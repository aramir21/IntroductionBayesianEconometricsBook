figE +  guides(color = guide_legend(title="Information"))
figE +  guides(guide_legend(title="Information"))
df <- expand.grid(X1 = 1:10, X2 = 1:10)
df$value <- df$X1 * df$X2
p1 <- ggplot(df, aes(X1, X2)) + geom_tile(aes(fill = value))
p2 <- p1 + geom_point(aes(size = value))
p1
figE <- ggplot(data = DataUp) +
geom_line(aes(lambda, Up1), color = "blue") +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Vague prior") +
geom_line(aes(lambda, Up2), color = "red")
figE
figE +  guides(guide_legend(title="Information"))
figE <- ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, color = "blue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Vague prior") +
geom_line(aes(lambda, Up2, color = "red"))
figE +  guides(guide_legend(title="Information"))
figE +  guides(color = guide_legend(title="Information"))
figE +  guides(color = guide_legend(title="Information")) +
scale_color_manual(labels = c("Update 1", "Update 2"), values = c("blue", "red"))
figE <- ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, color = "blue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Vague prior") +
geom_line(aes(lambda, Up2, color = "red")) +
geom_line(aes(lambda, Up3, color = "green"))
figE
figE <- ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, color = "blue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Vague prior") +
geom_line(aes(lambda, Up2, color = "red"))
figE
figE <- ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, color = "blue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Vague prior") +
geom_line(aes(lambda, Up2, color = "red")) +
geom_line(aes(lambda, Up3, color = "green"))
figE
######################################
ggplot(data = data) +
geom_line(aes(lambda, EBPrior, colour="darkblue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
geom_line(aes(lambda, LiksScale, colour="green")) +
geom_line(aes(lambda, PosteriorEB, colour="red")) +
scale_color_discrete(name = "Information", labels = c("Prior", "Likelihood", "Posterior"))
ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, colour="#999999")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
geom_line(aes(lambda, Up2, colour="#E69F00")) +
geom_line(aes(lambda, Up3, colour="#56B4E9")) +
scale_color_discrete(name = "Updates", labels = c("Iter 1", "Iter 2", "Iter 3"))
set.seed(010101)
y <- c(0, 3, 2, 1, 0) # Data
N <- length(y)
paste("The sample mean is", mean(y), sep = " ")
paste("The sample variance is", var(y), sep = " ")
ProbBo <- function(y, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- bn / (bn + 1) # Probability negative binomial density
Pr <- 1 - pnbinom(0, size = an, prob = (1 - p)) # Probability of visiting the Doctor at least once next year
# Observe that in R there is a slightly different parametrization.
return(Pr)
}
# Using a vague prior:
a0 <- 0.001 # Prior shape parameter
b0 <- 1 / 0.001 # Prior scale parameter
PriMeanV <- a0 * b0 # Prior mean
PriVarV <- a0 * b0^2 # Prior variance
paste("Prior mean and prior variance using vague information are", PriMeanV, "and", PriVarV, "respectively", sep = " ")
Pp <- ProbBo(y, a0 = 0.001, b0 = 1 / 0.001) # This setting is defining vague prior information.
paste("The probability of visiting the Doctor at least once next year using a vague prior is", Pp, sep = " ")
# Using Emprirical Bayes
LogMgLik <- function(theta, y){
N <- length(y) #sample size
a0 <- theta[1] # prior shape hyperparameter
b0 <- theta[2] # prior scale hyperparameter
an <- sum(y) + a0 # posterior shape parameter
if(a0 <= 0 || b0 <= 0){ #Avoiding negative values
lnp <- -Inf
}else{lnp <- lgamma(an) + sum(y)*log(b0/(N*b0+1)) - a0*log(N*b0+1) - lgamma(a0)} # log marginal likelihood
return(-lnp)
}
theta0 <- c(0.01, 1/0.1) # Initial values
control <- list(maxit = 1000) # Number of iterations in optimization
EmpBay <- optim(theta0, LogMgLik, method = "BFGS", control = control, hessian = TRUE, y = y) # Optimization
EmpBay$convergence # Checking convergence
EmpBay$value # Maximum
a0EB <- EmpBay$par[1] # Prior shape using empirical Bayes
b0EB <- EmpBay$par[2] # Prior scale using empirical Bayes
paste("The prior shape and scale parameters are", a0EB, "and", b0EB, "respectively", sep = " ")
PriMeanEB <- a0EB * b0EB # Prior mean
PriVarEB <- a0EB * b0EB^2 # Prior variance
paste("Prior mean and variance using empirical Bayes are", PriMeanEB, "and", PriVarEB, "respectively", sep = " ")
PpEB <- ProbBo(y, a0 = a0EB, b0 = b0EB) # This setting is using emprical Bayes.
paste("The probability of visiting the Doctor at least once next year using empirical Bayes is", PpEB, sep = " ")
# Density figures
lambda <- seq(0.01, 10, 0.01) # Values of lambda
VaguePrior <- dgamma(lambda, shape = a0, scale = b0)
EBPrior <- dgamma(lambda, shape = a0EB, scale = b0EB)
PosteriorV <- dgamma(lambda, shape = a0 + sum(y), scale = b0 / ((b0 * N) + 1))
PosteriorEB <- dgamma(lambda, shape = a0EB + sum(y), scale = b0EB / ((b0EB * N) + 1))
# Likelihood function
Likelihood <- function(theta, y){
LogL <- dpois(y, theta, log = TRUE)
Lik <- prod(exp(LogL))
return(Lik)
}
Liks <- sapply(lambda, function(par) {Likelihood(par, y = y)})
Sc <- max(PosteriorEB)/max(Liks) #Scale for displaying in figure
LiksScale <- Liks * Sc
data <- data.frame(cbind(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) #Data frame
require(ggplot2) # Cool figures
require(latex2exp) # LaTeX equations in figures
require(ggpubr) # Multiple figures in one page
fig1 <- ggplot(data = data, aes(lambda, VaguePrior)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior: Vague Gamma")
fig2 <- ggplot(data = data, aes(lambda, EBPrior)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior: Empirical Bayes Gamma")
fig3 <- ggplot(data = data, aes(lambda, PosteriorV)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Posterior: Vague Gamma")
fig4 <- ggplot(data = data, aes(lambda, PosteriorEB)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Posterior: Empirical Bayes Gamma")
FIG <- ggarrange(fig1, fig2, fig3, fig4,
ncol = 2, nrow = 2)
annotate_figure(FIG,
top = text_grob("Vague versus Empirical Bayes: Poisson-Gamma model", color = "black", face = "bold", size = 14))
ggplot(data = data) +
geom_line(aes(lambda, EBPrior, colour="darkblue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
geom_line(aes(lambda, LiksScale, colour="green")) +
geom_line(aes(lambda, PosteriorEB, colour="red")) +
scale_color_discrete(name = "Information", labels = c("Prior", "Likelihood", "Posterior"))
# Predictive distributions
PredDen <- function(y, y0, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- bn / (bn + 1) # Probability negative binomial density
Pr <- dnbinom(y0, size = an, prob = (1 - p)) # Predictive density
# Observe that in R there is a slightly different parametrization.
return(Pr)
}
y0 <- 0:10
PredVague <- PredDen(y = y, y0 = y0, a0 = a0, b0 = b0)
PredEB <- PredDen(y = y, y0 = y0, a0 = a0EB, b0 = b0EB)
dataPred <- as.data.frame(cbind(y0, PredVague, PredEB))
colnames(dataPred) <- c("y0", "PredictiveVague", "PredictiveEB")
ggplot(data = dataPred) +
geom_point(aes(y0, PredictiveVague, color = "red")) +
xlab(TeX("$y_0$")) + ylab("Density") +
ggtitle("Predictive density: Vague and Empirical Bayes priors") +
geom_point(aes(y0, PredictiveEB, color = "green")) +
guides(color = guide_legend(title="Prior")) +
scale_color_manual(labels = c("Vague", "Empirical Bayes"), values = c("red", "green")) +
scale_x_continuous(breaks=seq(0,10,by=1))
# Posterior odds: Vague vs Empirical Bayes
PO12 <- exp(-LogMgLik(c(a0EB, b0EB), y = y))/exp(-LogMgLik(c(a0, b0), y = y))
paste("The posterior odds: Vague prior vs Empirical Bayes prior is", PO12, sep = " ")
PostProMEM <- PO12/(1 + PO12) # Posterior model probability Empirical Bayes
PostProbMV <- 1 - PostProMEM # Posterior model probability vague prior
paste("These are the posterior model probabilities", PostProMEM, PostProbMV, "for the Empirical Bayes and vague priors, respectively")
# Bayesian model average (BMA)
PostMeanEB <- (a0EB + sum(y)) * (b0EB / (b0EB * N + 1)) # Posterior mean Empirical Bayes
PostMeanV <- (a0 + sum(y)) * (b0 / (b0 * N + 1)) # Posterior mean vague priors
BMAmean <- PostProMEM * PostMeanEB + PostProbMV * PostMeanV  # BMA posterior mean
PostVarEB <- (a0EB + sum(y)) * (b0EB / (b0EB * N + 1))^2 # Posterior variance Empirical Bayes
PostVarV <- (a0 + sum(y)) * (b0 / (b0 * N + 1))^2 # Posterior variance vague prior
BMAVar <- PostProMEM * PostVarEB + PostProbMV * PostVarV + PostProMEM * (PostMeanEB - BMAmean)^2 + PostProbMV * (PostMeanV - BMAmean)^2# BMA posterior variance
paste("The BMA posterior mean and variance are", BMAmean, "and", BMAVar, "respectively", sep = " ")
# BMA: Predictive
BMAPred <- PostProMEM * PredEB + PostProbMV * PredVague
dataPredBMA <- as.data.frame(cbind(y0, BMAPred))
colnames(dataPredBMA) <- c("y0", "PredictiveBMA")
ggplot(data = dataPredBMA) +
geom_point(aes(y0, PredictiveBMA, color = "red")) +
xlab(TeX("$y_0$")) + ylab("Density") +
ggtitle("Predictive density: BMA")
# Bayesian updating
BayUp <- function(y, lambda, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- dgamma(lambda, shape = an, scale = bn) # Posterior density
return(list(Post = p, a0New = an, b0New = bn))
}
PostUp <- NULL
for(i in 1:N){
if(i == 1){
PostUpi <- BayUp(y[i], lambda, a0 = 0.001, b0 = 1/0.001)}
else{
PostUpi <- BayUp(y[i], lambda, a0 = PostUpi$a0New, b0 = PostUpi$b0New)
}
PostUp <- cbind(PostUp, PostUpi$Post)
}
DataUp <- as.data.frame(cbind(lambda, PostUp))
colnames(DataUp) <- c("lambda", "Up1", "Up2", "Up3", "Up4", "Up5")
ggplot(data = DataUp) +
geom_line(aes(lambda, Up1, colour="black")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
geom_line(aes(lambda, Up2, colour="blue")) +
geom_line(aes(lambda, Up3, colour="red")) +
scale_color_discrete(name = "Updates", labels = c("Iter 1", "Iter 2", "Iter 3"))
ggplot(data = data) +
geom_line(aes(lambda, EBPrior, colour="darkblue")) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
geom_line(aes(lambda, LiksScale, colour="green")) +
geom_line(aes(lambda, PosteriorEB, colour="red")) +
scale_color_discrete(name = "Information", labels = c("Prior", "Likelihood", "Posterior"))
dataNew <- data.frame(cbind(rep(lambda, 3), c(EBPrior, PosteriorEB, LiksScale),
rep(1:3, each = 1000))) #Data frame
colnames(dataNew) <- c("Lambda", "Density", "Factor")
dataNew$Factor <- factor(dataNew$Factor, levels=c("1", "3", "2"),
labels=c("Prior", "Likelihood", "Posterior"))
ggplot(data = dataNew, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
guides(color=guide_legend(title="Information")) +
scale_color_manual(values = c("red", "green", "blue"))
A<- matrix(c(1,2,3,4,5,6), 3, 2)
A
c(A)
DataUp <- data.frame(cbind(rep(lambda, 5), c(PostUp),
rep(1:5, each = 1000))) #Data frame
colnames(DataUp) <- c("lambda", "Up1", "Up2", "Up3", "Up4", "Up5", "Factor")
colnames(DataUp) <- c("Lambda", "Density", "Factor")
DataUp$Factor <- factor(dataNew$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter1", "Iter2", "Iter3", "Iter4", "Iter5"))
View(DataUp)
DataUp$Factor <- factor(dataNew$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter1", "Iter2", "Iter3", "Iter4", "Iter5"))
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter1", "Iter2", "Iter3", "Iter4", "Iter5"))
ggplot(data = dataNew, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter1", "Iter2", "Iter3", "Iter4", "Iter5"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
DataUp <- data.frame(cbind(rep(lambda, 5), c(PostUp),
rep(1:5, each = 1000))) #Data frame
colnames(DataUp) <- c("Lambda", "Density", "Factor")
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "green", "blue", "yellow", "black"))
S <- 100000 # Posterior draws
PostMeanLambdaUps <- sapply(1:N, function(i) {mean(sample(lambda, S, replace = TRUE, prob = PostUp[ , i]))}) #Posterior mean update i
paste("Posterior means using all information and updating are:", PostMeanV, "and", PostMeanLambdaUps[5], sep = " ")
paste("Posterior means using all information and sequential updating are:", PostMeanV, "and", PostMeanLambdaUps[5], sep = " ")
paste("Posterior means using all information and sequential updating are:", round(PostMeanV, 3), "and", round(PostMeanLambdaUps[5], 3), sep = " ")
paste("Posterior means using all information and sequential updating are:", round(PostMeanV, 2), "and", round(PostMeanLambdaUps[5], 2), sep = " ")
paste("Posterior variances using all information and sequential updating are:", round(PostVarV, 3), "and", round(PostVarLambdaUps[5], 3), sep = " ")
paste("Posterior variances using all information and sequential updating are:", round(PostVarV, 2), "and", round(PostVarLambdaUps[5], 2), sep = " ")
set.seed(010101)
y <- c(0, 3, 2, 1, 0) # Data
N <- length(y)
paste("The sample mean is", mean(y), sep = " ")
paste("The sample variance is", var(y), sep = " ")
ProbBo <- function(y, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- bn / (bn + 1) # Probability negative binomial density
Pr <- 1 - pnbinom(0, size = an, prob = (1 - p)) # Probability of visiting the Doctor at least once next year
# Observe that in R there is a slightly different parametrization.
return(Pr)
}
# Using a vague prior:
a0 <- 0.001 # Prior shape parameter
b0 <- 1 / 0.001 # Prior scale parameter
PriMeanV <- a0 * b0 # Prior mean
PriVarV <- a0 * b0^2 # Prior variance
paste("Prior mean and prior variance using vague information are", PriMeanV, "and", PriVarV, "respectively", sep = " ")
Pp <- ProbBo(y, a0 = 0.001, b0 = 1 / 0.001) # This setting is defining vague prior information.
paste("The probability of visiting the Doctor at least once next year using a vague prior is", Pp, sep = " ")
# Using Emprirical Bayes
LogMgLik <- function(theta, y){
N <- length(y) #sample size
a0 <- theta[1] # prior shape hyperparameter
b0 <- theta[2] # prior scale hyperparameter
an <- sum(y) + a0 # posterior shape parameter
if(a0 <= 0 || b0 <= 0){ #Avoiding negative values
lnp <- -Inf
}else{lnp <- lgamma(an) + sum(y)*log(b0/(N*b0+1)) - a0*log(N*b0+1) - lgamma(a0)} # log marginal likelihood
return(-lnp)
}
theta0 <- c(0.01, 1/0.1) # Initial values
control <- list(maxit = 1000) # Number of iterations in optimization
EmpBay <- optim(theta0, LogMgLik, method = "BFGS", control = control, hessian = TRUE, y = y) # Optimization
EmpBay$convergence # Checking convergence
EmpBay$value # Maximum
a0EB <- EmpBay$par[1] # Prior shape using empirical Bayes
b0EB <- EmpBay$par[2] # Prior scale using empirical Bayes
paste("The prior shape and scale parameters are", a0EB, "and", b0EB, "respectively", sep = " ")
PriMeanEB <- a0EB * b0EB # Prior mean
PriVarEB <- a0EB * b0EB^2 # Prior variance
paste("Prior mean and variance using empirical Bayes are", PriMeanEB, "and", PriVarEB, "respectively", sep = " ")
PpEB <- ProbBo(y, a0 = a0EB, b0 = b0EB) # This setting is using emprical Bayes.
paste("The probability of visiting the Doctor at least once next year using empirical Bayes is", PpEB, sep = " ")
# Density figures
lambda <- seq(0.01, 10, 0.01) # Values of lambda
VaguePrior <- dgamma(lambda, shape = a0, scale = b0)
EBPrior <- dgamma(lambda, shape = a0EB, scale = b0EB)
PosteriorV <- dgamma(lambda, shape = a0 + sum(y), scale = b0 / ((b0 * N) + 1))
PosteriorEB <- dgamma(lambda, shape = a0EB + sum(y), scale = b0EB / ((b0EB * N) + 1))
# Likelihood function
Likelihood <- function(theta, y){
LogL <- dpois(y, theta, log = TRUE)
Lik <- prod(exp(LogL))
return(Lik)
}
Liks <- sapply(lambda, function(par) {Likelihood(par, y = y)})
Sc <- max(PosteriorEB)/max(Liks) #Scale for displaying in figure
LiksScale <- Liks * Sc
data <- data.frame(cbind(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) #Data frame
require(ggplot2) # Cool figures
require(latex2exp) # LaTeX equations in figures
require(ggpubr) # Multiple figures in one page
fig1 <- ggplot(data = data, aes(lambda, VaguePrior)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior: Vague Gamma")
fig2 <- ggplot(data = data, aes(lambda, EBPrior)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior: Empirical Bayes Gamma")
fig3 <- ggplot(data = data, aes(lambda, PosteriorV)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Posterior: Vague Gamma")
fig4 <- ggplot(data = data, aes(lambda, PosteriorEB)) +
geom_line() +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Posterior: Empirical Bayes Gamma")
FIG <- ggarrange(fig1, fig2, fig3, fig4,
ncol = 2, nrow = 2)
annotate_figure(FIG,
top = text_grob("Vague versus Empirical Bayes: Poisson-Gamma model", color = "black", face = "bold", size = 14))
dataNew <- data.frame(cbind(rep(lambda, 3), c(EBPrior, PosteriorEB, LiksScale),
rep(1:3, each = 1000))) #Data frame
colnames(dataNew) <- c("Lambda", "Density", "Factor")
dataNew$Factor <- factor(dataNew$Factor, levels=c("1", "3", "2"),
labels=c("Prior", "Likelihood", "Posterior"))
ggplot(data = dataNew, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Prior, likelihood and posterior: Empirical Bayes Gamma") +
guides(color=guide_legend(title="Information")) +
scale_color_manual(values = c("red", "yellow", "blue"))
# Predictive distributions
PredDen <- function(y, y0, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- bn / (bn + 1) # Probability negative binomial density
Pr <- dnbinom(y0, size = an, prob = (1 - p)) # Predictive density
# Observe that in R there is a slightly different parametrization.
return(Pr)
}
y0 <- 0:10
PredVague <- PredDen(y = y, y0 = y0, a0 = a0, b0 = b0)
PredEB <- PredDen(y = y, y0 = y0, a0 = a0EB, b0 = b0EB)
dataPred <- as.data.frame(cbind(y0, PredVague, PredEB))
colnames(dataPred) <- c("y0", "PredictiveVague", "PredictiveEB")
ggplot(data = dataPred) +
geom_point(aes(y0, PredictiveVague, color = "red")) +
xlab(TeX("$y_0$")) + ylab("Density") +
ggtitle("Predictive density: Vague and Empirical Bayes priors") +
geom_point(aes(y0, PredictiveEB, color = "yellow")) +
guides(color = guide_legend(title="Prior")) +
scale_color_manual(labels = c("Vague", "Empirical Bayes"), values = c("red", "yellow")) +
scale_x_continuous(breaks=seq(0,10,by=1))
# Posterior odds: Vague vs Empirical Bayes
PO12 <- exp(-LogMgLik(c(a0EB, b0EB), y = y))/exp(-LogMgLik(c(a0, b0), y = y))
paste("The posterior odds: Vague prior vs Empirical Bayes prior is", PO12, sep = " ")
PostProMEM <- PO12/(1 + PO12) # Posterior model probability Empirical Bayes
PostProbMV <- 1 - PostProMEM # Posterior model probability vague prior
paste("These are the posterior model probabilities", PostProMEM, PostProbMV, "for the Empirical Bayes and vague priors, respectively")
# Bayesian model average (BMA)
PostMeanEB <- (a0EB + sum(y)) * (b0EB / (b0EB * N + 1)) # Posterior mean Empirical Bayes
PostMeanV <- (a0 + sum(y)) * (b0 / (b0 * N + 1)) # Posterior mean vague priors
BMAmean <- PostProMEM * PostMeanEB + PostProbMV * PostMeanV  # BMA posterior mean
PostVarEB <- (a0EB + sum(y)) * (b0EB / (b0EB * N + 1))^2 # Posterior variance Empirical Bayes
PostVarV <- (a0 + sum(y)) * (b0 / (b0 * N + 1))^2 # Posterior variance vague prior
BMAVar <- PostProMEM * PostVarEB + PostProbMV * PostVarV + PostProMEM * (PostMeanEB - BMAmean)^2 + PostProbMV * (PostMeanV - BMAmean)^2# BMA posterior variance
paste("The BMA posterior mean and variance are", BMAmean, "and", BMAVar, "respectively", sep = " ")
# BMA: Predictive
BMAPred <- PostProMEM * PredEB + PostProbMV * PredVague
dataPredBMA <- as.data.frame(cbind(y0, BMAPred))
colnames(dataPredBMA) <- c("y0", "PredictiveBMA")
ggplot(data = dataPredBMA) +
geom_point(aes(y0, PredictiveBMA, color = "red")) +
xlab(TeX("$y_0$")) + ylab("Density") +
ggtitle("Predictive density: BMA")
# Bayesian updating
BayUp <- function(y, lambda, a0, b0){
N <- length(y)
an <- a0 + sum(y) # Posterior shape parameter
bn <- b0 / ((b0 * N) + 1) # Posterior scale parameter
p <- dgamma(lambda, shape = an, scale = bn) # Posterior density
return(list(Post = p, a0New = an, b0New = bn))
}
PostUp <- NULL
for(i in 1:N){
if(i == 1){
PostUpi <- BayUp(y[i], lambda, a0 = 0.001, b0 = 1/0.001)}
else{
PostUpi <- BayUp(y[i], lambda, a0 = PostUpi$a0New, b0 = PostUpi$b0New)
}
PostUp <- cbind(PostUp, PostUpi$Post)
}
DataUp <- data.frame(cbind(rep(lambda, 5), c(PostUp),
rep(1:5, each = 1000))) #Data frame
colnames(DataUp) <- c("Lambda", "Density", "Factor")
DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"),
labels=c("Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5"))
ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) +
geom_line(aes(color = Factor)) +
xlab(TeX("$\\lambda$")) + ylab("Density") +
ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
guides(color=guide_legend(title="Update")) +
scale_color_manual(values = c("red", "purple", "blue", "yellow", "black"))
S <- 100000 # Posterior draws
PostMeanLambdaUps <- sapply(1:N, function(i) {mean(sample(lambda, S, replace = TRUE, prob = PostUp[ , i]))}) #Posterior mean update i
paste("Posterior means using all information and sequential updating are:", round(PostMeanV, 2), "and", round(PostMeanLambdaUps[5], 2), sep = " ")
PostVarLambdaUps <- sapply(1:N, function(i) {var(sample(lambda, S, replace = TRUE, prob = PostUp[ , i]))}) #Posterior variance update i
paste("Posterior variances using all information and sequential updating are:", round(PostVarV, 2), "and", round(PostVarLambdaUps[5], 2), sep = " ")
shiny::runGitHub("besmarter/BSTApp", launch.browser=T)
shiny::runGitHub("besmarter/BSTApp", launch.browser=T)
shiny::runGitHub("besmarter/BSTApp", launch.browser=T)
3/8
-0.0003 + 0.3327*0.1117
0.1176^0.5
0.0267/0.0273
qt(0.025, 58)
qt(0.975, 58)
1.0788 - 2.001717*0.3880
1.0788 + 2.001717*0.3880
(1.0788 - 1)/0.388
qt(.95,4)
qt(.025,12)
1.5538 - 2.178813*0.2079
1.5538 + 2.178813*0.2079
27.3147^2
qf(0.95, 1, 58)
qt(0.025,7)
qt(0.05, 2584)
qt(0.01, 2584)
qt(0.005, 2584)
qt(0.01, 2584)
qt(0.01, 200)
