# Multivariate regression {#Chap7}

We describe how to perform Bayesian inference in multivariate response models, including multivariate regression, seemingly unrelated regression, instrumental variables, and the multivariate probit model. In particular, we present the posterior distributions of the parameters and demonstrate several applications and simulations. Additionally, we show how to perform inference in these models using three levels of programming skills: GUI, packages, and programming the algorithms from scratch. Finally, we provide some mathematical and computational exercises.

Remember that we can run our GUI typing `shiny::runGitHub("besmarter/BSTApp", launch.browser=T)` in the **R** console or any **R** code editor and execute it. However, users should see Chapter \@ref(Chap5) for details.

## Multivariate regression {#sec71}

A complete presentation of this model is given in Section \@ref(sec44). We show here the setting, and the posterior distributions for facility in exposition. In particular, there are \(M\) multiply dependent variables which share the same set of regressors, and their stochastic errors are contemporaneously correlated. In particular, \(\boldsymbol{Y} = \left[ \boldsymbol{y_{1}} \ \boldsymbol{y_{2}} \ \ldots \ \boldsymbol{y_{M}} \right]\) is an \( N \times M \) matrix that is generated by \(\boldsymbol{Y} = \boldsymbol{X} \boldsymbol{B} + \boldsymbol{U}\) where \(\boldsymbol{X}\) is an \( N \times K \) matrix of regressors, \(\boldsymbol{B} = \left[ \boldsymbol{\beta}_{1} \ \boldsymbol{\beta}_{2} \ldots \boldsymbol{\beta}_{M} \right]\) is a \( K \times M \) matrix of parameters, and \(\boldsymbol{U} = \left[ \boldsymbol{\mu}_{1} \ \boldsymbol{\mu}_{2} \ldots \boldsymbol{\mu}_{M} \right]\) is a matrix of stochastic random errors such that \(\boldsymbol{\mu}_i \sim N(\boldsymbol{0}, \boldsymbol{\Sigma})\), for \(i = 1, 2, \dots, N\), each row of \(\boldsymbol{U}\).

The prior is given by \(\boldsymbol{B} \mid \boldsymbol{\Sigma} \sim N(\boldsymbol{B}_0, \boldsymbol{V}_0, \boldsymbol{\Sigma})\) and \(\boldsymbol{\Sigma} \sim IW(\boldsymbol{\Psi}_0, \alpha_0)\). Therefore, the conditional posterior distributions are:

\[
\boldsymbol{B} \mid \boldsymbol{\Sigma}, \boldsymbol{Y}, \boldsymbol{X} \sim N(\boldsymbol{B}_n, \boldsymbol{V}_n, \boldsymbol{\Sigma}),
\]

\[
\boldsymbol{\Sigma} \mid \boldsymbol{Y}, \boldsymbol{X} \sim IW(\boldsymbol{\Psi}_n, \alpha_n),
\]

where 

\[
\boldsymbol{V}_n = (\boldsymbol{X}^{\top} \boldsymbol{X} + \boldsymbol{V}_0^{-1})^{-1}, \quad 
\boldsymbol{B}_n = \boldsymbol{V}_n (\boldsymbol{V}_0^{-1} \boldsymbol{B}_0 + \boldsymbol{X}^{\top} \boldsymbol{X} \hat{\boldsymbol{B}}),
\]

\[
\hat{\boldsymbol{B}} = (\boldsymbol{X}^{\top} \boldsymbol{X})^{-1} \boldsymbol{X}^{\top} \boldsymbol{Y}, \quad 
\boldsymbol{\Psi}_n = \boldsymbol{\Psi}_{0} + \boldsymbol{S} + \boldsymbol{B}_{0}^{\top} \boldsymbol{V}_{0}^{-1} \boldsymbol{B}_{0} + \hat{\boldsymbol{B}}^{\top} \boldsymbol{X}^{\top} \boldsymbol{X} \hat{\boldsymbol{B}} - \boldsymbol{B}_n^{\top} \boldsymbol{V}_n^{-1} \boldsymbol{B}_n,
\]

and 

\[
\alpha_n = \alpha_0 + N.
\]

We can use a Gibbs sampling algorithm in this model since the conditional posterior distributions are standard.

**Example: The effect of institutions on per capita gross domestic product**

To illustrate multivariate regression models, we use the dataset provided by @Acemoglu2001, who analyzed the effect of property rights on economic growth.

We begin with the following *simultaneous structural* economic model:^[This model captures the potential underlying economic relationship between the variables.]
\begin{align}
	\log(\text{pcGDP95}_i) &= \beta_1 + \beta_2 \text{PAER}_i + \beta_3 \text{Africa} + \beta_4 \text{Asia} + \beta_5 \text{Other} + u_{1i},
	(\#eq:str1)
\end{align}
\begin{align}
	\text{PAER}_i &= \alpha_1 + \alpha_2 \log(\text{pcGDP95}_i) + \alpha_3 \log(\text{Mort}_i) + u_{2i},
	(\#eq:str2)
\end{align}
where *pcGDP95*, *PAER*, and *Mort* represent the per capita gross domestic product (GDP) in 1995, the average index of protection against expropriation between 1985 and 1995, and the settler mortality rate during the period of colonization, respectively. *Africa*, *Asia*, and *Other* are indicator variables for continents, with *America* serving as the baseline group.

In this model, there is *reverse (simultaneous) causality* due to the contemporaneous effect of *GDP* on *PAER*, and vice versa.^[ *Simultaneous causality* is one of the most controversial causation issues from a philosophy of science perspective. The root of the issue is that causation is typically based on the time sequence of cause and effect.] Therefore, estimating Equations \@ref(eq:str1) and \@ref(eq:str2) without accounting for this phenomenon results in posterior mean estimates that are *biased* and *inconsistent* from a sampling (frequentist) perspective.^[Note that $\mathbb{E}[u_1\text{PAER}]\neq 0$, which means failing to meet a necessary condition for obtaining *unbiased* and *consistent* estimators of \boldsymbol{\beta}. See Exercise 1.] A potential strategy to address this issue is to estimate the *reduced-form* model, i.e., a model without *simultaneous causality*, where all *endogenous variables* are functions of *exogenous variables*. The former are determined within the model (e.g., $\log(\text{pcGDP95}_i)$ and *PAER* in this example), while the latter are determined outside the model (e.g., $\log(\text{Mort}_i)$, *Africa*, *Asia*, and *Other* in this example).

Replacing Equation \@ref(eq:str2) into Equation \@ref(eq:str1), and solving for $\log(\textit{pcGDP95})$,
\begin{align}
	\log(\text{pcGDP95}_i)=\pi_1+\pi_2\log(\text{Mort}_i)+\pi_3 \text{Africa}+\pi_4 \text{Asia}+\pi_5 \text{Other}+e_{1i}. 
	(\#eq:red1)
\end{align}
Then, by substituting Equation \@ref(eq:red1) into Equation \@ref(eq:str2), and solving for *PAER*, we obtain
\begin{align}
	\text{PAER}_i = \gamma_1 + \gamma_2 \log(\text{Mort}_i) + \gamma_3 \text{Africa} + \gamma_4 \text{Asia} + \gamma_5 \text{Other} + e_{2i},
	(\#eq:red2)
\end{align}
where $\pi_2 = \frac{\beta_2\alpha_3}{1 - \beta_2\alpha_2}$ and $\gamma_2 = \frac{\alpha_3}{1 - \beta_2\alpha_2}$, given that $\beta_2 \alpha_2 \neq 1$, i.e., independent equations (see Exercise 2).

Observe that Equations \@ref(eq:red1) and \@ref(eq:red2) have the form of a multivariate regression model, where the common set of regressors is 
\[
\boldsymbol{X} = \left[\log(\text{Mort}) \ \text{Africa} \ \text{Asia} \ \text{Other}\right]
\]
and the common set of dependent variables is 
\[
\boldsymbol{Y} = \left[\log(\text{pcGDP95}) \ \text{PAER}\right].
\]
Therefore, we can estimate this model using the setup outlined in this section.

In the first stage, we estimate the parameters of the *reduced-form* model (Equations \@ref(eq:red1) and \@ref(eq:red2)), but the main interest lies in estimating the parameters of the *structural* model (Equations \@ref(eq:str1) and \@ref(eq:str2)). A valid question is whether we can recover (identify) the *structural* parameters from the *reduced-form* parameters. There are two criteria to answer this question: the order condition, which is necessary, and the rank condition, which is both necessary and sufficient.  

**The order condition**

Given a system of equations with \(M\) endogenous variables, and \(K\) exogenous variables (including the intercept), there are two ways to assess the order condition:

- The parameters of an equation in the system are identified if there are at least \(M-1\) variables excluded from the equation (*exclusion restrictions*). The equation is *exactly identified* if the number of excluded variables is \(M-1\), and is *over identified* if the number of excluded variables is greater than \(M-1\).
- The parameters of equation \(m\) in the system are identified if \(K-K_m\geq M_m-1\), where \(K_m\) and \(M_m\) are the number of exogenous and endogenous variables in equation \(m\), respectively. The \(m\)-th equation is *exactly identified* if \(K-K_m = M_m-1\), and *over identified* if \(K-K_m > M_m-1\).

We can see from Equations \@ref(eq:str1) and \@ref(eq:str2) in this example that \(K=5\), \(M=2\), \(K_1=4\), \(K_2=2\), \(M_1=2\), and \(M_2=2\). This means that \(K-K_1=1=M-1\) and \(K-K_2=3>M-1=1\), that is, the order condition says that both equations satisfy the necessary condition of identification, the first equation would be *exactly identified*, and the second equation would be *over identified*. Observe that there is one excluded variable from the first equation, and there are three excluded variables from the second equation.  

**The rank condition**

The rank condition (necessary and sufficient) says that given a *structural* model with \(M\) equations (\(M\) endogenous variables), an equation is identified if and only if there is at least one determinant different from zero from a \((M-1)\times(M-1)\) matrix built using the excluded variables in the analyzed equation, but included in any other equation of the system.

It is useful to build the *identification matrix* to implement the *rank* condition. The next Table shows this matrix in this example.

```{r, echo=FALSE, message=FALSE}
suppressWarnings(library(kableExtra))

# Create the matrix as a data frame
table_data <- data.frame(
  Col1 = c("$\\log(\\text{pcGDP95})$", "1", "$-\\alpha_2$"),
  Col2 = c("$\\text{PAER}$", "$-\\beta_2$", "1"),
  Col3 = c("Constant", "$-\\beta_1$", "$-\\alpha_1$"),
  Col4 = c("$\\log(\\text{Mort})$", "0", "$-\\alpha_3$"),
  Col5 = c("Africa", "$-\\beta_3$", "0"),
  Col6 = c("Asia", "$-\\beta_4$", "0"),
  Col7 = c("Other", "$-\\beta_5$", "0")
)

# Render the table using kableExtra
kbl(table_data, format = "html", escape = FALSE, booktabs = TRUE, col.names = NULL, 
    caption = "Example: Rank condition") %>%
  kable_styling(full_width = FALSE, latex_options = "hold_position")
```

The only excluded variable in the $\log(\text{pcGDP95})$ equation is $\log(\text{Mort})$. Therefore, there is only one matrix that can be constructed using the excluded variables from this equation, which is $[-\alpha_3]$ (see column 4 in the Table). The determinant of this matrix is $-\alpha_3$, and as long as this coefficient is nonzero (i.e., $\alpha_3 \neq 0$), meaning that the mortality rate is relevant in the PAER equation, the coefficients in the $\log(\text{pcGDP95})$ equation are *exactly identified*. For example, $\beta_2 = \frac{\pi_2}{\gamma_2}$, which represents the effect of property rights on GDP, is exactly identified.

It is crucial to observe the importance of excluding $\log(\text{Mort})$ from the $\log(\text{pcGDP95})$ equation, while including $\log(\text{Mort})$ in the PAER equation. This is known as the *exclusion restriction*, which requires the presence of an exogenous source of variability in the PAER equation to help identify the $\log(\text{pcGDP95})$ equation. The presence of relevant exogenous sources of variability is an essential factor in the identification, estimation, and inference of *structural* parameters.

As for the identification of the *structural* parameters in the PAER equation, there are three potential matrices that can be constructed: $[-\beta_3]$, $[-\beta_4]$, and $[-\beta_5]$ (see columns 5, 6, and 7 in the Table). As long as any of these parameters are relevant in the $\log(\text{pcGDP95})$ equation, the PAER equation is identified. In this case, the PAER equation is *over-identified*, meaning there are multiple ways to estimate the parameters in this equation. For example, $\alpha_2 = \gamma_3/\pi_3 = \gamma_4/\pi_4 = \gamma_5/\pi_5$ (see Exercise 2).

In general, recovering the *structural* parameters from the *reduced-form* parameters can be challenging due to the need for relevant identification restrictions, which can be difficult to find in some applications.^[Good introductory-level textbooks on identification in linear systems include @gujarati2009basic, and @wooldridge2016introductory.]

For this example, we set non-informative priors: $\boldsymbol{B}_0 = \left[\boldsymbol{0}_5 \ \boldsymbol{0}_5\right]$, $\boldsymbol{V}_0 = 100 \boldsymbol{I}_K$, $\boldsymbol{\Psi}_0 = 5 \boldsymbol{I}_2$, and $\alpha_0 = 5$.^[Note that we are setting the priors in the *reduced-form* model. This may have unintended consequences for the posterior distributions of the *structural* parameters, which are ultimately the parameters of interest to researchers. For further discussion, see @koop2003bayesian.] Once our GUI is displayed (see the beginning of this chapter), we should follow the next Algorithm to run multivariate linear models in the GUI (see Chapter \@ref(Chap5) for details, particularly on how to set the data set).

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Multivariate Linear Model**  

1. Select *Multivariate Models* on the top panel  
2. Select *Simple Multivariate* model using the left radio button  
3. Upload the dataset selecting first if there is header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select the number of dependent variables in the box **Number of endogenous variables: m**  
6. Select the number of independent variables (including the intercept) in the box **Number of exogenous variables: k**  
7. Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors  
8. Click the *Go!* button  
9. Analyze results  
10. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following **R** code shows how to perform the Gibss sampling algorithm in this example using the dataset *4Institutions.csv*. We ask to run this example using the *rmultireg* command from the *bayesm* package as an exercise. We find that the posterior mean *structural* effect of property rights on GDP is 0.98, and the 95\% credible interval is (0.56, 2.87). This means that there is evidence supporting a positive effect of property rights on gross domestic product. 


```{r}
rm(list = ls())
set.seed(12345)
DataInst <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/4Institutions.csv", sep = ",", header = TRUE, quote = "")
attach(DataInst)
Y <- cbind(logpcGDP95, PAER)
X <- cbind(1, logMort, Africa, Asia, Other)
M <- dim(Y)[2]
K <- dim(X)[2]
N <- dim(Y)[1]
# Hyperparameters
B0 <- matrix(0, K, M)
c0 <- 100
V0 <- c0*diag(K)
Psi0 <- 5*diag(M)
a0 <- 5
# Posterior parameters
Bhat <- solve(t(X)%*%X)%*%t(X)%*%Y 
S <- t(Y - X%*%Bhat)%*%(Y - X%*%Bhat)
Vn <- solve(solve(V0) + t(X)%*%X) 
Bn <- Vn%*%(solve(V0)%*%B0 + t(X)%*%X%*%Bhat)
Psin <- Psi0 + S + t(B0)%*%solve(V0)%*%B0 + t(Bhat)%*%t(X)%*%X%*%Bhat - t(Bn)%*%solve(Vn)%*%Bn
an <- a0 + N
#Posterior draws
s <- 10000 #Number of posterior draws
SIGs <- replicate(s, LaplacesDemon::rinvwishart(an, Psin))
BsCond <- sapply(1:s, function(s) {MixMatrix::rmatrixnorm(n = 1, mean=Bn, U = Vn,V = SIGs[,,s])})
summary(coda::mcmc(t(BsCond)))
SIGMs <- t(sapply(1:s, function(l) {gdata::lowerTriangle(SIGs[,,l], diag=TRUE, byrow=FALSE)}))
summary(coda::mcmc(SIGMs))
hdiBs <- HDInterval::hdi(t(BsCond), credMass = 0.95) # Highest posterior density credible interval
hdiBs
hdiSIG <- HDInterval::hdi(SIGMs, credMass = 0.95) # Highest posterior density credible interval
hdiSIG
beta2 <- BsCond[2,]/BsCond[7,] 
summary(coda::mcmc(beta2)) # Effect of property rights on GDP
```

## Seemingly Unrelated Regression {#sec72}

In seemingly unrelated regression (SUR) models, there are \( M \) dependent variables, each with potentially different regressors, such that the stochastic errors are contemporaneously correlated. The model is given by:

\[
\boldsymbol{y}_m = \boldsymbol{X}_m \boldsymbol{\beta}_m + \boldsymbol{\mu}_m,
\]

where \( \boldsymbol{y}_m \) is an \( N \)-dimensional vector of observations, \( \boldsymbol{X}_m \) is an \( N \times K_m \) matrix of regressors, \( \boldsymbol{\beta}_m \) is a \( K_m \)-dimensional vector of location parameters, and \( \boldsymbol{\mu}_m \) is an \( N \)-dimensional vector of stochastic errors, for \( m = 1, 2, \dots, M \).

Let \( \boldsymbol{\mu}_i = \left[\mu_{i1} \ \mu_{i2} \ \dots \ \mu_{iM}\right]^{\top} \), where \( \boldsymbol{\mu}_i \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}) \). Stacking the \( M \) equations, we can write the model as:

\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\mu},
\]

where \( \boldsymbol{y} = \left[\boldsymbol{y}_1^{\top} \ \boldsymbol{y}_2^{\top} \ \dots \ \boldsymbol{y}_M^{\top}\right]^{\top} \) is an \( MN \)-dimensional vector, \( \boldsymbol{\beta} = \left[\boldsymbol{\beta}_1^{\top} \ \boldsymbol{\beta}_2^{\top} \ \dots \ \boldsymbol{\beta}_M^{\top}\right]^{\top} \) is a \( K \)-dimensional vector with \( K = \sum_{m=1}^M K_m \), and \( \boldsymbol{X} \) is an \( MN \times K \) block-diagonal matrix composed of the individual \( \boldsymbol{X}_m \), i.e.,

\[
\boldsymbol{X} = \begin{bmatrix}
    \boldsymbol{X}_1 & \boldsymbol{0} & \dots & \boldsymbol{0} \\
    \boldsymbol{0} & \boldsymbol{X}_2 & \dots & \boldsymbol{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \boldsymbol{0} & \boldsymbol{0} & \dots & \boldsymbol{X}_M
\end{bmatrix}.
\]

Similarly, the vector of errors is given by \( \boldsymbol{\mu} = \left[\boldsymbol{\mu}_1^{\top} \ \boldsymbol{\mu}_2^{\top} \ \dots \ \boldsymbol{\mu}_M^{\top}\right]^{\top} \), which is an \( MN \)-dimensional vector of stochastic errors, with \( \boldsymbol{\mu} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma} \otimes \boldsymbol{I}_N) \).

The likelihood function for the parameters is then:

\[
p(\boldsymbol{\beta}, \boldsymbol{\Sigma} \mid \boldsymbol{y}, \boldsymbol{X}) \propto |\boldsymbol{\Sigma}|^{-N/2} \exp\left\{ -\frac{1}{2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^{\top} (\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{I}_N) (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) \right\}.
\]

Using independent priors \( \pi(\boldsymbol{\beta}) \sim \mathcal{N}(\boldsymbol{\beta}_0, \boldsymbol{B}_0) \) and \( \pi(\boldsymbol{\Sigma}^{-1}) \sim W(\alpha_0, \boldsymbol{\Psi}_0) \), the posterior distributions are

\[
\boldsymbol{\beta} \mid \boldsymbol{\Sigma}, \boldsymbol{y}, \boldsymbol{X} \sim \mathcal{N}(\boldsymbol{\beta}_n, \boldsymbol{B}_n),
\]

\[
\boldsymbol{\Sigma}^{-1} \mid \boldsymbol{\beta}, \boldsymbol{y}, \boldsymbol{X} \sim W(\alpha_n, \boldsymbol{\Psi}_n),
\]

where \( \boldsymbol{B}_n = (\boldsymbol{X}^{\top} (\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{I}_N) \boldsymbol{X} + \boldsymbol{B}_0^{-1})^{-1} \), \( \boldsymbol{\beta}_n = \boldsymbol{B}_n (\boldsymbol{B}_0^{-1} \boldsymbol{\beta}_0 + \boldsymbol{X}^{\top} (\boldsymbol{\Sigma}^{-1} \otimes \boldsymbol{I}_N) \boldsymbol{y}) \), \( \alpha_n = \alpha_0 + N \) and \( \boldsymbol{\Psi}_n = (\boldsymbol{\Psi}_0^{-1} + \boldsymbol{U}^{\top} \boldsymbol{U})^{-1} \), where \( \boldsymbol{U} \) is an \( N \times M \) matrix whose columns are \( \boldsymbol{y}_m - \boldsymbol{X}_m \boldsymbol{\beta}_m \).

We can demonstrate, through straightforward yet tedious algebra, that by defining \( \boldsymbol{y}_i = [y_{i1} \ y_{i2} \ \dots \ y_{iM}] \) and

\[
\boldsymbol{X}_i = \begin{bmatrix}
    x_{1i}^{\top} & \boldsymbol{0} & \dots & \boldsymbol{0} \\
    \boldsymbol{0} & x_{2i}^{\top} & \dots & \boldsymbol{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \boldsymbol{0} & \boldsymbol{0} & \dots & x_{Mi}^{\top}
\end{bmatrix},
\]

we alternatively have \( \boldsymbol{B}_n = (\boldsymbol{B}_0^{-1} + \sum_{i=1}^N \boldsymbol{X}_i^{\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{X}_i)^{-1} \), \( \boldsymbol{\beta}_n = \boldsymbol{B}_n (\boldsymbol{B}_0^{-1} \boldsymbol{\beta}_0 + \sum_{i=1}^N \boldsymbol{X}_i^{\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{y}_i) \) and \( \boldsymbol{\Psi}_n = (\boldsymbol{\Psi}_0^{-1} + \sum_{i=1}^N (\boldsymbol{y}_i - \boldsymbol{X}_i^{\top} \boldsymbol{\beta}) (\boldsymbol{y}_i - \boldsymbol{X}_i^{\top} \boldsymbol{\beta})^{\top})^{-1} \).

Observe that we have standard conditional posteriors, thus, we can employ a Gibbs sampling algorithm to get the posterior draws.

**Example: Utility demand**

Let's use the dataset *Utilities.csv* to estimate a seemingly unrelated regression (SUR) model for utilities. We adopt the same setting as in Exercise 14 of Chapter \@ref(Chap3), where we estimate a multivariate regression model while omitting households with no consumption in any utility. In this exercise, we observe that not all regressors are relevant for the demand of electricity, water, and gas. Thus, we estimate the following model:

\begin{align*}
	\log(\text{electricity}_i) & = \beta_1 + \beta_2\log(\text{electricity price}_i) + \beta_3\log(\text{water price}_i) \\
	& + \beta_4\log(\text{gas price}_i) + \beta_5\text{IndSocio1}_i + \beta_6\text{IndSocio2}_i + \beta_7\text{Altitude}_i \\
	& + \beta_8\text{Nrooms}_i + \beta_9\text{HouseholdMem}_i + \beta_{10}\log(\text{Income}_i) + \mu_{i1} \\
	\log(\text{water}_i) & = \alpha_1 + \alpha_2\log(\text{electricity price}_i) + \alpha_3\log(\text{water price}_i) \\
	& + \alpha_4\log(\text{gas price}_i) + \alpha_5\text{IndSocio1}_i + \alpha_6\text{IndSocio2}_i \\
	& + \alpha_7\text{Nrooms}_i + \alpha_8\text{HouseholdMem}_i + \mu_{i2} \\
	\log(\text{gas}_i) & = \gamma_1 + \gamma_2\log(\text{electricity price}_i) + \gamma_3\log(\text{water price}_i) \\
	& + \gamma_4\log(\text{gas price}_i) + \gamma_5\text{IndSocio1}_i + \gamma_6\text{IndSocio2}_i + \gamma_7\text{Altitude}_i \\
	& + \gamma_8\text{Nrooms}_i + \gamma_9\text{HouseholdMem}_i + \mu_{i3},
\end{align*}

where electricity, water, and gas represent the monthly consumption of electricity (kWh), water (m$^3$), and gas (m$^3$) of Colombian households. The dataset includes information on 2,103 households, with details on the average prices of electricity (USD/kWh), water (USD/m$^3$), and gas (USD/m$^3$), as well as indicators of the socioeconomic conditions of the neighborhood where the household is located (IndSocio1 being the lowest and IndSocio3 the highest). Additionally, there is information on whether the household is located in a municipality situated at over 1,000 meters above sea level, the number of rooms in the house, the number of household members, and monthly income (USD).

Since each equation has a different set of regressors, and we suspect correlation between the stochastic errors of the three equations, we should estimate a seemingly unrelated regression (SUR) model. We expect unobserved correlation across these equations because we are modeling utilities, and in some cases, a single provider handles all three services and issues one bill.

The following Algorithm demonstrates how to estimate SUR models using our GUI. Our GUI utilizes the command *rsurGibbs* from the *bayesm* package in **R** software. See Chapter \@ref(Chap5) for further details, including instructions on how to set up the dataset, and check the templates available in our GitHub repository (**https://github.com/besmarter/BSTApp**) in the **DataApp** and **DataSim** folders.


::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Seemingly Unrelated Regression (SUR)**  

1. Select *Multivariate Models* on the top panel  
2. Select *Seemingly Unrelated Regression* model using the left radio button  
3. Upload the dataset, selecting first if there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select the number of dependent variables in the box **Number of endogenous variables: m**  
6. Select the number of independent variables in the box **TOTAL number of Exogenous Variables: k**. This is the sum of all exogenous variables over all equations, including intercepts. In the example of **Utility demand**, it is equal to 27  
7. Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors 
8. Click the *Go!* button  
9. Analyze results  
10. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following code shows how to program this application using this package. We use 10,000 MCMC iterations, \( \boldsymbol{\beta}_0 = \boldsymbol{0}_{27} \), \( \boldsymbol{B}_0 = 100\boldsymbol{I}_{27} \), \( \alpha_0 = 5 \) and \( \boldsymbol{\Psi} = 5\boldsymbol{I}_3 \).

We find that the posterior median estimates of the own-price elasticities of demand for electricity, water, and gas are -1.88, -0.36, and -0.62, respectively, and none of the 95% credible intervals encompass 0. This means that a 1% increase in the prices of electricity, water, and gas results in a 1.88%, 0.36%, and 0.62% decrease in the monthly consumption of these utilities, respectively.^[This is an example where concerns about *biased* and *inconsistent* posterior mean estimates may arise, for instance, due to *reverse causality* between quantity and demand. These concerns are valid; however, we are using micro-level data, which implies no demand-supply simultaneity. Additionally, the utility providers operate in regulated natural monopoly markets, which mitigates endogeneity from searching provider strategies. Finally, we took prices directly from provider records, which avoids potential price measurement errors @ramirez2024welfare.] In general, there is evidence supporting the relevance of all regressors in these equations, with a few exceptions, and unobserved correlation in the demand for these services, which further supports the use of a SUR model in this application.


```{r}
rm(list = ls())
set.seed(010101)
library(dplyr)
DataUt <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/Utilities.csv", sep = ",", header = TRUE, quote = "")
DataUtEst <- DataUt %>%  
filter(Electricity != 0 & Water !=0 & Gas != 0)
attach(DataUtEst)
y1 <- log(Electricity); y2 <- log(Water); y3 <- log(Gas)
X1 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem, Lnincome)
X2 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Nrooms, HouseholdMem)
X3 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem)
regdata <- NULL
regdata[[1]] <- list(y = y1, X = X1); regdata[[2]] <- list(y = y2, X = X2); regdata[[3]] <- list(y = y3, X = X3)
M <- length(regdata); K1 <- dim(X1)[2]; K2 <- dim(X2)[2]; K3 <- dim(X3)[2] 
K <- K1 + K2 + K3
# Hyperparameters
b0 <- rep(0, K); c0 <- 100; B0 <- c0*diag(K); V <- 5*diag(M); a0 <- M
Prior <- list(betabar = b0, A = solve(B0), nu = a0, V = V)
#Posterior draws
S <- 10000; keep <- 1; Mcmc <- list(R = S, keep = keep, nprint = 0)
PosteriorDraws <- bayesm::rsurGibbs(Data = list(regdata = regdata), Mcmc = Mcmc, Prior = Prior)
Bs <- PosteriorDraws[["betadraw"]]
Names <- c("Const", "LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", 
"Altitude", "Nrooms", "HouseholdMem", "Lnincome", "Const",
"LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", 
"Nrooms", "HouseholdMem","Const",
"LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", 
"Altitude", "Nrooms", "HouseholdMem")
colnames(Bs) <- Names
summary(coda::mcmc(Bs))
summary(PosteriorDraws[["Sigmadraw"]])
```

We ask in the Exercise 5 to run this application using our GUI and the information in the dataset *Utilities.csv*. Observe that this file should be modified to agree the structure that requires our GUI (see the dataset *5Institutions.csv* in the folder *DataApp* of our GitHub repository -**https://github.com/besmarter/BSTApp**- for a template). In addition, we ask to program from scratch the Gibbs sampler algorithm in this application.
