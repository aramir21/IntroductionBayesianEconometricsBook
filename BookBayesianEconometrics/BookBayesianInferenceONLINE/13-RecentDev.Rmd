# Machine learning {#Chap12}

Machine learning approaches are characterized by high-dimensional parameter spaces that are implicit in non-parametric inference. Take into account that non-parametric inference refers to models of potentially infinite parameters, rather than absence of these ones.

## Cross validation and Bayes factors {#sec12_1}
The issue of overfitting in Bayesian inference is mitigated due to its inherent shrinkage property when proper priors are used. Remember that the posterior distribution is a compromise between the sample information and the prior information.

## Regularization {#sec12_2}
The linear normal model using the conjugate family is ridge regression [@Ishwaran2005]. We can use empirical Bayes to select the scale parameter of the prior covariance matrix of the location parameters, which is in turn the regularization parameter in ridge regression (see my class notes in MSc in Data Science and Analytics).

### Bayesian LASSO {#sec12_21}

### Stochastic search variable selection {#sec12_22}

### Non-local priors {#sec12_23}

[@johnson2012bayesian]  
R package: mombf (Model Selection with Bayesian Methods and Information Criteria)  
link: https://cran.r-project.org/web/packages/mombf/index.html

## Bayesian additive regression trees {#sec12_3}

## Gaussian processes {#sec12_4}
