<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec11.html"/>
<link rel="next" href="sec14.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Dirichlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Non-parametric generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Bayesian framework: A brief summary of theory<a href="sec12.html#sec12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given an unknown parameter set <span class="math inline">\(\boldsymbol{\theta}\)</span>, and a particular realization of the data <span class="math inline">\(\mathbf{y}\)</span>, Bayes’ rule may be applied analogously,<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
<span class="math display" id="eq:121">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \mathbf{y})&amp;=\frac{p(\mathbf{y}\mid \boldsymbol{\theta}) \times \pi(\boldsymbol{\theta})}{p(\mathbf{y})},
    \tag{1.3}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\pi(\boldsymbol{\theta}\mid \mathbf{y})\)</span> is the posterior density function, <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> is the prior density, <span class="math inline">\(p(\mathbf{y}\mid \boldsymbol{\theta})\)</span> is the likelihood (statistical model), and</p>
<p><span class="math display" id="eq:121a">\[\begin{equation}
    p(\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{y}\mid \boldsymbol{\theta})\pi(\boldsymbol{\theta})d\boldsymbol{\theta}=\mathbb{E}\left[p(\mathbf{y}\mid \boldsymbol{\theta})\right]
    \tag{1.4}
\end{equation}\]</span></p>
<p>is the marginal likelihood or prior predictive. Observe that for this expected value to be meaningful, the prior should be a proper density, that is, it must integrate to one; otherwise, it does not make sense.</p>
<p>Observe that <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta})\)</span> is not a density in <span class="math inline">\(\boldsymbol{\theta}\)</span>. In addition, <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> does not have to integrate to 1, that is, <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> can be an improper density function, <span class="math inline">\(\int_{\mathbf{\Theta}} \pi(\boldsymbol{\theta}) d\boldsymbol{\theta} = \infty\)</span>. However, <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> is a proper density function, that is, <span class="math inline">\(\int_{\mathbf{\Theta}} \pi(\boldsymbol{\theta} \mid \mathbf{y}) d\boldsymbol{\theta} = 1\)</span>.</p>
<p>For instance, set <span class="math inline">\(\pi(\boldsymbol{\theta}) = c\)</span>, where <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(\int_{\mathbf{\Theta}} c d\boldsymbol{\theta} = \infty\)</span>. However,
<span class="math display">\[
\int_{\mathbf{\Theta}} \pi(\boldsymbol{\theta} \mid \mathbf{y}) d\boldsymbol{\theta} = \int_{\mathbf{\Theta}} \frac{p(\mathbf{y} \mid \boldsymbol{\theta}) \times c}{\int_{\mathbf{\Theta}} p(\mathbf{y} \mid \boldsymbol{\theta}) \times c \, d\boldsymbol{\theta}} d\boldsymbol{\theta} = 1
\]</span>
where <span class="math inline">\(c\)</span> cancels out.</p>
<p><span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> is a sample updated ``probabilistic belief” version of <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span>, where <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> is a prior probabilistic belief which can be constructed from previous empirical work, theoretical foundations, expert knowledge, and/or mathematical convenience. This prior usually depends on parameters, which are named . In addition, the Bayesian approach implies using a probabilistic model about <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(\boldsymbol{\theta}\)</span>, that is, <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta})\)</span>, where its integral over <span class="math inline">\(\mathbf{\Theta}\)</span>, <span class="math inline">\(p(\mathbf{y})\)</span>, is named due to being a measure of model fit to the data.</p>
<p>Observe that the Bayesian inferential approach is conditional, that is, what can we learn about an unknown object <span class="math inline">\(\boldsymbol{\theta}\)</span> given that we already observed <span class="math inline">\(\mathbf{ Y} =\mathbf{y}\)</span>? The answer is also conditional on the probabilistic model, that is, <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta})\)</span>. So, what if we want to compare different models, say <span class="math inline">\(\mathcal{M}_m\)</span>, <span class="math inline">\(m = \{1,2,\dots,M\}\)</span>? Then, we should make explicit this in the Bayes’ rule formulation:
<span class="math display" id="eq:122">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \mathbf{y},\mathcal{M}_m)&amp;=\frac{p(\mathbf{y}\mid \boldsymbol{\theta},\mathcal{M}_m) \times \pi(\boldsymbol{\theta}\mid \mathcal{M}_m)}{p(\mathbf{y}\mid \mathcal{M}_m)}.
    \tag{1.5}
\end{align}\]</span></p>
<p>The posterior model probability is
<span class="math display" id="eq:123">\[\begin{align}
    \pi(\mathcal{M}_m\mid \mathbf{y})&amp;=\frac{p(\mathbf{y}\mid \mathcal{M}_m) \times \pi(\mathcal{M}_m)}{p(\mathbf{y})},
    \tag{1.6}
\end{align}\]</span></p>
<p>where <span class="math inline">\(p(\mathbf{y}\mid \mathcal{M}_m)=\int_{\mathbf{\Theta}}p(\mathbf{y}\mid \boldsymbol{\theta},\mathcal{M}_m) \times \pi(\boldsymbol{\theta}\mid \mathcal{M}_m)d\boldsymbol{\theta}\)</span> due to equation <a href="sec12.html#eq:122">(1.5)</a>, and <span class="math inline">\(\pi(\mathcal{M}_m)\)</span> is the prior model probability.</p>
<p>Calculating <span class="math inline">\(p(\mathbf{y})\)</span> in equations <a href="sec12.html#eq:121">(1.3)</a> and <a href="sec12.html#eq:123">(1.6)</a> is very demanding in most of the realistic cases. Fortunately, it is not required when performing inference about <span class="math inline">\(\boldsymbol{\theta}\)</span> as this is integrated out from it. Then, all you need to know about the shape of <span class="math inline">\(\boldsymbol{\theta}\)</span> is in <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta}, \mathcal{M}_m) \times \pi(\boldsymbol{\theta} \mid \mathcal{M}_m)\)</span>, or without explicitly conditioning on <span class="math inline">\(\mathcal{M}_m\)</span>,
<span class="math display" id="eq:124">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \mathbf{y})&amp; \propto p(\mathbf{y}\mid \boldsymbol{\theta}) \times \pi(\boldsymbol{\theta}).
    \tag{1.7}
\end{align}\]</span></p>
<p>Equation <a href="sec12.html#eq:124">(1.7)</a> is a very good shortcut to perform Bayesian inference about <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>We can also avoid calculating <span class="math inline">\(p(\mathbf{y})\)</span> when performing model selection (hypothesis testing) using the posterior odds ratio, that is, comparing models <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span>,</p>
<p><span class="math display" id="eq:125">\[\begin{align}
    PO_{12}&amp;=\frac{\pi(\mathcal{M}_1\mid \mathbf{y})}{\pi(\mathcal{M}_2\mid \mathbf{y})} \nonumber \\
    &amp;=\frac{p(\mathbf{y}\mid \mathcal{M}_1)}{p(\mathbf{y}\mid \mathcal{M}_2)}\times\frac{\pi(\mathcal{M}_1)}{\pi(\mathcal{M}_2)},
    \tag{1.8}
\end{align}\]</span></p>
<p>where the first term in equation <a href="sec12.html#eq:125">(1.8)</a> is named the <em>Bayes factor</em>, and the second term is the <em>prior odds</em>. Observe that the Bayes factor is a ratio of ordinates for <span class="math inline">\(\mathbf{y}\)</span> under different models. Then, the Bayes factor is a measure of relative sample evidence in favor of model 1 compared to model 2.</p>
<p>However, we still need to calculate <span class="math inline">\(p(\mathbf{y}\mid \mathcal{M}_m) = \int_{\mathbf{\Theta}} p(\mathbf{y}\mid \boldsymbol{\theta}, \mathcal{M}_m) \pi(\boldsymbol{\theta}\mid \mathcal{M}_m) d\boldsymbol{\theta} = \mathbb{E}\left[ p(\mathbf{y}\mid \boldsymbol{\theta}, \mathcal{M}_m) \right]\)</span>. For this integral to be meaningful, the prior must be proper. Using an improper prior has unintended consequences when comparing models; for instance, parsimonious models are favored by posterior odds or Bayes factors, and these values may depend on units of measure (see Chapter <a href="Chap3.html#Chap3">3</a>).</p>
<p>A nice feature of comparing models using posterior odds is that if we have an exhaustive set of competing models such that <span class="math inline">\(\sum_{m=1}^M \pi(\mathcal{M}_m \mid \mathbf{y}) = 1\)</span>, then we can recover <span class="math inline">\(\pi(\mathcal{M}_m \mid \mathbf{y})\)</span> without calculating <span class="math inline">\(p(\mathbf{y})\)</span>. In particular, given two models <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span> such that <span class="math inline">\(\pi(\mathcal{M}_1 \mid \mathbf{y}) + \pi(\mathcal{M}_2 \mid \mathbf{y}) = 1\)</span>, we have:
<span class="math display">\[
\pi(\mathcal{M}_1 \mid \mathbf{y}) = \frac{PO_{12}}{1 + PO_{12}} \quad \text{and} \quad \pi(\mathcal{M}_2 \mid \mathbf{y}) = 1 - \pi(\mathcal{M}_1 \mid \mathbf{y}).
\]</span>
In general,
<span class="math display">\[
\pi(\mathcal{M}_m \mid \mathbf{y}) = \frac{p(\mathbf{y} \mid \mathcal{M}_m) \times \pi(\mathcal{M}_m)}{\sum_{l=1}^M p(\mathbf{y} \mid \mathcal{M}_l) \times \pi(\mathcal{M}_l)}.
\]</span>
These posterior model probabilities can be used to perform Bayesian model averaging.</p>
<p>Table <a href="sec12.html#tab:guide">1.1</a> shows guidelines for the interpretation of <span class="math inline">\(2\log(PO_{12})\)</span> <span class="citation">(<a href="#ref-Kass1995">R. E. Kass and Raftery 1995</a>)</span>. This transformation is done to replicate the structure of the likelihood ratio test statistic. However, posterior odds do not require nested models as the likelihood ratio test does.</p>
<table>
<caption><span id="tab:guide">Table 1.1: </span>Kass and Raftery guidelines</caption>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(2\log(PO_{12})\)</span></th>
<th align="left"><span class="math inline">\(PO_{12}\)</span></th>
<th align="left">Evidence against <span class="math inline">\(\mathcal{M}_{2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0 to 2</td>
<td align="left">1 to 3</td>
<td align="left">Not worth more than a bare mention</td>
</tr>
<tr class="even">
<td align="left">2 to 6</td>
<td align="left">3 to 20</td>
<td align="left">Positive</td>
</tr>
<tr class="odd">
<td align="left">6 to 10</td>
<td align="left">20 to 150</td>
<td align="left">Strong</td>
</tr>
<tr class="even">
<td align="left">&gt; 10</td>
<td align="left">&gt; 150</td>
<td align="left">Very strong</td>
</tr>
</tbody>
</table>
<p>Observe that the posterior odds ratio is a relative criterion, that is, we specify an exhaustive set of competing models and compare them. However, we may want to check the performance of a model on its own or use a non-informative prior. In this case, we can use <em>the posterior predictive p-value</em> <span class="citation">(<a href="#ref-Gelman1996">A. Gelman and Meng 1996</a>; <a href="#ref-gelman1996posterior">A. Gelman, Meng, and Stern 1996</a>)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>The intuition behind the predictive p-value is simple: analyze the discrepancy between the model’s assumptions and the data by checking a potential extreme tail-area probability. Observe that this approach does not check if a model is true; its focus is on potential discrepancies between the model and the data at hand.</p>
<p>This is done by simulating pseudo-data from our sampling model (<span class="math inline">\(\mathbf{y}^{(s)}, s=1,2,\dots,S\)</span>) using draws from the posterior distribution, and then calculating a discrepancy measure, <span class="math inline">\(D(\mathbf{y}^{(s)},\boldsymbol{\theta})\)</span>, to estimate the posterior predictive p-value,
<span class="math display">\[
p_D(\mathbf{y}) = P[D(\mathbf{y}^{(s)},\boldsymbol{\theta}) \geq D(\mathbf{y},\boldsymbol{\theta})],
\]</span>
using the proportion of the <span class="math inline">\(S\)</span> draws for which <span class="math inline">\(D(\mathbf{y}^{(s)},\boldsymbol{\theta}^{(s)}) \geq D(\mathbf{y},\boldsymbol{\theta}^{(s)})\)</span>. Extreme tail probabilities (<span class="math inline">\(p_D(\mathbf{y}) \leq 0.05\)</span> or <span class="math inline">\(p_D(\mathbf{y}) \geq 0.95\)</span>) suggest potential discrepancies between the data and the model. <span class="citation">A. Gelman, Meng, and Stern (<a href="#ref-gelman1996posterior">1996</a>)</span> also suggest the posterior predictive p-value based on the <em>minimum discrepancy</em>,
<span class="math display">\[
D_{\min}(\mathbf{y}) = \min_{\boldsymbol{\theta}} D(\mathbf{y}, \boldsymbol{\theta}),
\]</span>
and the <em>average discrepancy</em> statistic
<span class="math display">\[
D(\mathbf{y}) = \mathbb{E}[D(\mathbf{y}, \boldsymbol{\theta})] = \int_{\mathbf{\Theta}} D(\mathbf{y}, \boldsymbol{\theta}) \pi(\boldsymbol{\theta} \mid \mathbf{y}) d\boldsymbol{\theta}.
\]</span>
These alternatives can be more computationally demanding.</p>
<p>The Bayesian approach is also suitable to get probabilistic predictions, that is, we can obtain a posterior predictive density</p>
<p><span class="math display" id="eq:126">\[\begin{align}
    \pi(\mathbf{y}_0\mid \mathbf{y},\mathcal{M}_m) &amp; =\int_{\mathbf{\Theta}}\pi(\mathbf{y}_0,\boldsymbol{\theta}\mid \mathbf{y},\mathcal{M}_m)d\boldsymbol{\theta}\nonumber\\
    &amp;=\int_{\mathbf{\Theta}}\pi(\mathbf{y}_0\mid \boldsymbol{\theta},\mathbf{y},\mathcal{M}_m)\pi(\boldsymbol{\theta}\mid \mathbf{y},\mathcal{M}_m)d\boldsymbol{\theta}.
    \tag{1.9}
\end{align}\]</span></p>
<p>Observe that equation <a href="sec12.html#eq:126">(1.9)</a> is again an expectation <span class="math inline">\(\mathbb{E}[\pi(\mathbf{y}_0 \mid \boldsymbol{\theta}, \mathbf{y}, \mathcal{M}_m)]\)</span>, this time using the posterior distribution. Therefore, the Bayesian approach takes estimation error into account when performing prediction.</p>
<p>As we have shown many times, expectation (integration) is a common feature in Bayesian inference. That is why the remarkable relevance of computation based on <em>Monte Carlo integration</em> in the Bayesian framework.</p>
<p><em>Bayesian model averaging</em> (BMA) allows for considering model uncertainty in prediction or any unknown probabilistic object. In the case of the predictive density,</p>
<p><span class="math display">\[\begin{align}
    \pi(\mathbf{y}_0\mid \mathbf{y})&amp;=\sum_{m=1}^M \pi(\mathcal{M}_m\mid \mathbf{y})\pi(\mathbf{y}_0\mid \mathbf{y},\mathcal{M}_m).
\end{align}\]</span>
In the case of the posterior density of the parameters,
<span class="math display">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \mathbf{y})&amp;=\sum_{m=1}^M \pi(\mathcal{M}_m\mid \mathbf{y})\pi(\boldsymbol{\theta}\mid \mathbf{y},\mathcal{M}_m),
\end{align}\]</span>
where
<span class="math display" id="eq:127">\[\begin{align}
    \mathbb{E}(\boldsymbol{\theta}\mid \mathbf{y})=\sum_{m=1}^{M}\hat{\boldsymbol{\theta}}_m \pi(\mathcal{M}_m\mid \mathbf{y}),
    \tag{1.10}
\end{align}\]</span>
and
<span class="math display" id="eq:128">\[\begin{align}
    Var({\theta}_k\mid \mathbf{y})= \sum_{m=1}^{M}\pi(\mathcal{M}_m\mid \mathbf{y}) \widehat{Var} ({\theta}_{km}\mid \mathbf{y},\mathcal{M}_m)+\sum_{m=1}^{M} \pi(\mathcal{M}_m\mid \mathbf{y}) (\hat{{\theta}}_{km}-\mathbb{E}[{\theta}_{km}\mid \mathbf{y}])^2,
    \tag{1.11}
\end{align}\]</span>
<span class="math inline">\(\hat{\boldsymbol{\theta}}_m\)</span> is the posterior mean and <span class="math inline">\(\widehat{Var}({\theta}_{km}\mid \mathbf{y},\mathcal{M}_m)\)</span> is the posterior variance of the <span class="math inline">\(k\)</span>-th element of <span class="math inline">\(\boldsymbol{\theta}\)</span> under model <span class="math inline">\(\mathcal{M}_m\)</span>.</p>
<p>Observe how the variance in equation <a href="sec12.html#eq:128">(1.11)</a> captures the extra variability due to potential differences between the mean posterior estimates associated with each model, and the posterior mean that incorporates model uncertainty in equation <a href="sec12.html#eq:127">(1.10)</a>.</p>
<p>A significant advantage of the Bayesian approach, which is particularly useful in (see Chapter <a href="Chap8.html#Chap8">8</a>), is the way the posterior distribution updates with new sample information. Given <span class="math inline">\(\mathbf{y} = \mathbf{y}_{1:t+1}\)</span> as a sequence of observations from 1 to <span class="math inline">\(t+1\)</span>, then
<span class="math display" id="eq:128a">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \mathbf{y}_{1:t+1})&amp;\propto p(\mathbf{y}_{1:t+1}\mid \boldsymbol{\theta})\times \pi(\boldsymbol{\theta})\nonumber\\
    &amp;= p(y_{t+1}\mid \mathbf{y}_{1:t},\boldsymbol{\theta})\times p(\mathbf{y}_{1:t}\mid \boldsymbol{\theta})\times \pi(\boldsymbol{\theta})\nonumber\\
    &amp;\propto p(y_{t+1}\mid \mathbf{y}_{1:t},\boldsymbol{\theta})\times \pi(\boldsymbol{\theta}\mid \mathbf{y}_{1:t}).
    \tag{1.12}
\end{align}\]</span></p>
<p>We observe in Equation <a href="sec12.html#eq:128a">(1.12)</a> that the new prior is simply the posterior distribution based on the previous observations. This is particularly useful under the assumption of <em>conditional independence</em>, that is, <span class="math inline">\(Y_{t+1} \perp \mathbf{Y}_{1:t} \mid \boldsymbol{\theta}\)</span>, so that <span class="math inline">\(p(y_{t+1} \mid \mathbf{y}_{1:t}, \boldsymbol{\theta}) = p(y_{t+1} \mid \boldsymbol{\theta})\)</span>, allowing the posterior to be recovered recursively <span class="citation">(<a href="#ref-petris2009dynamic">Petris, Petrone, and Campagnoli 2009</a>)</span>. This facilitates online updating because all information up to time <span class="math inline">\(t\)</span> is captured in <span class="math inline">\(\boldsymbol{\theta}\)</span>. Therefore, <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y}_{1:t+1}) \propto p(y_{t+1} \mid \boldsymbol{\theta}) \times \pi(\boldsymbol{\theta} \mid \mathbf{y}_{1:t}) \propto \prod_{h=1}^{t+1} p(y_h \mid \boldsymbol{\theta}) \times \pi(\boldsymbol{\theta})\)</span>. This recursive expression can be computed more efficiently at any specific point in time <span class="math inline">\(t\)</span>, compared to a batch-mode algorithm, which requires processing all information up to time <span class="math inline">\(t\)</span> simultaneously.</p>
<p>It is also important to consider the sampling properties of “Bayesian estimators”. This topic has attracted the attention of statisticians and econometricians for a long time. For instance, asymptotic posterior concentration on the population parameter vector is discussed by <span class="citation">Bickel and Yahav (<a href="#ref-bickel1969some">1969</a>)</span>. The convergence of posterior distributions is stated by the Bernstein-von Mises theorem <span class="citation">(<a href="#ref-Lehmann2003">Lehmann and Casella 2003</a>; <a href="#ref-van2000asymptotic">Van der Vaart 2000</a>)</span>, which establishes a link between <em>credible intervals (sets)</em> and confidence intervals (sets), where a credible interval is an interval in the domain of the posterior distribution within which an unknown parameter falls with a particular probability. Credible intervals treat bounds as fixed and parameters as random, whereas confidence intervals reverse this. There are many settings in parametric models where Bayesian credible intervals with an <span class="math inline">\(\alpha\)</span> level converge asymptotically to confidence intervals at the <span class="math inline">\(\alpha\)</span> level. This suggests that Bayesian inference is asymptotically correct from a sampling perspective in these settings.</p>
<p>A heuristic approach to demonstrate this in the simplest case, where we assume random sampling and <span class="math inline">\(\theta \in \mathcal{R}\)</span>, is the following: <span class="math inline">\(p(\mathbf{y} \mid \theta) = \prod_{i=1}^N p(y_i \mid \theta)\)</span>, so the log likelihood is <span class="math inline">\(l(\mathbf{y} \mid \theta) \equiv \log p(\mathbf{y} \mid \theta) = \sum_{i=1}^N \log p(y_i \mid \theta) = N \times \bar{l}(\mathbf{y} \mid \theta)\)</span>, where <span class="math inline">\(\bar{l} \equiv \frac{1}{N} \sum_{i=1}^N \log p(y_i \mid \theta)\)</span> is the mean likelihood.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Then, the posterior distribution is proportional to</p>
<p><span class="math display">\[\begin{align}
    \pi(\theta\mid \mathbf{y})&amp;\propto p(\mathbf{y}\mid \theta) \times \pi(\theta)\nonumber\\
    &amp;=\exp\left\{N\times \bar{l}(\mathbf{y}\mid \theta)\right\} \times \pi(\theta).
\end{align}\]</span></p>
<p>Observe that as the sample size increases, that is, as <span class="math inline">\(N \to \infty\)</span>, the exponential term should dominate the prior distribution as long as the prior does not depend on <span class="math inline">\(N\)</span>, such that the likelihood determines the posterior distribution asymptotically.</p>
<p><em>Maximum likelihood</em> theory shows that <span class="math inline">\(\lim_{N \to \infty} \bar{l}(\mathbf{y} \mid \theta) \to \bar{l}(\mathbf{y} \mid \theta_0)\)</span>, where <span class="math inline">\(\theta_0\)</span> is the population parameter of the data-generating process. In addition, performing a second-order Taylor expansion of the log likelihood at the maximum likelihood estimator,</p>
<p><span class="math display">\[\begin{align*}
    l(\mathbf{y}\mid \theta)&amp;\approx l(\mathbf{y}\mid \hat{\theta})+\left.\frac{dl(\mathbf{y}\mid {\theta})}{d\theta}\right\vert_{\hat{\theta}}(\theta-\hat{\theta})+\frac{1}{2}\left.\frac{d^2l(\mathbf{y}\mid {\theta})}{d\theta^2}\right\vert_{\hat{\theta}}(\theta-\hat{\theta})^2\\
    &amp;= l(\mathbf{y}\mid \hat{\theta})+\frac{1}{2}\left.\sum_{i=1}^N\frac{d^2l(y_i\mid {\theta})}{d\theta^2}\right\vert_{\hat{\theta}}(\theta-\hat{\theta})^2\\
    &amp;= l(\mathbf{y}\mid \hat{\theta})-\frac{1}{2}\left.N\left[-\bar{l}&#39;&#39;\right\vert_{\hat{\theta}}\right](\theta-\hat{\theta})^2\\
    &amp;= l(\mathbf{y}\mid \hat{\theta})-\frac{N}{2\sigma^2}(\theta-\hat{\theta})^2
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\left.\frac{dl(\mathbf{y}\mid \theta)}{d\theta}\right\vert_{\hat{\theta}}=0\)</span>, <span class="math inline">\(\bar{l}&#39;&#39;\equiv\frac{1}{N}\left.\sum_{i=1}^N\frac{d^2l(y_i\mid {\theta})}{d\theta^2}\right\vert_{\hat{\theta}}\)</span> and <span class="math inline">\(\sigma^2:=\left[\left.-\bar{l}&#39;&#39;\right\vert_{\hat{\theta}}\right]^{-1}\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Then,</p>
<p><span class="math display">\[\begin{align*}
    \pi(\theta\mid \mathbf{y})&amp;\propto \exp\left\{{l}(\mathbf{y}\mid \theta)\right\} \times \pi(\theta)\\
    &amp;\approx \exp\left\{l(\mathbf{y}\mid \hat{\theta})-\frac{N}{2\sigma^2}(\theta-\hat{\theta})^2\right\} \times \pi(\theta)\\
    &amp;\propto \exp\left\{-\frac{N}{2\sigma^2}(\theta-\hat{\theta})^2\right\} \times \pi(\theta)\\
\end{align*}\]</span></p>
<p>Observe that the posterior density is proportional to the kernel of a normal density with mean <span class="math inline">\(\hat{\theta}\)</span> and variance <span class="math inline">\(\sigma^2 / N\)</span>, as long as <span class="math inline">\(\pi(\hat{\theta}) \neq 0\)</span>. This kernel dominates as the sample size increases due to the <span class="math inline">\(N\)</span> in the exponential term. It is also important to note that the prior should not exclude values of <span class="math inline">\(\theta\)</span> that are logically possible, such as <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><strong>Example: Health insurance</strong></p>
<p>Suppose that you are analyzing whether to buy health insurance next year. To make a better decision, you want to know <em>what the probability is that you will visit your doctor at least once next year?</em> To answer this question, you have records of the number of times you have visited your doctor over the last 5 years, <span class="math inline">\(\mathbf{y} = \{0, 3, 2, 1, 0\}\)</span>. How should you proceed?</p>
<p>Assuming that this is a random sample<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> from a data-generating process (statistical model) that is Poisson, i.e., <span class="math inline">\(Y_i \sim P(\lambda)\)</span>, and your probabilistic prior beliefs about <span class="math inline">\(\lambda\)</span> are well described by a Gamma distribution with shape and scale parameters <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span>, i.e., <span class="math inline">\(\lambda \sim G(\alpha_0, \beta_0)\)</span>, then you are interested in calculating the probability <span class="math inline">\(P(Y_0 &gt; 0 \mid \mathbf{y})\)</span>. To answer this, you need to calculate the posterior predictive density <span class="math inline">\(\pi(y_0 \mid \mathbf{y})\)</span> in a Bayesian way.</p>
<p>In this example, <span class="math inline">\(p(\mathbf{y} \mid \lambda)\)</span> is Poisson, and <span class="math inline">\(\pi(\lambda)\)</span> is Gamma. Therefore, using Equation <a href="sec12.html#eq:126">(1.9)</a>.</p>
<p><span class="math display">\[\begin{align*}
    \pi(y_0\mid \mathbf{y})=&amp;\int_{0}^{\infty}\frac{\lambda^{y_0}\exp\left\{-\lambda\right\}}{y_0!}\times \pi(\lambda\mid \mathbf{y})d\lambda,\\
\end{align*}\]</span></p>
<p>where the posterior distribution is<br />
<span class="math display">\[
\pi(\lambda\mid \mathbf{y})\propto \lambda^{\sum_{i=1}^N y_i + \alpha_0 - 1}\exp\left\{-\lambda\left(\frac{\beta_0 N+1}{\beta_0}\right)\right\}
\]</span>
by Equation <a href="sec12.html#eq:121">(1.3)</a>.</p>
<p>Observe that the last expression is the kernel of a Gamma distribution with parameters <span class="math inline">\(\alpha_n = \sum_{i=1}^N y_i + \alpha_0\)</span> and <span class="math inline">\(\beta_n = \frac{\beta_0}{\beta_0 N + 1}\)</span>. Given that <span class="math inline">\(\int_0^{\infty} \pi(\lambda \mid \mathbf{y}) d\lambda = 1\)</span>, the constant of proportionality in the last expression is <span class="math inline">\(\Gamma(\alpha_n) \beta_n^{\alpha_n}\)</span>, where <span class="math inline">\(\Gamma(\cdot)\)</span> is the Gamma function. Thus, the posterior density function <span class="math inline">\(\pi(\lambda \mid \mathbf{y})\)</span> is <span class="math inline">\(G(\alpha_n, \beta_n)\)</span>.</p>
<p>Observe that</p>
<p><span class="math display">\[\begin{align*}
    \mathbb{E}[\lambda\mid \mathbf{y}]&amp;=\alpha_n\beta_n\\
    &amp;=\left(\sum_{i=1}^N y_i + \alpha_0\right)\left(\frac{\beta_0}{\beta_0 N + 1}\right)\\
    &amp;=\bar{y}\left(\frac{N\beta_0}{N\beta_0+1}\right)+\alpha_0\beta_0\left(\frac{1}{N\beta_0+1}\right)\\
    &amp;=w\bar{y}+(1-w)\mathbb{E}[\lambda],
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\bar{y}\)</span> is the sample mean estimate, which is the maximum likelihood estimate of <span class="math inline">\(\lambda\)</span> in this example, <span class="math inline">\(w = \left(\frac{N\beta_0}{N\beta_0 + 1}\right)\)</span>, and <span class="math inline">\(\mathbb{E}[\lambda] = \alpha_0 \beta_0\)</span> is the prior mean. The posterior mean is a weighted average of the maximum likelihood estimator (sample information) and the prior mean. Observe that <span class="math inline">\(\lim_{N \to \infty} w = 1\)</span>, that is, the sample information asymptotically dominates.</p>
<p>The predictive distribution is</p>
<p><span class="math display">\[\begin{align*}
    \pi(y_0\mid \mathbf{y})=&amp;\int_{0}^{\infty}\frac{\lambda^{y_0}\exp\left\{-\lambda\right\}}{y_0!}\times \frac{1}{\Gamma(\alpha_n)\beta_n^{\alpha_n}}\lambda^{\alpha_n-1}\exp\left\{-\lambda/\beta_n\right\} d\lambda\\
    =&amp;\frac{1}{y_0!\Gamma(\alpha_n)\beta_n^{\alpha_n}}\int_{0}^{\infty}\lambda^{y_0+\alpha_n-1}\exp\left\{-\lambda\left(\frac{1+\beta_n}{\beta_n}\right)\right\}d\lambda\\
    =&amp;\frac{\Gamma(y_0+\alpha_n)\left(\frac{\beta_n}{\beta_n+1}\right)^{y_0+\alpha_n}}{y_0!\Gamma(\alpha_n)\beta_n^{\alpha_n}}\\
    =&amp;{y_0+\alpha_n-1 \choose y_0}\left(\frac{\beta_n}{\beta_n+1}\right)^{y_0}\left(\frac{1}{\beta_n+1}\right)^{\alpha_n}.
\end{align*}\]</span></p>
<p>The third equality follows from the kernel of a Gamma density, and the fourth from<br />
<span class="math display">\[
{y_0 + \alpha_n - 1 \choose y_0} = \frac{(y_0 + \alpha_n - 1)(y_0 + \alpha_n - 2)\dots\alpha_n}{y_0!} = \frac{\Gamma(y_0 + \alpha_n)}{\Gamma(\alpha_n) y_0!}
\]</span>
using a property of the Gamma function.</p>
<p>Observe that this is a Negative Binomial density, that is, <span class="math inline">\(Y_0 \mid \mathbf{y} \sim \text{NB}(\alpha_n, p_n)\)</span> where <span class="math inline">\(p_n = \frac{\beta_n}{\beta_n + 1}\)</span>.</p>
<p>Up to this point, we have said nothing about the hyperparameters, which are required to give a concrete response to this exercise. Thus, we show two approaches to set them. First, we set <span class="math inline">\(\alpha_0 = 0.001\)</span> and <span class="math inline">\(\beta_0 = \frac{1}{0.001}\)</span>, which imply vague prior information about <span class="math inline">\(\lambda\)</span> due to having a large degree of variability compared to the mean information.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> In particular, <span class="math inline">\(\mathbb{E}[\lambda] = 1\)</span> and <span class="math inline">\(\mathbb{V}ar[\lambda] = 1000\)</span>.</p>
<p>In this setting, <span class="math inline">\(P(Y_0 &gt; 0 \mid \mathbf{y}) = 1 - P(Y_0 = 0 \mid \mathbf{y}) \approx 0.67\)</span>. That is, the probability of visiting the doctor at least once next year is approximately 0.67.</p>
<p>Another approach is using <em>Empirical Bayes</em>, where we set the hyperparameters maximizing the logarithm of the marginal likelihood,<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> that is,
<span class="math display">\[
\left[\hat{\alpha}_0 \ \hat{\beta}_0\right]^{\top} = \underset{\alpha_0, \beta_0}{\mathrm{argmax}} \ \ln p(\mathbf{y})
\]</span>
where
<span class="math display">\[
\begin{align}
    p(\mathbf{y}) &amp;= \int_0^{\infty} \left\{ \frac{1}{\Gamma(\alpha_0)\beta_0^{\alpha_0}} \lambda^{\alpha_0 - 1} \exp\left\{-\lambda / \beta_0\right\} \prod_{i=1}^N \frac{\lambda^{y_i} \exp\left\{-\lambda\right\}}{ y_i!} \right\} d\lambda \\
    &amp;= \frac{\int_0^{\infty} \lambda^{\sum_{i=1}^N y_i + \alpha_0 - 1} \exp\left\{-\lambda \left( \frac{\beta_0 N + 1}{\beta_0} \right) \right\} d\lambda}{ \Gamma(\alpha_0) \beta_0^{\alpha_0} \prod_{i=1}^N y_i! } \\
    &amp;= \frac{\Gamma\left(\sum_{i=1}^N y_i + \alpha_0\right) \left( \frac{\beta_0}{N\beta_0 + 1} \right)^{\sum_{i=1}^N y_i} \left( \frac{1}{N\beta_0 + 1} \right)^{\alpha_0}}{ \Gamma(\alpha_0) \prod_{i=1}^N y_i }
\end{align}
\]</span></p>
<p>Using the empirical Bayes approach, we get <span class="math inline">\(\hat{\alpha}_0 = 51.8\)</span> and <span class="math inline">\(\hat{\beta}_0 = 0.023\)</span>, then <span class="math inline">\(P(Y_0 &gt; 0 \mid \mathbf{y}) = 1 - P(Y_0 = 0 \mid \mathbf{y}) \approx 0.70\)</span>.</p>
<p>Observe that we can calculate the posterior odds comparing the model using an Empirical Bayes prior (model 1) versus the vague prior (model 2). We assume that <span class="math inline">\(\pi(\mathcal{M}_1) = \pi(\mathcal{M}_2) = 0.5\)</span>, then
<span class="math display">\[
\begin{align}
    PO_{12} &amp;= \frac{p(\mathbf{y} \mid \text{Empirical Bayes})}{ p(\mathbf{y} \mid \text{Vague prior}) } \\
    &amp;= \frac{\frac{\Gamma\left(\sum_{i=1}^N y_i + 51.807\right) \left( \frac{0.023}{N \times 0.023 + 1} \right)^{\sum_{i=1}^N y_i} \left( \frac{1}{N \times 0.023 + 1} \right)^{51.807}}{\Gamma(51.807)}}{\frac{\Gamma\left(\sum_{i=1}^N y_i + 0.001\right) \left( \frac{1/0.001}{N/0.001 + 1} \right)^{\sum_{i=1}^N y_i} \left( \frac{1}{N/0.001 + 1} \right)^{0.001}}{\Gamma(0.001)}} \\
    &amp;\approx 919
\end{align}
\]</span></p>
<p>Then, <span class="math inline">\(2 \times \log(PO_{12}) = 13.64\)</span>, which provides very strong evidence against the vague prior model (see Table <a href="sec12.html#tab:guide">1.1</a>). In particular, <span class="math inline">\(\pi(\text{Empirical Bayes} \mid \mathbf{y}) = \frac{919}{1 + 919} = 0.999\)</span> and <span class="math inline">\(\pi(\text{Vague prior} \mid \mathbf{y}) = 1 - 0.999 = 0.001\)</span>. These probabilities can be used to perform Bayesian model averaging (BMA). In particular,
<span class="math display">\[
\begin{align}
    \mathbb{E}(\lambda \mid \mathbf{y}) &amp;= 1.2 \times 0.999 + 1.2 \times 0.001 = 1.2 \\
    \text{Var}(\lambda \mid \mathbf{y}) &amp;= 0.025 \times 0.999 + 0.24 \times 0.001 \\
    &amp;+ (1.2 - 1.2)^2 \times 0.999 + (1.2 - 1.2)^2 \times 0.001 = 0.025
\end{align}
\]</span></p>
<p>The BMA predictive distribution is a mix of negative binomial distributions, that is,
<span class="math display">\[
Y_0 \mid \mathbf{y} \sim 0.999 \times \text{NB}(57.8, 0.02) + 0.001 \times \text{NB}(6.001, 0.17)
\]</span></p>
<p>The following code shows how to perform this exercise in <strong>R</strong>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="sec12.html#cb9-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb9-2"><a href="sec12.html#cb9-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># Data</span></span>
<span id="cb9-3"><a href="sec12.html#cb9-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb9-4"><a href="sec12.html#cb9-4" tabindex="-1"></a>ProbBo <span class="ot">&lt;-</span> <span class="cf">function</span>(y, a0, b0){</span>
<span id="cb9-5"><a href="sec12.html#cb9-5" tabindex="-1"></a>    N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb9-6"><a href="sec12.html#cb9-6" tabindex="-1"></a>    <span class="co">#sample size</span></span>
<span id="cb9-7"><a href="sec12.html#cb9-7" tabindex="-1"></a>    an <span class="ot">&lt;-</span> a0 <span class="sc">+</span> <span class="fu">sum</span>(y) </span>
<span id="cb9-8"><a href="sec12.html#cb9-8" tabindex="-1"></a>    <span class="co"># Posterior shape parameter</span></span>
<span id="cb9-9"><a href="sec12.html#cb9-9" tabindex="-1"></a>    bn <span class="ot">&lt;-</span> b0 <span class="sc">/</span> ((b0 <span class="sc">*</span> N) <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb9-10"><a href="sec12.html#cb9-10" tabindex="-1"></a>    <span class="co"># Posterior scale parameter</span></span>
<span id="cb9-11"><a href="sec12.html#cb9-11" tabindex="-1"></a>    p <span class="ot">&lt;-</span> bn <span class="sc">/</span> (bn <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb9-12"><a href="sec12.html#cb9-12" tabindex="-1"></a>    <span class="co"># Probability negative binomial density</span></span>
<span id="cb9-13"><a href="sec12.html#cb9-13" tabindex="-1"></a>    Pr <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(<span class="dv">0</span>, <span class="at">size=</span>an,<span class="at">prob=</span>(<span class="dv">1</span> <span class="sc">-</span> p)) </span>
<span id="cb9-14"><a href="sec12.html#cb9-14" tabindex="-1"></a>    <span class="co"># Probability of visiting the Doctor at least once next year</span></span>
<span id="cb9-15"><a href="sec12.html#cb9-15" tabindex="-1"></a>    <span class="co"># Observe that in R there is a slightly different parametrization.</span></span>
<span id="cb9-16"><a href="sec12.html#cb9-16" tabindex="-1"></a>    <span class="fu">return</span>(Pr)</span>
<span id="cb9-17"><a href="sec12.html#cb9-17" tabindex="-1"></a>} </span>
<span id="cb9-18"><a href="sec12.html#cb9-18" tabindex="-1"></a><span class="co"># Using a vague prior:</span></span>
<span id="cb9-19"><a href="sec12.html#cb9-19" tabindex="-1"></a>a0 <span class="ot">&lt;-</span> <span class="fl">0.001</span> <span class="co"># Prior shape parameter</span></span>
<span id="cb9-20"><a href="sec12.html#cb9-20" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fl">0.001</span> <span class="co"># Prior scale parameter</span></span>
<span id="cb9-21"><a href="sec12.html#cb9-21" tabindex="-1"></a>PriMeanV <span class="ot">&lt;-</span> a0 <span class="sc">*</span> b0 <span class="co"># Prior mean</span></span>
<span id="cb9-22"><a href="sec12.html#cb9-22" tabindex="-1"></a>PriVarV <span class="ot">&lt;-</span> a0 <span class="sc">*</span> b0<span class="sc">^</span><span class="dv">2</span> <span class="co"># Prior variance</span></span>
<span id="cb9-23"><a href="sec12.html#cb9-23" tabindex="-1"></a>Pp <span class="ot">&lt;-</span> <span class="fu">ProbBo</span>(y, <span class="at">a0 =</span> <span class="fl">0.001</span>, <span class="at">b0 =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fl">0.001</span>) </span>
<span id="cb9-24"><a href="sec12.html#cb9-24" tabindex="-1"></a><span class="co"># This setting is vague prior information.</span></span>
<span id="cb9-25"><a href="sec12.html#cb9-25" tabindex="-1"></a>Pp</span></code></pre></div>
<pre><code>## [1] 0.6650961</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="sec12.html#cb11-1" tabindex="-1"></a><span class="co"># Using Empirical Bayes</span></span>
<span id="cb11-2"><a href="sec12.html#cb11-2" tabindex="-1"></a>LogMgLik <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, y){</span>
<span id="cb11-3"><a href="sec12.html#cb11-3" tabindex="-1"></a> N <span class="ot">&lt;-</span> <span class="fu">length</span>(y) </span>
<span id="cb11-4"><a href="sec12.html#cb11-4" tabindex="-1"></a> <span class="co">#sample size</span></span>
<span id="cb11-5"><a href="sec12.html#cb11-5" tabindex="-1"></a> a0 <span class="ot">&lt;-</span> theta[<span class="dv">1</span>] </span>
<span id="cb11-6"><a href="sec12.html#cb11-6" tabindex="-1"></a> <span class="co"># prior shape hyperparameter</span></span>
<span id="cb11-7"><a href="sec12.html#cb11-7" tabindex="-1"></a> b0 <span class="ot">&lt;-</span> theta[<span class="dv">2</span>] </span>
<span id="cb11-8"><a href="sec12.html#cb11-8" tabindex="-1"></a> <span class="co"># prior scale hyperparameter</span></span>
<span id="cb11-9"><a href="sec12.html#cb11-9" tabindex="-1"></a> an <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">+</span> a0 </span>
<span id="cb11-10"><a href="sec12.html#cb11-10" tabindex="-1"></a> <span class="co"># posterior shape parameter</span></span>
<span id="cb11-11"><a href="sec12.html#cb11-11" tabindex="-1"></a> <span class="cf">if</span>(a0 <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">||</span> b0 <span class="sc">&lt;=</span> <span class="dv">0</span>){ </span>
<span id="cb11-12"><a href="sec12.html#cb11-12" tabindex="-1"></a>  <span class="co">#Avoiding negative values</span></span>
<span id="cb11-13"><a href="sec12.html#cb11-13" tabindex="-1"></a>  lnp <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb11-14"><a href="sec12.html#cb11-14" tabindex="-1"></a>  }<span class="cf">else</span>{</span>
<span id="cb11-15"><a href="sec12.html#cb11-15" tabindex="-1"></a>  lnp <span class="ot">&lt;-</span> <span class="fu">lgamma</span>(an) <span class="sc">+</span> <span class="fu">sum</span>(y)<span class="sc">*</span><span class="fu">log</span>(b0<span class="sc">/</span>(N<span class="sc">*</span>b0<span class="sc">+</span><span class="dv">1</span>)) <span class="sc">-</span> a0<span class="sc">*</span><span class="fu">log</span>(N<span class="sc">*</span>b0<span class="sc">+</span><span class="dv">1</span>) <span class="sc">-</span> <span class="fu">lgamma</span>(a0)</span>
<span id="cb11-16"><a href="sec12.html#cb11-16" tabindex="-1"></a> } </span>
<span id="cb11-17"><a href="sec12.html#cb11-17" tabindex="-1"></a> <span class="co"># log marginal likelihood</span></span>
<span id="cb11-18"><a href="sec12.html#cb11-18" tabindex="-1"></a> <span class="fu">return</span>(<span class="sc">-</span>lnp)</span>
<span id="cb11-19"><a href="sec12.html#cb11-19" tabindex="-1"></a>}           </span>
<span id="cb11-20"><a href="sec12.html#cb11-20" tabindex="-1"></a>theta0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="dv">1</span><span class="sc">/</span><span class="fl">0.1</span>) </span>
<span id="cb11-21"><a href="sec12.html#cb11-21" tabindex="-1"></a><span class="co"># Initial values</span></span>
<span id="cb11-22"><a href="sec12.html#cb11-22" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">maxit =</span> <span class="dv">1000</span>) </span>
<span id="cb11-23"><a href="sec12.html#cb11-23" tabindex="-1"></a><span class="co"># Number of iterations in optimization</span></span>
<span id="cb11-24"><a href="sec12.html#cb11-24" tabindex="-1"></a>EmpBay <span class="ot">&lt;-</span> <span class="fu">optim</span>(theta0, LogMgLik, <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">control =</span> control, <span class="at">hessian =</span> <span class="cn">TRUE</span>, <span class="at">y =</span> y) </span>
<span id="cb11-25"><a href="sec12.html#cb11-25" tabindex="-1"></a><span class="co"># Optimization</span></span>
<span id="cb11-26"><a href="sec12.html#cb11-26" tabindex="-1"></a>EmpBay<span class="sc">$</span>convergence</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="sec12.html#cb13-1" tabindex="-1"></a>a0EB <span class="ot">&lt;-</span> EmpBay<span class="sc">$</span>par[<span class="dv">1</span>] </span>
<span id="cb13-2"><a href="sec12.html#cb13-2" tabindex="-1"></a><span class="co"># Prior shape using empirical Bayes</span></span>
<span id="cb13-3"><a href="sec12.html#cb13-3" tabindex="-1"></a>a0EB</span></code></pre></div>
<pre><code>## [1] 51.80696</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="sec12.html#cb15-1" tabindex="-1"></a>b0EB <span class="ot">&lt;-</span> EmpBay<span class="sc">$</span>par[<span class="dv">2</span>] </span>
<span id="cb15-2"><a href="sec12.html#cb15-2" tabindex="-1"></a><span class="co"># Prior scale using empirical Bayes</span></span>
<span id="cb15-3"><a href="sec12.html#cb15-3" tabindex="-1"></a>b0EB</span></code></pre></div>
<pre><code>## [1] 0.02318341</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="sec12.html#cb17-1" tabindex="-1"></a>PriMeanEB <span class="ot">&lt;-</span> a0EB <span class="sc">*</span> b0EB </span>
<span id="cb17-2"><a href="sec12.html#cb17-2" tabindex="-1"></a><span class="co"># Prior mean</span></span>
<span id="cb17-3"><a href="sec12.html#cb17-3" tabindex="-1"></a>PriVarEB <span class="ot">&lt;-</span> a0EB <span class="sc">*</span> b0EB<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb17-4"><a href="sec12.html#cb17-4" tabindex="-1"></a><span class="co"># Prior variance</span></span>
<span id="cb17-5"><a href="sec12.html#cb17-5" tabindex="-1"></a>PpEB <span class="ot">&lt;-</span> <span class="fu">ProbBo</span>(y, <span class="at">a0 =</span> a0EB, <span class="at">b0 =</span> b0EB) </span>
<span id="cb17-6"><a href="sec12.html#cb17-6" tabindex="-1"></a><span class="co"># This setting is using emprical Bayes.</span></span>
<span id="cb17-7"><a href="sec12.html#cb17-7" tabindex="-1"></a>PpEB</span></code></pre></div>
<pre><code>## [1] 0.6953668</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="sec12.html#cb19-1" tabindex="-1"></a><span class="co"># Density figures: </span></span>
<span id="cb19-2"><a href="sec12.html#cb19-2" tabindex="-1"></a><span class="co"># This code helps plotting densities</span></span>
<span id="cb19-3"><a href="sec12.html#cb19-3" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="fl">0.01</span>) </span>
<span id="cb19-4"><a href="sec12.html#cb19-4" tabindex="-1"></a><span class="co"># Values of lambda</span></span>
<span id="cb19-5"><a href="sec12.html#cb19-5" tabindex="-1"></a>VaguePrior <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(lambda,<span class="at">shape=</span>a0,<span class="at">scale =</span> b0)</span>
<span id="cb19-6"><a href="sec12.html#cb19-6" tabindex="-1"></a>EBPrior <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(lambda,<span class="at">shape=</span>a0EB,<span class="at">scale =</span> b0EB)</span>
<span id="cb19-7"><a href="sec12.html#cb19-7" tabindex="-1"></a>PosteriorV <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(lambda, <span class="at">shape =</span> a0 <span class="sc">+</span> <span class="fu">sum</span>(y), <span class="at">scale =</span> b0 <span class="sc">/</span> ((b0 <span class="sc">*</span> N) <span class="sc">+</span> <span class="dv">1</span>)) </span>
<span id="cb19-8"><a href="sec12.html#cb19-8" tabindex="-1"></a>PosteriorEB <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(lambda, <span class="at">shape =</span> a0EB<span class="sc">+</span><span class="fu">sum</span>(y), <span class="at">scale =</span> b0EB <span class="sc">/</span> ((b0EB <span class="sc">*</span> N) <span class="sc">+</span> <span class="dv">1</span>))         </span>
<span id="cb19-9"><a href="sec12.html#cb19-9" tabindex="-1"></a><span class="co"># Likelihood function</span></span>
<span id="cb19-10"><a href="sec12.html#cb19-10" tabindex="-1"></a>Likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, y){</span>
<span id="cb19-11"><a href="sec12.html#cb19-11" tabindex="-1"></a> LogL <span class="ot">&lt;-</span> <span class="fu">dpois</span>(y, theta, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-12"><a href="sec12.html#cb19-12" tabindex="-1"></a> Lik <span class="ot">&lt;-</span> <span class="fu">prod</span>(<span class="fu">exp</span>(LogL))</span>
<span id="cb19-13"><a href="sec12.html#cb19-13" tabindex="-1"></a> <span class="fu">return</span>(Lik)</span>
<span id="cb19-14"><a href="sec12.html#cb19-14" tabindex="-1"></a>}</span>
<span id="cb19-15"><a href="sec12.html#cb19-15" tabindex="-1"></a>Liks <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lambda, <span class="cf">function</span>(par) {<span class="fu">Likelihood</span>(par, <span class="at">y =</span> y)})</span>
<span id="cb19-16"><a href="sec12.html#cb19-16" tabindex="-1"></a>Sc <span class="ot">&lt;-</span> <span class="fu">max</span>(PosteriorEB)<span class="sc">/</span><span class="fu">max</span>(Liks) </span>
<span id="cb19-17"><a href="sec12.html#cb19-17" tabindex="-1"></a><span class="co">#Scale for displaying in figure</span></span>
<span id="cb19-18"><a href="sec12.html#cb19-18" tabindex="-1"></a>LiksScale <span class="ot">&lt;-</span> Liks <span class="sc">*</span> Sc</span>
<span id="cb19-19"><a href="sec12.html#cb19-19" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) <span class="co">#Data frame</span></span>
<span id="cb19-20"><a href="sec12.html#cb19-20" tabindex="-1"></a><span class="fu">require</span>(ggplot2) <span class="co"># Cool figures</span></span></code></pre></div>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="sec12.html#cb22-1" tabindex="-1"></a><span class="fu">require</span>(latex2exp) <span class="co"># LaTeX equations in figures</span></span></code></pre></div>
<pre><code>## Loading required package: latex2exp</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="sec12.html#cb24-1" tabindex="-1"></a><span class="fu">require</span>(ggpubr) <span class="co"># Multiple figures in one page</span></span></code></pre></div>
<pre><code>## Loading required package: ggpubr</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="sec12.html#cb26-1" tabindex="-1"></a>fig1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> data, <span class="fu">aes</span>(lambda, VaguePrior)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span>    <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">lambda$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Prior: Vague Gamma&quot;</span>) </span>
<span id="cb26-2"><a href="sec12.html#cb26-2" tabindex="-1"></a>fig2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> data, <span class="fu">aes</span>(lambda, EBPrior)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">lambda$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Prior: Empirical Bayes Gamma&quot;</span>)</span>
<span id="cb26-3"><a href="sec12.html#cb26-3" tabindex="-1"></a>fig3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> data, <span class="fu">aes</span>(lambda, PosteriorV)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">lambda$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Posterior: Vague Gamma&quot;</span>)</span>
<span id="cb26-4"><a href="sec12.html#cb26-4" tabindex="-1"></a>fig4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> data, <span class="fu">aes</span>(lambda, PosteriorEB)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">lambda$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Posterior: Empirical Bayes Gamma&quot;</span>)</span>
<span id="cb26-5"><a href="sec12.html#cb26-5" tabindex="-1"></a>FIGcap1 <span class="ot">&lt;-</span> <span class="fu">ggarrange</span>(fig1, fig2, fig3, fig4, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb26-6"><a href="sec12.html#cb26-6" tabindex="-1"></a><span class="fu">annotate_figure</span>(FIGcap1, <span class="at">top =</span> <span class="fu">text_grob</span>(<span class="st">&quot;Vague versus Empirical Bayes: Poisson-Gamma model&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="at">size =</span> <span class="dv">14</span>))</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-8-1.svg" width="672" /></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="sec12.html#cb27-1" tabindex="-1"></a>dataNew <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(<span class="fu">rep</span>(lambda, <span class="dv">3</span>), <span class="fu">c</span>(EBPrior, PosteriorEB, LiksScale), <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">each =</span> <span class="dv">1000</span>))) <span class="co">#Data frame</span></span>
<span id="cb27-2"><a href="sec12.html#cb27-2" tabindex="-1"></a><span class="fu">colnames</span>(dataNew) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Lambda&quot;</span>, <span class="st">&quot;Density&quot;</span>, <span class="st">&quot;Factor&quot;</span>)</span>
<span id="cb27-3"><a href="sec12.html#cb27-3" tabindex="-1"></a>dataNew<span class="sc">$</span>Factor <span class="ot">&lt;-</span> <span class="fu">factor</span>(dataNew<span class="sc">$</span>Factor, <span class="at">levels=</span><span class="fu">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;3&quot;</span>, <span class="st">&quot;2&quot;</span>), </span>
<span id="cb27-4"><a href="sec12.html#cb27-4" tabindex="-1"></a><span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;Prior&quot;</span>, <span class="st">&quot;Likelihood&quot;</span>, <span class="st">&quot;Posterior&quot;</span>))</span>
<span id="cb27-5"><a href="sec12.html#cb27-5" tabindex="-1"></a><span class="co"># ggplot(data = dataNew, aes_string(x = &quot;Lambda&quot;, y = &quot;Density&quot;, group = &quot;Factor&quot;)) + geom_line(aes(color = Factor)) + xlab(TeX(&quot;$\\lambda$&quot;)) + ylab(&quot;Density&quot;) + ggtitle(&quot;Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model&quot;) + guides(color=guide_legend(title=&quot;Information&quot;)) + scale_color_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;))</span></span>
<span id="cb27-6"><a href="sec12.html#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a href="sec12.html#cb27-7" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> dataNew, <span class="fu">aes</span>(<span class="at">x =</span> Lambda, <span class="at">y =</span> Density, <span class="at">group =</span> Factor, <span class="at">color =</span> Factor)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb27-8"><a href="sec12.html#cb27-8" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">lambda$&quot;</span>)) <span class="sc">+</span></span>
<span id="cb27-9"><a href="sec12.html#cb27-9" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb27-10"><a href="sec12.html#cb27-10" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model&quot;</span>) <span class="sc">+</span></span>
<span id="cb27-11"><a href="sec12.html#cb27-11" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">title =</span> <span class="st">&quot;Information&quot;</span>)) <span class="sc">+</span></span>
<span id="cb27-12"><a href="sec12.html#cb27-12" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;yellow&quot;</span>, <span class="st">&quot;blue&quot;</span>))</span>
<span id="cb27-13"><a href="sec12.html#cb27-13" tabindex="-1"></a>p1</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-8-2.svg" width="672" /></p>
<p>The first figure displays the prior and posterior densities based on vague and Empirical Bayes hyperparameters. We observe that the prior and posterior densities using the latter are more informative, as expected.</p>
<p>The second figure shows the prior, scaled likelihood, and posterior densities of <span class="math inline">\(\lambda\)</span> based on the hyperparameters from the Empirical Bayes approach. The posterior density is a compromise between prior and sample information.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="sec12.html#cb28-1" tabindex="-1"></a><span class="co"># Predictive distributions</span></span>
<span id="cb28-2"><a href="sec12.html#cb28-2" tabindex="-1"></a>PredDen <span class="ot">&lt;-</span> <span class="cf">function</span>(y, y0, a0, b0){</span>
<span id="cb28-3"><a href="sec12.html#cb28-3" tabindex="-1"></a>    N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb28-4"><a href="sec12.html#cb28-4" tabindex="-1"></a>    <span class="co">#sample size</span></span>
<span id="cb28-5"><a href="sec12.html#cb28-5" tabindex="-1"></a>    an <span class="ot">&lt;-</span> a0 <span class="sc">+</span> <span class="fu">sum</span>(y) </span>
<span id="cb28-6"><a href="sec12.html#cb28-6" tabindex="-1"></a>    <span class="co"># Posterior shape parameter</span></span>
<span id="cb28-7"><a href="sec12.html#cb28-7" tabindex="-1"></a>    bn <span class="ot">&lt;-</span> b0 <span class="sc">/</span> ((b0 <span class="sc">*</span> N) <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb28-8"><a href="sec12.html#cb28-8" tabindex="-1"></a>    <span class="co"># Posterior scale parameter</span></span>
<span id="cb28-9"><a href="sec12.html#cb28-9" tabindex="-1"></a>    p <span class="ot">&lt;-</span> bn <span class="sc">/</span> (bn <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb28-10"><a href="sec12.html#cb28-10" tabindex="-1"></a>    <span class="co"># Probability negative binomial density</span></span>
<span id="cb28-11"><a href="sec12.html#cb28-11" tabindex="-1"></a>    Pr <span class="ot">&lt;-</span> <span class="fu">dnbinom</span>(y0, <span class="at">size=</span>an, <span class="at">prob=</span>(<span class="dv">1</span> <span class="sc">-</span> p))</span>
<span id="cb28-12"><a href="sec12.html#cb28-12" tabindex="-1"></a>    <span class="co"># Predictive density</span></span>
<span id="cb28-13"><a href="sec12.html#cb28-13" tabindex="-1"></a>    <span class="co"># Observe that in R there is a slightly different parametrization.</span></span>
<span id="cb28-14"><a href="sec12.html#cb28-14" tabindex="-1"></a>    <span class="fu">return</span>(Pr)</span>
<span id="cb28-15"><a href="sec12.html#cb28-15" tabindex="-1"></a>}</span>
<span id="cb28-16"><a href="sec12.html#cb28-16" tabindex="-1"></a>y0 <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb28-17"><a href="sec12.html#cb28-17" tabindex="-1"></a>PredVague <span class="ot">&lt;-</span> <span class="fu">PredDen</span>(<span class="at">y=</span>y, <span class="at">y0=</span>y0, <span class="at">a0=</span>a0, <span class="at">b0=</span>b0)</span>
<span id="cb28-18"><a href="sec12.html#cb28-18" tabindex="-1"></a>PredEB <span class="ot">&lt;-</span> <span class="fu">PredDen</span>(<span class="at">y=</span>y, <span class="at">y0=</span>y0, <span class="at">a0=</span>a0EB, <span class="at">b0=</span>b0EB)</span>
<span id="cb28-19"><a href="sec12.html#cb28-19" tabindex="-1"></a>dataPred <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(y0, PredVague, PredEB))</span>
<span id="cb28-20"><a href="sec12.html#cb28-20" tabindex="-1"></a><span class="fu">colnames</span>(dataPred) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;y0&quot;</span>, <span class="st">&quot;PredictiveVague&quot;</span>, <span class="st">&quot;PredictiveEB&quot;</span>)</span>
<span id="cb28-21"><a href="sec12.html#cb28-21" tabindex="-1"></a>FIGchappred <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> dataPred) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(y0, PredictiveVague, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)) <span class="sc">+</span> </span>
<span id="cb28-22"><a href="sec12.html#cb28-22" tabindex="-1"></a><span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$y_0$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Predictive density: Vague and Empirical Bayes priors&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(y0, PredictiveEB, <span class="at">color =</span> <span class="st">&quot;yellow&quot;</span>)) <span class="sc">+</span></span>
<span id="cb28-23"><a href="sec12.html#cb28-23" tabindex="-1"></a><span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">title=</span><span class="st">&quot;Prior&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Vague&quot;</span>, <span class="st">&quot;Empirical Bayes&quot;</span>), <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;yellow&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="dv">1</span>))</span>
<span id="cb28-24"><a href="sec12.html#cb28-24" tabindex="-1"></a>FIGchappred</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-1.svg" width="672" /></p>
<p>This figure displays the predictive probability mass of not having any visits to a physician next year, as well as having one, two, and so on, using Empirical Bayes and vague hyperparameters. The predictive probabilities of not having any visits are approximately 30% and 33% based on the Empirical Bayes and vague hyperparameters, respectively.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="sec12.html#cb29-1" tabindex="-1"></a><span class="co"># Posterior odds: Vague vs Empirical Bayes</span></span>
<span id="cb29-2"><a href="sec12.html#cb29-2" tabindex="-1"></a>PO12 <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">LogMgLik</span>(<span class="fu">c</span>(a0EB, b0EB), <span class="at">y =</span> y))<span class="sc">/</span><span class="fu">exp</span>(<span class="sc">-</span><span class="fu">LogMgLik</span>(<span class="fu">c</span>(a0, b0), <span class="at">y =</span> y))</span>
<span id="cb29-3"><a href="sec12.html#cb29-3" tabindex="-1"></a>PO12</span></code></pre></div>
<pre><code>## [1] 919.0069</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="sec12.html#cb31-1" tabindex="-1"></a>PostProMEM <span class="ot">&lt;-</span> PO12<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> PO12) </span>
<span id="cb31-2"><a href="sec12.html#cb31-2" tabindex="-1"></a>PostProMEM</span></code></pre></div>
<pre><code>## [1] 0.9989131</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="sec12.html#cb33-1" tabindex="-1"></a><span class="co"># Posterior model probability Empirical Bayes</span></span>
<span id="cb33-2"><a href="sec12.html#cb33-2" tabindex="-1"></a>PostProbMV <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> PostProMEM </span>
<span id="cb33-3"><a href="sec12.html#cb33-3" tabindex="-1"></a>PostProbMV</span></code></pre></div>
<pre><code>## [1] 0.001086948</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="sec12.html#cb35-1" tabindex="-1"></a><span class="co"># Posterior model probability vague prior</span></span>
<span id="cb35-2"><a href="sec12.html#cb35-2" tabindex="-1"></a><span class="co"># Bayesian model average (BMA)</span></span>
<span id="cb35-3"><a href="sec12.html#cb35-3" tabindex="-1"></a>PostMeanEB <span class="ot">&lt;-</span> (a0EB <span class="sc">+</span> <span class="fu">sum</span>(y)) <span class="sc">*</span> (b0EB <span class="sc">/</span> (b0EB <span class="sc">*</span> N <span class="sc">+</span> <span class="dv">1</span>)) </span>
<span id="cb35-4"><a href="sec12.html#cb35-4" tabindex="-1"></a><span class="co"># Posterior mean Empirical Bayes </span></span>
<span id="cb35-5"><a href="sec12.html#cb35-5" tabindex="-1"></a>PostMeanV <span class="ot">&lt;-</span> (a0 <span class="sc">+</span> <span class="fu">sum</span>(y)) <span class="sc">*</span> (b0 <span class="sc">/</span> (b0 <span class="sc">*</span> N <span class="sc">+</span> <span class="dv">1</span>)) </span>
<span id="cb35-6"><a href="sec12.html#cb35-6" tabindex="-1"></a><span class="co"># Posterior mean vague priors</span></span>
<span id="cb35-7"><a href="sec12.html#cb35-7" tabindex="-1"></a>BMAmean <span class="ot">&lt;-</span> PostProMEM <span class="sc">*</span> PostMeanEB <span class="sc">+</span> PostProbMV <span class="sc">*</span> PostMeanV  </span>
<span id="cb35-8"><a href="sec12.html#cb35-8" tabindex="-1"></a>BMAmean</span></code></pre></div>
<pre><code>## [1] 1.200951</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="sec12.html#cb37-1" tabindex="-1"></a><span class="co"># BMA posterior mean</span></span>
<span id="cb37-2"><a href="sec12.html#cb37-2" tabindex="-1"></a>PostVarEB <span class="ot">&lt;-</span> (a0EB <span class="sc">+</span> <span class="fu">sum</span>(y)) <span class="sc">*</span> (b0EB<span class="sc">/</span>(b0EB <span class="sc">*</span> N <span class="sc">+</span> <span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb37-3"><a href="sec12.html#cb37-3" tabindex="-1"></a><span class="co"># Posterior variance Empirical Bayes</span></span>
<span id="cb37-4"><a href="sec12.html#cb37-4" tabindex="-1"></a>PostVarV <span class="ot">&lt;-</span> (a0 <span class="sc">+</span> <span class="fu">sum</span>(y)) <span class="sc">*</span> (b0 <span class="sc">/</span> (b0 <span class="sc">*</span> N <span class="sc">+</span> <span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb37-5"><a href="sec12.html#cb37-5" tabindex="-1"></a><span class="co"># Posterior variance vague prior </span></span>
<span id="cb37-6"><a href="sec12.html#cb37-6" tabindex="-1"></a>BMAVar <span class="ot">&lt;-</span> PostProMEM <span class="sc">*</span> PostVarEB <span class="sc">+</span> PostProbMV<span class="sc">*</span>PostVarV <span class="sc">+</span> PostProMEM <span class="sc">*</span> (PostMeanEB <span class="sc">-</span> BMAmean)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> PostProbMV <span class="sc">*</span> (PostMeanV <span class="sc">-</span> BMAmean)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb37-7"><a href="sec12.html#cb37-7" tabindex="-1"></a><span class="co"># BMA posterior variance   </span></span>
<span id="cb37-8"><a href="sec12.html#cb37-8" tabindex="-1"></a>BMAVar</span></code></pre></div>
<pre><code>## [1] 0.02518372</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="sec12.html#cb39-1" tabindex="-1"></a><span class="co"># BMA: Predictive</span></span>
<span id="cb39-2"><a href="sec12.html#cb39-2" tabindex="-1"></a>BMAPred <span class="ot">&lt;-</span> PostProMEM <span class="sc">*</span> PredEB<span class="sc">+</span>PostProbMV <span class="sc">*</span> PredVague</span>
<span id="cb39-3"><a href="sec12.html#cb39-3" tabindex="-1"></a>dataPredBMA <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(y0, BMAPred))</span>
<span id="cb39-4"><a href="sec12.html#cb39-4" tabindex="-1"></a><span class="fu">colnames</span>(dataPredBMA) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;y0&quot;</span>, <span class="st">&quot;PredictiveBMA&quot;</span>)</span>
<span id="cb39-5"><a href="sec12.html#cb39-5" tabindex="-1"></a>FIGchappred1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> dataPredBMA) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(y0, PredictiveBMA, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)) <span class="sc">+</span>  <span class="fu">xlab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$y_0$&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Predictive density: BMA&quot;</span>) <span class="sc">+</span> <span class="fu">guides</span>(<span class="at">color =</span> <span class="fu">guide_legend</span>(<span class="at">title=</span><span class="st">&quot;BMA&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Probability&quot;</span>), <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="dv">1</span>))</span>
<span id="cb39-6"><a href="sec12.html#cb39-6" tabindex="-1"></a>FIGchappred1</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-1.svg" width="672" /></p>
<p>The first figure displays the predictive density using Bayesian model averaging based on the vague and Empirical Bayes hyperparameters. This figure closely resembles the predictive probability mass function based on the Empirical Bayes framework, as the posterior model probability for that setting is nearly one.</p>
<p>The second figure shows how the posterior distribution updates with new sample information, starting from an initial non-informative prior (iteration 1). We observe that iteration 5 incorporates all the sample information in our example. As a result, the posterior density in iteration 5 is identical to the posterior density.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Bayarri2000" class="csl-entry">
Bayarri, M., and J. Berger. 2000. <span>“P–Values for Composite Null Models.”</span> <em>Journal of American Statistical Association</em> 95: 1127–42.
</div>
<div id="ref-bickel1969some" class="csl-entry">
Bickel, Peter J, and Joseph A Yahav. 1969. <span>“Some Contributions to the Asymptotic Theory of Bayes Solutions.”</span> <em>Zeitschrift f<span>ü</span>r Wahrscheinlichkeitstheorie Und Verwandte Gebiete</em> 11 (4): 257–76.
</div>
<div id="ref-casella2024statistical" class="csl-entry">
Casella, George, and Roger Berger. 2024. <em>Statistical Inference</em>. CRC Press.
</div>
<div id="ref-Gelman1996" class="csl-entry">
Gelman, A., and X. Meng. 1996. <span>“Model Checking and Model Improvement.”</span> In <em>In Markov Chain Monte Carlo in Practice</em>, edited by Gilks, Richardson, and Speigelhalter. Springer US.
</div>
<div id="ref-gelman1996posterior" class="csl-entry">
Gelman, A., X. Meng, and H. Stern. 1996. <span>“Posterior Predictive Assessment of Model Fitness via Realized Discrepancies.”</span> <em>Statistica Sinica</em>, 733–60.
</div>
<div id="ref-gelman2006prior" class="csl-entry">
Gelman, Andrew et al. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).”</span> <em>Bayesian Analysis</em> 1 (3): 515–34.
</div>
<div id="ref-Kass1995" class="csl-entry">
Kass, R E, and A E Raftery. 1995. <span>“<span class="nocase">Bayes factors</span>.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 773–95.
</div>
<div id="ref-Lehmann2003" class="csl-entry">
Lehmann, E. L., and George Casella. 2003. <em>Theory of Point Estimation</em>. Second Edition. Springer.
</div>
<div id="ref-petris2009dynamic" class="csl-entry">
Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. <span>“Dynamic Linear Models.”</span> In <em>Dynamic Linear Models with r</em>, 31–84. Springer.
</div>
<div id="ref-van2000asymptotic" class="csl-entry">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>From a Bayesian perspective, <span class="math inline">\(\boldsymbol{\theta}\)</span> is fixed but unknown. Then, it is treated as a random object despite the lack of variability (see Chapter <a href="Chap2.html#Chap2">2</a>).<a href="sec12.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><span class="citation">M. Bayarri and Berger (<a href="#ref-Bayarri2000">2000</a>)</span> show potential issues due to using data twice in the construction of the predictive p-values. They also present alternative proposals, for instance, <em>the partial posterior predictive p-value</em>.<a href="sec12.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Note that in the likelihood function the argument is <span class="math inline">\(\theta\)</span>, but we keep the notation for convenience in exposition.<a href="sec12.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The last definition follows from standard theory in maximum likelihood estimation (see <span class="citation">George Casella and Berger (<a href="#ref-casella2024statistical">2024</a>)</span> and <span class="citation">Jeffrey M. Wooldridge (<a href="#ref-wooldridge2010econometric">2010</a>)</span>).<a href="sec12.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Independent and identically distributed draws.<a href="sec12.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>We should be aware that there may be technical problems using this kind of hyperparameters in this setting <span class="citation">(<a href="#ref-gelman2006prior">Andrew Gelman et al. 2006</a>)</span><a href="sec12.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Empirical Bayes methods are criticized due to double-using the data. First to set the hyperparameters, and second, to perform Bayesian inference.<a href="sec12.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec11.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec14.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
