<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.1 Foundation | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="10.1 Foundation | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.1 Foundation | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Chap10.html"/>
<link rel="next" href="sec102.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Dirichlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Non-parametric generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec10_1" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Foundation<a href="sec10_1.html#sec10_1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Remember from Chapter <a href="Chap1.html#Chap1">1</a> that Bayesian model averaging (BMA) is an approach which takes into account model uncertainty. In particular, we consider uncertainty in the regressors (variable selection) in a regression framework where there are <span class="math inline">\(K\)</span> possible explanatory variables.<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a> This implies <span class="math inline">\(2^K\)</span> potential models indexed by parameters <span class="math inline">\(\boldsymbol{\theta}_m\)</span>, <span class="math inline">\(m=1,2,\dots,2^K\)</span>.</p>
<p>Following <span class="citation">Simmons et al. (<a href="#ref-Simmons2010">2010</a>)</span>, the posterior model probability is
<span class="math display">\[\begin{equation*}
    \pi(\mathcal{M}_j |\boldsymbol{y})=\frac{p(\boldsymbol{y} | \mathcal{M}_j)\pi(\mathcal{M}_j)}{\sum_{m=1}^{2^K}p(\boldsymbol{y} | \mathcal{M}_m)\pi(\mathcal{M}_m)},
\end{equation*}\]</span>
where <span class="math inline">\(\pi(\mathcal{M}_j)\)</span> is the prior model probability,<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>
<span class="math display">\[\begin{equation*}
    p(\boldsymbol{y} | \mathcal{M}_j)=\int_{\boldsymbol{\Theta}_j} p(\boldsymbol{y}| \boldsymbol{\theta}_j,\mathcal{M}_j)\pi(\boldsymbol{\theta}_j | \mathcal{M}_j) d\boldsymbol{\theta}_{j}
\end{equation*}\]</span>
is the marginal likelihood, and <span class="math inline">\(\pi(\boldsymbol{\theta}_j | \mathcal{M}_j)\)</span> is the prior distribution of <span class="math inline">\(\boldsymbol{\theta}_j\)</span> conditional on model <span class="math inline">\(\mathcal{M}_j\)</span>.</p>
<p>Following <span class="citation">Adrian E. Raftery (<a href="#ref-Raftery93">1993</a>)</span>, the posterior distribution of <span class="math inline">\(\boldsymbol{\theta}\)</span> is
<span class="math display">\[\begin{equation*}
    \pi(\boldsymbol{\theta}|\boldsymbol{y})= \sum_{m=1}^{2^K}\pi(\boldsymbol{\theta}_m|\boldsymbol{y},\mathcal{M}_m) \pi(\mathcal{M}_m|\boldsymbol{y})
\end{equation*}\]</span>
The posterior distribution of the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> under model <span class="math inline">\(\mathcal{M}_m\)</span> is denoted as <span class="math inline">\(\pi(\boldsymbol{\theta}_m|\boldsymbol{y}, \mathcal{M}_m)\)</span>. The posterior mean of <span class="math inline">\(\boldsymbol{\theta}\)</span> is given by:
<span class="math display">\[
\mathbb{E}[\boldsymbol{\theta}|\boldsymbol{y}] = \sum_{m=1}^{2^K} \hat{\boldsymbol{\theta}}_m \, \pi(\mathcal{M}_m|\boldsymbol{y}),
\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\theta}}_m\)</span> represents the posterior mean under model <span class="math inline">\(\mathcal{M}_m\)</span>.</p>
<p>The variance of the <span class="math inline">\(k\)</span>-th element of <span class="math inline">\(\boldsymbol{\theta}\)</span> given the data <span class="math inline">\(\boldsymbol{y}\)</span> is:
<span class="math display">\[
\text{Var}(\theta_{km}|\boldsymbol{y}) = \sum_{m=1}^{2^K} \pi(\mathcal{M}_m|\boldsymbol{y}) \, \widehat{\text{Var}}(\theta_{km}|\boldsymbol{y}, \mathcal{M}_m) + \sum_{m=1}^{2^K} \pi(\mathcal{M}_m|\boldsymbol{y}) \left( \hat{\theta}_{km} - \mathbb{E}[\theta_{km}|\boldsymbol{y}] \right)^2,
\]</span></p>
<p>where <span class="math inline">\(\widehat{\text{Var}}(\theta_{km}|\boldsymbol{y}, \mathcal{M}_m)\)</span> denotes the posterior variance of the <span class="math inline">\(k\)</span>-th element of <span class="math inline">\(\boldsymbol{\theta}\)</span> under model <span class="math inline">\(\mathcal{M}_m\)</span>.</p>
<p>The posterior variance highlights how BMA accounts for model uncertainty. The first term represents the weighted variance of each model, averaged across all potential models, while the second term reflects the stability of the estimates across models. The greater the variation in estimates between models, the higher the posterior variance.</p>
<p>The posterior predictive distribution is
<span class="math display">\[\begin{equation*}
    \pi(\boldsymbol{y}_0|\boldsymbol{y})= \sum_{m=1}^{2^K}p_m(\boldsymbol{y}_0|\boldsymbol{y},\mathcal{M}_m) \pi(M_m|\boldsymbol{y})
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(p_m(\boldsymbol{y}_0|\boldsymbol{y},\mathcal{M}_m)=\int_{\boldsymbol{\Theta}_m} p(\boldsymbol{y}_0|\boldsymbol{y},\boldsymbol{\theta}_m,\mathcal{M}_m)\pi(\boldsymbol{\theta}_m |\boldsymbol{y}, \mathcal{M}_m) d\boldsymbol{\theta}_{m}\)</span> is the posterior predictive distribution under model <span class="math inline">\(\mathcal{M}_m\)</span>.</p>
<p>Another important statistic in BMA is the posterior inclusion probability associated with variable <span class="math inline">\(\boldsymbol{x}_k\)</span>, <span class="math inline">\(k=1,2,\dots,K\)</span>, which is</p>
<p><span class="math display">\[\begin{equation*}
    PIP(\boldsymbol{x}_k)=\sum_{m=1}^{2^K}\pi(\mathcal{M}_m|\boldsymbol{y})\times \mathbb{1}_{k,m},
\end{equation*}\]</span>
where
<span class="math inline">\(\mathbb{1}_{k,m}= \left\{ \begin{array}{lcc}  1&amp; if &amp; \boldsymbol{x}_{k}\in \mathcal{M}_m \\  \\ 0 &amp; if &amp; \boldsymbol{x}_{k}\not \in \mathcal{M}_m \end{array} \right\}.\)</span></p>
<p><span class="citation">R. E. Kass and Raftery (<a href="#ref-Kass1995">1995</a>)</span> suggest that posterior inclusion probabilities (PIP) less than 0.5 are evidence against the regressor, <span class="math inline">\(0.5\leq PIP&lt;0.75\)</span> is weak evidence, <span class="math inline">\(0.75\leq PIP&lt;0.95\)</span> is positive evidence, <span class="math inline">\(0.95\leq PIP&lt;0.99\)</span> is strong evidence, and <span class="math inline">\(PIP\geq 0.99\)</span> is very strong evidence.</p>
<p>There are two main computational issues in implementing BMA based on variable selection. First, the number of models in the model space is <span class="math inline">\(2^K\)</span>, which sometimes can be enormous. For instance, three regressors imply just eight models, see the next Table, but 40 regressors implies approximately 1.1e+12 models. Take into account that models always include the intercept, and all regressors should be standardized to avoid scale issues.<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> The second computational issue is calculating the marginal likelihood <span class="math inline">\(p(\boldsymbol{y} | \mathcal{M}_j)=\int_{\boldsymbol{\Theta}_j} p(\boldsymbol{y}| \boldsymbol{\theta}_j,\mathcal{M}_j)\pi(\boldsymbol{\theta}_j | \mathcal{M}_j) d\boldsymbol{\theta}_{j}\)</span>, which most of the time does not have an analytic solution.</p>
<table>
<caption><span id="tab:spacemodels">Table 10.1: </span>Space of models</caption>
<colgroup>
<col width="12%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left"><span class="math inline">\(M_{1}\)</span></th>
<th align="left"><span class="math inline">\(M_{2}\)</span></th>
<th align="left"><span class="math inline">\(M_{3}\)</span></th>
<th align="left"><span class="math inline">\(M_{4}\)</span></th>
<th align="left"><span class="math inline">\(M_{5}\)</span></th>
<th align="left"><span class="math inline">\(M_{6}\)</span></th>
<th align="left"><span class="math inline">\(M_{7}\)</span></th>
<th align="left"><span class="math inline">\(M_{8}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(x_1\)</span></td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(x_2\)</span></td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(x_3\)</span></td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>The first computational issue is basically a problem of ranking models. This can be tackled using different approaches, such as Occam’s window criterion <span class="citation">(<a href="#ref-Madigan1994">D. Madigan and Raftery 1994</a>; <a href="#ref-Raftery1997">Adrian E. Raftery, Madigan, and Hoeting 1997</a>)</span>, reversible jump Markov chain Monte Carlo computation <span class="citation">(<a href="#ref-Green1995">Green 1995</a>)</span>, Markov chain Monte Carlo model composition <span class="citation">(<a href="#ref-madigan95">D. Madigan, York, and Allard 1995</a>)</span>, and multiple testing using intrinsic priors <span class="citation">(<a href="#ref-Casella2006">G. Casella and Moreno 2006</a>)</span> or nonlocal prior densities <span class="citation">(<a href="#ref-Johnson2012">Jhonson and Rossell 2012</a>)</span>. We focus on Occam’s window and Markov chain Monte Carlo model composition in our GUI.<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a></p>
<p>In Occam’s window, a model is discarded if its predictive performance is much worse than that of the best model <span class="citation">(<a href="#ref-Madigan1994">D. Madigan and Raftery 1994</a>; <a href="#ref-Raftery1997">Adrian E. Raftery, Madigan, and Hoeting 1997</a>)</span>.
Thus, models not belonging to <span class="math inline">\(\mathcal{M}&#39;=\left\{\mathcal{M}_j:\frac{\max_m {\pi(\mathcal{M}_m|\boldsymbol{y})}}{\pi(\mathcal{M}_j|\boldsymbol{y})}\leq c\right\}\)</span> should be discarded, where <span class="math inline">\(c\)</span> is chosen by the user (<span class="citation">D. Madigan and Raftery (<a href="#ref-Madigan1994">1994</a>)</span> propose <span class="math inline">\(c=20\)</span>).
In addition, complicated models than are less supported by the data than simpler models are also discarded, that is, <span class="math inline">\(\mathcal{M}&#39;&#39;=\left\{\mathcal{M}_j:\exists \mathcal{M}_m\in\mathcal{M}&#39;,\mathcal{M}_m\subset \mathcal{M}_j,\frac{\pi(\mathcal{M}_m|\boldsymbol{y})}{\pi(\mathcal{M}_j|\boldsymbol{y})}&gt;1\right\}\)</span>. Then, the set of models used in BMA is <span class="math inline">\(\mathcal{M}^*=\mathcal{M}&#39;\cap \mathcal{M}&#39;&#39;^c\in\mathcal{M}\)</span>. <span class="citation">Adrian E. Raftery, Madigan, and Hoeting (<a href="#ref-Raftery1997">1997</a>)</span> find that the number of models in <span class="math inline">\(\mathcal{M}^*\)</span> is normally less than 25.</p>
<p>However, the previous theoretical framework requires finding the model with the maximum a posteriori model probability (<span class="math inline">\(\max_m {\pi(\mathcal{M}_m|\boldsymbol{y})}\)</span>), which implies calculating all possible models in <span class="math inline">\(\mathcal{M}\)</span>. This is computationally burdensome. Hence, a heuristic approach is proposed by <span class="citation">Adrian Raftery et al. (<a href="#ref-Raftery2012">2012</a>)</span> based on ideas of <span class="citation">D. Madigan and Raftery (<a href="#ref-Madigan1994">1994</a>)</span>. The search strategy is based on a series of nested comparisons of ratios of posterior model probabilities. Let <span class="math inline">\(\mathcal{M}_0\)</span> be a model with one regressor less than model <span class="math inline">\(\mathcal{M}_1\)</span>, then:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(\log(\pi(\mathcal{M}_0|\boldsymbol{y})/\pi(\mathcal{M}_1|\boldsymbol{y}))&gt;\log(O_R)\)</span>, then <span class="math inline">\(\mathcal{M}_1\)</span> is rejected and <span class="math inline">\(\mathcal{M}_0\)</span> is considered.</p></li>
<li><p>If <span class="math inline">\(\log(\pi(\mathcal{M}_0|\boldsymbol{y})/\pi(\mathcal{M}_1|\boldsymbol{y}))\leq -\log(O_L)\)</span>, then <span class="math inline">\(\mathcal{M}_0\)</span> is rejected, and <span class="math inline">\(\mathcal{M}_1\)</span> is considered.</p></li>
<li><p>If <span class="math inline">\(\log(O_L)&lt;\log(\pi(\mathcal{M}_0|\boldsymbol{y})/\pi(\mathcal{M}_1|\boldsymbol{y}))\leq \log(O_R\)</span>), <span class="math inline">\(\mathcal{M}_0\)</span> and <span class="math inline">\(\mathcal{M}_1\)</span> are considered.</p></li>
</ol>
<p>Here <span class="math inline">\(O_R\)</span> is a number specifying the maximum ratio for excluding models in Occam’s window, and <span class="math inline">\(O_L=1/O_R^{2}\)</span> is defined by default in <span class="citation">Adrian Raftery et al. (<a href="#ref-Raftery2012">2012</a>)</span>. The search strategy can be “up’‘, adding one regressor, or “down’’, dropping one regressor (see <span class="citation">D. Madigan and Raftery (<a href="#ref-Madigan1994">1994</a>)</span> for details about the down and up algorithms). The leaps and bounds algorithm <span class="citation">(<a href="#ref-Furnival1974">Furnival and Wilson 1974</a>)</span> is implemented to improve the computational efficiency of this search strategy <span class="citation">(<a href="#ref-Raftery2012">Adrian Raftery et al. 2012</a>)</span>. Once the set of potentially acceptable models is defined, we discard all the models that are not in <span class="math inline">\(\mathcal{M}&#39;\)</span>, and the models that are in <span class="math inline">\(\mathcal{M}&#39;&#39;\)</span> where 1 is replaced by <span class="math inline">\(\exp\left\{O_R\right\}\)</span> due to the leaps and bounds algorithm giving an approximation to BIC, so as to ensure that no good models are discarded.</p>
<p>The second approach that we consider in our GUI to tackle the model space size issue is Markov chain Monte Carlo model composition (MC3) <span class="citation">(<a href="#ref-madigan1995bayesian1">David Madigan, York, and Allard 1995</a>)</span>.
In particular, given the space of models <span class="math inline">\(\mathcal{M}_m\)</span>, we simulate a chain of <span class="math inline">\(\mathcal{M}_s\)</span> models, <span class="math inline">\(s = 1, 2, ..., S&lt;&lt;2^K\)</span>, where the algorithm randomly extracts a candidate model <span class="math inline">\(\mathcal{M}_c\)</span> from a neighborhood of models (<span class="math inline">\(nbd(\mathcal{M}_m)\)</span>) that consists of the actual model itself and the set of models with either one variable more or one variable less <span class="citation">(<a href="#ref-Raftery1997">Adrian E. Raftery, Madigan, and Hoeting 1997</a>)</span>. Therefore, there is a transition kernel in the space of models <span class="math inline">\(q(\mathcal{M}_m\rightarrow \mathcal{M}_c)\)</span>, such that <span class="math inline">\(q(\mathcal{M}_m\rightarrow \mathcal{M}_{c})=0 \ \forall \mathcal{M}_{c}\notin nbd(\mathcal{M}_m)\)</span> and <span class="math inline">\(q(\mathcal{M}_m\rightarrow \mathcal{M}_{c})=\frac{1}{|nbd(\mathcal{M}_m)|} \ \forall \mathcal{M}_m\in nbd(\mathcal{M}_m)\)</span>, <span class="math inline">\(|nbd(\mathcal{M}_m)|\)</span> being the number of neighbors of <span class="math inline">\(\mathcal{M}_m\)</span>. This candidate model is accepted with probability</p>
<p><span class="math display">\[\begin{equation*}
    \alpha (\mathcal{M}_{s-1},\mathcal{M}_{c})=\min \bigg \{ \frac{|nbd(\mathcal{M}_m)|p(\boldsymbol{y} | \mathcal{M}_c)\pi(\mathcal{M}_c)}{|nbd(\mathcal{M}^{c})|p(\boldsymbol{y}| \mathcal{M}_{(s-1)})\pi(\mathcal{M}_{(s-1)})},1 \bigg \}.
\end{equation*}\]</span></p>
<p>Observe that by construction <span class="math inline">\(|nbd(\mathcal{M}_m)|=|nbd(\mathcal{M}_c)|=k\)</span>, except in extreme cases where a model has only one regressor or has all regressors.</p>
<p>The Bayesian information criterion is a possible solution for the second computational issue in BMA, that is, calculating the marginal likelihood when there is no an analytic solution. Defining <span class="math inline">\(h(\boldsymbol{\theta}|\mathcal{M}_j)=-\frac{\log(p(\boldsymbol{y}| \boldsymbol{\theta}_j,\mathcal{M}_j)\pi(\boldsymbol{\theta}_j | \mathcal{M}_j))}{N}\)</span>, then <span class="math inline">\(p(\boldsymbol{y} | \mathcal{M}_j)=\int_{\boldsymbol{\Theta}_j} \exp\left\{-N h(\boldsymbol{\theta}|\mathcal{M}_j)\right\} d\boldsymbol{\theta}_{j}\)</span>. If <span class="math inline">\(N\)</span> is sufficiently large (technically <span class="math inline">\(N\to \infty\)</span>), we can make the following assumptions <span class="citation">(<a href="#ref-Hoeting1999">Hoeting et al. 1999</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>We can use the Laplace method for approximating integrals <span class="citation">(<a href="#ref-Tierney1986">Tierney and Kadane 1986</a>)</span>.</li>
<li>The posterior mode is reached at the same point as the maximum likelihood estimator (MLE), denoted by <span class="math inline">\(\hat{\boldsymbol{\theta}}_{MLE}\)</span>.</li>
</ol>
<p>We get the following results under these assumptions:
<span class="math display">\[\begin{align*}
    p(\boldsymbol{y} | \mathcal{M}_j)\approx&amp;\left( \frac{2\pi}{N}\right)^{K_j/2}|\boldsymbol{\Sigma}|^{-1/2} \exp\left\{-N h(\boldsymbol{\hat{\theta}}_j^{MLE}|\mathcal{M}_j)\right\}, \ N\rightarrow\infty,
\end{align*}\]</span>
where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the inverse of the Hessian matrix of <span class="math inline">\(h(\boldsymbol{\hat{\theta}}_j^{MLE}|\mathcal{M}_j)\)</span>, and <span class="math inline">\(K_j=dim\left\{\boldsymbol{\theta}_j\right\}\)</span>.</p>
<p>This implies
<span class="math display">\[\begin{align*}
    \log\left(p(\boldsymbol{y} | \mathcal{M}_j)\right)\approx&amp; \frac{K_j}{2}\log(2\pi)- \frac{K_j}{2}\log(N) -\frac{1}{2}\log(|\boldsymbol{\Sigma}|) + \log(p(\boldsymbol{y}| \boldsymbol{\hat{\theta}}_j^{MLE},\mathcal{M}_j))+\log(\pi(\boldsymbol{\hat{\theta}}_j^{MLE} | \mathcal{M}_j)), \ N\rightarrow\infty.
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(\frac{K_j}{2}\log(2\pi)\)</span> and <span class="math inline">\(\log(\pi(\boldsymbol{\hat{\theta}}_j^{MLE} | \mathcal{M}_j))\)</span> are constants as functions of <span class="math inline">\(\boldsymbol{y}\)</span>, and <span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> is bounded by a finite constant, we have
<span class="math display">\[\begin{align*}
    log\left(p(\boldsymbol{y} | \mathcal{M}_j)\right)\approx&amp; -\frac{K_j}{2}\log(N)+\log(p(\boldsymbol{y}| \boldsymbol{\hat{\theta}}_j^{MLE},\mathcal{M}_j))= -\frac{BIC}{2}, \ N \rightarrow \infty.
\end{align*}\]</span></p>
<p>The marginal likelihood thus asymptotically converges to a linear transformation of the Bayesian Information Criterion (BIC), significantly simplifying its calculation. In addition, the BIC is consistent, that is, the probability of uncovering the population statistical model converges to one as the sample size converges to infinity given a <span class="math inline">\(\mathcal{M}\)</span>-closed view <span class="citation">(<a href="#ref-Bernardo1994">J. Bernardo and Smith 1994</a>)</span>, that is, one of the models in consideration is the population statistical model (data generating process) <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz 1978</a>; <a href="#ref-burnham2004multimodel">Burnham and Anderson 2004</a>)</span>. In case that there is an <span class="math inline">\(\mathcal{M}\)</span>-completed view of nature, that is, there is a true data generating process, but the space of models that we are comparing does not include it, the BIC asymptotically selects the model that minimizes the Kullback-Leiber (KL) divergence to the true (population) model <span class="citation">(<a href="#ref-claeskens2008model">Claeskens and Hjort 2008</a>)</span>.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Bernardo1994" class="csl-entry">
Bernardo, J., and A. Smith. 1994. <em>Bayesian Theory</em>. Chichester: Wiley.
</div>
<div id="ref-burnham2004multimodel" class="csl-entry">
Burnham, Kenneth P, and David R Anderson. 2004. <span>“Multimodel Inference: Understanding AIC and BIC in Model Selection.”</span> <em>Sociological Methods &amp; Research</em> 33 (2): 261–304.
</div>
<div id="ref-Casella2006" class="csl-entry">
Casella, G., and E. Moreno. 2006. <span>“Objective <span>B</span>ayesian Variable Selection.”</span> <em>Journal of the American Statistical Association</em> 101 (473): 157–67.
</div>
<div id="ref-claeskens2008model" class="csl-entry">
Claeskens, Gerda, and Nils Lid Hjort. 2008. <span>“Model Selection and Model Averaging.”</span> <em>Cambridge Books</em>.
</div>
<div id="ref-Furnival1974" class="csl-entry">
Furnival, George M., and Robert W. Wilson. 1974. <span>“Regressions by Leaps and Bounds.”</span> <em>Technometrics</em> 16 (4): 499–511.
</div>
<div id="ref-George1993" class="csl-entry">
George, E., and R. McCulloch. 1993. <span>“Variable Selection via <span>G</span>ibbs Sampling.”</span> <em>Journal of the American Statistical Association</em> 88 (423): 881–89.
</div>
<div id="ref-George1997" class="csl-entry">
———. 1997. <span>“Approaches for <span>B</span>ayesian Variable Selection.”</span> <em>Statistica Sinica</em> 7: 339–73.
</div>
<div id="ref-Green1995" class="csl-entry">
Green, P. J. 1995. <span>“Reversible Jump <span>M</span>arkov Chain <span>M</span>onte <span>C</span>arlo Computation and <span>B</span>ayesian Model Determination.”</span> <em>Biometrika</em> 82: 711–32.
</div>
<div id="ref-Hoeting1999" class="csl-entry">
Hoeting, J., D. Madigan, A. Raftery, and C. Volinsky. 1999. <span>“Bayesian Model Averaging: A Tutorial.”</span> <em>Statistical Science</em> 14 (4): 382–417.
</div>
<div id="ref-Johnson2012" class="csl-entry">
Jhonson, V. E., and D. Rossell. 2012. <span>“Bayesian Model Selection in High-Dimensional Settings.”</span> <em>Journal of the American Statistical Association</em> 107 (498): 649–60.
</div>
<div id="ref-Kass1995" class="csl-entry">
Kass, R E, and A E Raftery. 1995. <span>“<span class="nocase">Bayes factors</span>.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 773–95.
</div>
<div id="ref-ley2009effect" class="csl-entry">
Ley, Eduardo, and Mark FJ Steel. 2009. <span>“On the Effect of Prior Assumptions in <span>B</span>ayesian Model Averaging with Applications to Growth Regression.”</span> <em>Journal of Applied Econometrics</em> 24 (4): 651–74.
</div>
<div id="ref-madigan1995bayesian1" class="csl-entry">
Madigan, David, Jeremy York, and Denis Allard. 1995. <span>“Bayesian Graphical Models for Discrete Data.”</span> <em>International Statistical Review</em> 63: 215–32.
</div>
<div id="ref-Madigan1994" class="csl-entry">
Madigan, D., and A. E. Raftery. 1994. <span>“<span class="nocase">Model selection and accounting for model uncertainty in graphical models using <span>O</span>ccam’s window</span>.”</span> <em>Journal of the American Statistical Association</em> 89 (428): 1535–46.
</div>
<div id="ref-madigan95" class="csl-entry">
Madigan, D., J. C. York, and D. Allard. 1995. <span>“Bayesian Graphical Models for Discrete Data.”</span> <em>International Statistical Review</em> 63 (2): 215–32.
</div>
<div id="ref-Park2008" class="csl-entry">
Park, T., and G. Casella. 2008. <span>“The <span>B</span>ayesian Lasso.”</span> <em>Journal of the American Statistical Association</em> 103 (482): 681–86.
</div>
<div id="ref-Raftery93" class="csl-entry">
Raftery, Adrian E. 1993. <span>“Bayesian Model Selection in Structural Equation Models.”</span> <em>Sage Focus Editions</em> 154: 163–63.
</div>
<div id="ref-Raftery1997" class="csl-entry">
Raftery, Adrian E., David Madigan, and Jennifer A. Hoeting. 1997. <span>“Bayesian Model Averaging for Linear Regression Models.”</span> <em>Journal of the American Statistical Association</em> 92 (437): 179–91.
</div>
<div id="ref-Raftery2012" class="csl-entry">
Raftery, Adrian, Jennifer Hoeting, Chris Volinsky, Ian Painter, and Ka Yee Yeung. 2012. <em>Package <span>BMA</span></em>. <a href="https://CRAN.R-project.org/package=BMA">https://CRAN.R-project.org/package=BMA</a>.
</div>
<div id="ref-schwarz1978estimating" class="csl-entry">
Schwarz, Gideon. 1978. <span>“Estimating the Dimension of a Model.”</span> <em>The Annals of Statistics</em>, 461–64.
</div>
<div id="ref-Simmons2010" class="csl-entry">
Simmons, Susan J, Fang Fang, Qijun Fang, and Karl Ricanek. 2010. <span>“Markov Chain <span>M</span>onte <span>C</span>arlo Model Composition Search Strategy for Quantitative Trait Loci in a <span>B</span>ayesian Hierarchical Model.”</span> <em>World Academy of Science, Engineering and Technology</em> 63: 58–61.
</div>
<div id="ref-Tierney1986" class="csl-entry">
Tierney, Luke, and Joseph B Kadane. 1986. <span>“Accurate Approximations for Posterior Moments and Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 81 (393): 82–86.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="54">
<li id="fn54"><p>Take into account that <span class="math inline">\(K\)</span> can increase when interaction terms and/or polynomial terms of the original control variables are included.<a href="sec10_1.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p>We attach equal prior probabilities to each model in our GUI. However, this choice gives more prior probability to the set of models of medium size (think about the <span class="math inline">\(k\)</span>-th row of Pascal’s triangle). An interesting alternative is to use the Beta-Binomial prior proposed by <span class="citation">Ley and Steel (<a href="#ref-ley2009effect">2009</a>)</span>.<a href="sec10_1.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>Scaling variables is always an important step in variable selection.<a href="sec10_1.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p>Variable selection (model selection or regularization) is a topic related to model uncertainty. Approaches such as stochastic search variable selection (spike and slab) <span class="citation">(<a href="#ref-George1993">George and McCulloch 1993</a>, <a href="#ref-George1997">1997</a>)</span> and Bayesian Lasso <span class="citation">(<a href="#ref-Park2008">Park and Casella 2008</a>)</span> are good examples of how to tackle this issue. See Chapter <a href="Chap12.html#Chap12">12</a>.<a href="sec10_1.html#fnref57" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Chap10.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec102.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/11-BMA.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
