<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12_8.html"/>
<link rel="next" href="sec13_11.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Identification setting</a></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Randomized controlled trial (RCT)</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Conditional independence assumption (CIA)</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Instrumental variables (IV)</a></li>
<li class="chapter" data-level="13.5" data-path="Chap13_5.html"><a href="Chap13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference-in-differences design (DiD)</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="Chap13_5.html"><a href="Chap13_5.html#sec13_51"><i class="fa fa-check"></i><b>13.5.1</b> Classic DiD: two-group and two-period (<span class="math inline">\(2\times2\)</span>) setup</a></li>
<li class="chapter" data-level="13.5.2" data-path="Chap13_5.html"><a href="Chap13_5.html#sec13_52"><i class="fa fa-check"></i><b>13.5.2</b> Staggered (SDiD): G-group and T-period (<span class="math inline">\(G\times T\)</span>) setup</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="sec_7.html"><a href="sec_7.html"><i class="fa fa-check"></i><b>13.6</b> Regression discontinuity design (RD)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="sec_7.html"><a href="sec_7.html#sharp-regression-discontinuity-design-srd"><i class="fa fa-check"></i><b>13.6.1</b> Sharp regression discontinuity design (SRD)</a></li>
<li class="chapter" data-level="13.6.2" data-path="sec_7.html"><a href="sec_7.html#fuzzy-regression-discontinuity-design-frd"><i class="fa fa-check"></i><b>13.6.2</b> Fuzzy regression discontinuity design (FRD)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="sec12_8.html"><a href="sec12_8.html"><i class="fa fa-check"></i><b>13.7</b> Sample selection</a></li>
<li class="chapter" data-level="13.8" data-path="sec12_9.html"><a href="sec12_9.html"><i class="fa fa-check"></i><b>13.8</b> Bayesian exponentially tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.9" data-path="sec13_11.html"><a href="sec13_11.html"><i class="fa fa-check"></i><b>13.9</b> Doubly robust</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12_9" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> Bayesian exponentially tilted empirical likelihood<a href="sec12_9.html#sec12_9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian parametric approaches are often criticized because they require distributional assumptions that may be arbitrary or remain unchecked. For example, in this chapter we model a continuous outcome as normally distributed. This choice is defensible: among all distributions with a fixed mean and variance, the normal imposes the least prior structure (it maximizes entropy; see Exercise 2), and the same reasoning extends to regression via Gaussian errors with fixed conditional variance <span class="citation">(<a href="#ref-zellner1996bmom">Zellner 1996a</a>)</span>. Nevertheless, in some settings it may be preferable to use partial information methods that rely only on moment conditions rather than full distributional assumptions. The trade-off is familiar: unless the parametric model is correctly specified, these semiparametric approaches typically reduce efficiency relative to a well-specified parametric model.</p>
<p>The point of departure is a set of moment conditions</p>
<p><span class="math display">\[
\mathbb{E}\!\big[\mathbf{g}(\mathbf{W},\boldsymbol{\theta})\big]=\mathbf{0}_{d},
\]</span></p>
<p>where the expectation is with respect to the population distribution, <span class="math inline">\(\mathbf{W}_{1:N}:=[\mathbf{W}_1 \ \mathbf{W}_2 \ \dots \ \mathbf{W}_N]\)</span> is a random sample from <span class="math inline">\(\mathbf{W}\subset \mathbb{R}^{d_w}\)</span>, <span class="math inline">\(\mathbf{g}:\mathbb{R}^{d_w}\times\boldsymbol{\Theta}\to\mathbb{R}^{d}\)</span> is a vector of known functions, and
<span class="math inline">\(\boldsymbol{\theta}=[\theta_{1}\ \theta_{2}\ \dots\ \theta_{p}]^{\top}\in\boldsymbol{\Theta}\subset\mathbb{R}^{p}\)</span>.
If <span class="math inline">\(d&gt;p\)</span> the model is <em>over-identified</em>; if <span class="math inline">\(d=p\)</span> it is <em>exactly identified</em> (and if <span class="math inline">\(d&lt;p\)</span> it is <em>under-identified</em>).</p>
<p><strong>Example: Linear regression</strong></p>
<p>Let</p>
<p><span class="math display">\[
y_i=\mathbf{X}_i^{\top}\boldsymbol{\beta}+\mu_i,\qquad \mathbb{E}[\mu_i\mid \mathbf{X}_i]=0,
\]</span></p>
<p>with <span class="math inline">\(\mathbf{X}_i\in\mathbb{R}^{p}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\in\mathbb{R}^{p}\)</span>. Then the unconditional moment conditions are</p>
<p><span class="math display">\[
\mathbb{E}\!\left[\mathbf{X}_i\,\mu_i\right]
=\mathbb{E}\!\left[\mathbf{X}_i\,(y_i-\mathbf{X}_i^{\top}\boldsymbol{\beta})\right]
=\mathbf{0}_{p}.
\]</span></p>
<p><strong>Example: Instrumental variables</strong></p>
<p>If there is endogeneity, <span class="math inline">\(\mathbb{E}[\mu_i\mid \mathbf{X}_i]\neq 0\)</span>, but there exist instruments <span class="math inline">\(\mathbf{Z}_i\in\mathbb{R}^{d}\)</span> that are exogenous, <span class="math inline">\(\mathbb{E}[\mu_i\mid \mathbf{Z}_i]=0\)</span>, and relevant, <span class="math inline">\(\operatorname{rank}\!\big(\mathbb{E}[\mathbf{Z}_i\mathbf{X}_i^{\top}]\big)=p\)</span>, then</p>
<p><span class="math display">\[
\mathbb{E}\!\left[\mathbf{Z}_i\,\mu_i\right]
=\mathbb{E}\!\left[\mathbf{Z}_i\,(y_i-\mathbf{X}_i^{\top}\boldsymbol{\beta})\right]
=\mathbf{0}_{d},
\]</span></p>
<p>with <span class="math inline">\(d\ge p\)</span>.</p>
<p>Moment conditions can be used in a Bayesian framework via <em>Bayesian Empirical Likelihood</em> (BEL) <span class="citation">(<a href="#ref-lazar2003bel">Nicole A. Lazar 2003</a>)</span> and <em>Bayesian Exponentially Tilted Empirical Likelihood</em> (BETEL) <span class="citation">(<a href="#ref-schennach2005betel">Schennach 2005</a>)</span>. We focus on BETEL because, while BEL inherits the attractive properties of empirical likelihood under correct specification, it can lose them under model misspecification. In contrast, Exponentially Tilted Empirical Likelihood (ETEL) remains well behaved under misspecification and retains root-<span class="math inline">\(n\)</span> consistency and asymptotic normality <span class="citation">(<a href="#ref-schennach2007etel">Schennach 2007</a>)</span>.</p>
<p>Thus, the posterior distribution is</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}\mid \mathbf{W}_{1:N})
\;\propto\;
\pi(\boldsymbol{\theta})\; L_{\mathrm{ETEL}}(\boldsymbol{\theta}),
\qquad
L_{\mathrm{ETEL}}(\boldsymbol{\theta})=\prod_{i=1}^N p_i^{*}(\boldsymbol{\theta}),
\]</span></p>
<p>where <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> is the prior and <span class="math inline">\(L_{\mathrm{ETEL}}\)</span> is the exponentially tilted empirical likelihood. The weights
<span class="math inline">\(\big(p_1^{*}(\boldsymbol{\theta}),\dots,p_N^{*}(\boldsymbol{\theta})\big)\)</span> are obtained from the maximum-entropy problem</p>
<p><span class="math display">\[
\max_{\{p_i\}_{i=1}^N}\;\Big\{-\sum_{i=1}^N p_i\log p_i\Big\}
\quad\text{subject to}\quad
\sum_{i=1}^N p_i=1,\;\; p_i\ge 0,\;\;
\sum_{i=1}^N p_i\,\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})=\mathbf{0}_d.
\]</span></p>
<p>Equivalently (dual/saddlepoint form; see <span class="citation">Schennach (<a href="#ref-schennach2005betel">2005</a>)</span>;<span class="citation">Schennach (<a href="#ref-schennach2007etel">2007</a>)</span>;<span class="citation">Chib, Shin, and Simoni (<a href="#ref-chib2018moment">2018</a>)</span>),</p>
<p><span class="math display">\[
p_i^{*}(\boldsymbol{\theta})
=\frac{\exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)}
{\sum_{j=1}^N \exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_j,\boldsymbol{\theta})\big)},
\quad\text{where}\quad
\sum_{i=1}^N p_i^{*}(\boldsymbol{\theta})\,\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})=\mathbf{0}_d.
\]</span></p>
<p>Equivalently, <span class="math inline">\(\boldsymbol{\lambda}(\boldsymbol{\theta})\)</span> can be characterized as</p>
<p><span class="math display">\[
\boldsymbol{\lambda}(\boldsymbol{\theta})
=\arg\min_{\boldsymbol{\lambda}\in\mathbb{R}^{d}}
\;\log\!\left(\frac{1}{N}\sum_{i=1}^N
\exp\!\big(\boldsymbol{\lambda}^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)\right),
\]</span></p>
<p>whose gradient condition is precisely the moment constraint above. Therefore, the BETEL posterior is</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}\mid \mathbf{w}_{1:N})
\;\propto\;
\pi(\boldsymbol{\theta})\;
\prod_{i=1}^N
\frac{\exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)}
{\sum_{j=1}^N \exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_j,\boldsymbol{\theta})\big)}.
\]</span></p>
<p>Posterior inference of the BETEL can be performed using a Metropolis-Hastings algorithm where the proposal distribution is <span class="math inline">\(q(\boldsymbol{\theta}\mid \mathbf{W}_{1:N})\)</span>. See the following algorithm <span class="citation">(<a href="#ref-chib2018moment">Chib, Shin, and Simoni 2018</a>)</span>.</p>
<figcaption><b>Algorithm: Bayesian Exponentially Tilted Empirical Likelihood (BETEL) – Metropolis-Hastings</b></figcaption>
<figure id="alg:BETEL">
<pre>
Set θ<sup>(0)</sup> in the support of π(θ|W<sub>1:N</sub>) 
For s=1,2,...,S do
  Propose θ<sup>c</sup> from q(θ|W<sub>1:N</sub>)
  Solve for p<sub>i</sub><sup>*</sup>(θ<sup>c</sup>), i=1,...,N using the equation that characterized λ
  Calculate α(θ<sup>(s-1)</sup>,θ<sup>c</sup>|W<sub>1:N</sub>) =
      min(1, [π(θ<sup>c</sup>|W<sub>1:N</sub>)/π(θ<sup>(s-1)</sup>|W<sub>1:N</sub>)] × 
                [q(θ<sup>(s-1)</sup>|W<sub>1:N</sub>)/q(θ<sup>c</sup>|W<sub>1:N</sub>)])
  Draw U ~ U(0,1)
  θ<sup>(s)</sup> = θ<sup>c</sup> if U ≤ α(θ<sup>(s-1)</sup>,θ<sup>c</sup>|W<sub>1:N</sub>)
  θ<sup>(s)</sup> = θ<sup>(s-1)</sup> otherwise
End for
</pre>
</figure>
<p><strong>Example: Classical measurement error in regressor</strong></p>
<p>Let’s set the unobserved (latent) regressor <span class="math inline">\(X_i^*\)</span> such that</p>
<p><span class="math display">\[
X_i = X_i^* + \nu_i,
\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> is the observed regressor, and <span class="math inline">\(\nu_i\)</span> is a <em>classical measurement error</em> such that
<span class="math inline">\(\mathbb{E}[\nu_i]=0\)</span> and <span class="math inline">\(\nu_i \perp \{X_i^*,\mu_i\}\)</span>, where <span class="math inline">\(\mu_i\)</span> is the stochastic error of the structural model</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1X_i^*+\mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[\mu_i]=0\)</span> and <span class="math inline">\(\mu_i\perp X_i^*\)</span>.</p>
<p>If we perform the regression using the observed regressor, then</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1(X_i-\nu_i)+\mu_i
=\beta_0+\beta_1X_i+\underbrace{\mu_i-\beta_1\nu_i}_{\epsilon_i},
\]</span></p>
<p>where the new error term is <span class="math inline">\(\epsilon_i=\mu_i-\beta_1\nu_i\)</span>. We will show that</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i\mid X_i]\neq 0.
\]</span></p>
<p>By the law of iterated expectations,</p>
<p><span class="math display">\[
\mathbb{E}[\nu_i X_i] = \mathbb{E}\!\left[X_i \, \mathbb{E}[\nu_i \mid X_i]\right].
\]</span></p>
<p>This implies:</p>
<ul>
<li><p>If <span class="math inline">\(\mathbb{E}[\nu_i \mid X_i] = 0\)</span> almost surely, then <span class="math inline">\(\mathbb{E}[\nu_i X_i] = 0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\mathbb{E}[\nu_i X_i] \neq 0\)</span>, then <span class="math inline">\(\mathbb{E}[\nu_i \mid X_i]\)</span> cannot be equal to zero almost surely.</p></li>
</ul>
<p>Now compute</p>
<p><span class="math display">\[
\mathbb{E}[\nu_iX_i]
= \mathbb{E}[\nu_i(X_i^*+\nu_i)]
= \underbrace{\mathbb{E}[\nu_i X_i^*]}_{=0} + \mathbb{E}[\nu_i^2]
= \sigma^2_{\nu} \neq 0.
\]</span></p>
<p>Hence it must be that <span class="math inline">\(\mathbb{E}[\nu_i\mid X_i]\neq 0\)</span>. Therefore,</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i\mid X_i]
=\underbrace{\mathbb{E}[\mu_i\mid X_i]}_{=0}
- \beta_1\underbrace{\mathbb{E}[\nu_i\mid X_i]}_{\neq 0}
\neq 0,
\]</span></p>
<p>that is, the regressor is not exogenous, and consequently,</p>
<p><span class="math display">\[
\mathbb{E}[X_i\underbrace{(Y_i-\beta_0-\beta_1X_i)}_{\epsilon_i}]\neq 0,
\]</span></p>
<p>thus, we need a relevant (strong) (<span class="math inline">\(\mathbb{E}[Z_iX_i^*]\neq 0\)</span>) and exogeneous (<span class="math inline">\(\mathbb{E}[Z_i\mid \mu_i]=\mathbb{E}[Z_i\mid \nu_i]= 0\)</span>) instrument to identify the causal effect. Therefore,</p>
<p><span class="math display">\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    Z_i
\end{bmatrix}(Y_i-\beta_0-\beta_1X_i)\right]=\mathbf{0}.
\]</span></p>
<p>The following DAG illustrates the situation of measurement error, and how the instrument helps to identify the causal effect. The instrument solves the endogeneity problem because it exploits variation in the regressor <span class="math inline">\(X_i\)</span> that is correlated with the true latent regressor <span class="math inline">\(X_i^*\)</span> but uncorrelated with the measurement error <span class="math inline">\(\nu_i\)</span>. See Chapter 9 in <span class="citation">Hernán and Robins (<a href="#ref-hernan2020causal">2020</a>)</span> for a nice review of the effects of measurement error in causal inference.`</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="figures/FigChap13_13.png" alt="DAG with measurement error and instrument $Z$. Observed $X$ depends on the latent $X^{*}$ and error $
u$. Because the composed error in the estimated equation includes $
u$, $X$ is endogenous when regressed on $Y$. Instrument $Z$ isolates exogenous variation in $X^{*}$." width="200px" />
<p class="caption">
Figure 13.13: DAG with measurement error and instrument <span class="math inline">\(Z\)</span>. Observed <span class="math inline">\(X\)</span> depends on the latent <span class="math inline">\(X^{*}\)</span> and error $
u$. Because the composed error in the estimated equation includes $
u$, <span class="math inline">\(X\)</span> is endogenous when regressed on <span class="math inline">\(Y\)</span>. Instrument <span class="math inline">\(Z\)</span> isolates exogenous variation in <span class="math inline">\(X^{*}\)</span>.
</p>
</div>
<p>We simulate the latent process <span class="math inline">\(X_i^* = 0.5Z_i + e_i\)</span>, with the observed regressor defined as <span class="math inline">\(X_i = X_i^* + \nu_i\)</span>, and the outcome equation specified as <span class="math inline">\(Y_i = 1 + 1.2X_i^* + \mu_i\)</span>. The variables <span class="math inline">\(Z_i\)</span>, <span class="math inline">\(e_i\)</span>, and <span class="math inline">\(\nu_i\)</span> are standard normal, while <span class="math inline">\(\mu_i\)</span> follows a mixture of two normal distributions with means <span class="math inline">\(0.5\)</span> and <span class="math inline">\(-0.5\)</span>, and standard deviations <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1.2\)</span>. The sample size is 2,000, the burn-in is 1,000, and the number of MCMC draws retained after burn-in is 10,000.</p>
<p>We perform Bayesian exponentially tilted empirical likelihood (BETEL) using the package <em>betel</em>, and adopt the default hyperparameter values provided there.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This package implements Bayesian estimation and marginal likelihood computation for moment condition models, employing a Student-t prior distribution and a Student-t proposal distribution following <span class="citation">Chib, Shin, and Simoni (<a href="#ref-chib2018moment">2018</a>)</span>. We also compare the results with a model that assumes <span class="math inline">\(X_i\)</span> is exogenous, and with an instrumental Gibbs sampler that assumes <span class="math inline">\(Y_i\)</span> is normally distributed. We also use the default hyperparameters of the packages employed. The following code shows the implementation.</p>
<p>The figure displays the posterior distributions. We see that ignoring measurement error produces a biased posterior distribution. In particular, the absolute value of the causal effect is smaller than the population value; this is called <em>attenuation bias</em>. By contrast, methods that account for measurement error and use valid instruments yield well-centered posterior distributions.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="sec12_9.html#cb39-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb39-2"><a href="sec12_9.html#cb39-2" tabindex="-1"></a><span class="fu">library</span>(betel); <span class="fu">library</span>(ucminf)</span></code></pre></div>
<pre><code>## Loading required package: devtools</code></pre>
<pre><code>## Loading required package: usethis</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="sec12_9.html#cb42-1" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb42-2"><a href="sec12_9.html#cb42-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">2000</span>; d <span class="ot">&lt;-</span> <span class="dv">2</span>; k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb42-3"><a href="sec12_9.html#cb42-3" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.5</span>; beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.2</span>)</span>
<span id="cb42-4"><a href="sec12_9.html#cb42-4" tabindex="-1"></a><span class="co"># Mixture</span></span>
<span id="cb42-5"><a href="sec12_9.html#cb42-5" tabindex="-1"></a>mum1 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>; mum2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span></span>
<span id="cb42-6"><a href="sec12_9.html#cb42-6" tabindex="-1"></a>mu1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, mum1, <span class="fl">0.5</span>); mu2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, mum2, <span class="fl">1.2</span>)</span>
<span id="cb42-7"><a href="sec12_9.html#cb42-7" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="cf">function</span>(i){<span class="fu">sample</span>(<span class="fu">c</span>(mu1[i], mu2[i]), <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))})</span>
<span id="cb42-8"><a href="sec12_9.html#cb42-8" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb42-9"><a href="sec12_9.html#cb42-9" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N) <span class="co"># Instrument</span></span>
<span id="cb42-10"><a href="sec12_9.html#cb42-10" tabindex="-1"></a>xlat <span class="ot">&lt;-</span> gamma<span class="sc">*</span>z <span class="sc">+</span> e <span class="co"># Unobserved regressor</span></span>
<span id="cb42-11"><a href="sec12_9.html#cb42-11" tabindex="-1"></a>nu <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N) <span class="co"># Measurement error</span></span>
<span id="cb42-12"><a href="sec12_9.html#cb42-12" tabindex="-1"></a>x <span class="ot">&lt;-</span> xlat <span class="sc">+</span> nu <span class="co"># Observed regressor</span></span>
<span id="cb42-13"><a href="sec12_9.html#cb42-13" tabindex="-1"></a>Xlat <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, xlat)</span>
<span id="cb42-14"><a href="sec12_9.html#cb42-14" tabindex="-1"></a>y <span class="ot">&lt;-</span> Xlat<span class="sc">%*%</span>beta <span class="sc">+</span> mu</span>
<span id="cb42-15"><a href="sec12_9.html#cb42-15" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x, z) <span class="co"># Data</span></span>
<span id="cb42-16"><a href="sec12_9.html#cb42-16" tabindex="-1"></a><span class="co"># Function g_i by row in BETEL</span></span>
<span id="cb42-17"><a href="sec12_9.html#cb42-17" tabindex="-1"></a>gfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">psi =</span> psi, <span class="at">y =</span> y, <span class="at">dat =</span> dat) {</span>
<span id="cb42-18"><a href="sec12_9.html#cb42-18" tabindex="-1"></a>    X <span class="ot">&lt;-</span> dat[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb42-19"><a href="sec12_9.html#cb42-19" tabindex="-1"></a>    e <span class="ot">&lt;-</span> y <span class="sc">-</span> X <span class="sc">%*%</span> psi</span>
<span id="cb42-20"><a href="sec12_9.html#cb42-20" tabindex="-1"></a>    E <span class="ot">&lt;-</span> e <span class="sc">%*%</span> <span class="fu">rep</span>(<span class="dv">1</span>,d)</span>
<span id="cb42-21"><a href="sec12_9.html#cb42-21" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> dat[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)]</span>
<span id="cb42-22"><a href="sec12_9.html#cb42-22" tabindex="-1"></a>    G <span class="ot">&lt;-</span> E <span class="sc">*</span> Z;</span>
<span id="cb42-23"><a href="sec12_9.html#cb42-23" tabindex="-1"></a>    <span class="fu">return</span>(G)</span>
<span id="cb42-24"><a href="sec12_9.html#cb42-24" tabindex="-1"></a>}</span>
<span id="cb42-25"><a href="sec12_9.html#cb42-25" tabindex="-1"></a>nt <span class="ot">&lt;-</span> <span class="fu">round</span>(N <span class="sc">*</span> <span class="fl">0.1</span>, <span class="dv">0</span>); <span class="co"># training sample size for prior</span></span>
<span id="cb42-26"><a href="sec12_9.html#cb42-26" tabindex="-1"></a>psi0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y[<span class="dv">1</span><span class="sc">:</span>nt]<span class="sc">~</span>x[<span class="dv">1</span><span class="sc">:</span>nt])<span class="sc">$</span>coefficients <span class="co"># Starting value of psi = (theta, v), v is the slack parameter in CSS (2018)</span></span>
<span id="cb42-27"><a href="sec12_9.html#cb42-27" tabindex="-1"></a><span class="fu">names</span>(psi0) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>,<span class="st">&quot;beta&quot;</span>)</span>
<span id="cb42-28"><a href="sec12_9.html#cb42-28" tabindex="-1"></a>psi0_ <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(psi0) <span class="co"># Prior mean of psi </span></span>
<span id="cb42-29"><a href="sec12_9.html#cb42-29" tabindex="-1"></a>Psi0_ <span class="ot">&lt;-</span> <span class="dv">5</span><span class="sc">*</span><span class="fu">rep</span>(<span class="dv">1</span>,k) <span class="co"># Prior dispersions of psi</span></span>
<span id="cb42-30"><a href="sec12_9.html#cb42-30" tabindex="-1"></a>lam0 <span class="ot">&lt;-</span> .<span class="dv">5</span><span class="sc">*</span><span class="fu">rnorm</span>(d) <span class="co"># Starting value of lambda</span></span>
<span id="cb42-31"><a href="sec12_9.html#cb42-31" tabindex="-1"></a>nu <span class="ot">&lt;-</span> <span class="fl">2.5</span> <span class="co"># df of the prior student-t</span></span>
<span id="cb42-32"><a href="sec12_9.html#cb42-32" tabindex="-1"></a>nuprop <span class="ot">&lt;-</span> <span class="dv">15</span> <span class="co"># df of the student-t proposal</span></span>
<span id="cb42-33"><a href="sec12_9.html#cb42-33" tabindex="-1"></a>n0 <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># burn-in</span></span>
<span id="cb42-34"><a href="sec12_9.html#cb42-34" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># iterations beyond burn-in</span></span>
<span id="cb42-35"><a href="sec12_9.html#cb42-35" tabindex="-1"></a><span class="co"># MCMC ESTIMATION BY THE CSS (2018) method</span></span>
<span id="cb42-36"><a href="sec12_9.html#cb42-36" tabindex="-1"></a>psim <span class="ot">&lt;-</span> betel<span class="sc">::</span><span class="fu">bayesetel</span>(<span class="at">gfunc =</span> gfunc, <span class="at">y =</span> y[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>nt)], <span class="at">dat =</span> dat[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>nt),], <span class="at">psi0 =</span> psi0, <span class="at">lam0 =</span> lam0, <span class="at">psi0_ =</span> psi0_, <span class="at">Psi0_ =</span> Psi0_, <span class="at">nu =</span> nu, <span class="at">nuprop =</span> nuprop,</span>
<span id="cb42-37"><a href="sec12_9.html#cb42-37" tabindex="-1"></a> <span class="at">controlpsi =</span> <span class="fu">list</span>(<span class="at">maxiterpsi =</span> <span class="dv">50</span>, <span class="at">mingrpsi =</span> <span class="fl">1.0e-8</span>), <span class="co">#  list of parameters in maximizing likelihood over psi</span></span>
<span id="cb42-38"><a href="sec12_9.html#cb42-38" tabindex="-1"></a><span class="at">controllam =</span> <span class="fu">list</span>(<span class="at">maxiterlam =</span> <span class="dv">50</span>, <span class="co"># list of parameters in minimizing dual over lambda</span></span>
<span id="cb42-39"><a href="sec12_9.html#cb42-39" tabindex="-1"></a><span class="at">mingrlam =</span> <span class="fl">1.0e-7</span>),</span>
<span id="cb42-40"><a href="sec12_9.html#cb42-40" tabindex="-1"></a><span class="at">n0 =</span> n0, <span class="at">m =</span> m)</span></code></pre></div>
<pre><code>## this is psi0 1.022954 0.6541617 
## tailoring to find the proposal has started ... 
## $muprop
##    alpha     beta 
## 1.026519 1.284899 
## 
## $Pprop
##           [,1]      [,2]
## [1,] 671.68877  14.90052
## [2,]  14.90052 135.87597
## 
## $value
## [1] 13497.24
## 
## $grad
## [1] -3.375095e-04 -7.844614e-05
## 
## this is g 2000 
## this is psi1 1.016084 1.293286 
## this is counter/g 0.8775 
## this is g 4000 
## this is psi1 1.072923 1.257938 
## this is counter/g 0.876 
## this is g 6000 
## this is psi1 1.033308 1.266278 
## this is counter/g 0.8748333 
## this is g 8000 
## this is psi1 1.014174 1.494646 
## this is counter/g 0.874375 
## this is g 10000 
## this is psi1 1.082801 1.311398 
## this is counter/g 0.8734 
## this is g in numerator of C-J 2000 
## this is g in numerator of C-J 4000 
## this is g in numerator of C-J 6000 
## this is g in numerator of C-J 8000 
## this is g in numerator of C-J 10000 
## this is g in denominator of C-J 2000 
## this is g in denominator of C-J 4000 
## this is g in denominator of C-J 6000 
## this is g in denominator of C-J 8000 
## this is g in denominator of C-J 10000</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="sec12_9.html#cb44-1" tabindex="-1"></a>MCMCexg <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">MCMCregress</span>(y <span class="sc">~</span> x, <span class="at">burnin =</span> n0, <span class="at">mcmc =</span> m)</span>
<span id="cb44-2"><a href="sec12_9.html#cb44-2" tabindex="-1"></a>Data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y =</span> <span class="fu">c</span>(y), <span class="at">x =</span> x, <span class="at">z =</span> <span class="fu">matrix</span>(z, N, <span class="dv">1</span>), <span class="at">w =</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span>, N), N, <span class="dv">1</span>))</span>
<span id="cb44-3"><a href="sec12_9.html#cb44-3" tabindex="-1"></a>Mcmc <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">R =</span> m)</span>
<span id="cb44-4"><a href="sec12_9.html#cb44-4" tabindex="-1"></a>MCMCivr <span class="ot">&lt;-</span> bayesm<span class="sc">::</span><span class="fu">rivGibbs</span>(Data, <span class="at">Mcmc =</span> Mcmc)</span></code></pre></div>
<pre><code>##  
## Starting Gibbs Sampler for Linear IV Model
##  
##  nobs=  2000 ;  1  instruments;  1  included exog vars
##      Note: the numbers above include intercepts if in z or w
##  
## Prior Parms: 
## mean of delta 
## [1] 0
## Adelta
##      [,1]
## [1,] 0.01
## mean of beta/gamma
## [1] 0 0
## Abeta/gamma
##      [,1] [,2]
## [1,] 0.01 0.00
## [2,] 0.00 0.01
## Sigma Prior Parms
## nu=  3  V=
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
##  
## MCMC parms: 
## R=  10000  keep=  1  nprint=  100
##  
##  MCMC Iteration (est time to end - min) 
##  100 (0.0)
##  200 (0.0)
##  300 (0.0)
##  400 (0.0)
##  500 (0.0)
##  600 (0.0)
##  700 (0.0)
##  800 (0.0)
##  900 (0.0)
##  1000 (0.0)
##  1100 (0.0)
##  1200 (0.0)
##  1300 (0.0)
##  1400 (0.0)
##  1500 (0.0)
##  1600 (0.0)
##  1700 (0.0)
##  1800 (0.0)
##  1900 (0.0)
##  2000 (0.0)
##  2100 (0.0)
##  2200 (0.0)
##  2300 (0.0)
##  2400 (0.0)
##  2500 (0.0)
##  2600 (0.0)
##  2700 (0.0)
##  2800 (0.0)
##  2900 (0.0)
##  3000 (0.0)
##  3100 (0.0)
##  3200 (0.0)
##  3300 (0.0)
##  3400 (0.0)
##  3500 (0.0)
##  3600 (0.0)
##  3700 (0.0)
##  3800 (0.0)
##  3900 (0.0)
##  4000 (0.0)
##  4100 (0.0)
##  4200 (0.0)
##  4300 (0.0)
##  4400 (0.0)
##  4500 (0.0)
##  4600 (0.0)
##  4700 (0.0)
##  4800 (0.0)
##  4900 (0.0)
##  5000 (0.0)
##  5100 (0.0)
##  5200 (0.0)
##  5300 (0.0)
##  5400 (0.0)
##  5500 (0.0)
##  5600 (0.0)
##  5700 (0.0)
##  5800 (0.0)
##  5900 (0.0)
##  6000 (0.0)
##  6100 (0.0)
##  6200 (0.0)
##  6300 (0.0)
##  6400 (0.0)
##  6500 (0.0)
##  6600 (0.0)
##  6700 (0.0)
##  6800 (0.0)
##  6900 (0.0)
##  7000 (0.0)
##  7100 (0.0)
##  7200 (0.0)
##  7300 (0.0)
##  7400 (0.0)
##  7500 (0.0)
##  7600 (0.0)
##  7700 (0.0)
##  7800 (0.0)
##  7900 (0.0)
##  8000 (0.0)
##  8100 (0.0)
##  8200 (0.0)
##  8300 (0.0)
##  8400 (0.0)
##  8500 (0.0)
##  8600 (0.0)
##  8700 (0.0)
##  8800 (0.0)
##  8900 (0.0)
##  9000 (0.0)
##  9100 (0.0)
##  9200 (0.0)
##  9300 (0.0)
##  9400 (0.0)
##  9500 (0.0)
##  9600 (0.0)
##  9700 (0.0)
##  9800 (0.0)
##  9900 (0.0)
##  10000 (0.0)
##  Total Time Elapsed: 0.02</code></pre>
<p><strong>Example: Omission of relevant correlated regressor</strong></p>
<p>Consider the true model</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \gamma W_i + \mu_i,
\qquad \mathbb{E}[\mu_i \mid X_i, W_i] = 0.
\]</span></p>
<p>Suppose that the relevant regressor <span class="math inline">\(W_i\)</span> is omitted from the specification, with <span class="math inline">\(\gamma \neq 0\)</span> and <span class="math inline">\(\operatorname{Cov}(X_i, W_i) \neq 0\)</span>.</p>
<p>Thus, if we regress <span class="math inline">\(Y_i\)</span> only on <span class="math inline">\(X_i\)</span>, the composite error is</p>
<p><span class="math display">\[
\epsilon_i = \gamma W_i + \mu_i.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i \mid X_i]
= \gamma \, \mathbb{E}[W_i \mid X_i] + \mathbb{E}[\mu_i \mid X_i]
= \gamma \, \mathbb{E}[W_i \mid X_i] \neq 0,
\]</span></p>
<p>since <span class="math inline">\(\mathbb{E}[W_i \mid X_i]\)</span> is nonconstant whenever <span class="math inline">\(X_i\)</span> and <span class="math inline">\(W_i\)</span> are correlated, that is, the expected value of <span class="math inline">\(W_i\)</span> is a function of <span class="math inline">\(X_i\)</span>.</p>
<p>Consequently,</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i X_i]
= \gamma \, \mathbb{E}[W_i X_i] + \mathbb{E}[\mu_i X_i].
\]</span></p>
<p>Because <span class="math inline">\(\mathbb{E}[\mu_i X_i]=0\)</span>, we obtain</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i X_i] = \gamma (\operatorname{Cov}(W_i,X_i)
+ \mathbb{E}[W_i] \, \mathbb{E}[X_i])\neq 0.
\]</span></p>
<p>In this situation, we can use an instrument that is relevant (<span class="math inline">\(\mathbb{E}[Z_i X_i] \neq 0\)</span>) and exogenous (<span class="math inline">\(\mathbb{E}[Z_i \mid \mu_i] = \mathbb{E}[Z_i \mid W_i] = 0\)</span>) to address the endogeneity problem, and identify the causal effect.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Therefore,</p>
<p><span class="math display">\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    Z_i
\end{bmatrix}(Y_i-\beta_0-\beta_1X_i)\right]=\mathbf{0}.
\]</span></p>
<p>The figure illustrates the case of an omitted relevant regressor that is correlated with <span class="math inline">\(X_i\)</span>, and how an instrument can be used to identify the causal effect. The instrument resolves the endogeneity problem by exploiting variation in <span class="math inline">\(X_i\)</span> that is driven by <span class="math inline">\(Z_i\)</span>, which is orthogonal to the problematic error term. In other words, it replaces “bad” correlation with “clean” variation.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="figures/FigChap13_14.png" alt="DAG with omitted relevant regressor $W$ (dashed, unobserved) that is correlated with $X$ and affects $Y$, inducing endogeneity of $X$ in a regression of $Y$ on $X$. A valid instrument $Z$ affects $X$ (relevance) but has no direct path to $Y$ and is independent of $W$ and $u$ (exclusion/independence), enabling identification of the causal effect $X  o Y$." width="200px" />
<p class="caption">
Figure 13.14: DAG with omitted relevant regressor <span class="math inline">\(W\)</span> (dashed, unobserved) that is correlated with <span class="math inline">\(X\)</span> and affects <span class="math inline">\(Y\)</span>, inducing endogeneity of <span class="math inline">\(X\)</span> in a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. A valid instrument <span class="math inline">\(Z\)</span> affects <span class="math inline">\(X\)</span> (relevance) but has no direct path to <span class="math inline">\(Y\)</span> and is independent of <span class="math inline">\(W\)</span> and <span class="math inline">\(u\)</span> (exclusion/independence), enabling identification of the causal effect <span class="math inline">\(X    o Y\)</span>.
</p>
</div>
<p>We ask in Exercise 10 to perform a simulation exercise to assess the ability of BETEL to identify the causal effect when an instrument is used to address the omission of relevant regressors.</p>
<p><strong>Example: Simultaneous causality</strong></p>
<p>This example is based on <span class="citation">Guido W. Imbens (<a href="#ref-imbens2014ivperspective">2014</a>)</span>, where Professor Imbens illustrates the problem of analyzing the causal effect on traded quantity of a tax of <span class="math inline">\(100 \times r\%\)</span> in a market. He defines the average causal effect on the logarithm of traded quantity as</p>
<p><span class="math display">\[
\tau = \mathbb{E}[q_i(r) - q_i(0)],
\]</span></p>
<p>where <span class="math inline">\(q_i(r) = \log Q_i(r)\)</span> and <span class="math inline">\(Q_i(r)\)</span> denotes the potential traded quantity if the tax were <span class="math inline">\(r\)</span>%.</p>
<p>This situation is more challenging than in the standard treatment effects literature, because we do not observe any unit facing the tax. Instead, we only observe all units facing no tax, that is, <span class="math inline">\(Q_i^{\text{Obs}} = Q_i(0)\)</span>. This setting requires the use of a <em>structural model</em> to define the counterfactual scenarios of the potential outcomes.</p>
<p>The starting point for inference on the treatment effect of this new tax is the price determination mechanism, that is, the assignment mechanism in the potential outcome framework. We specify a <em>structural demand function</em> that defines the potential demand given the price (treatment) and other exogenous variables:</p>
<p><span class="math display">\[
q_i^d(p) = \beta_1 + \beta_2 p + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1},
\]</span></p>
<p>where <span class="math inline">\(q^d\)</span> is demand, and <span class="math inline">\(p\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(pc\)</span>, and <span class="math inline">\(ps\)</span> are the logarithms of price, income, the price of a complementary good, and the price of a substitute good, respectively. All coefficients are interpreted as demand elasticities. The term <span class="math inline">\(u_{i1}\)</span> represents unobserved demand factors such that <span class="math inline">\(\mathbb{E}[u_{i1}]=0\)</span>. Therefore,</p>
<p><span class="math display">\[
\mathbb{E}[q_i^d(p)\mid p, y_i, pc_i, ps_i] = \beta_1 + \beta_2 p + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i.
\]</span></p>
<p>This expectation does not represent the conditional expectation of the observed quantity in markets where the observed price equals <span class="math inline">\(p\)</span>. Rather, it is the expectation of potential demand functions given <span class="math inline">\(p\)</span> and other exogenous controls, irrespective of the realized market price.</p>
<p>Similarly, the assignment mechanism requires specifying the <em>structural supply function</em>:</p>
<p><span class="math display">\[
q_i^s(p) = \alpha_1 + \alpha_2 p + \alpha_3 er_i + u_{i2},
\]</span></p>
<p>which represents the quantity that sellers are willing to supply given the price and the (exogenous) exchange rate <span class="math inline">\(er_i\)</span>. The unobserved supply factors satisfy <span class="math inline">\(\mathbb{E}[u_{i2}]=0\)</span>, so that</p>
<p><span class="math display">\[
\mathbb{E}[q_i^s(p)\mid p, er_i] = \alpha_1 + \alpha_2 p + \alpha_3 er_i.
\]</span></p>
<p>This expectation represents the average of all potential supply functions given the price and exogenous controls, again irrespective of the realized market price.</p>
<p>Thus, the assignment mechanism is given by the market equilibrium where the observed price is such that the observed quantity is equal to the demand and supply potential outcomes at the observed price,</p>
<p><span class="math display">\[
q_i^{Obs}=q_i^d(p_i^{Obs})=q_i^s(p_i^{Obs}).
\]</span></p>
<p>Using this market equilibrium condition, we get</p>
<p><span class="math display">\[
p_i^{Obs}=\pi_1+\pi_2 er_i + \pi_3 y_i + \pi_4 pc_i + \pi_5 ps_i + v_{i1},
\]</span></p>
<p>where <span class="math inline">\(\pi_1=\frac{\alpha_1-\beta_1}{\beta_2-\alpha_2}\)</span>, <span class="math inline">\(\pi_2=\frac{\alpha_3}{\beta_2-\alpha_2}\)</span>, <span class="math inline">\(\pi_3=\frac{-\beta_3}{\beta_2-\alpha_2}\)</span>, <span class="math inline">\(\pi_4=\frac{-\beta_4}{\beta_2-\alpha_2}\)</span>, <span class="math inline">\(\pi_5=\frac{-\beta_5}{\beta_2-\alpha_2}\)</span>, and <span class="math inline">\(v_{i1}=\frac{u_{i2}-u_{i1}}{\beta_2-\alpha_2}\)</span> given <span class="math inline">\(\beta_2\neq\alpha_2\)</span>, that is, the equations should be independent. This condition is given by economic theory due to <span class="math inline">\(\beta_2&lt;0\)</span> and <span class="math inline">\(\alpha_2&gt;0\)</span>, the effect of price on demand and supply should be negative and positive, respectively.</p>
<p>The equation of price into the demand equation gives</p>
<p><span class="math display">\[
q_i^{Obs}=\tau_1+\tau_2 er_i + \tau_3 y_i + \tau_4 pc_i + \tau_5 ps_i + v_{i2},
\]</span></p>
<p>where <span class="math inline">\(\tau_1=\beta_1+\beta_2\pi_1\)</span>, <span class="math inline">\(\tau_2=\beta_2\pi_2\)</span>, <span class="math inline">\(\tau_3=\beta_2\pi_3+\beta_3\)</span>, <span class="math inline">\(\tau_4=\beta_2\pi_4+\beta_4\)</span>, <span class="math inline">\(\tau_5=\beta_2\pi_5+\beta_5\)</span>, and <span class="math inline">\(v_{i2}=\beta_2v_{i1}+u_{i1}\)</span>.</p>
<p>The expressions for <span class="math inline">\(p_i^{Obs}\)</span> and <span class="math inline">\(q_i^{Obs}\)</span> are called the <em>reduced-form</em> representations. In Section <a href="sec71.html#sec71">7.1</a>, we presented the order condition, which is necessary, and the rank condition, which is both necessary and sufficient, to identify the structural parameters from the reduced-form parameters. A key point to note is that only the prior distribution of the reduced-form parameters is updated by the sample information, while the updating of the structural parameters occurs solely through the reduced-form parameters, that is,</p>
<p><span class="math display">\[
\pi(\boldsymbol{\beta},\boldsymbol{\alpha}\mid \boldsymbol{\gamma},\boldsymbol{\pi},\mathbf{W}_{1:N})
\;\propto\;
\pi(\boldsymbol{\beta},\boldsymbol{\alpha}\mid \boldsymbol{\gamma},\boldsymbol{\pi}).
\]</span></p>
<p>See Section 9.3 in <span class="citation">Zellner (<a href="#ref-zellner1996introduction">1996b</a>)</span> for details about identification in Bayesian inference. This implies that in <em>under-identified</em> models, the posterior distribution of the structural parameters is not concentrated at a single point, but rather remains spread out (e.g., uniformly) over a range of values.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>To analyze the causal effects of the new tax, we can find the new equilibrium,</p>
<p><span class="math display">\[
q_i^d(P_i(r)\times (1+r))=q_i^s(P_i(r)),
\]</span></p>
<p>where <span class="math inline">\(P_i(r)\)</span> is the price level (<span class="math inline">\(p_i(r)=\log P_i(r)\)</span>) that sellers get, and <span class="math inline">\((P_i(r)\times (1+r)\)</span> is the price that buyers pay.</p>
<p>Let’s define the (log) price that buyers pay <span class="math inline">\(p_b = p + \log(1+r)\)</span> while sellers receive <span class="math inline">\(p\)</span>. The demand function becomes</p>
<p><span class="math display">\[
q_i^d\big(p + \log(1+r)\big) = \beta_1 + \beta_2\left(p + \log(1+r)\right) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1},
\]</span></p>
<p>and the supply function remains</p>
<p><span class="math display">\[
q_i^s(p) = \alpha_1 + \alpha_2 p + \alpha_3 er_i + u_{i2}.
\]</span></p>
<p>Thus, the equilibrium price is given by</p>
<p><span class="math display">\[
p_i^*(r) = \frac{(\beta_1 - \alpha_1) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i - \alpha_3 er_i + (u_{i1} - u_{i2}) + \beta_2 \log(1+r)}{\alpha_2 - \beta_2}.
\]</span></p>
<p>The equilibrium quantity is</p>
<p><span class="math display">\[
q_i^*(r) = \beta_1 + \beta_2\left(p_i^*(r) + \log(1+r)\right) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1}.
\]</span></p>
<p>Thus, the expected equilibrium price and quantity are</p>
<p><span class="math display">\[
\mathbb{E}[p_i^*(r)\mid y_i, pc_i, ps_i, er_i]
=
\frac{(\beta_1 - \alpha_1) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i - \alpha_3 er_i + \beta_2 \log(1+r)}{\alpha_2 - \beta_2},
\]</span></p>
<p><span class="math display">\[
\mathbb{E}[q_i^*(r)\mid y_i, pc_i, ps_i, er_i]
=
\alpha_1 + \alpha_2 \,\mathbb{E}[p_i^*(r)\mid \cdot] + \alpha_3 er_i.
\]</span></p>
<p>We can see that given <span class="math inline">\(\beta_2 &lt; 0 &lt; \alpha_2\)</span>, we have:</p>
<p><span class="math display">\[
\frac{d p^*(r)}{dr} = \frac{\beta_2}{(\alpha_2 - \beta_2)(1+r)} &lt; 0, \quad
\frac{d p_b^*(r)}{dr} = \frac{\alpha_2}{(\alpha_2 - \beta_2)(1+r)} &gt; 0, \quad
\frac{d q^*(r)}{dr} = \alpha_2 \cdot \frac{d p^*(r)}{dr} &lt; 0.
\]</span></p>
<p>Thus, the price received by sellers decreases with the tax, the price paid by buyers increases, and the equilibrium quantity falls.</p>
<p>Thus, the average causal effect on the logarithm of traded quantity is</p>
<p><span class="math display">\[
\tau = \mathbb{E}[q_i(r) - q_i(0)]= \frac{\alpha_2 \, \beta_2}{\alpha_2 - \beta_2}\,\log(1+r).
\]</span></p>
<p>Note that the treatment effect depends on the price elasticities of supply and demand. Therefore, we need to identify the demand and supply functions using instruments, since the observed quantities and prices cannot be used directly due to the issue of simultaneous causality. In particular, given the assumption of exogeneity of the other control variables, the only part of <span class="math inline">\(p_i^{Obs}\)</span> that can correlate with <span class="math inline">\(u_{i1}\)</span> or <span class="math inline">\(u_{i2}\)</span> is the error component</p>
<p><span class="math display">\[
\frac{u_{i2}-u_{i1}}{\beta_2-\alpha_2}.
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\mathbb{E}[u_{i1}p_i^{Obs}]
=\frac{1}{\beta_2-\alpha_2}\,\mathbb{E}\!\big[u_{i1}(u_{i2}-u_{i1})\big]
=\frac{\operatorname{Cov}(u_{i1},u_{i2})-\operatorname{Var}(u_{i1})}{\beta_2-\alpha_2}\neq 0,
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}[u_{i2}p_i^{Obs}]
=\frac{1}{\beta_2-\alpha_2}\,\mathbb{E}\!\big[u_{i2}(u_{i2}-u_{i1})\big]
=\frac{\operatorname{Var}(u_{i2})-\operatorname{Cov}(u_{i1},u_{i2})}{\beta_2-\alpha_2}\neq 0,
\]</span></p>
<p>due to the usual slopes <span class="math inline">\(\beta_2&lt;0&lt;\alpha_2\)</span> so that <span class="math inline">\(\alpha_2-\beta_2&gt;0\)</span>.</p>
<p>We can use supply shifters to identify the demand, and demand shifters to identify the supply. In this setting, the exchange rate serves as an instrument to identify the demand, while income together with the prices of complementary and substitute goods serve as instruments to identify the supply. Thus, we can use the moment conditions,</p>
<p><span class="math display">\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    y_i\\
    pc_i\\
    ps_i\\
    er_i
\end{bmatrix}(q_i^{Obs}-\beta_1-\beta_2p_i^{Obs}-\beta_3y_i-\beta_4pc_i-\beta_5ps_i)\right]=\mathbf{0},
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    y_i\\
    pc_i\\
    ps_i\\
    er_i
\end{bmatrix}(q_i^{Obs}-\alpha_1-\alpha_2p_i^{Obs}-\alpha_3er_i)\right]=\mathbf{0}
\]</span></p>
<p>to identify the demand and supply functions. Note that the demand equation is <em>exactly identified</em>, whereas the supply equation is <em>over-identified</em>. See Section <a href="sec71.html#sec71">7.1</a> for details about identification of linear systems of equations.</p>
<p>We perform a simulation exercise to analyze the hypothetical causal effects of a new tax rate of 10% simulating the demand and supply equations using the following structural parameters <span class="math inline">\(\boldsymbol{\beta} = \left[ 5 \ -0.5 \ 0.8 \ -0.4 \ 0.7 \right]^{\top}\)</span>, <span class="math inline">\(\boldsymbol{\alpha} = \left[ -2 \ 0.5 \ -0.4 \right]^{\top}\)</span>, <span class="math inline">\(u_1 \sim N(0, 0.5^2)\)</span>, and <span class="math inline">\(u_2 \sim N(0, 0.5^2)\)</span> assuming a sample size equal to 5,000. Additionally, assume that <span class="math inline">\(y \sim N(10, 1)\)</span>, <span class="math inline">\(pc \sim N(5, 1)\)</span>, <span class="math inline">\(ps \sim N(5, 1)\)</span>, and <span class="math inline">\(er \sim N(15, 1)\)</span>.</p>
<p>The population causal effect of the tax on quantity is</p>
<p><span class="math display">\[
\tau=\frac{(-0.5)\times 0.5}{0.5-(-0.5)}\log(1+0.1)\approx -0.0238,
\]</span></p>
<p>which implies that a 10% tax reduces traded quantity by approximately 2.4%.</p>
<p>In Exercise 11, you are asked to program a BETEL algorithm from scratch to perform inference in this example. Th following figure displays the posterior distribution of the causal effect. The 95% credible interval contains the population value, and the posterior mean lies close to it.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="figures/FigChap13_15.png" alt="Posterior distribution of the causal effect of a tax: BETEL" width="200px" />
<p class="caption">
Figure 13.15: Posterior distribution of the causal effect of a tax: BETEL
</p>
</div>
<p><span class="citation">Chib, Shin, and Simoni (<a href="#ref-chib2018moment">2018</a>)</span> propose a unified framework based on marginal likelihoods and Bayes factors for comparing different moment-restricted models and for discarding misspecified restrictions. They demonstrate the model selection consistency of the marginal likelihood, showing that it favors the specification with the fewest parameters and the largest number of valid moment restrictions. When models are misspecified, the marginal likelihood procedure selects the model that is closest to the (unknown) true data-generating process in terms of Kullback–Leibler divergence. See <span class="citation">Nicole A. Lazar (<a href="#ref-lazar2021review">2021</a>)</span> and <span class="citation">Liu and Zhao (<a href="#ref-liu2023review">2023</a>)</span> for comprehensive reviews of recent advances in empirical likelihood and exponentially tilted empirical likelihood methods, including their Bayesian variants.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-chib2018moment" class="csl-entry">
Chib, Siddhartha, Minchul Shin, and Anna Simoni. 2018. <span>“Bayesian Estimation and Comparison of Moment Condition Models.”</span> <em>Journal of the American Statistical Association</em> 113 (524): 1656–68. <a href="https://doi.org/10.1080/01621459.2017.1358172">https://doi.org/10.1080/01621459.2017.1358172</a>.
</div>
<div id="ref-hernan2020causal" class="csl-entry">
Hernán, Miguel A., and James M. Robins. 2020. <em>Causal Inference: What If</em>. Boca Raton, FL: Chapman &amp; Hall/CRC. <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/">https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/</a>.
</div>
<div id="ref-imbens2014ivperspective" class="csl-entry">
Imbens, Guido W. 2014. <span>“Instrumental Variables: An Econometrician’s Perspective.”</span> <em>Statistical Science</em> 29 (3): 323–58. <a href="https://doi.org/10.1214/14-STS480">https://doi.org/10.1214/14-STS480</a>.
</div>
<div id="ref-lazar2021review" class="csl-entry">
Lazar, Nicole A. 2021. <span>“A Review of Empirical Likelihood.”</span> <em>Annual Review of Statistics and Its Application</em> 8 (1): 329–44.
</div>
<div id="ref-lazar2003bel" class="csl-entry">
Lazar, Nicole A. 2003. <span>“Bayesian Empirical Likelihood.”</span> <em>Biometrika</em> 90 (2): 319–26. <a href="https://doi.org/10.1093/biomet/90.2.319">https://doi.org/10.1093/biomet/90.2.319</a>.
</div>
<div id="ref-liu2023review" class="csl-entry">
Liu, Pangpang, and Yichuan Zhao. 2023. <span>“A Review of Recent Advances in Empirical Likelihood.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 15 (3): e1599.
</div>
<div id="ref-schennach2005betel" class="csl-entry">
Schennach, Susanne M. 2005. <span>“Bayesian Exponentially Tilted Empirical Likelihood.”</span> <em>Biometrika</em> 92 (1): 31–46. <a href="https://doi.org/10.1093/biomet/92.1.31">https://doi.org/10.1093/biomet/92.1.31</a>.
</div>
<div id="ref-schennach2007etel" class="csl-entry">
———. 2007. <span>“Point Estimation with Exponentially Tilted Empirical Likelihood.”</span> <em>The Annals of Statistics</em> 35 (2): 634–72. <a href="https://doi.org/10.1214/009053606000001208">https://doi.org/10.1214/009053606000001208</a>.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
<div id="ref-zellner1996bmom" class="csl-entry">
Zellner, Arnold. 1996a. <span>“Bayesian Method of Moments (BMOM) Analysis of Mean and Regression Models.”</span> In <em>Modelling and Prediction Honoring Seymour Geisser</em>, edited by Jack C. Lee, Wesley O. Johnson, and Arnold Zellner, 61–72. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-4612-2414-3_4">https://doi.org/10.1007/978-1-4612-2414-3_4</a>.
</div>
<div id="ref-zellner1996introduction" class="csl-entry">
———. 1996b. <span>“Introduction to Bayesian Inference in Econometrics.”</span>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Available at <em><a href="https://apps.olin.wustl.edu/faculty/chib/rpackages/betel/" class="uri">https://apps.olin.wustl.edu/faculty/chib/rpackages/betel/</a></em>.<a href="sec12_9.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Another potential solution for the omission of relevant variables is the use of <em>proxy variables</em>; see <span class="citation">Wooldridge (<a href="#ref-wooldridge2010econometric">2010</a>)</span> for details.<a href="sec12_9.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>This identification issue is not peculiar to simultaneous equation models, it arises in other econometric/statistical models <span class="citation">Zellner (<a href="#ref-zellner1996introduction">1996b</a>)</span>. See for instance the example of the effects of vitamin A.<a href="sec12_9.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12_8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec13_11.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/10-Diagnostics.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
