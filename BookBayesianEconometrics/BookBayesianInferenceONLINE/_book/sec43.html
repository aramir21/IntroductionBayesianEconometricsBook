<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-05-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec42.html"/>
<link rel="next" href="sec44.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec43" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Linear regression: The conjugate normal-normal/inverse gamma model<a href="sec43.html#sec43" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this setting, we analyze the conjugate normal-normal/inverse gamma model, which is a cornerstone in econometrics. In this model, the dependent variable <span class="math inline">\(y_i\)</span> is related to a set of regressors <span class="math inline">\(\boldsymbol{x}_i = [x_{i1} \ x_{i2} \ \dots \ x_{iK}]^{\top}\)</span> in a linear way, that is:
<span class="math display">\[
y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_K x_{iK} + \mu_i = \boldsymbol{x}_i^{\top} \boldsymbol{\beta} + \mu_i,
\]</span>
where <span class="math inline">\(\boldsymbol{\beta} = [\beta_1 \ \beta_2 \ \dots \ \beta_K]^{\top}\)</span> and <span class="math inline">\(\mu_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span> is a stochastic error such that <span class="math inline">\(\mathbb{E}[\mu_i \mid \boldsymbol{x}_i] = 0\)</span>.</p>
<p>Defining the vectors and matrices:
<span class="math display">\[
\boldsymbol{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{bmatrix}, \quad
\boldsymbol{X} = \begin{bmatrix} x_{11} &amp; x_{12} &amp; \dots &amp; x_{1K} \\ x_{21} &amp; x_{22} &amp; \dots &amp; x_{2K} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ x_{N1} &amp; x_{N2} &amp; \dots &amp; x_{NK} \end{bmatrix}, \quad
\boldsymbol{\mu} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_N \end{bmatrix},
\]</span></p>
<p>we can write the model in matrix form as:
<span class="math display">\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\mu},
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I})\)</span>. This implies that:
<span class="math display">\[
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}).\]</span></p>
<p>In regression analysis, to simplify notation, we depart from the conventional statistical notation, which defines lowercase letters as realizations of random variables, typically denoted by uppercase letters. We hope it is clear from the context when we refer to random vectors and matrices, and their realizations. Thus, we use bold lowercase letters for vectors and bold uppercase letters for matrices. This applies to the rest of the book.</p>
<p>Thus, the likelihood function is:
<span class="math display">\[\begin{align*}
    p({\boldsymbol{y}}\mid \boldsymbol{\beta}, \sigma^2, {{\boldsymbol{X}}}) &amp; = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta})^{\top}({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta}) \right\}  \\
    &amp; \propto (\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta})^{\top}({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta}) \right\}.
\end{align*}\]</span></p>
<p>The conjugate priors for the parameters are
<span class="math display">\[\begin{align*}
    \boldsymbol{\beta}\mid \sigma^2 &amp; \sim N(\boldsymbol{\beta}_0, \sigma^2 {\boldsymbol{B}}_0),\\
    \sigma^2 &amp; \sim IG(\alpha_0/2, \delta_0/2).
\end{align*}\]</span></p>
<p>Then, the posterior distribution is
<span class="math display">\[\begin{align*}
    \pi(\boldsymbol{\beta},\sigma^2\mid \boldsymbol{y},\boldsymbol{X})&amp;\propto (\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta})^{\top}({\boldsymbol{y}} - {\boldsymbol{X}}\boldsymbol{\beta}) \right\} \\
    &amp; \times (\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\boldsymbol{\beta} - \boldsymbol{\beta}_0)^{\top}{\boldsymbol{B}}_0^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_0)\right\} \\
    &amp; \times \frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp \left\{-\frac{\delta_0}{2\sigma^2} \right\} \\
    &amp; \propto (\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} [\boldsymbol{\beta}^{\top}({\boldsymbol{B}}_0^{-1} + {\boldsymbol{X}}^{\top}{\boldsymbol{X}})\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\top}({\boldsymbol{B}}_0^{-1}\boldsymbol{\beta}_0 + {\boldsymbol{X}}^{\top}{\boldsymbol{X}}\hat{\boldsymbol{\beta}})] \right\} \\
    &amp; \times \left(\frac{1}{\sigma^2}\right)^{(\alpha_0+N)/2+1}\exp \left\{-\frac{\delta_0+ {\boldsymbol{y}}^{\top}{\boldsymbol{y}} + \boldsymbol{\beta}_0^{\top}{\boldsymbol{B}}_0^{-1}\boldsymbol{\beta}_0}{2\sigma^2} \right\},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\beta}}=({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}{\boldsymbol{X}}^{\top}{\boldsymbol{y}}\)</span> is the maximum likelihood estimator.</p>
<p>Adding and subtracting <span class="math inline">\(\boldsymbol{\beta}_n^{\top}{{\boldsymbol{B}}}_n^{-1} \boldsymbol{\beta}_n\)</span> to complete the square, where <span class="math inline">\(\boldsymbol{B}_n = (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_n = \boldsymbol{B}_n(\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0 + \boldsymbol{X}^{\top}\boldsymbol{X}\hat{\boldsymbol{\beta}})\)</span>,
<span class="math display">\[\begin{align*}
    \pi(\boldsymbol{\beta},\sigma^2\mid \boldsymbol{y},\boldsymbol{X})&amp;\propto \underbrace{(\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}{\boldsymbol{B}}^{-1}_n(\boldsymbol{\beta}-\boldsymbol{\beta}_n) \right\}}_1 \\
    &amp; \times \underbrace{(\sigma^2)^{-\left(\frac{\alpha_n}{2}+1 \right)} \exp \left\{-\frac{\delta_n}{2\sigma^2} \right\}}_2.
\end{align*}\]</span></p>
<p>The first expression is the kernel of a normal density function, <span class="math inline">\(\boldsymbol{\beta}\mid \sigma^2, \boldsymbol{y}, \boldsymbol{X} \sim N(\boldsymbol{\beta}_n, \sigma^2\boldsymbol{B}_n)\)</span>. The second expression is the kernel of a inverse gamma density, <span class="math inline">\(\sigma^2\mid  \boldsymbol{y}, \boldsymbol{X}\sim IG(\alpha_n/2, \delta_n/2)\)</span>, where <span class="math inline">\(\alpha_n = \alpha_0 + N\)</span> and <span class="math inline">\(\delta_n = \delta_0 + \boldsymbol{y}^{\top}\boldsymbol{y} + \boldsymbol{\beta}_0^{\top}\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0 - \boldsymbol{\beta}_n^{\top}\boldsymbol{B}_n^{-1}\boldsymbol{\beta}_n\)</span>.</p>
<p>Taking into account that
<span class="math display">\[\begin{align*}\boldsymbol{\beta}_n &amp; = (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top}\boldsymbol{X})^{-1}(\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0 + \boldsymbol{X}^{\top}\boldsymbol{X}\hat{\boldsymbol{\beta}})\\
    &amp; = (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0 + (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top}\boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\boldsymbol{X}\hat{\boldsymbol{\beta}},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(({\boldsymbol{B}}_0^{-1} + {\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}{\boldsymbol{B}}_0^{-1}=\boldsymbol{I}_K-({\boldsymbol{B}}_0^{-1} + {\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}\)</span> <span class="citation">(<a href="#ref-Smith1973">Smith 1973</a>)</span>. Setting <span class="math inline">\({\boldsymbol{W}}=({\boldsymbol{B}}_0^{-1} + {\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}\)</span> we have <span class="math inline">\(\boldsymbol{\beta}_n=(\boldsymbol{I}_K-{\boldsymbol{W}})\boldsymbol{\beta}_0+{\boldsymbol{W}}\hat{\boldsymbol{\beta}}\)</span>, that is, the posterior mean of <span class="math inline">\(\boldsymbol{\beta}\)</span> is a weighted average between the sample and prior information, where the weights depend on the precision of each piece of information. Observe that when the prior covariance matrix is highly vague (non–informative), such that <span class="math inline">\({\boldsymbol{B}}_0^{-1}\rightarrow \boldsymbol{0}_K\)</span>, we obtain <span class="math inline">\({\boldsymbol{W}} \rightarrow I_K\)</span>, such that <span class="math inline">\(\boldsymbol{\beta}_n \rightarrow \hat{\boldsymbol{\beta}}\)</span>, that is, the posterior mean location parameter converges to the maximum likelihood estimator.</p>
<p>In addition, we know that the posterior conditional covariance matrix of the location parameters <span class="math inline">\(\sigma^2({\boldsymbol{B}}_0^{-1} + {\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}=\sigma^2({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}-\sigma^2\left(({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}({\boldsymbol{B}}_0 + ({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1})^{-1}({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}\right)\)</span> is positive semi-definite.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Given that <span class="math inline">\(\sigma^2({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}\)</span> is the covariance matrix of the maximum likelihood estimator, we observe that prior information reduces estimation uncertainty.</p>
<p>Another way to see this model is by considering that both <span class="math inline">\(\boldsymbol{y}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> are treated as random variables under the Bayesian framework. Thus, we can express the joint distribution of these two vectors as follows:
<span class="math display">\[\begin{align*}
    \begin{bmatrix}
        \boldsymbol{\beta}\\
        \boldsymbol{y}
    \end{bmatrix}\sim N\left [ \begin{pmatrix}
        \boldsymbol{\beta}_{0} \\
        \boldsymbol{X}\boldsymbol{\beta}_{0}
    \end{pmatrix} , \sigma^2\begin{pmatrix}
        \boldsymbol{B}_{0} &amp; \boldsymbol{B}_{0} \boldsymbol{X}^{\top} \\
        \boldsymbol{X}\boldsymbol{B}_{0}^{\top} &amp; \boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+\boldsymbol{I}_N
    \end{pmatrix}\right ],
\end{align*}\]</span>
where we use that
<span class="math display">\[\begin{align*}
Cov[\boldsymbol{\beta},\boldsymbol{y}\mid \boldsymbol{X}]&amp;=\mathbb{E}[\boldsymbol{\beta}\boldsymbol{y}^{\top}]-\mathbb{E}[\boldsymbol{\beta}]\mathbb{E}[\boldsymbol{y}^{\top}]\\
&amp;=\mathbb{E}[\boldsymbol{\beta}(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\mu})^{\top}]-\mathbb{E}[\boldsymbol{\beta}]\mathbb{E}[\boldsymbol{y}^{\top}]\\
&amp;=[Var[\boldsymbol{\beta}]+\mathbb{E}[\boldsymbol{\beta}]\mathbb{E}[\boldsymbol{\beta}^{\top}]]\boldsymbol{X}^{\top}-\mathbb{E}[\boldsymbol{\beta}]\mathbb{E}[\boldsymbol{y}^{\top}]\\
&amp;=\sigma^2\boldsymbol{B}_0\boldsymbol{X}^{\top}+\boldsymbol{\beta}_0\boldsymbol{\beta}_0^{\top}\boldsymbol{X}^{\top}-\boldsymbol{\beta}_0\boldsymbol{\beta}_0^{\top}\boldsymbol{X}^{\top}\\
&amp;=\sigma^2\boldsymbol{B}_0\boldsymbol{X}^{\top}.
\end{align*}\]</span></p>
<p>Then, we can obtain the conditional distribution of <span class="math inline">\(\boldsymbol{\beta} \mid \boldsymbol{y}\)</span> using the properties of the multivariate normal distribution. This distribution is normal with mean equal to
<span class="math display" id="eq:416">\[\begin{align}
\boldsymbol{\beta}_{0} + \boldsymbol{B}_{0} \boldsymbol{X}^{\top} \left( \boldsymbol{X} \boldsymbol{B}_{0} \boldsymbol{X}^{\top} + \boldsymbol{I}_N \right)^{-1} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}_{0}),
\tag{3.5}
\end{align}\]</span></p>
<p>and covariance matrix
<span class="math display">\[
\sigma^2 \left( \boldsymbol{B}_{0} - \boldsymbol{B}_{0} \boldsymbol{X}^{\top} \left( \boldsymbol{X} \boldsymbol{B}_{0} \boldsymbol{X}^{\top} + \boldsymbol{I}_N \right)^{-1} \boldsymbol{X} \boldsymbol{B}_{0}^{\top} \right).
\]</span></p>
<p>Observe that in this representation, the posterior mean is equal to the prior mean plus a correction term that takes into account the deviation between the observations and the prior expected value (<span class="math inline">\(\boldsymbol{X} \boldsymbol{\beta}_{0}\)</span>). The weight of this correction is given by the matrix <span class="math inline">\(\boldsymbol{B}_{0} \boldsymbol{X}^{\top} \left( \boldsymbol{X} \boldsymbol{B}_{0} \boldsymbol{X}^{\top} + \boldsymbol{I}_N \right)^{-1}\)</span>.</p>
<p>This form of expressing the posterior distribution is relevant for gaining some intuition on Bayesian inference in time series models within the <em>Gaussian linear state-space representation</em> in Chapter <a href="Chap8.html#Chap8">8</a>, also known as the Kalman filter in time series literature.</p>
<p>We can show that both conditional posterior distributions are the same. In particular, the posterior mean in Equation <a href="sec43.html#eq:416">(3.5)</a> is <span class="math inline">\([\boldsymbol{I}_K-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+ \boldsymbol{I}_N)^{-1}\boldsymbol{X}]\boldsymbol{\beta}_{0}+\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+ \boldsymbol{I}_N)^{-1}\boldsymbol{y}\)</span>, where
<span class="math display">\[\begin{align*}
        \boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+ \boldsymbol{I}_N)^{-1}
        &amp;=\boldsymbol{B}_{0}\boldsymbol{X}^{\top}[\boldsymbol{I}_N-\boldsymbol{I}_N\boldsymbol{X}(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{I}_N\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{I}_N]\\
        &amp;=\boldsymbol{B}_{0}[\boldsymbol{I}_K-\boldsymbol{X}^{\top}\boldsymbol{I}_N\boldsymbol{X}(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{I}_N\boldsymbol{X})^{-1}]\boldsymbol{X}^{\top}\\
        &amp;=\boldsymbol{B}_{0}[\boldsymbol{I}_K-[\boldsymbol{I}_K-\boldsymbol{B}_0^{-1}(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{I}_N\boldsymbol{X})^{-1}]]\boldsymbol{X}^{\top}\\
        &amp;=(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top},
\end{align*}\]</span>
where the first equality uses the Woodbury matrix identity (matrix inversion lemma), and the third equality uses <span class="math inline">\(\boldsymbol{D}(\boldsymbol{D}+\boldsymbol{E})^{-1}=\boldsymbol{I}-\boldsymbol{E}(\boldsymbol{D}+\boldsymbol{E})^{-1}\)</span>.</p>
<p>Thus,
<span class="math display">\[\begin{align*}
\boldsymbol{\beta}_n&amp;=[\boldsymbol{I}_K-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+ \boldsymbol{I}_N)^{-1}\boldsymbol{X}]\boldsymbol{\beta}_{0}+\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^{\top}+ \boldsymbol{I}_N)^{-1}\boldsymbol{y}\\
&amp;=[\boldsymbol{I}_K-(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{X}]\boldsymbol{\beta}_{0}+(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{y}\\
&amp;=[\boldsymbol{I}_K-(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{X}]\boldsymbol{\beta}_{0}+(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{X}\hat{\boldsymbol{\beta}}
\end{align*}\]</span>
Again, we see that the posterior mean is a weighted average between the prior mean, and the maximum likelihood estimator.</p>
<p>The equality of variances of both approaches is as follows:
<span class="math display">\[\begin{align*}
        Var[\boldsymbol{\beta}\mid \boldsymbol{y}]&amp;
        = \sigma^2(\boldsymbol{B}_{0}-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{B}_{0}\boldsymbol{X}^\top+\boldsymbol{I}_N)^{-1} \boldsymbol{X}\boldsymbol{B}_{0})\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}(\boldsymbol{I}_N- \boldsymbol{I}_N\boldsymbol{X}(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{I}_N\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{I}_N)\boldsymbol{X}\boldsymbol{B}_{0})\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}\boldsymbol{X}\boldsymbol{B}_{0}+ \boldsymbol{B}_{0}\boldsymbol{X}^{\top}\boldsymbol{X}(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{X}^{\top}\boldsymbol{X}\boldsymbol{B}_{0})\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}\boldsymbol{X}\boldsymbol{B}_{0}+ \boldsymbol{B}_{0}\boldsymbol{X}^{\top}\boldsymbol{X}[\boldsymbol{I}_K-(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}\boldsymbol{B}_{0}^{-1}]\boldsymbol{B}_{0})\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}-\boldsymbol{B}_{0}\boldsymbol{X}^{\top}\boldsymbol{X}(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1})\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}[\boldsymbol{I}_K-\boldsymbol{X}^{\top}\boldsymbol{X}(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1}])\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}[\boldsymbol{I}_K-(\boldsymbol{I}_K-\boldsymbol{B}_{0}^{-1}(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1})])\\
        &amp;=\sigma^2(\boldsymbol{B}_{0}^{-1}+\boldsymbol{X}^{\top}\boldsymbol{X})^{-1},
\end{align*}\]</span>
where the second equality uses the Woodbury matrix identity, the fourth equality uses <span class="math inline">\((\boldsymbol{D}+\boldsymbol{E})^{-1}\boldsymbol{D}=\boldsymbol{I}-(\boldsymbol{D}+\boldsymbol{E})^{-1}\boldsymbol{E}\)</span>, and the seventh equality uses <span class="math inline">\(\boldsymbol{D}(\boldsymbol{D}+\boldsymbol{E})^{-1}=\boldsymbol{I}-\boldsymbol{E}(\boldsymbol{D}+\boldsymbol{E})^{-1}\)</span>.</p>
<p>Now, we calculate the posterior marginal distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> following the standard approach,
<span class="math display">\[\begin{align*}
    \pi(\boldsymbol{\beta}\mid {\boldsymbol{y}},{\boldsymbol{X}}) &amp; = \int_0^{\infty} \pi(\boldsymbol{\beta}, \sigma^2\mid {\boldsymbol{y}},{\boldsymbol{X}}) d\sigma^2 \\
    &amp; = \int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+K}{2} + 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2,
\end{align*}\]</span>
where <span class="math inline">\(s = \delta_n + (\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}{{\boldsymbol{B}}}_n^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)\)</span>. Then we can write
<span class="math display">\[\begin{align*}
    \pi(\boldsymbol{\beta}\mid {\boldsymbol{y}},{\boldsymbol{X}}) &amp; = \int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+K}{2} + 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2 \\
    &amp; = \frac{\Gamma((\alpha_n+K)/2)}{(s/2)^{(\alpha_n+K)/2}} \int_0^{\infty} \frac{(s/2)^{(\alpha_n+K)/2}}{\Gamma((\alpha_n+K)/2)} (\sigma^2)^{-(\alpha_n+K)/2 - 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2.
\end{align*}\]</span></p>
<p>The right term is the integral of the probability density function of an inverse gamma distribution with parameters <span class="math inline">\(\nu = (\alpha_n+K)/2\)</span> and <span class="math inline">\(\tau = s/2\)</span>. Since we are integrating over the whole support of <span class="math inline">\(\sigma^2\)</span>, the integral is equal to 1, and therefore
<span class="math display">\[\begin{align*}
    \pi(\boldsymbol{\beta}\mid {\boldsymbol{y}},{\boldsymbol{X}}) &amp; = \frac{\Gamma((\alpha_n+K)/2)}{(s/2)^{(\alpha_n+K)/2}} \\
    &amp; \propto s^{-(\alpha_n+K)/2} \\
    &amp; = [\delta_n + (\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}{{\boldsymbol{B}}}_n^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)]^{-(\alpha_n+K)/2} \\
    &amp; = \left[1 + \frac{(\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}\left(\frac{\delta_n}{\alpha_n}{{\boldsymbol{B}}}_n\right)^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)}{\alpha_n}\right]^{-(\alpha_n+K)/2}(\delta_n)^{-(\alpha_N+K)/2} \\
    &amp; \propto \left[1 + \frac{(\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}{\boldsymbol{H}}_n^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)}{\alpha_n}\right]^{-(\alpha_n+K)/2},
\end{align*}\]</span>
where <span class="math inline">\({\boldsymbol{H}}_n = \frac{\delta_n}{\alpha_n}{\boldsymbol{B}}_n\)</span>. This last expression is a multivariate t distribution, that is, <span class="math inline">\(\boldsymbol{\beta}\mid {\boldsymbol{y}},{\boldsymbol{X}} \sim t_K(\alpha_n, \boldsymbol{\beta}_n, {\boldsymbol{H}}_n)\)</span>.</p>
<p>Observe that as we have incorporated the uncertainty of the variance, the posterior for <span class="math inline">\(\boldsymbol{\beta}\)</span> changes from a normal to a t distribution, which has heavier tails, indicating more uncertainty.</p>
<p>The marginal likelihood of this model is
<span class="math display">\[\begin{align*}
    p({\boldsymbol{y}})=\int_0^{\infty}\int_{R^K}\pi (\boldsymbol{\beta} \mid  \sigma^2,{\boldsymbol{B}}_0,\boldsymbol{\beta}_0 )\pi(\sigma^2\mid  \alpha_0/2, \delta_0/2)p({\boldsymbol{y}}\mid \boldsymbol{\beta}, \sigma^2, {\boldsymbol{X}})d\sigma^2 d\boldsymbol{\beta}.
\end{align*}\]</span></p>
<p>Taking into account that <span class="math inline">\(({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})^{\top}({\boldsymbol{y}}-{\boldsymbol{X}}\boldsymbol{\beta})+(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\top}{\boldsymbol{B}}_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)=(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}{\boldsymbol{B}}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)+m\)</span>, where <span class="math inline">\(m={\boldsymbol{y}}^{\top}{\boldsymbol{y}}+\boldsymbol{\beta}_0^{\top}{\boldsymbol{B}}_0^{-1}\boldsymbol{\beta}_0-\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n\)</span>, we have that</p>
<p><span class="math display">\[\begin{align*}
    p({\boldsymbol{y}})&amp;=\int_0^{\infty}\int_{R^K}\pi (\boldsymbol{\beta} \mid  \sigma^2)\pi(\sigma^2)p({\boldsymbol{y}}\mid \boldsymbol{\beta}, \sigma^2, {\boldsymbol{X}})d\sigma^2 d\boldsymbol{\beta}\\
    &amp;=\int_0^{\infty}\pi(\sigma^2) \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{1}{(2\pi\sigma^2)^{K/2}|{\boldsymbol{B}}_0|^{1/2}}\\
    &amp;\times\int_{R^K}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}{\boldsymbol{B}}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)\right\}d\sigma^2 d\boldsymbol{\beta}\\
    &amp;=\int_0^{\infty}\pi(\sigma^2) \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{|{\boldsymbol{B}}_n|^{1/2}}{|{\boldsymbol{B}}_0|^{1/2}}d\sigma^2\\
    &amp;=\int_{0}^{\infty} \frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp\left\{\left(-\frac{\delta_0}{2\sigma^2}\right)\right\} \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{|{\boldsymbol{B}}_n|^{1/2}}{|{\boldsymbol{B}}_0|^{1/2}} d\sigma^2\\
    &amp;= \frac{1}{(2\pi)^{N/2}}\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\frac{|{\boldsymbol{B}}_n|^{1/2}}{|{\boldsymbol{B}}_0|^{1/2}}\int_{0}^{\infty}\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1}\exp\left\{\left(-\frac{\delta_0+m}{2\sigma^2}\right)\right\}d\sigma^2\\
    &amp;= \frac{1}{\pi^{N/2}}\frac{\delta_0^{\alpha_0/2}}{\delta_n^{\alpha_n/2}}\frac{|{\boldsymbol{B}}_n|^{1/2}}{|{\boldsymbol{B}}_0|^{1/2}}\frac{\Gamma(\alpha_n/2)}{\Gamma(\alpha_0/2)}.
\end{align*}\]</span></p>
<p>We can show that
<span class="math display">\[\begin{align*}
\delta_n&amp;=\delta_0 + {\boldsymbol{y}}^{\top}{\boldsymbol{y}} + \boldsymbol{\beta}_0^{\top}{\boldsymbol{B}}_0^{-1}\boldsymbol{\beta}_0 - \boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n\\
&amp;=\delta_0+({\boldsymbol{y}}-{\boldsymbol{X}}\hat{\boldsymbol{\beta}})^{\top}({\boldsymbol{y}}-{\boldsymbol{X}}\hat{\boldsymbol{\beta}})+(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0)^{\top}(({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}+{\boldsymbol{B}}_0)^{-1}(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0).
\end{align*}\]</span>
See Exercise 7.</p>
<p>Therefore, if we want to compare two models under this setting, the Bayes factor is
<span class="math display">\[\begin{align*}
    BF_{12}&amp;=\frac{p(\boldsymbol{y}\mid \mathcal{M}_1)}{p(\boldsymbol{y}\mid \mathcal{M}_2)}\\
    &amp;=\frac{\frac{\delta_{10}^{\alpha_{10}/2}}{\delta_{1n}^{\alpha_{1n}/2}}\frac{|{\boldsymbol{B}}_{1n}|^{1/2}}{|{\boldsymbol{B}}_{10}|^{1/2}}\frac{\Gamma(\alpha_{1n}/2)}{\Gamma(\alpha_{10}/2)}}{\frac{\delta_{20}^{\alpha_{20}/2}}{\delta_{2n}^{\alpha_{2n}/2}}\frac{|{\boldsymbol{B}}_{2n}|^{1/2}}{|{\boldsymbol{B}}_{20}|^{1/2}}\frac{\Gamma(\alpha_{2n}/2)}{\Gamma(\alpha_{20}/2)}},
\end{align*}\]</span></p>
<p>where subscripts 1 and 2 refer to each model.</p>
<p>Observe that, <em>ceteris paribus</em>, the model with better fit, coherence between sample and prior information regarding location parameters, higher prior to posterior precision, and fewer parameters is favored by the Bayes factor. The Bayes factor rewards model fit, as the sum of squared errors appears in <span class="math inline">\(\delta_n\)</span>; the better the fit (i.e., the lower the sum of squared errors), the better the Bayes factor. In addition, a weighted distance between sample and prior location parameters also appears in <span class="math inline">\(\delta_n\)</span>. The greater this distance, the worse the model support. The ratio of determinants between posterior and prior covariance matrices is also present; the higher this ratio, the better the Bayes factor supports a model due to information gains.</p>
<p>To see the effect of a model’s parsimony, let’s consider the common situation in applications where <span class="math inline">\(\boldsymbol{B}_{j0} = c \boldsymbol{I}_{K_j}\)</span>, then <span class="math inline">\(| \boldsymbol{B}_{j0} | = c^{K_j}\)</span>. Hence,
<span class="math display">\[
\left( \frac{| \boldsymbol{B}_{20} |}{| \boldsymbol{B}_{10} |} \right)^{1/2} = \left( \frac{c^{K_2/2}}{c^{K_1/2}} \right),
\]</span></p>
<p>if <span class="math inline">\(\frac{K_2}{K_1} &gt; 1\)</span> and <span class="math inline">\(c \to \infty\)</span> (the latter implying a non-informative prior), then <span class="math inline">\(BF_{12} \to \infty\)</span>. This means infinite evidence supporting the parsimonious model, no matter what the sample information says.</p>
<p>Comparing models having the same number of regressors (<span class="math inline">\(K_1 = K_2\)</span>) is not a safe ground, as <span class="math inline">\(| \boldsymbol{B}_0 |\)</span> depends on the measurement units of the regressors. Conclusions regarding model selection depend on this, which is not a nice property. This prevents using non-informative priors when performing model selection in the Bayesian framework. However, this is not the case when <span class="math inline">\(\alpha_0 \to 0\)</span> and <span class="math inline">\(\delta_0 \to 0\)</span>, which implies a non-informative prior for the variance parameter.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>We observe that <span class="math inline">\(\Gamma(\alpha_{j0})\)</span> cancels out, as <span class="math inline">\(\alpha_0 \to 0\)</span> implies <span class="math inline">\(\alpha_{jn} \to N\)</span>, and
<span class="math display">\[
\delta_{jn} \to ({\boldsymbol{y}} - {\boldsymbol{X}}_j \hat{\boldsymbol{\beta}}_j)^{\top} ({\boldsymbol{y}} - {\boldsymbol{X}}_j \hat{\boldsymbol{\beta}}_j) + (\hat{\boldsymbol{\beta}}_j - \boldsymbol{\beta}_{j0})^{\top} \left( ({\boldsymbol{X}}_j^{\top} {\boldsymbol{X}}_j)^{-1} + \boldsymbol{B}_{j0} \right)^{-1} (\hat{\boldsymbol{\beta}}_j - \boldsymbol{\beta}_{j0}),
\]</span>
when <span class="math inline">\(\delta_0 \to 0,\)</span> therefore, there is no effect. This is due to <span class="math inline">\(\sigma^2\)</span> being a common parameter in both models.</p>
<p>In general, we can use non-informative priors for common parameters across all models, but we cannot use non-informative priors for non-common parameters when performing model selection using the Bayes factor. This issue raises the question of how to set informative priors. On one hand, we have those who advocate for <em>subjective</em> priors <span class="citation">(<a href="#ref-Ramsey1926">Ramsey 1926</a>; <a href="#ref-deFinetti1937">De Finetti 1937</a>; <a href="#ref-savage1954">Savage 1954</a>; <a href="#ref-Lindley2000">Lindley 2000</a>)</span>; on the other hand, those who prefer <em>objective</em> priors <span class="citation">(<a href="#ref-Bayes1763">T. Bayes 1763</a>; <a href="#ref-Laplace1812">Laplace 1812</a>; <a href="#ref-Jeffreys1961">H. Jeffreys 1961</a>; <a href="#ref-Berger2006">J. Berger 2006</a>)</span>.</p>
<p>Regarding the former, eliciting <em>subjective</em> priors, i.e., “formulating a person’s knowledge and beliefs about one or more uncertain quantities into a (joint) probability distribution for those quantities” <span class="citation">(<a href="#ref-garthwaite05">Garthwaite, Kadane, and O’Hagan 2005</a>)</span>, is a very difficult task due to human beings’ heuristics and biases associated with representativeness, information availability, conservatism, overconfidence, and anchoring-and-adjustment issues <span class="citation">(<a href="#ref-tversky74">Tversky and Kahneman 1974</a>)</span>. However, there have been good efforts using predictive and structural elicitation procedures <span class="citation">(<a href="#ref-Kadane80">J. B. Kadane 1980</a>; <a href="#ref-kadane98">J. Kadane and Wolfson 1998</a>)</span>.</p>
<p>Regarding the latter, there are <em>reference priors</em> that are designed to have minimal impact on the posterior distribution and to be invariant to different parametrizations of the model <span class="citation">(<a href="#ref-bernardo2009bayesian">Bernardo and Smith 2009</a>)</span>. A remarkable example of <em>reference priors</em> is the <em>Jeffreys’ prior</em> <span class="citation">(<a href="#ref-jeffreys1946invariant">Harold Jeffreys 1946</a>)</span>, which originated from the critique of <em>non-informative priors</em> that were not invariant to transformations of the parameter space. In particular, the <em>Jeffreys’ prior</em> is given by:
<span class="math display">\[
\pi(\boldsymbol{\theta}) \propto |I(\boldsymbol{\theta})|^{1/2},
\]</span></p>
<p>where <span class="math inline">\(I(\boldsymbol{\theta}) = \mathbb{E}\left(-\frac{\partial^2 \log p(\boldsymbol{y} \mid \boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^{\top}}\right)\)</span>, i.e., <span class="math inline">\(I(\boldsymbol{\theta})\)</span> is the Fisher information matrix. However, the <em>Jeffreys’ prior</em> is often improper, meaning it does not work well for model selection.</p>
<p>Thus, a standard <em>objective</em> approach is to use <em>intrinsic priors</em> <span class="citation">(<a href="#ref-berger1996intrinsic">J. O. Berger and Pericchi 1996</a>)</span>, where a <em>minimal training</em> dataset is used with a <em>reference prior</em> to obtain a proper posterior distribution. This proper distribution is then used as a prior, and the standard Bayesian procedures are followed using the remaining dataset. In this way, we end up with meaningful Bayes factors for model selection.</p>
<p>Regardless of using a <em>subjective</em> or <em>objective</em> approach to define a prior distribution, it is always a good idea to assess the sensitivity of the posterior results to the prior assumptions. This is commonly done using local or pointwise assessments, such as partial derivatives <span class="citation">(<a href="#ref-giordano2022evaluating">Giordano et al. 2022</a>; <a href="#ref-Jacobi2022">Jacobi, Zhu, and Joshi 2022</a>; <a href="#ref-gustafson2000local">Gustafson 2000</a>)</span> or, more often, in terms of multiple evaluations (<em>scenario analysis</em>) <span class="citation">(<a href="#ref-richardson1997bayesian">Richardson and Green 1997</a>; <a href="#ref-kim1999has">Kim and Nelson 1999</a>; <a href="#ref-an2007bayesian">An and Schorfheide 2007</a>)</span>. Recently, <span class="citation">Jacobi et al. (<a href="#ref-jacobi2024posterior">2024</a>)</span> extend these approaches to perform sensitivity analysis in high-dimensional hyperparameter settings.</p>
<p>Returning to the linear model, the posterior predictive is equal to
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;=\int_{0}^{\infty}\int_{R^K}p({\boldsymbol{Y}}_0\mid \boldsymbol{\beta},\sigma^2,{\boldsymbol{y}})\pi(\boldsymbol{\beta}\mid \sigma^2,{\boldsymbol{y}})\pi(\sigma^2\mid {\boldsymbol{y}})d\boldsymbol{\beta} d\sigma^2\\
    &amp;=\int_{0}^{\infty}\int_{R^K}p({\boldsymbol{Y}}_0\mid \boldsymbol{\beta},\sigma^2)\pi(\boldsymbol{\beta}\mid \sigma^2,{\boldsymbol{y}})\pi(\sigma^2\mid {\boldsymbol{y}})d\boldsymbol{\beta} d\sigma^2,
\end{align*}\]</span></p>
<p>where we take into account independence between <span class="math inline">\({\boldsymbol{y}}_0\)</span> and <span class="math inline">\({\boldsymbol{y}}\)</span>. Given <span class="math inline">\({\boldsymbol{X}}_0\)</span>, which is the <span class="math inline">\(N_0\times K\)</span> matrix of regressors associated with <span class="math inline">\({\boldsymbol{y}}_0\)</span>, Then,
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;=\int_{0}^{\infty}\int_{R^K}\left\{ (2\pi\sigma^2)^{-\frac{N_0}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\boldsymbol{y}}_0 - {\boldsymbol{X}}_0\boldsymbol{\beta})^{\top}({\boldsymbol{y}}_0 - {\boldsymbol{X}}_0\boldsymbol{\beta})^{\top} \right\}\right. \\
    &amp; \times (2\pi\sigma^2)^{-\frac{K}{2}} |{\boldsymbol{B}}_n|^{-1/2} \exp \left\{-\frac{1}{2\sigma^2} (\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}{\boldsymbol{B}}_n^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)\right\} \\
    &amp; \left. \times \frac{(\delta_n/2)^{\alpha_n/2}}{\Gamma(\alpha_n/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_n/2+1}\exp \left\{-\frac{\delta_n}{2\sigma^2} \right\}\right\}d\boldsymbol{\beta} d\sigma^2. \\
\end{align*}\]</span></p>
<p>Setting <span class="math inline">\({\boldsymbol{M}}=({\boldsymbol{X}}_0^{\top}{\boldsymbol{X}}_0+{\boldsymbol{B}}_n^{-1})\)</span> and <span class="math inline">\(\boldsymbol{\beta}_*={\boldsymbol{M}}^{-1}({\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{X}}_0^{\top}{\boldsymbol{y}}_0)\)</span>, we have
<span class="math inline">\(({\boldsymbol{y}}_0 - {\boldsymbol{X}}_0\boldsymbol{\beta})^{\top}({\boldsymbol{y}}_0 - {\boldsymbol{X}}_0\boldsymbol{\beta})^{\top}+(\boldsymbol{\beta} - \boldsymbol{\beta}_n)^{\top}{\boldsymbol{B}}_n^{-1}(\boldsymbol{\beta} - \boldsymbol{\beta}_n)=(\boldsymbol{\beta} - \boldsymbol{\beta}_*)^{\top}{\boldsymbol{M}}(\boldsymbol{\beta} - \boldsymbol{\beta}_*)+\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-\boldsymbol{\beta}_*^{\top}{\boldsymbol{M}}\boldsymbol{\beta}_*\)</span>.
Thus,</p>
<p><span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;\propto\int_{0}^{\infty}\left\{\left(\frac{1}{\sigma^2}\right)^{-\frac{K+N_0+\alpha_n}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-\boldsymbol{\beta}_*^{\top}{\boldsymbol{M}}\boldsymbol{\beta}_*+\delta_n)\right\}\right.\\
    &amp;\times\left.\int_{R^K}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{\beta} - \boldsymbol{\beta}_*)^{\top}{\boldsymbol{M}}(\boldsymbol{\beta} - \boldsymbol{\beta}_*)\right\}d\boldsymbol{\beta}\right\} d\sigma^2,\\
\end{align*}\]</span></p>
<p>where the term in the second integral is the kernel of a multivariate normal density with mean <span class="math inline">\(\boldsymbol{\beta}_*\)</span> and covariance matrix <span class="math inline">\(\sigma^2{\boldsymbol{M}}^{-1}\)</span>. Then,
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;\propto\int_{0}^{\infty}\left(\frac{1}{\sigma^2}\right)^{\frac{N_0+\alpha_n}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-\boldsymbol{\beta}_*^{\top}{\boldsymbol{M}}\boldsymbol{\beta}_*+\delta_n)\right\}d\sigma^2,\\
\end{align*}\]</span></p>
<p>which is the kernel of an inverse gamma density. Thus,
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;\propto \left[\frac{\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-\boldsymbol{\beta}_*^{\top}{\boldsymbol{M}}\boldsymbol{\beta}_*+\delta_n}{2}\right]^{-\frac{\alpha_n+N_0}{2}}.
\end{align*}\]</span></p>
<p>Setting <span class="math inline">\({\boldsymbol{C}}^{-1}={\boldsymbol{I}}_{N_0}+{\boldsymbol{X}}_0{\boldsymbol{B}}_n{\boldsymbol{X}}_0^{\top}\)</span> such that <span class="math inline">\({\boldsymbol{C}}={\boldsymbol{I}}_{N_0}-{\boldsymbol{X}}_0({\boldsymbol{B}}_n^{-1}+{\boldsymbol{X}}_0^{\top}{\boldsymbol{X}}_0)^{-1}{\boldsymbol{X}}_0^{\top}={\boldsymbol{I}}_{N_0}-{\boldsymbol{X}}_0{\boldsymbol{M}}^{-1}{\boldsymbol{X}}_0^{\top}\)</span>, and <span class="math inline">\({\boldsymbol{\boldsymbol{\beta}}}_{**}={\boldsymbol{C}}^{-1}{\boldsymbol{X}}_0{\boldsymbol{M}}^{-1}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n\)</span>, then</p>
<p><span class="math display">\[\begin{align*}
    \boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-\boldsymbol{\beta}_*^{\top}{\boldsymbol{M}}\boldsymbol{\beta}_*&amp;=
    \boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{y}}_0-(\boldsymbol{\beta}_n^{\top}{\boldsymbol{B}}_n^{-1}+{\boldsymbol{y}}_0^{\top}{\boldsymbol{X}}_0){\boldsymbol{M}}^{-1}({\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{X}}_0^{\top}{\boldsymbol{y}}_0)\\
    &amp;=\boldsymbol{\beta}_n^{\top}({\boldsymbol{B}}_n^{-1}-{\boldsymbol{B}}_n^{-1}{\boldsymbol{M}}^{-1}{\boldsymbol{B}}_n^{-1})\boldsymbol{\beta}_n+{\boldsymbol{y}}_0^{\top}{\boldsymbol{C}}{\boldsymbol{y}}_0\\
    &amp;-2{\boldsymbol{y}}_0^{\top}{\boldsymbol{C}}{\boldsymbol{C}}^{-1}{\boldsymbol{X}}_0{\boldsymbol{M}}^{-1}{\boldsymbol{B}}_n^{-1}\boldsymbol{\beta}_n+{\boldsymbol{\boldsymbol{\beta}}}_{**}^{\top}{\boldsymbol{C}}{\boldsymbol{\boldsymbol{\beta}}}_{**}-{\boldsymbol{\boldsymbol{\beta}}}_{**}^{\top}{\boldsymbol{C}}{\boldsymbol{\boldsymbol{\beta}}}_{**}\\
    &amp;=\boldsymbol{\beta}_n^{\top}({\boldsymbol{B}}_n^{-1}-{\boldsymbol{B}}_n^{-1}{\boldsymbol{M}}^{-1}{\boldsymbol{B}}_n^{-1})\boldsymbol{\beta}_n+({\boldsymbol{y}}_0-{\boldsymbol{\boldsymbol{\beta}}}_{**})^{\top}{\boldsymbol{C}}({\boldsymbol{y}}_0-{\boldsymbol{\boldsymbol{\beta}}}_{**})\\
    &amp;-{\boldsymbol{\boldsymbol{\beta}}}_{**}^{\top}{\boldsymbol{C}}{\boldsymbol{\boldsymbol{\beta}}}_{**},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\beta}_n^{\top}({\boldsymbol{B}}_n^{-1}-{\boldsymbol{B}}_n^{-1}{\boldsymbol{M}}^{-1}{\boldsymbol{B}}_n^{-1})\boldsymbol{\beta}_n={\boldsymbol{\boldsymbol{\beta}}}_{**}^{\top}{\boldsymbol{C}}{\boldsymbol{\boldsymbol{\beta}}}_{**}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{**}={\boldsymbol{X}}_0\boldsymbol{\beta}_n\)</span> (see Exercise 8).</p>
<p>Then,
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{y}}_0\mid {\boldsymbol{y}})&amp;\propto\left[\frac{({\boldsymbol{y}}_0-{\boldsymbol{X}}_0\boldsymbol{\beta}_n)^{\top}{\boldsymbol{C}}({\boldsymbol{y}}_0-{\boldsymbol{X}}_0\boldsymbol{\beta}_n)+\delta_n}{2}\right]^{-\frac{\alpha_n+N_0}{2}}\\
    &amp;\propto\left[\frac{({\boldsymbol{y}}_0-{\boldsymbol{X}}_0\boldsymbol{\beta}_n)^{\top}\left(\frac{{\boldsymbol{C}}\alpha_n}{\delta_n}\right)({\boldsymbol{y}}_0-{\boldsymbol{X}}_0\boldsymbol{\beta}_n)}{\alpha_n}+1\right]^{-\frac{\alpha_n+N_0}{2}}.
\end{align*}\]</span></p>
<p>The posterior predictive is a multivariate t distribution, <span class="math inline">\({\boldsymbol{y}}_0\mid {\boldsymbol{y}}\sim t\left({\boldsymbol{X}}_0\boldsymbol{\beta}_n,\frac{\delta_n({\boldsymbol{I}}_{N_0}+{\boldsymbol{X}}_0{\boldsymbol{B}}_n{\boldsymbol{X}}_0^{\top})}{\alpha_n},\alpha_n\right)\)</span>.</p>
<p><strong>Example: Demand of electricity</strong></p>
<p>We study in this example the determinants of the monthly demand for electricity by Colombian households. The data consists of information from 2103 households, including the following variables: the average price (USD/kWh), indicators of the socioeconomic conditions of the neighborhood where the household is located (with <em>IndSocio1</em> being the lowest and <em>IndSocio3</em> being the highest), an indicator for whether the household is located in a municipality that is above 1000 meters above sea level, the number of rooms in the house, the number of members in the household, the presence of children in the household (where 1 indicates yes), and the monthly income (USD). The specification is as follows:
<span class="math display">\[\begin{align*}
    \log(\text{Electricity}_i) &amp; = \beta_1\log(\text{price}_i) + \beta_2\text{IndSocio1}_i + \beta_3\text{IndSocio2}_i + \beta_4\text{Altitude}_i \\
    &amp; + \beta_5\text{Nrooms}_i + \beta_6\text{HouseholdMem}_i + \beta_7\text{Children}_i\\
    &amp; + \beta_8\log(\text{Income}_i) + \beta_9 + \mu_i.
\end{align*}\]</span></p>
<p>We use a non-informative vague prior setting such that <span class="math inline">\(\alpha_0=\delta_0=0.001\)</span>, <span class="math inline">\(\boldsymbol{\beta}_0=\boldsymbol{0}\)</span> and <span class="math inline">\(\boldsymbol{B}_0=c_0\boldsymbol{I}_K\)</span>, where <span class="math inline">\(c_0=1000\)</span> and <span class="math inline">\(K\)</span> is the number of regressors.</p>
<p>The results from the <strong>R</strong> code (see below) indicate that the posterior mean of the own-price elasticity of electricity demand is -1.09, and the 95% symmetric credible interval is (-1.47, -0.71). Households in neighborhoods with low socioeconomic conditions and those located in municipalities situated more than 1000 meters above sea level consume less electricity, with reductions of 32.7% and 19.7% on average, respectively. An additional room leads to an 8.7% increase in electricity consumption, and each additional household member increases consumption by 5.9% on average. The mean estimate for income elasticity is 0.074, meaning that a 10% increase in income results in a 0.74% increase in electricity demand.</p>
<p>We want to check the results of the Bayes factor comparing the previous specification (model 1) with other specification without considering the price of electricity (model 2), that is,
<span class="math display">\[\begin{align*}
    \log(\text{Electricity}_i) &amp; = \beta_1\text{IndSocio1}_i + \beta_2\text{IndSocio2}_i + \beta_3\text{Altitude}_i + \beta_4\text{Nrooms}_i\\
    &amp; + \beta_5\text{HouseholdMem}_i + \beta_6\text{Children}_i + \beta_7\log(\text{Income}_i)\\
    &amp; + \beta_8 + \mu_i
\end{align*}\]</span></p>
<p>In particular, we examine what happens as <span class="math inline">\(c_0\)</span> increases from <span class="math inline">\(10^{0}\)</span> to <span class="math inline">\(10^{20}\)</span>. We observe that when <span class="math inline">\(c_0 = 1\)</span>, <span class="math inline">\(BF_{12} = 8.68 \times 10^{+16}\)</span>, which indicates very strong evidence in favor of the model including the price of electricity. However, as <span class="math inline">\(c_0\)</span> increases, the Bayes factor decreases, which suggests evidence supporting model 2. For instance, when <span class="math inline">\(c_0 = 10^{20}\)</span>, <span class="math inline">\(BF_{12} = 3.11 \times 10^{-4}\)</span>. This is an example of the issue with using non-informative priors to calculate the Bayes factor: there is very strong evidence supporting the parsimonious model as <span class="math inline">\(c_0 \rightarrow \infty\)</span>.</p>
<p>We can obtain the posterior predictive distribution of the monthly electricity demand for a household located in the lowest socioeconomic condition in a municipality situated below 1000 meters above sea level, with 2 rooms, 3 members (with children), a monthly income of USD 500, and an electricity price of USD 0.15/kWh. The next Figure shows the histogram of the predictive posterior distribution. The highest posterior density credible interval at 95% is between 44.4 kWh and 373.9 kWh, and the posterior mean is 169.4 kWh.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="sec43.html#cb53-1" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb53-2"><a href="sec43.html#cb53-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb53-3"><a href="sec43.html#cb53-3" tabindex="-1"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb53-4"><a href="sec43.html#cb53-4" tabindex="-1"></a><span class="fu">library</span>(MCMCpack)</span>
<span id="cb53-5"><a href="sec43.html#cb53-5" tabindex="-1"></a><span class="fu">library</span>(LaplacesDemon)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;LaplacesDemon&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     partial</code></pre>
<pre><code>## The following objects are masked from &#39;package:MCMCpack&#39;:
## 
##     BayesFactor, ddirichlet, dinvgamma, rdirichlet, rinvgamma</code></pre>
<pre><code>## The following object is masked from &#39;package:sirt&#39;:
## 
##     rmvn</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="sec43.html#cb58-1" tabindex="-1"></a><span class="fu">library</span>(coda)</span>
<span id="cb58-2"><a href="sec43.html#cb58-2" tabindex="-1"></a><span class="fu">library</span>(HDInterval)</span>
<span id="cb58-3"><a href="sec43.html#cb58-3" tabindex="-1"></a></span>
<span id="cb58-4"><a href="sec43.html#cb58-4" tabindex="-1"></a><span class="co"># Load electricity demand data</span></span>
<span id="cb58-5"><a href="sec43.html#cb58-5" tabindex="-1"></a>data_util <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(</span>
<span id="cb58-6"><a href="sec43.html#cb58-6" tabindex="-1"></a>  <span class="st">&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/Utilities.csv&quot;</span>,</span>
<span id="cb58-7"><a href="sec43.html#cb58-7" tabindex="-1"></a>  <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span></span>
<span id="cb58-8"><a href="sec43.html#cb58-8" tabindex="-1"></a>)</span>
<span id="cb58-9"><a href="sec43.html#cb58-9" tabindex="-1"></a></span>
<span id="cb58-10"><a href="sec43.html#cb58-10" tabindex="-1"></a><span class="co"># Filter out households with zero electricity consumption</span></span>
<span id="cb58-11"><a href="sec43.html#cb58-11" tabindex="-1"></a>data_est <span class="ot">&lt;-</span> data_util <span class="sc">%&gt;%</span></span>
<span id="cb58-12"><a href="sec43.html#cb58-12" tabindex="-1"></a>  <span class="fu">filter</span>(Electricity <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb58-13"><a href="sec43.html#cb58-13" tabindex="-1"></a></span>
<span id="cb58-14"><a href="sec43.html#cb58-14" tabindex="-1"></a><span class="co"># Define dependent variable: log of monthly electricity consumption</span></span>
<span id="cb58-15"><a href="sec43.html#cb58-15" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">log</span>(data_est<span class="sc">$</span>Electricity)</span>
<span id="cb58-16"><a href="sec43.html#cb58-16" tabindex="-1"></a></span>
<span id="cb58-17"><a href="sec43.html#cb58-17" tabindex="-1"></a><span class="co"># Define regressors including intercept</span></span>
<span id="cb58-18"><a href="sec43.html#cb58-18" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">with</span>(data_est, <span class="fu">cbind</span>(</span>
<span id="cb58-19"><a href="sec43.html#cb58-19" tabindex="-1"></a>  LnPriceElect, IndSocio1, IndSocio2, Altitude, Nrooms,</span>
<span id="cb58-20"><a href="sec43.html#cb58-20" tabindex="-1"></a>  HouseholdMem, Children, Lnincome, <span class="dv">1</span></span>
<span id="cb58-21"><a href="sec43.html#cb58-21" tabindex="-1"></a>))</span>
<span id="cb58-22"><a href="sec43.html#cb58-22" tabindex="-1"></a></span>
<span id="cb58-23"><a href="sec43.html#cb58-23" tabindex="-1"></a><span class="co"># Dimensions</span></span>
<span id="cb58-24"><a href="sec43.html#cb58-24" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb58-25"><a href="sec43.html#cb58-25" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb58-26"><a href="sec43.html#cb58-26" tabindex="-1"></a></span>
<span id="cb58-27"><a href="sec43.html#cb58-27" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb58-28"><a href="sec43.html#cb58-28" tabindex="-1"></a>d_0 <span class="ot">&lt;-</span> <span class="fl">0.001</span></span>
<span id="cb58-29"><a href="sec43.html#cb58-29" tabindex="-1"></a>a_0 <span class="ot">&lt;-</span> <span class="fl">0.001</span></span>
<span id="cb58-30"><a href="sec43.html#cb58-30" tabindex="-1"></a>b_0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, k)</span>
<span id="cb58-31"><a href="sec43.html#cb58-31" tabindex="-1"></a>B_0 <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="sc">*</span> <span class="fu">diag</span>(k)</span>
<span id="cb58-32"><a href="sec43.html#cb58-32" tabindex="-1"></a></span>
<span id="cb58-33"><a href="sec43.html#cb58-33" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb58-34"><a href="sec43.html#cb58-34" tabindex="-1"></a>b_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb58-35"><a href="sec43.html#cb58-35" tabindex="-1"></a>B_n <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">forceSymmetric</span>(<span class="fu">solve</span>(<span class="fu">solve</span>(B_0) <span class="sc">+</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X)))</span>
<span id="cb58-36"><a href="sec43.html#cb58-36" tabindex="-1"></a>b_n <span class="ot">&lt;-</span> B_n <span class="sc">%*%</span> (<span class="fu">solve</span>(B_0) <span class="sc">%*%</span> b_0 <span class="sc">+</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">%*%</span> b_hat)</span>
<span id="cb58-37"><a href="sec43.html#cb58-37" tabindex="-1"></a>d_n <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(d_0 <span class="sc">+</span> <span class="fu">t</span>(y) <span class="sc">%*%</span> y <span class="sc">+</span> <span class="fu">t</span>(b_0) <span class="sc">%*%</span> <span class="fu">solve</span>(B_0) <span class="sc">%*%</span> b_0 <span class="sc">-</span> <span class="fu">t</span>(b_n) <span class="sc">%*%</span> <span class="fu">solve</span>(B_n) <span class="sc">%*%</span> b_n)</span>
<span id="cb58-38"><a href="sec43.html#cb58-38" tabindex="-1"></a>a_n <span class="ot">&lt;-</span> a_0 <span class="sc">+</span> n</span>
<span id="cb58-39"><a href="sec43.html#cb58-39" tabindex="-1"></a>H_n <span class="ot">&lt;-</span> B_n <span class="sc">*</span> d_n <span class="sc">/</span> a_n</span>
<span id="cb58-40"><a href="sec43.html#cb58-40" tabindex="-1"></a></span>
<span id="cb58-41"><a href="sec43.html#cb58-41" tabindex="-1"></a><span class="co"># Posterior draws</span></span>
<span id="cb58-42"><a href="sec43.html#cb58-42" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb58-43"><a href="sec43.html#cb58-43" tabindex="-1"></a>sigma2_samples <span class="ot">&lt;-</span> <span class="fu">rinvgamma</span>(S, <span class="at">shape =</span> a_n <span class="sc">/</span> <span class="dv">2</span>, <span class="at">scale =</span> d_n <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb58-44"><a href="sec43.html#cb58-44" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">mcmc</span>(sigma2_samples))</span></code></pre></div>
<pre><code>## 
## Iterations = 1:10000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##      2.361e-01      7.524e-03      7.524e-05      7.524e-05 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## 0.2219 0.2309 0.2359 0.2411 0.2513</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="sec43.html#cb60-1" tabindex="-1"></a>beta_samples <span class="ot">&lt;-</span> <span class="fu">rmvt</span>(S, b_n, H_n, <span class="at">df =</span> a_n)</span>
<span id="cb60-2"><a href="sec43.html#cb60-2" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">mcmc</span>(beta_samples))</span></code></pre></div>
<pre><code>## 
## Iterations = 1:10000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##                  Mean      SD  Naive SE Time-series SE
## LnPriceElect -1.09036 0.19366 0.0019366      0.0019366
## IndSocio1    -0.32714 0.05233 0.0005233      0.0005233
## IndSocio2    -0.05622 0.04621 0.0004621      0.0004621
## Altitude     -0.19758 0.02406 0.0002406      0.0002327
## Nrooms        0.08740 0.01092 0.0001092      0.0001092
## HouseholdMem  0.06007 0.01343 0.0001343      0.0001378
## Children      0.05679 0.03069 0.0003069      0.0003069
## Lnincome      0.07442 0.01209 0.0001209      0.0001209
##               2.52160 0.34868 0.0034868      0.0034868
## 
## 2. Quantiles for each variable:
## 
##                   2.5%      25%      50%      75%    97.5%
## LnPriceElect -1.470327 -1.22358 -1.08954 -0.95828 -0.71714
## IndSocio1    -0.430053 -0.36227 -0.32732 -0.29170 -0.22531
## IndSocio2    -0.148235 -0.08713 -0.05633 -0.02490  0.03381
## Altitude     -0.244686 -0.21361 -0.19747 -0.18142 -0.15081
## Nrooms        0.066066  0.08018  0.08755  0.09482  0.10841
## HouseholdMem  0.033540  0.05117  0.06004  0.06910  0.08621
## Children     -0.002609  0.03629  0.05694  0.07770  0.11685
## Lnincome      0.050594  0.06634  0.07445  0.08266  0.09794
##               1.835193  2.28510  2.51997  2.76009  3.20715</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="sec43.html#cb62-1" tabindex="-1"></a><span class="co"># Function to compute log marginal likelihood (negative for optimization)</span></span>
<span id="cb62-2"><a href="sec43.html#cb62-2" tabindex="-1"></a>log_marginal_likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(X, c_0) {</span>
<span id="cb62-3"><a href="sec43.html#cb62-3" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb62-4"><a href="sec43.html#cb62-4" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb62-5"><a href="sec43.html#cb62-5" tabindex="-1"></a>  B_0 <span class="ot">&lt;-</span> c_0 <span class="sc">*</span> <span class="fu">diag</span>(k)</span>
<span id="cb62-6"><a href="sec43.html#cb62-6" tabindex="-1"></a>  b_0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, k)</span>
<span id="cb62-7"><a href="sec43.html#cb62-7" tabindex="-1"></a>  </span>
<span id="cb62-8"><a href="sec43.html#cb62-8" tabindex="-1"></a>  b_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb62-9"><a href="sec43.html#cb62-9" tabindex="-1"></a>  B_n <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">forceSymmetric</span>(<span class="fu">solve</span>(<span class="fu">solve</span>(B_0) <span class="sc">+</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X)))</span>
<span id="cb62-10"><a href="sec43.html#cb62-10" tabindex="-1"></a>  b_n <span class="ot">&lt;-</span> B_n <span class="sc">%*%</span> (<span class="fu">solve</span>(B_0) <span class="sc">%*%</span> b_0 <span class="sc">+</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">%*%</span> b_hat)</span>
<span id="cb62-11"><a href="sec43.html#cb62-11" tabindex="-1"></a>  d_n <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(d_0 <span class="sc">+</span> <span class="fu">t</span>(y) <span class="sc">%*%</span> y <span class="sc">+</span> <span class="fu">t</span>(b_0) <span class="sc">%*%</span> <span class="fu">solve</span>(B_0) <span class="sc">%*%</span> b_0 <span class="sc">-</span> <span class="fu">t</span>(b_n) <span class="sc">%*%</span> <span class="fu">solve</span>(B_n) <span class="sc">%*%</span> b_n)</span>
<span id="cb62-12"><a href="sec43.html#cb62-12" tabindex="-1"></a>  a_n <span class="ot">&lt;-</span> a_0 <span class="sc">+</span> n</span>
<span id="cb62-13"><a href="sec43.html#cb62-13" tabindex="-1"></a>  </span>
<span id="cb62-14"><a href="sec43.html#cb62-14" tabindex="-1"></a>  log_py <span class="ot">&lt;-</span> (n <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">/</span> pi) <span class="sc">+</span></span>
<span id="cb62-15"><a href="sec43.html#cb62-15" tabindex="-1"></a>    (a_0 <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">log</span>(d_0) <span class="sc">-</span></span>
<span id="cb62-16"><a href="sec43.html#cb62-16" tabindex="-1"></a>    (a_n <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">log</span>(d_n) <span class="sc">+</span></span>
<span id="cb62-17"><a href="sec43.html#cb62-17" tabindex="-1"></a>    <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">det</span>(B_n) <span class="sc">/</span> <span class="fu">det</span>(B_0)) <span class="sc">+</span></span>
<span id="cb62-18"><a href="sec43.html#cb62-18" tabindex="-1"></a>    <span class="fu">lgamma</span>(a_n <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">-</span> <span class="fu">lgamma</span>(a_0 <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb62-19"><a href="sec43.html#cb62-19" tabindex="-1"></a>  </span>
<span id="cb62-20"><a href="sec43.html#cb62-20" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>log_py)</span>
<span id="cb62-21"><a href="sec43.html#cb62-21" tabindex="-1"></a>}</span>
<span id="cb62-22"><a href="sec43.html#cb62-22" tabindex="-1"></a></span>
<span id="cb62-23"><a href="sec43.html#cb62-23" tabindex="-1"></a><span class="co"># Prior variances</span></span>
<span id="cb62-24"><a href="sec43.html#cb62-24" tabindex="-1"></a>c_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1e3</span>, <span class="fl">1e6</span>, <span class="fl">1e10</span>, <span class="fl">1e12</span>, <span class="fl">1e15</span>, <span class="fl">1e20</span>)</span>
<span id="cb62-25"><a href="sec43.html#cb62-25" tabindex="-1"></a></span>
<span id="cb62-26"><a href="sec43.html#cb62-26" tabindex="-1"></a><span class="co"># Compute log marginal likelihoods</span></span>
<span id="cb62-27"><a href="sec43.html#cb62-27" tabindex="-1"></a>log_ml <span class="ot">&lt;-</span> <span class="fu">sapply</span>(c_values, <span class="cf">function</span>(c) <span class="sc">-</span><span class="fu">log_marginal_likelihood</span>(<span class="at">X =</span> X, <span class="at">c_0 =</span> c))</span>
<span id="cb62-28"><a href="sec43.html#cb62-28" tabindex="-1"></a></span>
<span id="cb62-29"><a href="sec43.html#cb62-29" tabindex="-1"></a><span class="co"># Regressors without price</span></span>
<span id="cb62-30"><a href="sec43.html#cb62-30" tabindex="-1"></a>X_new <span class="ot">&lt;-</span> <span class="fu">with</span>(data_est, <span class="fu">cbind</span>(</span>
<span id="cb62-31"><a href="sec43.html#cb62-31" tabindex="-1"></a>  IndSocio1, IndSocio2, Altitude, Nrooms,</span>
<span id="cb62-32"><a href="sec43.html#cb62-32" tabindex="-1"></a>  HouseholdMem, Children, Lnincome, <span class="dv">1</span></span>
<span id="cb62-33"><a href="sec43.html#cb62-33" tabindex="-1"></a>))</span>
<span id="cb62-34"><a href="sec43.html#cb62-34" tabindex="-1"></a></span>
<span id="cb62-35"><a href="sec43.html#cb62-35" tabindex="-1"></a>log_ml_new <span class="ot">&lt;-</span> <span class="fu">sapply</span>(c_values, <span class="cf">function</span>(c) <span class="sc">-</span><span class="fu">log_marginal_likelihood</span>(<span class="at">X =</span> X_new, <span class="at">c_0 =</span> c))</span>
<span id="cb62-36"><a href="sec43.html#cb62-36" tabindex="-1"></a></span>
<span id="cb62-37"><a href="sec43.html#cb62-37" tabindex="-1"></a><span class="co"># Bayes Factor</span></span>
<span id="cb62-38"><a href="sec43.html#cb62-38" tabindex="-1"></a>bf <span class="ot">&lt;-</span> <span class="fu">exp</span>(log_ml <span class="sc">-</span> log_ml_new)</span>
<span id="cb62-39"><a href="sec43.html#cb62-39" tabindex="-1"></a>bf</span></code></pre></div>
<pre><code>## [1] 8.687289e+16 1.006665e+05 3.108374e+03 3.108299e+01 3.108303e+00
## [6] 9.829315e-02 3.108302e-04</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="sec43.html#cb64-1" tabindex="-1"></a><span class="co"># Predictive distribution</span></span>
<span id="cb64-2"><a href="sec43.html#cb64-2" tabindex="-1"></a>x_pred <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">log</span>(<span class="fl">0.15</span>), <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="fu">log</span>(<span class="dv">500</span>), <span class="dv">1</span>)</span>
<span id="cb64-3"><a href="sec43.html#cb64-3" tabindex="-1"></a>mean_pred <span class="ot">&lt;-</span> x_pred <span class="sc">%*%</span> b_n</span>
<span id="cb64-4"><a href="sec43.html#cb64-4" tabindex="-1"></a>H_pred <span class="ot">&lt;-</span> d_n <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">t</span>(x_pred) <span class="sc">%*%</span> B_n <span class="sc">%*%</span> x_pred) <span class="sc">/</span> a_n</span>
<span id="cb64-5"><a href="sec43.html#cb64-5" tabindex="-1"></a>expected_kwh <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">rmvt</span>(S, mean_pred, H_pred, <span class="at">df =</span> a_n))</span>
<span id="cb64-6"><a href="sec43.html#cb64-6" tabindex="-1"></a></span>
<span id="cb64-7"><a href="sec43.html#cb64-7" tabindex="-1"></a><span class="fu">summary</span>(expected_kwh)</span></code></pre></div>
<pre><code>##        V1         
##  Min.   :  21.63  
##  1st Qu.: 123.23  
##  Median : 170.92  
##  Mean   : 193.31  
##  3rd Qu.: 238.12  
##  Max.   :1124.50</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="sec43.html#cb66-1" tabindex="-1"></a>hdi_interval <span class="ot">&lt;-</span> <span class="fu">hdi</span>(expected_kwh, <span class="at">credMass =</span> <span class="fl">0.95</span>)</span>
<span id="cb66-2"><a href="sec43.html#cb66-2" tabindex="-1"></a>hdi_interval</span></code></pre></div>
<pre><code>##            [,1]
## lower  48.78708
## upper 396.47748
## attr(,&quot;credMass&quot;)
## [1] 0.95</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="sec43.html#cb68-1" tabindex="-1"></a><span class="fu">hist</span>(expected_kwh,</span>
<span id="cb68-2"><a href="sec43.html#cb68-2" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Histogram: Monthly demand of electricity&quot;</span>,</span>
<span id="cb68-3"><a href="sec43.html#cb68-3" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Monthly kWh&quot;</span>,</span>
<span id="cb68-4"><a href="sec43.html#cb68-4" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">breaks =</span> <span class="dv">50</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-3-1.svg" width="672" /></p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-an2007bayesian" class="csl-entry">
An, Sungbae, and Frank Schorfheide. 2007. <span>“Bayesian Analysis of DSGE Models.”</span> <em>Econometric Reviews</em> 26 (2-4): 113–72.
</div>
<div id="ref-Bayes1763" class="csl-entry">
Bayes, T. 1763. <span>“An Essay Towards Solving a Problem in the Doctrine of Chances.”</span> <em>Philosophical Transactions of the Royal Society of London</em> 53: 370–416.
</div>
<div id="ref-Berger2006" class="csl-entry">
Berger, J. 2006. <span>“The Case for Objective Bayesian Analysis.”</span> <em>Bayesian Analysis</em> 1 (3): 385–402.
</div>
<div id="ref-berger1996intrinsic" class="csl-entry">
Berger, James O, and Luis R Pericchi. 1996. <span>“The Intrinsic Bayes Factor for Model Selection and Prediction.”</span> <em>Journal of the American Statistical Association</em> 91 (433): 109–22.
</div>
<div id="ref-bernardo2009bayesian" class="csl-entry">
Bernardo, José M, and Adrian FM Smith. 2009. <em>Bayesian Theory</em>. Vol. 405. John Wiley &amp; Sons.
</div>
<div id="ref-deFinetti1937" class="csl-entry">
De Finetti, Bruno. 1937. <span>“Foresight: Its Logical Laws, Its Subjective Sources.”</span> In <em>Studies in Subjective Probability</em>, edited by H. E. Kyburg and H. E. Smokler. New York: Krieger.
</div>
<div id="ref-garthwaite05" class="csl-entry">
Garthwaite, P., J. Kadane, and A. O’Hagan. 2005. <span>“Statistical Methods for Eliciting Probability Distributions.”</span> <em>Journal of American Statistical Association</em> 100 (470): 680–701.
</div>
<div id="ref-gelman2006prior" class="csl-entry">
Gelman, Andrew et al. 2006. <span>“Prior Distributions for Variance Parameters in Hierarchical Models (Comment on Article by Browne and Draper).”</span> <em>Bayesian Analysis</em> 1 (3): 515–34.
</div>
<div id="ref-giordano2022evaluating" class="csl-entry">
Giordano, Ryan, Runjing Liu, Michael I Jordan, and Tamara Broderick. 2022. <span>“Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian Nonparametrics.”</span> <em>Bayesian Analysis</em> 1 (1): 1–34.
</div>
<div id="ref-gustafson2000local" class="csl-entry">
Gustafson, Paul. 2000. <span>“Local Robustness in Bayesian Analysis.”</span> In <em>Robust Bayesian Analysis</em>, 71–88. Springer.
</div>
<div id="ref-jacobi2024posterior" class="csl-entry">
Jacobi, Liana, Chun Fung Kwok, Andrés Ramı́rez-Hassan, and Nhung Nghiem. 2024. <span>“Posterior Manifolds over Prior Parameter Regions: Beyond Pointwise Sensitivity Assessments for Posterior Statistics from MCMC Inference.”</span> <em>Studies in Nonlinear Dynamics &amp; Econometrics</em> 28 (2): 403–34.
</div>
<div id="ref-Jacobi2022" class="csl-entry">
Jacobi, Liana, Dan Zhu, and Mark Joshi. 2022. <span>“Estimating Posterior Sensitivities with Application to Structural Analysis of Bayesian Vector Autoregressions.”</span> <em>Available at SSRN 3347399</em>. <a href="https://ssrn.com/abstract=3347399 or http://dx.doi.org/10.2139/ssrn.3347399">https://ssrn.com/abstract=3347399 or http://dx.doi.org/10.2139/ssrn.3347399</a>.
</div>
<div id="ref-Jeffreys1961" class="csl-entry">
Jeffreys, H. 1961. <em>Theory of Probability</em>. London: Oxford University Press.
</div>
<div id="ref-jeffreys1946invariant" class="csl-entry">
Jeffreys, Harold. 1946. <span>“An Invariant Form for the Prior Probability in Estimation Problems.”</span> <em>Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences</em> 186 (1007): 453–61.
</div>
<div id="ref-Kadane80" class="csl-entry">
Kadane, J. B. 1980. <span>“Predictive and Structural Methods for Eliciting Prior Distributions.”</span> In <em>Bayesian Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys,</em> edited by A Zellner, 89–93. Amsterdam: North–Holland Publishing Company,.
</div>
<div id="ref-kadane98" class="csl-entry">
Kadane, Joseph, and Lara Wolfson. 1998. <span>“Experiences in Elitation.”</span> <em>The Statiscian</em> 47 (1): 3–19.
</div>
<div id="ref-kim1999has" class="csl-entry">
Kim, Chang-Jin, and Charles R Nelson. 1999. <span>“Has the US Economy Become More Stable? A Bayesian Approach Based on a Markov-Switching Model of the Business Cycle.”</span> <em>Review of Economics and Statistics</em> 81 (4): 608–16.
</div>
<div id="ref-Laplace1812" class="csl-entry">
Laplace, P. 1812. <em>Théorie Analytique Des Probabilités</em>. Courcier.
</div>
<div id="ref-Lindley2000" class="csl-entry">
Lindley, D. V. 2000. <span>“The Philosophy of Statistics.”</span> <em>The Statistician</em> 49 (3): 293–337.
</div>
<div id="ref-Ramsey1926" class="csl-entry">
Ramsey, F. 1926. <span>“Truth and Probability.”</span> In <em>The Foundations of Mathematics and Other Logical Essays</em>, edited by Routledge and Kegan Paul. London: New York: Harcourt, Brace; Company.
</div>
<div id="ref-richardson1997bayesian" class="csl-entry">
Richardson, Sylvia, and Peter J Green. 1997. <span>“On Bayesian Analysis of Mixtures with an Unknown Number of Components (with Discussion).”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 59 (4): 731–92.
</div>
<div id="ref-savage1954" class="csl-entry">
Savage, L. J. 1954. <em>The Foundations of Statistics</em>. New York: John Wiley &amp; Sons, Inc.
</div>
<div id="ref-Smith1973" class="csl-entry">
Smith, A. F. M. 1973. <span>“<span>A General Bayesian Linear Model</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological).</em> 35 (1): 67–75.
</div>
<div id="ref-tversky74" class="csl-entry">
Tversky, A., and D. Kahneman. 1974. <span>“Judgement Under Uncertainty: Heuristics and Biases.”</span> <em>Science</em> 185: 1124–31.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>A particular case of the Woodbury matrix identity, <span class="math inline">\((\boldsymbol{A}+\boldsymbol{U}\boldsymbol{C}\boldsymbol{V})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{U}(\boldsymbol{C}^{-1}+\boldsymbol{V}\boldsymbol{A}^{-1}\boldsymbol{U})^{-1}\boldsymbol{V}\boldsymbol{A}^{-1}\)</span>.<a href="sec43.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>See <span class="citation">Gelman et al. (<a href="#ref-gelman2006prior">2006</a>)</span> for advice against this common practice.<a href="sec43.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec42.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec44.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
