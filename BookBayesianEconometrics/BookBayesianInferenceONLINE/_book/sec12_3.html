<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.3 Bayesian additive regression trees | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="12.3 Bayesian additive regression trees | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.3 Bayesian additive regression trees | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12_2.html"/>
<link rel="next" href="sec12_4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12_3" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Bayesian additive regression trees<a href="sec12_3.html#sec12_3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Classification and Regression Tree (CART) is a method used to predict outcomes based on inputs without assuming a parametric model. It is referred to as a classification tree when the outcome variable is qualitative, and as a regression tree when the outcome variable is quantitative. The method works by recursively partitioning the dataset into smaller, more homogeneous subsets using decision rules based on the input variables. For regression tasks, CART selects splits that minimize prediction error, typically measured by the sum of squared residuals. For classification problems, it aims to create the purest possible groups, using criteria such as Gini impurity or entropy. The result is a tree-like structure in which each internal node represents a decision based on one variable, and each leaf node corresponds to a prediction. CART was popularized in the statistical community by <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification">1984</a>)</span>.</p>
<p>The following figure displays a binary regression tree with two variables: one continuous (<span class="math inline">\(x_1\)</span>) and one categorical (<span class="math inline">\(x_2 \in \{A, B, C\}\)</span>). The tree has seven nodes in total, four of which are terminal nodes (<span class="math inline">\(B = 4\)</span>), dividing the input space <span class="math inline">\(\mathbf{x} = (x_1, x_2)\)</span> into four non-overlapping regions. Each internal node indicates the splitting variable and rule, while each terminal node shows the value <span class="math inline">\(\theta_b\)</span>, representing the conditional mean of <span class="math inline">\(y\)</span> given <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>The first split is based on the rule <span class="math inline">\(x_1 \leq 5\)</span> (left) versus <span class="math inline">\(x_1 &gt; 5\)</span> (right). The leftmost terminal node corresponds to <span class="math inline">\(x_2 \in \{A\}\)</span> with <span class="math inline">\(\mu_1 = 2\)</span>. The second terminal node, with <span class="math inline">\(\mu_2 = 3\)</span>, is defined by <span class="math inline">\(x_1 \leq 3\)</span> and <span class="math inline">\(x_2 \in \{B, C\}\)</span>. The third node assigns <span class="math inline">\(\mu_3 = 5\)</span> for <span class="math inline">\(3 &lt; x_1 \leq 5\)</span> and <span class="math inline">\(x_2 \in \{B, C\}\)</span>. Finally, the rightmost terminal node assigns <span class="math inline">\(\mu_4 = 8\)</span> for all observations with <span class="math inline">\(x_1 &gt; 5\)</span>.</p>
<p><img src="figures/BART.png" width="600px" height="350px" style="display: block; margin: auto;" /></p>
<p>We can view a CART model as specifying the conditional distribution of <span class="math inline">\(y\)</span> given the vector of features <span class="math inline">\(\mathbf{x} = [x_1, x_2, \dots, x_K]^{\top}\)</span>. There are two main components: (i) the binary tree structure <span class="math inline">\(T\)</span>, which consists of a sequence of binary decision rules of the form <span class="math inline">\(x_k \in A\)</span> versus <span class="math inline">\(x_k \notin A\)</span>, where <span class="math inline">\(A\)</span> is a subset of the range of <span class="math inline">\(x_k\)</span>, and <span class="math inline">\(B\)</span> terminal nodes that define a non-overlapping and exhaustive partition of the input space; and (ii) the parameter vector <span class="math inline">\(\boldsymbol{\theta} = [\mu_1, \mu_2, \dots, \mu_B]^{\top}\)</span> and <span class="math inline">\(\sigma^2\)</span>, where each <span class="math inline">\(\mu_b\)</span> corresponds to the parameter associated with terminal node <span class="math inline">\(b\)</span>, and <span class="math inline">\(\sigma^2\)</span> is the variance. Consequently, the response <span class="math inline">\(y \mid \mathbf{x} \sim p(y \mid \mu_b,\sigma^2)\)</span> if <span class="math inline">\(\mathbf{x}\)</span> belongs to the region associated with terminal node <span class="math inline">\(b\)</span>, where <span class="math inline">\(p\)</span> denotes a parametric distribution indexed by <span class="math inline">\(\mu_b\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Assuming that <span class="math inline">\(y_{bi}\)</span> is independently and identically distributed within each terminal node and independently across nodes, for <span class="math inline">\(b = 1, 2, \dots, B\)</span> and <span class="math inline">\(i = 1, 2, \dots, n_b\)</span>, we have:</p>
<p><span class="math display">\[
p(\mathbf{y} \mid \mathbf{x}, T, \boldsymbol{\theta}, \sigma^2) = \prod_{b=1}^B p(\mathbf{y}_b \mid \mu_b,\sigma^2) = \prod_{b=1}^B \prod_{i=1}^{n_b} p(y_{bi} \mid \mu_b,\sigma^2),
\]</span></p>
<p>where <span class="math inline">\(\mathbf{y}_b = [y_{b1}, y_{b2}, \dots, y_{bn_b}]^{\top}\)</span> is the set of observations in terminal node <span class="math inline">\(b\)</span>.</p>
<p><span class="citation">Chipman, George, and McCulloch (<a href="#ref-chipman1998bayesian">1998</a>)</span> introduced a Bayesian formulation of CART models, in which inference is carried out through a combination of prior specification on the binary tree structure <span class="math inline">\(T\)</span> and the parameters <span class="math inline">\(\boldsymbol{\theta}, \sigma^2 \mid T\)</span>, along with a stochastic search strategy based on a Metropolis-Hastings algorithm. The transition kernels used in the algorithm include operations such as growing, pruning, changing, and swapping tree branches, and candidate trees are evaluated based on their marginal likelihood. This approach enables exploration of a richer set of potential tree models and offers a more flexible and effective alternative to the greedy algorithms commonly used in classical CART.</p>
<p>While CART is a simple yet powerful tool, it is prone to overfitting. To mitigate this, ensemble methods such as boosting, bagging, and random forests are often used. <em>Boosting</em> combines multiple weak trees sequentially, each correcting the errors of its predecessor, to create a strong predictive model <span class="citation">(<a href="#ref-freund1997decision">Freund and Schapire 1997</a>)</span>. <em>Bagging</em> builds multiple models on bootstrapped datasets and averages their predictions to reduce variance <span class="citation">(<a href="#ref-breiman1996bagging">Breiman 1996</a>)</span>, and <em>random forests</em> extend bagging by using decision trees and adding random input selection at each split to increase model diversity <span class="citation">(<a href="#ref-breiman2001random">Breiman 2001</a>)</span>. Although a single tree might overfit and generalize poorly, aggregating many randomized trees typically yields more accurate and stable predictions.</p>
<p><span class="citation">Chipman, George, and McCulloch (<a href="#ref-chipman2010bart">2010</a>)</span> introduced Bayesian Additive Regression Trees (BART). The starting point is the model:</p>
<p><span class="math display">\[
y_i = f(\mathbf{x}_i) + \mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mu_i \sim N(0, \sigma^2)\)</span>.</p>
<p>Thus, the conditional expectation <span class="math inline">\(\mathbb{E}[y_i \mid \mathbf{x}_i] = f(\mathbf{x}_i)\)</span> is approximated as</p>
<p><span class="math display">\[\begin{equation}
\label{eq:BART}
f(\mathbf{x}_i) \approx h(\mathbf{x}_i) = \sum_{j=1}^J g_j(\mathbf{x}_i \mid T_j, \boldsymbol{\theta}_j),
\end{equation}\]</span></p>
<p>that is, <span class="math inline">\(f(\mathbf{x}_i)\)</span> is approximated by the sum of <span class="math inline">\(J\)</span> regression trees. Each tree is defined by a structure <span class="math inline">\(T_j\)</span> and a corresponding set of terminal node parameters <span class="math inline">\(\boldsymbol{\theta}_j\)</span>, where <span class="math inline">\(g_j(\mathbf{x}_i \mid T_j, \boldsymbol{\theta}_j)\)</span> denotes the function that assigns the value <span class="math inline">\(\mu_{bj} \in \boldsymbol{\theta}_j\)</span> to <span class="math inline">\(\mathbf{x}_i\)</span> according to the partition defined by <span class="math inline">\(T_j\)</span>.</p>
<p>The main idea is to construct this sum-of-trees model by imposing a prior that regularizes the fit, ensuring that the individual contribution of each tree remains small. Thus, each tree <span class="math inline">\(g_j\)</span> explains a small and distinct portion of <span class="math inline">\(f\)</span>. This is achieved through Bayesian backfitting MCMC <span class="citation">(<a href="#ref-hastie2000bayesian">Hastie and Tibshirani 2000</a>)</span>, where successive fits to the residuals are added. In this sense, BART can be viewed as a Bayesian version of boosting.</p>
<p><span class="citation">Chipman, George, and McCulloch (<a href="#ref-chipman2010bart">2010</a>)</span> use the following prior structure:</p>
<p><span class="math display">\[\begin{align*}
\pi\left(\{(T_1,\boldsymbol{\theta}_1), \dots, (T_J,\boldsymbol{\theta}_J), \sigma^2\}\right) &amp; = \left[\prod_{j=1}^J \pi(T_j,\boldsymbol{\theta}_j)\right]\pi(\sigma)\\
&amp; = \left[\prod_{j=1}^J \pi(\boldsymbol{\theta}_j \mid T_j) \pi(T_j)\right] \pi(\sigma)\\
&amp; = \left[\prod_{j=1}^J \prod_{b=1}^B \pi(\mu_{bj} \mid T_j) \pi(T_j)\right] \pi(\sigma).
\end{align*}\]</span></p>
<p>The prior for the binary tree structure has three components:<br />
(i) the probability that a node at depth <span class="math inline">\(d = 0, 1, \dots\)</span> is nonterminal, given by <span class="math inline">\(\alpha(1 + d)^{-\beta}\)</span>, where <span class="math inline">\(\alpha \in (0,1)\)</span> and <span class="math inline">\(\beta \in [0, \infty)\)</span>, the default values are <span class="math inline">\(\alpha = 0.95\)</span> and <span class="math inline">\(\beta = 2\)</span>; (ii) a uniform prior over the set of available regressors to define the distribution of splitting variable assignments at each interior node; (iii) a uniform prior over the discrete set of available splitting values to define the splitting rule assignment, conditional on the chosen splitting variable.</p>
<p>The prior for the terminal node parameters <span class="math inline">\(\pi(\mu_{bj} \mid T_j)\)</span> is specified as <span class="math inline">\(N(0, 0.5 / (k \sqrt{J}))\)</span>. The observed values of <span class="math inline">\(y\)</span> are scaled and shifted to lie within the range <span class="math inline">\(y_{\text{min}} = -0.5\)</span> to <span class="math inline">\(y_{\text{max}} = 0.5\)</span>, and the default value <span class="math inline">\(k = 2\)</span> ensures that</p>
<p><span class="math display">\[
P_{\pi(\mu_{bj} \mid T_j)}\left(\mathbb{E}[y \mid \mathbf{x}] \in (-0.5, 0.5)\right) = 0.95.
\]</span></p>
<p>Note that this prior shrinks the effect of individual trees toward zero, ensuring that each tree contributes only a small amount to the overall prediction. Moreover, although the dependent variable is transformed, there is no need to transform the input variables, since the tree-splitting rules are invariant to monotonic transformations of the regressors.</p>
<p>The prior for <span class="math inline">\(\sigma^2\)</span> is specified as <span class="math inline">\(\pi(\sigma^2) \sim v\lambda / \chi^2_v\)</span>. <span class="citation">Chipman, George, and McCulloch (<a href="#ref-chipman2010bart">2010</a>)</span> recommend setting <span class="math inline">\(v = 3\)</span>, and choosing <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(P(\sigma &lt; \hat{\sigma}) = q, q = 0.9\)</span>, where <span class="math inline">\(\hat{\sigma}\)</span> is an estimate of the residual standard deviation from a saturated linear model, i.e., a model including all available regressors when <span class="math inline">\(K &lt; N\)</span>, or the standard deviation of <span class="math inline">\(y\)</span> when <span class="math inline">\(K \geq N\)</span>.</p>
<p>Finally, regarding the number of trees <span class="math inline">\(J\)</span>, the default value is 200. As <span class="math inline">\(J\)</span> increases from 1, BART’s predictive performance typically improves substantially until it reaches a plateau, after which performance may degrade very slowly. However, if the primary goal is variable selection, using a smaller <span class="math inline">\(J\)</span> is preferable, as it facilitates identifying the most important regressors by enhancing the internal competition among variables within a limited number of trees.</p>
<p>To sum up, the default hyperparameters are <span class="math inline">\((\alpha, \beta, k, J, v, q) = (0.95, 2, 2, 200, 3, 0.9)\)</span>; however, cross-validation can be used to tune these hyperparameters if desired. BART’s predictive performance appears to be relatively robust to the choice of hyperparameters, provided they are set to sensible values, except in cases where <span class="math inline">\(K &gt; N\)</span>, in which cross-validated tuning tends to yield better performance, albeit at the cost of increased computational time.</p>
<p>Given this specification, we can use a Gibbs sampler that cycles through the <span class="math inline">\(J\)</span> trees. At each iteration, we sample from the conditional posterior distribution:</p>
<p><span class="math display">\[
\pi(T_j, \boldsymbol{\theta}_j \mid R_j, \sigma) = \pi(T_j \mid R_j, \sigma) \times \pi(\boldsymbol{\theta}_j \mid T_j, R_j, \sigma),
\]</span></p>
<p>where <span class="math inline">\(R_j = y - \sum_{k \neq j} g_k(\mathbf{x} \mid T_k, \boldsymbol{\theta}_k)\)</span> represents the residuals excluding the contribution of the <span class="math inline">\(j\)</span>-th tree.</p>
<p>The posterior distribution <span class="math inline">\(\pi(T_j \mid R_j, \sigma)\)</span> is explored using a Metropolis-Hastings algorithm, where the candidate tree is generated by one of the following moves <span class="citation">(<a href="#ref-chipman1998bayesian">Chipman, George, and McCulloch 1998</a>)</span>:<br />
(i) growing a terminal node with probability 0.25;<br />
(ii) pruning a pair of terminal nodes with probability 0.25;<br />
(iii) changing a nonterminal splitting rule with probability 0.4; or<br />
(iv) swapping a rule between a parent and child node with probability 0.1.</p>
<p>The posterior distribution of <span class="math inline">\(\boldsymbol{\theta}_j\)</span> is the product of the posterior distributions <span class="math inline">\(\pi(\mu_{jb} \mid T_j, R_j, \sigma)\)</span>, which are Gaussian. The posterior distribution of <span class="math inline">\(\sigma\)</span> is inverse-gamma. The posterior draws of <span class="math inline">\(\mu_{bj}\)</span> are then used to update the residuals <span class="math inline">\(R_{j+1}\)</span>, allowing the sampler to proceed to the next tree in the cycle. The number of iterations in the Gibbs sampler does not need to be very large; for instance, 200 burn-in iterations and 1,000 post-burn-in iterations typically work well.</p>
<p>As we obtain posterior draws from the sum-of-trees model, we can compute point estimates at each <span class="math inline">\(\mathbf{x}_i\)</span> using the posterior mean, <span class="math inline">\(\mathbb{E}[y \mid \mathbf{x}_i] = f(\mathbf{x}_i)\)</span>. Additionally, pointwise measures of uncertainty can be derived from the quantiles of the posterior draws. We can also identify the most relevant predictors by tracking the relative frequency with which each regressor appears in the sum-of-trees model across iterations. Moreover, we can perform inference on functionals of <span class="math inline">\(f\)</span>, such as the <em>partial dependence functions</em> <span class="citation">(<a href="#ref-friedman2001greedy">Friedman 2001</a>)</span>, which quantify the marginal effects of the regressors.</p>
<p>Specifically, if we are interested in the effect of <span class="math inline">\(\mathbf{x}_s\)</span> on <span class="math inline">\(y\)</span>, while marginalizing over the remaining variables <span class="math inline">\(\mathbf{x}_c\)</span>, such that <span class="math inline">\(\mathbf{x} = [\mathbf{x}_s^{\top}, \mathbf{x}_c^{\top}]^{\top}\)</span>, the partial dependence function is defined as:</p>
<p><span class="math display">\[
f(\mathbf{x}_s) = \frac{1}{N} \sum_{i=1}^N f(\mathbf{x}_s, \mathbf{x}_{ic}),
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}_{ic}\)</span> denotes the <span class="math inline">\(i\)</span>-th observed value of <span class="math inline">\(\mathbf{x}_c\)</span> in the dataset.</p>
<p>Note that the calculation of the partial dependence function assumes that a subset of the variables is held fixed while averaging over the remainder. This assumption may be questionable when strong dependence exists among input variables, as fixing some variables while varying others may result in unrealistic or implausible combinations. Therefore, caution is warranted when interpreting the results.</p>
<p><span class="citation">Linero (<a href="#ref-linero2018bayesian">2018</a>)</span> extended BART models to high-dimensional settings for prediction and variable selection, while <span class="citation">Hill (<a href="#ref-hill2011bayesian">2011</a>)</span> and <span class="citation">Hahn, Murray, and Carvalho (<a href="#ref-hahn2020bayesian">2020</a>)</span> applied them to causal inference. The asymptotic properties of BART models have been studied by <span class="citation">Ročková and Saha (<a href="#ref-rockova2019on">2019</a>)</span>, <span class="citation">Ročková and Pas (<a href="#ref-rockova2020posterior">2020</a>)</span>, and <span class="citation">Ročková (<a href="#ref-rockova2020semiparametric">2020</a>)</span>.</p>
<p><strong>Example: Simulation exercise to study BART performance</strong></p>
<p>We use the <em>BART</em> package <span class="citation">(<a href="#ref-sparapani2021nonparametric">Sparapani, Spanbauer, and McCulloch 2021</a>)</span> in the <strong>R</strong> software environment to perform estimation, prediction, inference, and marginal analysis using Bayesian Additive Regression Trees. In addition to modeling continuous outcomes, this package also supports dichotomous, categorical, and time-to-event outcomes.</p>
<p>We adopt the simulation setting proposed by <span class="citation">Friedman (<a href="#ref-friedman1991multivariate">1991</a>)</span>, which is also used by <span class="citation">Chipman, George, and McCulloch (<a href="#ref-chipman2010bart">2010</a>)</span>:</p>
<p><span class="math display">\[
y_i = 10\sin(\pi x_{i1}x_{i2}) + 20(x_{i3}-0.5)^2 + 10 x_{i4} + 5 x_{i5} + \mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mu_i \sim N(0,1)\)</span>, for <span class="math inline">\(i = 1, 2, \dots, 500\)</span>.</p>
<p>We set the hyperparameters to <span class="math inline">\((\alpha, \beta, k, J, v, q) = (0.95, 2, 2, 200, 3, 0.9)\)</span>, with a burn-in of 100 iterations, a thinning parameter of 1, and 1,000 post-burn-in MCMC iterations.</p>
<p>We analyze the trace plot of the posterior draws of <span class="math inline">\(\sigma\)</span> to assess convergence, compare the true values of <span class="math inline">\(y\)</span> with the posterior mean and 95% predictive intervals for both the training and test sets (using 80% of the data for training and 20% for testing), and visualize the relative importance of the regressors across different values of <span class="math inline">\(J = 10, 20, 50, 100, 200\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="sec12_3.html#cb27-1" tabindex="-1"></a><span class="do">####### Bayesian Additive Regression Trees #######</span></span>
<span id="cb27-2"><a href="sec12_3.html#cb27-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb27-3"><a href="sec12_3.html#cb27-3" tabindex="-1"></a><span class="fu">library</span>(BART); <span class="fu">library</span>(tidyr)</span></code></pre></div>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## 
## Attaching package: &#39;nlme&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="sec12_3.html#cb32-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2); <span class="fu">library</span>(dplyr)</span>
<span id="cb32-2"><a href="sec12_3.html#cb32-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">500</span>; K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb32-3"><a href="sec12_3.html#cb32-3" tabindex="-1"></a><span class="co"># Simulate the dataset</span></span>
<span id="cb32-4"><a href="sec12_3.html#cb32-4" tabindex="-1"></a>MeanFunct <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb32-5"><a href="sec12_3.html#cb32-5" tabindex="-1"></a>    f <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">sin</span>(pi<span class="sc">*</span>x[,<span class="dv">1</span>]<span class="sc">*</span>x[,<span class="dv">2</span>]) <span class="sc">+</span> <span class="dv">20</span><span class="sc">*</span>(x[,<span class="dv">3</span>]<span class="sc">-</span>.<span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">10</span><span class="sc">*</span>x[,<span class="dv">4</span>]<span class="sc">+</span><span class="dv">5</span><span class="sc">*</span>x[,<span class="dv">5</span>]</span>
<span id="cb32-6"><a href="sec12_3.html#cb32-6" tabindex="-1"></a>    <span class="fu">return</span>(f)</span>
<span id="cb32-7"><a href="sec12_3.html#cb32-7" tabindex="-1"></a>}</span>
<span id="cb32-8"><a href="sec12_3.html#cb32-8" tabindex="-1"></a>sig2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb32-9"><a href="sec12_3.html#cb32-9" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, sig2<span class="sc">^</span><span class="fl">0.5</span>)</span>
<span id="cb32-10"><a href="sec12_3.html#cb32-10" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(N<span class="sc">*</span>K),N,K)</span>
<span id="cb32-11"><a href="sec12_3.html#cb32-11" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">MeanFunct</span>(X[,<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]) <span class="sc">+</span> e</span>
<span id="cb32-12"><a href="sec12_3.html#cb32-12" tabindex="-1"></a><span class="co"># Train and test</span></span>
<span id="cb32-13"><a href="sec12_3.html#cb32-13" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb32-14"><a href="sec12_3.html#cb32-14" tabindex="-1"></a>Ntrain <span class="ot">&lt;-</span> <span class="fu">floor</span>(c <span class="sc">*</span> N)</span>
<span id="cb32-15"><a href="sec12_3.html#cb32-15" tabindex="-1"></a>Ntest <span class="ot">&lt;-</span> N <span class="sc">-</span> Ntrain</span>
<span id="cb32-16"><a href="sec12_3.html#cb32-16" tabindex="-1"></a>X.train <span class="ot">&lt;-</span> X[<span class="dv">1</span><span class="sc">:</span>Ntrain, ]</span>
<span id="cb32-17"><a href="sec12_3.html#cb32-17" tabindex="-1"></a>y.train <span class="ot">&lt;-</span> y[<span class="dv">1</span><span class="sc">:</span>Ntrain]</span>
<span id="cb32-18"><a href="sec12_3.html#cb32-18" tabindex="-1"></a>X.test <span class="ot">&lt;-</span> X[(Ntrain<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>N, ]</span>
<span id="cb32-19"><a href="sec12_3.html#cb32-19" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[(Ntrain<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>N]</span>
<span id="cb32-20"><a href="sec12_3.html#cb32-20" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb32-21"><a href="sec12_3.html#cb32-21" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.95</span>; beta <span class="ot">&lt;-</span> <span class="dv">2</span>; k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb32-22"><a href="sec12_3.html#cb32-22" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="dv">3</span>; q <span class="ot">&lt;-</span> <span class="fl">0.9</span>; J <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb32-23"><a href="sec12_3.html#cb32-23" tabindex="-1"></a><span class="co"># MCMC parameters</span></span>
<span id="cb32-24"><a href="sec12_3.html#cb32-24" tabindex="-1"></a>MCMCiter <span class="ot">&lt;-</span> <span class="dv">1000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">100</span>; thinning <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb32-25"><a href="sec12_3.html#cb32-25" tabindex="-1"></a><span class="co"># Estimate BART</span></span>
<span id="cb32-26"><a href="sec12_3.html#cb32-26" tabindex="-1"></a>BARTfit <span class="ot">&lt;-</span> <span class="fu">wbart</span>(<span class="at">x.train =</span> X.train, <span class="at">y.train =</span> y.train, <span class="at">x.test =</span> X.test, <span class="at">base =</span> alpha,</span>
<span id="cb32-27"><a href="sec12_3.html#cb32-27" tabindex="-1"></a><span class="at">power =</span> beta, <span class="at">k =</span> k, <span class="at">sigdf =</span> v, <span class="at">sigquant =</span> q, <span class="at">ntree =</span> J,</span>
<span id="cb32-28"><a href="sec12_3.html#cb32-28" tabindex="-1"></a><span class="at">ndpost =</span> MCMCiter, <span class="at">nskip =</span> burnin, <span class="at">keepevery =</span> thinning)</span></code></pre></div>
<pre><code>## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 200
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.481183,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 7s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="sec12_3.html#cb34-1" tabindex="-1"></a><span class="co"># Trace plot sigma</span></span>
<span id="cb34-2"><a href="sec12_3.html#cb34-2" tabindex="-1"></a>keep <span class="ot">&lt;-</span> <span class="fu">seq</span>(burnin <span class="sc">+</span> <span class="dv">1</span>, MCMCiter <span class="sc">+</span> burnin, thinning)</span>
<span id="cb34-3"><a href="sec12_3.html#cb34-3" tabindex="-1"></a>df_sigma <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">iteration =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(keep), <span class="at">sigma =</span> BARTfit<span class="sc">$</span>sigma[keep])</span>
<span id="cb34-4"><a href="sec12_3.html#cb34-4" tabindex="-1"></a><span class="fu">ggplot</span>(df_sigma, <span class="fu">aes</span>(<span class="at">x =</span> iteration, <span class="at">y =</span> sigma)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Trace Plot of Sigma&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Iteration&quot;</span>, <span class="at">y =</span> <span class="fu">expression</span>(sigma)) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-3-1.svg" width="672" /></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="sec12_3.html#cb35-1" tabindex="-1"></a><span class="co"># Prediction plot training</span></span>
<span id="cb35-2"><a href="sec12_3.html#cb35-2" tabindex="-1"></a>train_preds <span class="ot">&lt;-</span> <span class="fu">data.frame</span>( <span class="at">y_true =</span> y.train, <span class="at">mean =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.train, <span class="dv">2</span>, mean),</span>
<span id="cb35-3"><a href="sec12_3.html#cb35-3" tabindex="-1"></a><span class="at">lower =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.train, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>), <span class="at">upper =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.train, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(y_true) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">index =</span> <span class="fu">row_number</span>())</span>
<span id="cb35-4"><a href="sec12_3.html#cb35-4" tabindex="-1"></a><span class="fu">ggplot</span>(train_preds, <span class="fu">aes</span>(<span class="at">x =</span> index)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper, <span class="at">fill =</span> <span class="st">&quot;95% Interval&quot;</span>), <span class="at">alpha =</span> <span class="fl">0.4</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean, <span class="at">color =</span> <span class="st">&quot;Predicted Mean&quot;</span>)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y_true, <span class="at">color =</span> <span class="st">&quot;True y&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&quot;Line&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted Mean&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;True y&quot;</span> <span class="ot">=</span> <span class="st">&quot;black&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_fill_manual</span>(<span class="at">name =</span> <span class="st">&quot;Interval&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;95% Interval&quot;</span> <span class="ot">=</span> <span class="st">&quot;lightblue&quot;</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Training Data: Ordered Predictions with 95% Intervals&quot;</span>,</span>
<span id="cb35-5"><a href="sec12_3.html#cb35-5" tabindex="-1"></a><span class="at">x =</span> <span class="st">&quot;Ordered Index&quot;</span>, <span class="at">y =</span> <span class="st">&quot;y&quot;</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-3-2.svg" width="672" /></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="sec12_3.html#cb36-1" tabindex="-1"></a><span class="co"># Prediction plot test</span></span>
<span id="cb36-2"><a href="sec12_3.html#cb36-2" tabindex="-1"></a>test_preds <span class="ot">&lt;-</span> <span class="fu">data.frame</span>( <span class="at">y_true =</span> y.test, <span class="at">mean =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.test, <span class="dv">2</span>, mean), <span class="at">lower =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.test, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>), <span class="at">upper =</span> <span class="fu">apply</span>(BARTfit<span class="sc">$</span>yhat.test, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>)) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(y_true) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">index =</span> <span class="fu">row_number</span>())</span>
<span id="cb36-3"><a href="sec12_3.html#cb36-3" tabindex="-1"></a></span>
<span id="cb36-4"><a href="sec12_3.html#cb36-4" tabindex="-1"></a><span class="fu">ggplot</span>(test_preds, <span class="fu">aes</span>(<span class="at">x =</span> index)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper, <span class="at">fill =</span> <span class="st">&quot;95% Interval&quot;</span>), <span class="at">alpha =</span> <span class="fl">0.4</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean, <span class="at">color =</span> <span class="st">&quot;Predicted Mean&quot;</span>)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y_true, <span class="at">color =</span> <span class="st">&quot;True y&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">name =</span> <span class="st">&quot;Line&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Predicted Mean&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;True y&quot;</span> <span class="ot">=</span> <span class="st">&quot;black&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_fill_manual</span>(<span class="at">name =</span> <span class="st">&quot;Interval&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;95% Interval&quot;</span> <span class="ot">=</span> <span class="st">&quot;lightblue&quot;</span>)) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Test Data: Ordered Predictions with 95% Intervals&quot;</span>,</span>
<span id="cb36-5"><a href="sec12_3.html#cb36-5" tabindex="-1"></a><span class="at">x =</span> <span class="st">&quot;Ordered Index&quot;</span>, <span class="at">y =</span> <span class="st">&quot;y&quot;</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-3-3.svg" width="672" /></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="sec12_3.html#cb37-1" tabindex="-1"></a><span class="co"># Relevant regressors</span></span>
<span id="cb37-2"><a href="sec12_3.html#cb37-2" tabindex="-1"></a>Js <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>)</span>
<span id="cb37-3"><a href="sec12_3.html#cb37-3" tabindex="-1"></a>VarImportance <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(Js), K)</span>
<span id="cb37-4"><a href="sec12_3.html#cb37-4" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb37-5"><a href="sec12_3.html#cb37-5" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> Js){</span>
<span id="cb37-6"><a href="sec12_3.html#cb37-6" tabindex="-1"></a>    BARTfit <span class="ot">&lt;-</span> <span class="fu">wbart</span>(<span class="at">x.train =</span> X.train, <span class="at">y.train =</span> y.train, <span class="at">x.test =</span> X.test, <span class="at">base =</span> alpha,</span>
<span id="cb37-7"><a href="sec12_3.html#cb37-7" tabindex="-1"></a>    <span class="at">power =</span> beta, <span class="at">k =</span> k, <span class="at">sigdf =</span> v, <span class="at">sigquant =</span> q, <span class="at">ntree =</span> j,</span>
<span id="cb37-8"><a href="sec12_3.html#cb37-8" tabindex="-1"></a>    <span class="at">ndpost =</span> MCMCiter, <span class="at">nskip =</span> burnin, <span class="at">keepevery =</span> thinning)</span>
<span id="cb37-9"><a href="sec12_3.html#cb37-9" tabindex="-1"></a>    VarImportance[l, ] <span class="ot">&lt;-</span> BARTfit[[<span class="st">&quot;varcount.mean&quot;</span>]]<span class="sc">/</span>j</span>
<span id="cb37-10"><a href="sec12_3.html#cb37-10" tabindex="-1"></a>    l <span class="ot">&lt;-</span> l <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb37-11"><a href="sec12_3.html#cb37-11" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 10
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,2.151914,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 0s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000
## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 20
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,1.521633,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 1s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000
## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 50
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.962365,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 2s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000
## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 100
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.680495,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 4s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000
## *****Into main of wbart
## *****Data:
## data:n,p,np: 400, 10, 100
## y1,yn: -4.287055, -6.380098
## x1,x[n*p]: 0.031887, 0.455171
## xp1,xp[np*p]: 0.373776, 0.051661
## *****Number of Trees: 200
## *****Number of Cut Points: 100 ... 100
## *****burn and ndpost: 100, 1000
## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.481183,3.000000,1.534927
## *****sigma: 2.807107
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0
## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000
## *****printevery: 100
## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1
## 
## MCMC
## done 0 (out of 1100)
## done 100 (out of 1100)
## done 200 (out of 1100)
## done 300 (out of 1100)
## done 400 (out of 1100)
## done 500 (out of 1100)
## done 600 (out of 1100)
## done 700 (out of 1100)
## done 800 (out of 1100)
## done 900 (out of 1100)
## done 1000 (out of 1100)
## time: 7s
## check counts
## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="sec12_3.html#cb39-1" tabindex="-1"></a><span class="fu">rownames</span>(VarImportance) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;10&quot;</span>, <span class="st">&quot;20&quot;</span>, <span class="st">&quot;50&quot;</span>, <span class="st">&quot;100&quot;</span>, <span class="st">&quot;200&quot;</span>)</span>
<span id="cb39-2"><a href="sec12_3.html#cb39-2" tabindex="-1"></a><span class="fu">colnames</span>(VarImportance) <span class="ot">&lt;-</span> <span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb39-3"><a href="sec12_3.html#cb39-3" tabindex="-1"></a>importance_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(VarImportance) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">trees =</span> <span class="fu">rownames</span>(.)) <span class="sc">%&gt;%</span></span>
<span id="cb39-4"><a href="sec12_3.html#cb39-4" tabindex="-1"></a><span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">-</span>trees, <span class="at">names_to =</span> <span class="st">&quot;variable&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;percent_used&quot;</span>)</span>
<span id="cb39-5"><a href="sec12_3.html#cb39-5" tabindex="-1"></a>importance_df<span class="sc">$</span>variable <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(importance_df<span class="sc">$</span>variable)</span>
<span id="cb39-6"><a href="sec12_3.html#cb39-6" tabindex="-1"></a>importance_df<span class="sc">$</span>trees <span class="ot">&lt;-</span> <span class="fu">factor</span>(importance_df<span class="sc">$</span>trees, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;10&quot;</span>, <span class="st">&quot;20&quot;</span>, <span class="st">&quot;50&quot;</span>, <span class="st">&quot;100&quot;</span>, <span class="st">&quot;200&quot;</span>))</span>
<span id="cb39-7"><a href="sec12_3.html#cb39-7" tabindex="-1"></a></span>
<span id="cb39-8"><a href="sec12_3.html#cb39-8" tabindex="-1"></a><span class="fu">ggplot</span>(importance_df, <span class="fu">aes</span>(<span class="at">x =</span> variable, <span class="at">y =</span> percent_used, <span class="at">color =</span> trees, <span class="at">linetype =</span> trees)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;10&quot;</span> <span class="ot">=</span> <span class="st">&quot;red&quot;</span>, <span class="st">&quot;20&quot;</span> <span class="ot">=</span> <span class="st">&quot;green&quot;</span>, <span class="st">&quot;50&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;100&quot;</span> <span class="ot">=</span> <span class="st">&quot;cyan&quot;</span>, <span class="st">&quot;200&quot;</span> <span class="ot">=</span> <span class="st">&quot;magenta&quot;</span>)) <span class="sc">+</span> <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;variable&quot;</span>, <span class="at">y =</span> <span class="st">&quot;percent used&quot;</span>, <span class="at">color =</span> <span class="st">&quot;#trees&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;#trees&quot;</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-3-4.svg" width="672" /></p>
<p>The first figure displays the trace plot of <span class="math inline">\(\sigma\)</span>, which appears to have reached a stationary distribution. However, the posterior draws are slightly lower than the true value (1).</p>
<p>The second and third figures compare the true values of <span class="math inline">\(y\)</span> with the posterior mean and 95% predictive intervals. The BART model performs well in both sets, and the intervals in the test set are notably wider than those in the training set.</p>
<p>The last figure shows the relative frequency with which each variable is used in the trees, a measure of variable relevance. When the number of trees is small, the algorithm more clearly identifies the most relevant predictors (variables 1–5). As the number of trees increases, this discrimination gradually disappears.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-breiman1996bagging" class="csl-entry">
Breiman, Leo. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24 (2): 123–40.
</div>
<div id="ref-breiman2001random" class="csl-entry">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-breiman1984classification" class="csl-entry">
Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. <em>Classification and Regression Trees</em>. Belmont, CA: Wadsworth International Group.
</div>
<div id="ref-chipman1998bayesian" class="csl-entry">
Chipman, Hugh A, Edward I George, and Robert E McCulloch. 1998. <span>“Bayesian CART Model Search.”</span> <em>Journal of the American Statistical Association</em> 93 (443): 935–48. <a href="https://doi.org/10.1080/01621459.1998.10473750">https://doi.org/10.1080/01621459.1998.10473750</a>.
</div>
<div id="ref-chipman2010bart" class="csl-entry">
———. 2010. <span>“BART: Bayesian Additive Regression Trees.”</span> <em>The Annals of Applied Statistics</em> 4 (1): 266–98.
</div>
<div id="ref-freund1997decision" class="csl-entry">
Freund, Yoav, and Robert E Schapire. 1997. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.
</div>
<div id="ref-friedman1991multivariate" class="csl-entry">
Friedman, Jerome H. 1991. <span>“Multivariate Adaptive Regression Splines.”</span> <em>Annals of Statistics</em> 19 (1): 1–67. <a href="https://doi.org/10.1214/aos/1176347963">https://doi.org/10.1214/aos/1176347963</a>.
</div>
<div id="ref-friedman2001greedy" class="csl-entry">
———. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em> 29 (5): 1189–1232.
</div>
<div id="ref-hahn2020bayesian" class="csl-entry">
Hahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020. <span>“Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).”</span> <em>Bayesian Analysis</em> 15 (3): 965–1056.
</div>
<div id="ref-hastie2000bayesian" class="csl-entry">
Hastie, Trevor, and Robert Tibshirani. 2000. <span>“Bayesian Backfitting (with Discussion).”</span> <em>Journal of the American Statistical Association</em> 95 (452): 1228–40. <a href="https://doi.org/10.1080/01621459.2000.10474339">https://doi.org/10.1080/01621459.2000.10474339</a>.
</div>
<div id="ref-hill2011bayesian" class="csl-entry">
Hill, Jennifer L. 2011. <span>“Bayesian Nonparametric Modeling for Causal Inference.”</span> <em>Journal of Computational and Graphical Statistics</em> 20 (1): 217–40.
</div>
<div id="ref-linero2018bayesian" class="csl-entry">
Linero, Antonio R. 2018. <span>“Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.”</span> <em>Journal of the American Statistical Association</em> 113 (522): 626–36.
</div>
<div id="ref-rockova2020semiparametric" class="csl-entry">
Ročková, Veronika. 2020. <span>“On Semi-Parametric Inference for <span>BART</span>.”</span> In <em>Proceedings of the 37th International Conference on Machine Learning</em>, edited by Hal Daumé III and Aarti Singh, 119:8137–46. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v119/rockova20a.html">https://proceedings.mlr.press/v119/rockova20a.html</a>.
</div>
<div id="ref-rockova2020posterior" class="csl-entry">
Ročková, Veronika, and Stephanie van der Pas. 2020. <span>“Posterior Concentration for Bayesian Regression Trees and Forests.”</span> <em>The Annals of Statistics</em> 48 (4): 2103–30. <a href="https://doi.org/10.1214/19-AOS1879">https://doi.org/10.1214/19-AOS1879</a>.
</div>
<div id="ref-rockova2019on" class="csl-entry">
Ročková, Veronika, and Enakshi Saha. 2019. <span>“On Theory for BART.”</span> In <em>Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics</em>, edited by Kamalika Chaudhuri and Masashi Sugiyama, 89:2839–48. Proceedings of Machine Learning Research. PMLR.
</div>
<div id="ref-sparapani2021nonparametric" class="csl-entry">
Sparapani, Rodney A., Charles Spanbauer, and Robert McCulloch. 2021. <span>“Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART r Package.”</span> <em>Journal of Statistical Software</em> 97 (1): 1–66. <a href="https://doi.org/10.18637/jss.v097.i01">https://doi.org/10.18637/jss.v097.i01</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12_2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec12_4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/13-RecentDev.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
