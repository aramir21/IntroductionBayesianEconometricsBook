<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.2 The Gaussian linear model | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="10.2 The Gaussian linear model | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.2 The Gaussian linear model | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-11-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec10_1.html"/>
<link rel="next" href="sec103.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded toolkit using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model averaging</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal/Panel data models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Identification setting</a></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Randomized controlled trial (RCT)</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Conditional independence assumption (CIA)</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Instrumental variables (IV)</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference-in-differences design (DiD)</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="sec13_5.html"><a href="sec13_5.html#sec13_51"><i class="fa fa-check"></i><b>13.5.1</b> Classic DiD: two-group and two-period (<span class="math inline">\(2\times2\)</span>) setup</a></li>
<li class="chapter" data-level="13.5.2" data-path="sec13_5.html"><a href="sec13_5.html#sec13_52"><i class="fa fa-check"></i><b>13.5.2</b> Staggered (SDiD): G-group and T-period (<span class="math inline">\(G\times T\)</span>) setup</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Regression discontinuity design (RD)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="sec13_6.html"><a href="sec13_6.html#sec13_61"><i class="fa fa-check"></i><b>13.6.1</b> Sharp regression discontinuity design (SRD)</a></li>
<li class="chapter" data-level="13.6.2" data-path="sec13_6.html"><a href="sec13_6.html#sec13_62"><i class="fa fa-check"></i><b>13.6.2</b> Fuzzy regression discontinuity design (FRD)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Sample selection</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Bayesian exponentially tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.9" data-path="sec13_9.html"><a href="sec13_9.html"><i class="fa fa-check"></i><b>13.9</b> General Bayes posteriors</a></li>
<li class="chapter" data-level="13.10" data-path="sec13_10.html"><a href="sec13_10.html"><i class="fa fa-check"></i><b>13.10</b> Doubly robust Bayesian inferential framework (DRB)</a></li>
<li class="chapter" data-level="13.11" data-path="sec13_11.html"><a href="sec13_11.html"><i class="fa fa-check"></i><b>13.11</b> Summary</a></li>
<li class="chapter" data-level="13.12" data-path="sec13_12.html"><a href="sec13_12.html"><i class="fa fa-check"></i><b>13.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="packages-and-commands-in-besmarter-gui.html"><a href="packages-and-commands-in-besmarter-gui.html"><i class="fa fa-check"></i>Packages and commands in BEsmarter GUI</a></li>
<li class="chapter" data-level="" data-path="datasets-templates-in-folder-datasim.html"><a href="datasets-templates-in-folder-datasim.html"><i class="fa fa-check"></i>Datasets templates in folder DataSim</a></li>
<li class="chapter" data-level="" data-path="real-datasets-in-folder-dataapp.html"><a href="real-datasets-in-folder-dataapp.html"><i class="fa fa-check"></i>Real datasets in folder DataApp</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec102" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> The Gaussian linear model<a href="sec102.html#sec102" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Gaussian linear model specifies <span class="math inline">\(\boldsymbol{y}=\alpha\boldsymbol{i}_N+\boldsymbol{X}_m\boldsymbol{\beta}_m+\boldsymbol{\mu}_m\)</span> such that <span class="math inline">\(\boldsymbol{\mu}_m\sim{N}(\boldsymbol{0},\sigma^2\boldsymbol{I}_n)\)</span>, <span class="math inline">\(\boldsymbol{i}_N\)</span> is a <span class="math inline">\(N\)</span>-dimensional column of ones, <span class="math inline">\(\boldsymbol{X}_m\)</span> is the design matrix without the column of ones, and <span class="math inline">\(N\)</span> is the sample size. Following <span class="citation">G. M. Koop (<a href="#ref-koop2003bayesian">2003</a>)</span>, the conjugate prior for the location parameters is <span class="math inline">\(\boldsymbol{\beta}_m|\sigma^2 \sim {N}(\boldsymbol{\beta}_{m0}, \sigma^2 \boldsymbol{B}_{m0})\)</span>, and the priors for <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\alpha\)</span> can be improper, as these parameters are common to all models <span class="math inline">\(\mathcal{M}_m\)</span>. Particularly, <span class="math inline">\(\pi(\sigma^2)\propto 1/\sigma^2\)</span> (Jeffreys’ prior for the linear Gaussian model, see <span class="citation">Ibrahim and Laud (<a href="#ref-prior1991bayesian">1991</a>)</span>) and <span class="math inline">\(\pi(\alpha)\propto 1\)</span>.</p>
<p>The selection of the hyperparameters of <span class="math inline">\(\boldsymbol{\beta}_m\)</span> is more critical, as these parameters are not common to all models. A very common prior for the location parameters in the BMA literature is the Zellner’s prior <span class="citation">(<a href="#ref-zellner1986assessing">Zellner 1986</a>)</span>, where <span class="math inline">\(\boldsymbol{\beta}_{m0}=\boldsymbol{0}_m\)</span> and <span class="math inline">\(\boldsymbol{B}_{m0}=(g_m\boldsymbol{X}_m^{\top}\boldsymbol{X}_m)^{-1}\)</span>. Observe that this covariance matrix is similar to the covariance matrix of the ordinary least squares estimator of the location parameters. This suggests that there is compatibility between the prior information and the sample information, and the only parameter to elicit is <span class="math inline">\(g_m\geq 0\)</span>, which facilitates the elicitation process, as eliciting covariance matrices is a very hard endeavor.</p>
<p>Following same steps as in Section <a href="sec43.html#sec43">3.3</a>, the posterior conditional distribution of <span class="math inline">\(\boldsymbol{\beta}_m\)</span> has covariance matrix <span class="math inline">\(\sigma^2\boldsymbol{B}_{mn}\)</span>, where <span class="math inline">\(\boldsymbol{B}_{mn}=((1+g_m)\boldsymbol{X}_m^{\top}\boldsymbol{X}_m)^{-1}\)</span> (Exercise 1), which means that <span class="math inline">\(g_m=0\)</span> implies a non-informative prior, whereas <span class="math inline">\(g_m=1\)</span> implies that prior and data information have same weights. We follow <span class="citation">Fernandez, Ley, and Steel (<a href="#ref-fernandez2001benchmark">2001</a>)</span>, who recommend
<span class="math display">\[\begin{align*}
    g_m &amp; =
    \begin{Bmatrix}
        1/K^2, &amp; N \leq K^2\\
        1/N, &amp; N&gt;K^2
    \end{Bmatrix},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of regressors.</p>
<p>Given the likelihood function,
<span class="math display">\[\begin{equation*}
    p(\boldsymbol{\beta}_m, \sigma^2|\boldsymbol{y}, \boldsymbol{X}_m) = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\boldsymbol{y} - \alpha\boldsymbol{i}_N - \boldsymbol{X}_m\boldsymbol{\beta}_m)^{\top}(\boldsymbol{y} - \alpha\boldsymbol{i}_N - \boldsymbol{X}_m\boldsymbol{\beta}_m) \right\},
\end{equation*}\]</span>
the marginal likelihood associated with model <span class="math inline">\(\mathcal{M}_m\)</span> is proportional to (Exercise 1)
<span class="math display">\[\begin{align*}
    p(\boldsymbol{y}|\mathcal{M}_m)&amp;\propto \left(\frac{g_m}{1+g_m}\right)^{k_m/2} \left[(\boldsymbol{y}-\bar{y}\boldsymbol{i}_N)^{\top}(\boldsymbol{y}-\bar{y}\boldsymbol{i}_N)-\frac{1}{1+g_m}(\boldsymbol{y}^{\top}\boldsymbol{P}_{X_m}\boldsymbol{y})\right]^{-(N-1)/2},
\end{align*}\]</span>
where all parameters are indexed to model <span class="math inline">\(\mathcal{M}_m\)</span>, <span class="math inline">\(\boldsymbol{P}_{X_m}=\boldsymbol{X}_m(\boldsymbol{X}_m^{\top}\boldsymbol{X}_m)^{-1}\boldsymbol{X}_m\)</span> is the projection matrix on the space generated by the columns of <span class="math inline">\(\boldsymbol{X}_m\)</span>, and <span class="math inline">\(\bar{y}\)</span> is the sample mean of <span class="math inline">\(\boldsymbol{y}\)</span>.</p>
<p>We implement in our GUI four approaches to perform BMA in the Gaussian linear model: the BIC approximation using the Occam’s window approach, the MC3 algorithm using the analytical expression for calculating the marginal likelihood, an instrumental variable approach based on conditional likelihoods, and dynamic variable selection.</p>
<p><strong>Example: Simulation exercise</strong></p>
<p>Let’s perform a simulation exercise to assess the performance of the BIC approximation using the Occam’s window, and the Markov chain Monte Carlo model composition approaches. Let’s set a model where the computational burden is low and we know the data generating process (population statistical model). In particular, we set 10 regressors such that <span class="math inline">\(x_k\sim N(1, 1)\)</span>, <span class="math inline">\(k =1,\dots,6\)</span>, and <span class="math inline">\(x_k\sim B(0.5)\)</span>, <span class="math inline">\(k=7,\dots,10\)</span>. We set <span class="math inline">\(\boldsymbol{\beta}=[1 \ 0 \ 0 \ 0 \ 0.5 \ 0, 0, 0, 0, -0.7]^{\top}\)</span> such that just <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_5\)</span> and <span class="math inline">\(x_{10}\)</span> are relevant to drive <span class="math inline">\(y_i=1+\boldsymbol{x}^{\top}\boldsymbol{\beta}+\mu_i\)</span>, <span class="math inline">\(\mu_i\sim N(0,0.5^2)\)</span>. Observe that we just have <span class="math inline">\(2^{10}=1,024\)</span> models in this setting, thus, we can calculate the posterior model probability for each model.</p>
<p>Our GUI uses the commands <em>bicreg</em> and <em>MC3.REG</em> from the package <em>BMA</em> to perform Bayesian model averaging in the linear regression model using the BIC approximation and MC3, respectively. These commands in turn are based on <span class="citation">A. Raftery (<a href="#ref-Raftery1995">1995</a>)</span> and <span class="citation">Adrian E. Raftery, Madigan, and Hoeting (<a href="#ref-Raftery1997">1997</a>)</span>. The following code shows how to perform the simulation and get the posterior mean and standard deviation using these commands with the default values of hyperparameters and tuning parameters.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="sec102.html#cb466-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb466-2"><a href="sec102.html#cb466-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb466-3"><a href="sec102.html#cb466-3" tabindex="-1"></a>K1 <span class="ot">&lt;-</span> <span class="dv">6</span>; K2 <span class="ot">&lt;-</span> <span class="dv">4</span>; K <span class="ot">&lt;-</span> K1 <span class="sc">+</span> K2</span>
<span id="cb466-4"><a href="sec102.html#cb466-4" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N<span class="sc">*</span>K1,<span class="dv">1</span> ,<span class="dv">1</span>), N, K1)</span>
<span id="cb466-5"><a href="sec102.html#cb466-5" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rbinom</span>(N<span class="sc">*</span>K2, <span class="dv">1</span>, <span class="fl">0.5</span>), N, K2)</span>
<span id="cb466-6"><a href="sec102.html#cb466-6" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, X2); e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb466-7"><a href="sec102.html#cb466-7" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="sc">-</span><span class="fl">0.7</span>)</span>
<span id="cb466-8"><a href="sec102.html#cb466-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> X<span class="sc">%*%</span>B <span class="sc">+</span> e</span>
<span id="cb466-9"><a href="sec102.html#cb466-9" tabindex="-1"></a>BMAglm <span class="ot">&lt;-</span> BMA<span class="sc">::</span><span class="fu">bicreg</span>(X, y, <span class="at">strict =</span> <span class="cn">FALSE</span>, <span class="at">OR =</span> <span class="dv">50</span>) </span>
<span id="cb466-10"><a href="sec102.html#cb466-10" tabindex="-1"></a><span class="fu">summary</span>(BMAglm)</span></code></pre></div>
<pre><code>## 
## Call:
## BMA::bicreg(x = X, y = y, strict = FALSE, OR = 50)
## 
## 
##   8  models were selected
##  Best  5  models (cumulative posterior probability =  0.9176 ): 
## 
##            p!=0    EV         SD        model 1     model 2     model 3   
## Intercept  100.0   1.0321222  0.031290   1.032e+00   1.047e+00   1.041e+00
## X1         100.0   1.0018025  0.015274   1.002e+00   1.002e+00   1.002e+00
## X2           3.0  -0.0002625  0.002997       .           .      -8.826e-03
## X3           2.6   0.0001208  0.002530       .           .           .    
## X4           2.9   0.0002341  0.002910       .           .           .    
## X5         100.0   0.4976248  0.015668   4.976e-01   4.975e-01   4.976e-01
## X6           3.9  -0.0005920  0.004256       .      -1.509e-02       .    
## X7           2.8   0.0004292  0.005739       .           .           .    
## X8           2.9   0.0004508  0.005860       .           .           .    
## X9           2.9   0.0004729  0.005914       .           .           .    
## X10        100.0  -0.7035270  0.030939  -7.036e-01  -7.036e-01  -7.031e-01
##                                                                           
## nVar                                       3           4           4      
## r2                                       0.855       0.855       0.855    
## BIC                                     -1.912e+03  -1.906e+03  -1.906e+03
## post prob                                0.791       0.039       0.030    
##            model 4     model 5   
## Intercept   1.024e+00   1.024e+00
## X1          1.002e+00   1.002e+00
## X2              .           .    
## X3              .           .    
## X4          8.150e-03       .    
## X5          4.975e-01   4.976e-01
## X6              .           .    
## X7              .           .    
## X8              .       1.569e-02
## X9              .           .    
## X10        -7.029e-01  -7.034e-01
##                                  
## nVar          4           4      
## r2          0.855       0.855    
## BIC        -1.906e+03  -1.906e+03
## post prob   0.029       0.029</code></pre>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="sec102.html#cb468-1" tabindex="-1"></a>BMAreg <span class="ot">&lt;-</span> BMA<span class="sc">::</span><span class="fu">MC3.REG</span>(y, X, <span class="at">num.its=</span><span class="dv">500</span>)</span></code></pre></div>
<pre><code>## Warning in covMcd(X, alpha = alpha, use.correction = use.correction): The 505-th order statistic of the absolute deviation of variable 7 is
## zero.
## There are 505 observations (in the entire dataset of 1000 obs.) lying
## on the hyperplane with equation a_1*(x_i1 - m_1) + ... + a_p*(x_ip -
## m_p) = 0 with (m_1, ..., m_p) the mean of these observations and
## coefficients a_i from the vector a &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0)</code></pre>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="sec102.html#cb470-1" tabindex="-1"></a>Models <span class="ot">&lt;-</span> <span class="fu">unique</span>(BMAreg[[<span class="st">&quot;variables&quot;</span>]])</span>
<span id="cb470-2"><a href="sec102.html#cb470-2" tabindex="-1"></a>nModels <span class="ot">&lt;-</span> <span class="fu">dim</span>(Models)[<span class="dv">1</span>]</span>
<span id="cb470-3"><a href="sec102.html#cb470-3" tabindex="-1"></a>nVistModels <span class="ot">&lt;-</span> <span class="fu">dim</span>(BMAreg[[<span class="st">&quot;variables&quot;</span>]])[<span class="dv">1</span>]</span>
<span id="cb470-4"><a href="sec102.html#cb470-4" tabindex="-1"></a>PMP <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb470-5"><a href="sec102.html#cb470-5" tabindex="-1"></a><span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nModels){</span>
<span id="cb470-6"><a href="sec102.html#cb470-6" tabindex="-1"></a>    idModm <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb470-7"><a href="sec102.html#cb470-7" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nVistModels){</span>
<span id="cb470-8"><a href="sec102.html#cb470-8" tabindex="-1"></a>        <span class="cf">if</span>(<span class="fu">sum</span>(Models[m,] <span class="sc">==</span> BMAreg[[<span class="st">&quot;variables&quot;</span>]][j,]) <span class="sc">==</span> K){</span>
<span id="cb470-9"><a href="sec102.html#cb470-9" tabindex="-1"></a>            idModm <span class="ot">&lt;-</span> <span class="fu">c</span>(idModm, j)</span>
<span id="cb470-10"><a href="sec102.html#cb470-10" tabindex="-1"></a>        }<span class="cf">else</span>{</span>
<span id="cb470-11"><a href="sec102.html#cb470-11" tabindex="-1"></a>            idModm <span class="ot">&lt;-</span> idModm</span>
<span id="cb470-12"><a href="sec102.html#cb470-12" tabindex="-1"></a>        } </span>
<span id="cb470-13"><a href="sec102.html#cb470-13" tabindex="-1"></a>    }</span>
<span id="cb470-14"><a href="sec102.html#cb470-14" tabindex="-1"></a>    PMPm <span class="ot">&lt;-</span> <span class="fu">sum</span>(BMAreg[[<span class="st">&quot;post.prob&quot;</span>]][idModm])</span>
<span id="cb470-15"><a href="sec102.html#cb470-15" tabindex="-1"></a>    PMP <span class="ot">&lt;-</span> <span class="fu">c</span>(PMP, PMPm)</span>
<span id="cb470-16"><a href="sec102.html#cb470-16" tabindex="-1"></a>}</span>
<span id="cb470-17"><a href="sec102.html#cb470-17" tabindex="-1"></a>PIP <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb470-18"><a href="sec102.html#cb470-18" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K){</span>
<span id="cb470-19"><a href="sec102.html#cb470-19" tabindex="-1"></a>    PIPk <span class="ot">&lt;-</span> <span class="fu">sum</span>(PMP[<span class="fu">which</span>(Models[,k] <span class="sc">==</span> <span class="dv">1</span>)])</span>
<span id="cb470-20"><a href="sec102.html#cb470-20" tabindex="-1"></a>    PIP <span class="ot">&lt;-</span> <span class="fu">c</span>(PIP, PIPk)</span>
<span id="cb470-21"><a href="sec102.html#cb470-21" tabindex="-1"></a>}</span>
<span id="cb470-22"><a href="sec102.html#cb470-22" tabindex="-1"></a><span class="fu">plot</span>(PIP)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-51-1.svg" width="672" /></p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="sec102.html#cb471-1" tabindex="-1"></a>Means <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb471-2"><a href="sec102.html#cb471-2" tabindex="-1"></a>Vars <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb471-3"><a href="sec102.html#cb471-3" tabindex="-1"></a><span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nModels){</span>
<span id="cb471-4"><a href="sec102.html#cb471-4" tabindex="-1"></a>    idXs <span class="ot">&lt;-</span> <span class="fu">which</span>(Models[m,] <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb471-5"><a href="sec102.html#cb471-5" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">length</span>(idXs) <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb471-6"><a href="sec102.html#cb471-6" tabindex="-1"></a>        Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb471-7"><a href="sec102.html#cb471-7" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb471-8"><a href="sec102.html#cb471-8" tabindex="-1"></a>        Xm <span class="ot">&lt;-</span> X[, idXs]</span>
<span id="cb471-9"><a href="sec102.html#cb471-9" tabindex="-1"></a>        Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> Xm)</span>
<span id="cb471-10"><a href="sec102.html#cb471-10" tabindex="-1"></a>        SumRegm <span class="ot">&lt;-</span> <span class="fu">summary</span>(Regm)</span>
<span id="cb471-11"><a href="sec102.html#cb471-11" tabindex="-1"></a>        Means[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb471-12"><a href="sec102.html#cb471-12" tabindex="-1"></a>        Vars[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb471-13"><a href="sec102.html#cb471-13" tabindex="-1"></a>    } </span>
<span id="cb471-14"><a href="sec102.html#cb471-14" tabindex="-1"></a>}</span>
<span id="cb471-15"><a href="sec102.html#cb471-15" tabindex="-1"></a>BMAmeans <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Means<span class="sc">*</span>PMP)</span>
<span id="cb471-16"><a href="sec102.html#cb471-16" tabindex="-1"></a>BMAsd <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(PMP<span class="sc">*</span>Vars)  <span class="sc">+</span> <span class="fu">colSums</span>(PMP<span class="sc">*</span>(Means<span class="sc">-</span><span class="fu">matrix</span>(<span class="fu">rep</span>(BMAmeans, <span class="at">each =</span> nModels), nModels, K))<span class="sc">^</span><span class="dv">2</span>))<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb471-17"><a href="sec102.html#cb471-17" tabindex="-1"></a>BMAmeans</span></code></pre></div>
<pre><code>##  [1]  1.001771e+00 -5.322016e-05  6.635422e-06  3.721457e-07  4.976335e-01
##  [6] -1.271339e-04  1.000932e-08  2.107441e-05  6.578654e-06 -7.035557e-01</code></pre>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="sec102.html#cb473-1" tabindex="-1"></a>BMAsd</span></code></pre></div>
<pre><code>##  [1] 1.527261e-02 1.353624e-03 5.936816e-04 1.163947e-04 1.566698e-02
##  [6] 1.987360e-03 2.778896e-05 1.270579e-03 6.997305e-04 3.093389e-02</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="sec102.html#cb475-1" tabindex="-1"></a>BMAmeans<span class="sc">/</span>BMAsd</span></code></pre></div>
<pre><code>##  [1]  6.559266e+01 -3.931680e-02  1.117673e-02  3.197272e-03  3.176320e+01
##  [6] -6.397124e-02  3.601905e-04  1.658647e-02  9.401697e-03 -2.274385e+01</code></pre>
<p>We can see from the results that the BIC approximation with the Occam’s window, and the MC3 algorithm perform a good job finding the relevant regressors, and their posterior BMA means are very close to the population values. We also see that the BMA results are very similar in the two approaches.</p>
<p>We can perform Bayesian model averaging in our GUI for linear Gaussian models using the BIC approximation and MC3 using the following Algorithms. We ask in Exercise 2 to perform BMA using the dataset <em>10ExportDiversificationHHI.csv</em> from <span class="citation">M. Jetter and Ramírez Hassan (<a href="#ref-Jetter2015">2015</a>)</span>.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Bayesian Model Averaging in Linear Gaussian Models using the Bayesian Information Criterion</strong></p>
<ol style="list-style-type: decimal">
<li><p>Select <em>Bayesian Model Averaging</em> on the top panel</p></li>
<li><p>Select <em>Normal data</em> model using the left radio button</p></li>
<li><p>Select <em>BIC</em> using the right radio button under <strong>Which type do you want to perform?</strong></p></li>
<li><p>Upload the dataset, selecting first if there is a header in the file and the kind of separator in the <em>csv</em> file of the dataset (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Type the <em>OR</em> number of the Occam’s window in the box under <strong>OR: Number between 5 and 50</strong> (this step is optional, as the default value is 50)</p></li>
<li><p>Click the <em>Go!</em> button</p></li>
<li><p>Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset, the PIP (posterior inclusion probability, <strong>p!=0</strong>), the BMA posterior mean (<strong>EV</strong>), the BMA standard deviation (<strong>SD</strong>), and the posterior mean for models with the highest PMP. At the bottom of the table, for the models with the largest PMP, the number of variables (<strong>nVar</strong>), the coefficient of determination (<strong>r2</strong>), the BIC, and the PMP (<strong>post prob</strong>) are displayed</p></li>
<li><p>Download posterior results using the <em>Download results using BIC</em> button. Two files are provided:</p>
<ul>
<li>The first file contains the best models by row according to the PMP (last column), indicating variable inclusion with a 1 (0 indicates no inclusion)<br />
</li>
<li>The second file contains the PIP, the BMA expected value, and the standard deviation for each variable in the dataset</li>
</ul></li>
</ol>
</div>
</div>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Bayesian Model Averaging in Linear Gaussian Models using Markov Chain Monte Carlo Model Composition</strong></p>
<ol style="list-style-type: decimal">
<li><p>Select <em>Bayesian Model Averaging</em> on the top panel</p></li>
<li><p>Select <em>Normal data</em> model using the left radio button</p></li>
<li><p>Select <em>MC3</em> using the right radio button under <strong>Which type do you want to perform?</strong></p></li>
<li><p>Upload the dataset, selecting first if there is a header in the file and the kind of separator in the <em>csv</em> file of the dataset (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Select MC3 iterations using the <em>Range slider</em> under the label <strong>MC3 iterations:</strong></p></li>
<li><p>Click the <em>Go!</em> button</p></li>
<li><p>Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset, the PIP (posterior inclusion probability, <strong>p!=0</strong>), the BMA posterior mean (<strong>EV</strong>), the BMA standard deviation (<strong>SD</strong>), and the posterior mean for models with the highest PMP. At the bottom of the table, for the models with the largest PMP, the number of variables (<strong>nVar</strong>), the coefficient of determination (<strong>r2</strong>), the BIC, and the PMP (<strong>post prob</strong>) are displayed</p></li>
<li><p>Download posterior results using the <em>Download results using BIC</em> button. Two files are provided:</p>
<ul>
<li>The first file contains the best models by row according to the PMP (last column), indicating variable inclusion with a 1 (0 indicates no inclusion)<br />
</li>
<li>The second file contains the PIP, the BMA expected value, and the standard deviation for each variable in the dataset</li>
</ul></li>
</ol>
</div>
</div>
<p>We show in the following code how to program a MC3 algorithm from scratch to perform BMA using the setting from the previous simulation exercise. The first part of the code is the function to calculate the log marginal likelihood. This is a small simulation setting, thus we can calculate the marginal likelihood for all 1,024 models, and then calculate the posterior model probability standardizing using the model with the largest log marginal likelihood. We see from the results that this model is the data generating process (population statistical model). We also find that the posterior inclusion probabilities for <span class="math inline">\(x_{1}\)</span>, <span class="math inline">\(x_{5}\)</span> and <span class="math inline">\(x_{10}\)</span> are 1, whereas the PIP for the other variables are less than 0.05.</p>
<p>Although BMA allows incorporating model uncertainty in a regression framework, sometimes it is desirable to select just one model. Two compelling alternatives are the model with the largest posterior model probability, and the median probability model. The latter is the model which includes every predictor that has posterior inclusion probability higher than 0.5. The first model is the best alternative for prediction in the case of a 0–1 loss function <span class="citation">(<a href="#ref-Clyde2004">Clyde and George 2004</a>)</span>, whereas the second is the best alternative when there is a quadratic loss function in prediction <span class="citation">(<a href="#ref-Barbieri2004">Barbieri and Berger 2004</a>)</span>. In this simulation, the two criteria indicate selection of the data generating process.</p>
<p>We also show how to estimate the posterior mean and standard deviation based on BMA in this code. We see that the posterior means are very close to the population parameters.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="sec102.html#cb477-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb477-2"><a href="sec102.html#cb477-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb477-3"><a href="sec102.html#cb477-3" tabindex="-1"></a>K1 <span class="ot">&lt;-</span> <span class="dv">6</span>; K2 <span class="ot">&lt;-</span> <span class="dv">4</span>; K <span class="ot">&lt;-</span> K1 <span class="sc">+</span> K2</span>
<span id="cb477-4"><a href="sec102.html#cb477-4" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N<span class="sc">*</span>K1,<span class="dv">1</span> ,<span class="dv">1</span>), N, K1)</span>
<span id="cb477-5"><a href="sec102.html#cb477-5" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rbinom</span>(N<span class="sc">*</span>K2, <span class="dv">1</span>, <span class="fl">0.5</span>), N, K2)</span>
<span id="cb477-6"><a href="sec102.html#cb477-6" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, X2); e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb477-7"><a href="sec102.html#cb477-7" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="sc">-</span><span class="fl">0.7</span>)</span>
<span id="cb477-8"><a href="sec102.html#cb477-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> X<span class="sc">%*%</span>B <span class="sc">+</span> e</span>
<span id="cb477-9"><a href="sec102.html#cb477-9" tabindex="-1"></a>LogMLfunt <span class="ot">&lt;-</span> <span class="cf">function</span>(Model){</span>
<span id="cb477-10"><a href="sec102.html#cb477-10" tabindex="-1"></a>    indr <span class="ot">&lt;-</span> Model <span class="sc">==</span> <span class="dv">1</span></span>
<span id="cb477-11"><a href="sec102.html#cb477-11" tabindex="-1"></a>    kr <span class="ot">&lt;-</span> <span class="fu">sum</span>(indr)</span>
<span id="cb477-12"><a href="sec102.html#cb477-12" tabindex="-1"></a>    <span class="cf">if</span>(kr <span class="sc">&gt;</span> <span class="dv">0</span>){</span>
<span id="cb477-13"><a href="sec102.html#cb477-13" tabindex="-1"></a>        gr <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(N <span class="sc">&gt;</span> kr<span class="sc">^</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">/</span>N, kr<span class="sc">^</span>(<span class="sc">-</span><span class="dv">2</span>))</span>
<span id="cb477-14"><a href="sec102.html#cb477-14" tabindex="-1"></a>        Xr <span class="ot">&lt;-</span> <span class="fu">matrix</span>(Xnew[ , indr], <span class="at">ncol =</span> kr)</span>
<span id="cb477-15"><a href="sec102.html#cb477-15" tabindex="-1"></a>        PX <span class="ot">&lt;-</span> Xr<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(Xr)<span class="sc">%*%</span>Xr)<span class="sc">%*%</span><span class="fu">t</span>(Xr)</span>
<span id="cb477-16"><a href="sec102.html#cb477-16" tabindex="-1"></a>        s2pos <span class="ot">&lt;-</span> <span class="fu">c</span>((<span class="fu">t</span>(y <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">%*%</span>(y <span class="sc">-</span> <span class="fu">mean</span>(y))) <span class="sc">-</span> <span class="fu">t</span>(y)<span class="sc">%*%</span>PX<span class="sc">%*%</span>y<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> gr))</span>
<span id="cb477-17"><a href="sec102.html#cb477-17" tabindex="-1"></a>        mllMod <span class="ot">&lt;-</span> (kr<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(gr<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>gr))<span class="sc">-</span>(N<span class="dv">-1</span>)<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(s2pos)</span>
<span id="cb477-18"><a href="sec102.html#cb477-18" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb477-19"><a href="sec102.html#cb477-19" tabindex="-1"></a>        gr <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(N <span class="sc">&gt;</span> kr<span class="sc">^</span><span class="dv">2</span>, <span class="dv">1</span><span class="sc">/</span>N, kr<span class="sc">^</span>(<span class="sc">-</span><span class="dv">2</span>))</span>
<span id="cb477-20"><a href="sec102.html#cb477-20" tabindex="-1"></a>        s2pos <span class="ot">&lt;-</span> <span class="fu">c</span>((<span class="fu">t</span>(y <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">%*%</span>(y <span class="sc">-</span> <span class="fu">mean</span>(y))))</span>
<span id="cb477-21"><a href="sec102.html#cb477-21" tabindex="-1"></a>        mllMod <span class="ot">&lt;-</span> (kr<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(gr<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>gr))<span class="sc">-</span>(N<span class="dv">-1</span>)<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(s2pos)</span>
<span id="cb477-22"><a href="sec102.html#cb477-22" tabindex="-1"></a>    }</span>
<span id="cb477-23"><a href="sec102.html#cb477-23" tabindex="-1"></a>    <span class="fu">return</span>(mllMod)</span>
<span id="cb477-24"><a href="sec102.html#cb477-24" tabindex="-1"></a>}</span>
<span id="cb477-25"><a href="sec102.html#cb477-25" tabindex="-1"></a>combs <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb477-26"><a href="sec102.html#cb477-26" tabindex="-1"></a>Xnew <span class="ot">&lt;-</span> <span class="fu">apply</span>(X, <span class="dv">2</span>, scale)</span>
<span id="cb477-27"><a href="sec102.html#cb477-27" tabindex="-1"></a>mll <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span><span class="sc">^</span>K, <span class="cf">function</span>(s){<span class="fu">LogMLfunt</span>(<span class="fu">matrix</span>(combs[s,], <span class="dv">1</span>, K))})</span>
<span id="cb477-28"><a href="sec102.html#cb477-28" tabindex="-1"></a>MaxPMP <span class="ot">&lt;-</span> <span class="fu">which.max</span>(mll); StMarLik <span class="ot">&lt;-</span> <span class="fu">exp</span>(mll<span class="sc">-</span><span class="fu">max</span>(mll))</span>
<span id="cb477-29"><a href="sec102.html#cb477-29" tabindex="-1"></a>PMP <span class="ot">&lt;-</span> StMarLik<span class="sc">/</span><span class="fu">sum</span>(StMarLik)</span>
<span id="cb477-30"><a href="sec102.html#cb477-30" tabindex="-1"></a>PMP[MaxPMP]</span></code></pre></div>
<pre><code>## [1] 0.7705196</code></pre>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="sec102.html#cb479-1" tabindex="-1"></a>combs[MaxPMP,]</span></code></pre></div>
<pre><code>##     Var1 Var2 Var3 Var4 Var5 Var6 Var7 Var8 Var9 Var10
## 530    1    0    0    0    1    0    0    0    0     1</code></pre>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="sec102.html#cb481-1" tabindex="-1"></a>PIP <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb481-2"><a href="sec102.html#cb481-2" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K){</span>
<span id="cb481-3"><a href="sec102.html#cb481-3" tabindex="-1"></a>    PIPk <span class="ot">&lt;-</span> <span class="fu">sum</span>(PMP[<span class="fu">which</span>(combs[,k] <span class="sc">==</span> <span class="dv">1</span>)]); PIP <span class="ot">&lt;-</span> <span class="fu">c</span>(PIP, PIPk)</span>
<span id="cb481-4"><a href="sec102.html#cb481-4" tabindex="-1"></a>}</span>
<span id="cb481-5"><a href="sec102.html#cb481-5" tabindex="-1"></a>PIP</span></code></pre></div>
<pre><code>##  [1] 1.00000000 0.03617574 0.03208369 0.03516743 1.00000000 0.04795509
##  [7] 0.03457102 0.03468819 0.03510209 1.00000000</code></pre>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="sec102.html#cb483-1" tabindex="-1"></a>nModels <span class="ot">&lt;-</span> <span class="fu">dim</span>(combs)[<span class="dv">1</span>]; Means <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb483-2"><a href="sec102.html#cb483-2" tabindex="-1"></a>Vars <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb483-3"><a href="sec102.html#cb483-3" tabindex="-1"></a><span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nModels){</span>
<span id="cb483-4"><a href="sec102.html#cb483-4" tabindex="-1"></a>    idXs <span class="ot">&lt;-</span> <span class="fu">which</span>(combs[m,] <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb483-5"><a href="sec102.html#cb483-5" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">length</span>(idXs) <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb483-6"><a href="sec102.html#cb483-6" tabindex="-1"></a>        Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb483-7"><a href="sec102.html#cb483-7" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb483-8"><a href="sec102.html#cb483-8" tabindex="-1"></a>        Xm <span class="ot">&lt;-</span> X[, idXs]; Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> Xm)</span>
<span id="cb483-9"><a href="sec102.html#cb483-9" tabindex="-1"></a>        SumRegm <span class="ot">&lt;-</span> <span class="fu">summary</span>(Regm)</span>
<span id="cb483-10"><a href="sec102.html#cb483-10" tabindex="-1"></a>        Means[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb483-11"><a href="sec102.html#cb483-11" tabindex="-1"></a>        Vars[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb483-12"><a href="sec102.html#cb483-12" tabindex="-1"></a>    }</span>
<span id="cb483-13"><a href="sec102.html#cb483-13" tabindex="-1"></a>}</span>
<span id="cb483-14"><a href="sec102.html#cb483-14" tabindex="-1"></a>BMAmeans <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Means<span class="sc">*</span>PMP)</span>
<span id="cb483-15"><a href="sec102.html#cb483-15" tabindex="-1"></a>BMAmeans</span></code></pre></div>
<pre><code>##  [1]  1.0018105888 -0.0003196423  0.0001489711  0.0002853524  0.4976225353
##  [6] -0.0007229563  0.0005342718  0.0005441905  0.0005758708 -0.7035206822</code></pre>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="sec102.html#cb485-1" tabindex="-1"></a>BMAsd <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(PMP<span class="sc">*</span>Vars)  <span class="sc">+</span> <span class="fu">colSums</span>(PMP<span class="sc">*</span>(Means<span class="sc">-</span><span class="fu">matrix</span>(<span class="fu">rep</span>(BMAmeans, <span class="at">each =</span> nModels), nModels, K))<span class="sc">^</span><span class="dv">2</span>))<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb485-2"><a href="sec102.html#cb485-2" tabindex="-1"></a>BMAsd </span></code></pre></div>
<pre><code>##  [1] 0.015274980 0.003304115 0.002814491 0.003214722 0.015668278 0.004694003
##  [7] 0.006400541 0.006435695 0.006528471 0.030940753</code></pre>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="sec102.html#cb487-1" tabindex="-1"></a>BMAmeans<span class="sc">/</span>BMAsd </span></code></pre></div>
<pre><code>##  [1]  65.58506579  -0.09674068   0.05293002   0.08876427  31.75987477
##  [6]  -0.15401700   0.08347292   0.08455816   0.08820914 -22.73767175</code></pre>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="sec102.html#cb489-1" tabindex="-1"></a><span class="do">#### MC3 Algorithm ####</span></span>
<span id="cb489-2"><a href="sec102.html#cb489-2" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb489-3"><a href="sec102.html#cb489-3" tabindex="-1"></a>Models <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rbinom</span>(K<span class="sc">*</span>M, <span class="dv">1</span>, <span class="at">p =</span> <span class="fl">0.5</span>), <span class="at">ncol=</span>K, <span class="at">nrow =</span> M)</span>
<span id="cb489-4"><a href="sec102.html#cb489-4" tabindex="-1"></a>mllnew <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>M,<span class="cf">function</span>(s){<span class="fu">LogMLfunt</span>(<span class="fu">matrix</span>(Models[s,], <span class="dv">1</span>, K))})</span>
<span id="cb489-5"><a href="sec102.html#cb489-5" tabindex="-1"></a>oind <span class="ot">&lt;-</span> <span class="fu">order</span>(mllnew, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb489-6"><a href="sec102.html#cb489-6" tabindex="-1"></a>mllnew <span class="ot">&lt;-</span> mllnew[oind]; Models <span class="ot">&lt;-</span> Models[oind, ]; iter <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb489-7"><a href="sec102.html#cb489-7" tabindex="-1"></a>pb <span class="ot">&lt;-</span> <span class="fu">txtProgressBar</span>(<span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> iter, <span class="at">style =</span> <span class="dv">3</span>); s <span class="ot">&lt;-</span> <span class="dv">1</span></span></code></pre></div>
<pre><code>##   |                                                                              |                                                                      |   0%</code></pre>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="sec102.html#cb491-1" tabindex="-1"></a><span class="cf">while</span>(s <span class="sc">&lt;=</span> iter){</span>
<span id="cb491-2"><a href="sec102.html#cb491-2" tabindex="-1"></a>    ActModel <span class="ot">&lt;-</span> Models[M,]; idK <span class="ot">&lt;-</span> <span class="fu">which</span>(ActModel <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb491-3"><a href="sec102.html#cb491-3" tabindex="-1"></a>    Kact <span class="ot">&lt;-</span> <span class="fu">length</span>(idK)</span>
<span id="cb491-4"><a href="sec102.html#cb491-4" tabindex="-1"></a>    <span class="cf">if</span>(Kact <span class="sc">&lt;</span> K <span class="sc">&amp;</span> Kact <span class="sc">&gt;</span> <span class="dv">1</span>){</span>
<span id="cb491-5"><a href="sec102.html#cb491-5" tabindex="-1"></a>        CardMol <span class="ot">&lt;-</span> K; opt <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb491-6"><a href="sec102.html#cb491-6" tabindex="-1"></a>        <span class="cf">if</span>(opt <span class="sc">==</span> <span class="dv">1</span>){ <span class="co"># Same</span></span>
<span id="cb491-7"><a href="sec102.html#cb491-7" tabindex="-1"></a>            CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-8"><a href="sec102.html#cb491-8" tabindex="-1"></a>        }<span class="cf">else</span>{</span>
<span id="cb491-9"><a href="sec102.html#cb491-9" tabindex="-1"></a>            <span class="cf">if</span>(opt <span class="sc">==</span> <span class="dv">2</span>){ <span class="co"># Add</span></span>
<span id="cb491-10"><a href="sec102.html#cb491-10" tabindex="-1"></a>                All <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>K; NewX <span class="ot">&lt;-</span> <span class="fu">sample</span>(All[<span class="sc">-</span>idK], <span class="dv">1</span>)</span>
<span id="cb491-11"><a href="sec102.html#cb491-11" tabindex="-1"></a>                CandModel <span class="ot">&lt;-</span> ActModel; CandModel[NewX] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb491-12"><a href="sec102.html#cb491-12" tabindex="-1"></a>            }<span class="cf">else</span>{ <span class="co"># Subtract</span></span>
<span id="cb491-13"><a href="sec102.html#cb491-13" tabindex="-1"></a>                LessX <span class="ot">&lt;-</span> <span class="fu">sample</span>(idK, <span class="dv">1</span>); CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-14"><a href="sec102.html#cb491-14" tabindex="-1"></a>                CandModel[LessX] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb491-15"><a href="sec102.html#cb491-15" tabindex="-1"></a>            }</span>
<span id="cb491-16"><a href="sec102.html#cb491-16" tabindex="-1"></a>        }</span>
<span id="cb491-17"><a href="sec102.html#cb491-17" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb491-18"><a href="sec102.html#cb491-18" tabindex="-1"></a>        CardMol <span class="ot">&lt;-</span> K <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb491-19"><a href="sec102.html#cb491-19" tabindex="-1"></a>        <span class="cf">if</span>(Kact <span class="sc">==</span> K){</span>
<span id="cb491-20"><a href="sec102.html#cb491-20" tabindex="-1"></a>            opt <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb491-21"><a href="sec102.html#cb491-21" tabindex="-1"></a>            <span class="cf">if</span>(opt <span class="sc">==</span> <span class="dv">1</span>){ <span class="co"># Same</span></span>
<span id="cb491-22"><a href="sec102.html#cb491-22" tabindex="-1"></a>                CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-23"><a href="sec102.html#cb491-23" tabindex="-1"></a>            }<span class="cf">else</span>{ <span class="co"># Subtract</span></span>
<span id="cb491-24"><a href="sec102.html#cb491-24" tabindex="-1"></a>                LessX <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>K, <span class="dv">1</span>); CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-25"><a href="sec102.html#cb491-25" tabindex="-1"></a>                CandModel[LessX] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb491-26"><a href="sec102.html#cb491-26" tabindex="-1"></a>            }</span>
<span id="cb491-27"><a href="sec102.html#cb491-27" tabindex="-1"></a>        }<span class="cf">else</span>{</span>
<span id="cb491-28"><a href="sec102.html#cb491-28" tabindex="-1"></a>            <span class="cf">if</span>(K <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb491-29"><a href="sec102.html#cb491-29" tabindex="-1"></a>                opt <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb491-30"><a href="sec102.html#cb491-30" tabindex="-1"></a>                <span class="cf">if</span>(opt <span class="sc">==</span> <span class="dv">1</span>){ <span class="co"># Same</span></span>
<span id="cb491-31"><a href="sec102.html#cb491-31" tabindex="-1"></a>                    CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-32"><a href="sec102.html#cb491-32" tabindex="-1"></a>                }<span class="cf">else</span>{</span>
<span id="cb491-33"><a href="sec102.html#cb491-33" tabindex="-1"></a>                    <span class="cf">if</span>(opt <span class="sc">==</span> <span class="dv">2</span>){ <span class="co"># Add</span></span>
<span id="cb491-34"><a href="sec102.html#cb491-34" tabindex="-1"></a>                        All <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>K; NewX <span class="ot">&lt;-</span> <span class="fu">sample</span>(All[<span class="sc">-</span>idK], <span class="dv">1</span>)</span>
<span id="cb491-35"><a href="sec102.html#cb491-35" tabindex="-1"></a>                        CandModel <span class="ot">&lt;-</span> ActModel; CandModel[NewX] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb491-36"><a href="sec102.html#cb491-36" tabindex="-1"></a>                    }<span class="cf">else</span>{ <span class="co"># Subtract</span></span>
<span id="cb491-37"><a href="sec102.html#cb491-37" tabindex="-1"></a>                        LessX <span class="ot">&lt;-</span> <span class="fu">sample</span>(idK, <span class="dv">1</span>); CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-38"><a href="sec102.html#cb491-38" tabindex="-1"></a>                        CandModel[LessX] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb491-39"><a href="sec102.html#cb491-39" tabindex="-1"></a>                    }</span>
<span id="cb491-40"><a href="sec102.html#cb491-40" tabindex="-1"></a>                }</span>
<span id="cb491-41"><a href="sec102.html#cb491-41" tabindex="-1"></a>            }<span class="cf">else</span>{ <span class="co"># Add</span></span>
<span id="cb491-42"><a href="sec102.html#cb491-42" tabindex="-1"></a>                NewX <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>K, <span class="dv">1</span>); CandModel <span class="ot">&lt;-</span> ActModel</span>
<span id="cb491-43"><a href="sec102.html#cb491-43" tabindex="-1"></a>                CandModel[NewX] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb491-44"><a href="sec102.html#cb491-44" tabindex="-1"></a>            }</span>
<span id="cb491-45"><a href="sec102.html#cb491-45" tabindex="-1"></a>        }</span>
<span id="cb491-46"><a href="sec102.html#cb491-46" tabindex="-1"></a>    }</span>
<span id="cb491-47"><a href="sec102.html#cb491-47" tabindex="-1"></a>    LogMLact <span class="ot">&lt;-</span> <span class="fu">LogMLfunt</span>(<span class="fu">matrix</span>(ActModel, <span class="dv">1</span>, K))</span>
<span id="cb491-48"><a href="sec102.html#cb491-48" tabindex="-1"></a>    LogMLcand <span class="ot">&lt;-</span> <span class="fu">LogMLfunt</span>(<span class="fu">matrix</span>(CandModel, <span class="dv">1</span>, K))</span>
<span id="cb491-49"><a href="sec102.html#cb491-49" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="dv">1</span>, <span class="fu">exp</span>(LogMLcand<span class="sc">-</span>LogMLact))</span>
<span id="cb491-50"><a href="sec102.html#cb491-50" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb491-51"><a href="sec102.html#cb491-51" tabindex="-1"></a>    <span class="cf">if</span>(u <span class="sc">&lt;=</span> alpha){</span>
<span id="cb491-52"><a href="sec102.html#cb491-52" tabindex="-1"></a>        mllnew[M] <span class="ot">&lt;-</span> LogMLcand; Models[M, ] <span class="ot">&lt;-</span> CandModel</span>
<span id="cb491-53"><a href="sec102.html#cb491-53" tabindex="-1"></a>        oind <span class="ot">&lt;-</span> <span class="fu">order</span>(mllnew, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb491-54"><a href="sec102.html#cb491-54" tabindex="-1"></a>        mllnew <span class="ot">&lt;-</span> mllnew[oind]; Models <span class="ot">&lt;-</span> Models[oind, ]</span>
<span id="cb491-55"><a href="sec102.html#cb491-55" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb491-56"><a href="sec102.html#cb491-56" tabindex="-1"></a>        mllnew <span class="ot">&lt;-</span> mllnew; Models <span class="ot">&lt;-</span> Models</span>
<span id="cb491-57"><a href="sec102.html#cb491-57" tabindex="-1"></a>    }</span>
<span id="cb491-58"><a href="sec102.html#cb491-58" tabindex="-1"></a>    s <span class="ot">&lt;-</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb491-59"><a href="sec102.html#cb491-59" tabindex="-1"></a>    <span class="fu">setTxtProgressBar</span>(pb, s)</span>
<span id="cb491-60"><a href="sec102.html#cb491-60" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>##   |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |==                                                                    |   4%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |=====                                                                 |   8%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |=========                                                             |  14%  |                                                                              |==========                                                            |  14%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  16%  |                                                                              |============                                                          |  17%  |                                                                              |============                                                          |  18%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |==============                                                        |  21%  |                                                                              |===============                                                       |  21%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |================                                                      |  24%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |=====================                                                 |  31%  |                                                                              |======================                                                |  31%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |=======================                                               |  34%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |==========================                                            |  38%  |                                                                              |===========================                                           |  38%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |============================                                          |  41%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |==============================                                        |  44%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |=================================                                     |  48%  |                                                                              |==================================                                    |  48%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |=====================================                                 |  54%  |                                                                              |======================================                                |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  62%  |                                                                              |============================================                          |  63%  |                                                                              |============================================                          |  64%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |===================================================                   |  74%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  79%  |                                                                              |========================================================              |  80%  |                                                                              |========================================================              |  81%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |==========================================================            |  84%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |===============================================================       |  91%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |=================================================================     |  94%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================|  99%  |                                                                              |======================================================================| 100%</code></pre>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="sec102.html#cb493-1" tabindex="-1"></a><span class="fu">close</span>(pb)</span></code></pre></div>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="sec102.html#cb494-1" tabindex="-1"></a>ModelsUni <span class="ot">&lt;-</span> <span class="fu">unique</span>(Models)</span>
<span id="cb494-2"><a href="sec102.html#cb494-2" tabindex="-1"></a>mllnewUni <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(ModelsUni)[<span class="dv">1</span>], <span class="cf">function</span>(s){<span class="fu">LogMLfunt</span>(<span class="fu">matrix</span>(ModelsUni[s,], <span class="dv">1</span>, K))})</span>
<span id="cb494-3"><a href="sec102.html#cb494-3" tabindex="-1"></a>StMarLik <span class="ot">&lt;-</span> <span class="fu">exp</span>(mllnewUni<span class="sc">-</span>mllnewUni[<span class="dv">1</span>])</span>
<span id="cb494-4"><a href="sec102.html#cb494-4" tabindex="-1"></a>PMP <span class="ot">&lt;-</span> StMarLik<span class="sc">/</span><span class="fu">sum</span>(StMarLik) <span class="co"># PMP based on unique selected models</span></span>
<span id="cb494-5"><a href="sec102.html#cb494-5" tabindex="-1"></a>nModels <span class="ot">&lt;-</span> <span class="fu">dim</span>(ModelsUni)[<span class="dv">1</span>]</span>
<span id="cb494-6"><a href="sec102.html#cb494-6" tabindex="-1"></a>StMarLik <span class="ot">&lt;-</span> <span class="fu">exp</span>(mllnew<span class="sc">-</span>mllnew[<span class="dv">1</span>])</span>
<span id="cb494-7"><a href="sec102.html#cb494-7" tabindex="-1"></a>PMPold <span class="ot">&lt;-</span> StMarLik<span class="sc">/</span><span class="fu">sum</span>(StMarLik) <span class="co"># PMP all selected models</span></span>
<span id="cb494-8"><a href="sec102.html#cb494-8" tabindex="-1"></a>PMPot <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb494-9"><a href="sec102.html#cb494-9" tabindex="-1"></a>PMPap <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb494-10"><a href="sec102.html#cb494-10" tabindex="-1"></a>FreqMod <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb494-11"><a href="sec102.html#cb494-11" tabindex="-1"></a><span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nModels){</span>
<span id="cb494-12"><a href="sec102.html#cb494-12" tabindex="-1"></a>    idModm <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb494-13"><a href="sec102.html#cb494-13" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M){</span>
<span id="cb494-14"><a href="sec102.html#cb494-14" tabindex="-1"></a>        <span class="cf">if</span>(<span class="fu">sum</span>(ModelsUni[m,] <span class="sc">==</span> Models[j,]) <span class="sc">==</span> K){</span>
<span id="cb494-15"><a href="sec102.html#cb494-15" tabindex="-1"></a>            idModm <span class="ot">&lt;-</span> <span class="fu">c</span>(idModm, j)</span>
<span id="cb494-16"><a href="sec102.html#cb494-16" tabindex="-1"></a>        }<span class="cf">else</span>{</span>
<span id="cb494-17"><a href="sec102.html#cb494-17" tabindex="-1"></a>            idModm <span class="ot">&lt;-</span> idModm</span>
<span id="cb494-18"><a href="sec102.html#cb494-18" tabindex="-1"></a>        }</span>
<span id="cb494-19"><a href="sec102.html#cb494-19" tabindex="-1"></a>    }</span>
<span id="cb494-20"><a href="sec102.html#cb494-20" tabindex="-1"></a>    PMPm <span class="ot">&lt;-</span> <span class="fu">sum</span>(PMPold[idModm]) <span class="co"># PMP unique models using sum of all selected models</span></span>
<span id="cb494-21"><a href="sec102.html#cb494-21" tabindex="-1"></a>    PMPot <span class="ot">&lt;-</span> <span class="fu">c</span>(PMPot, PMPm)</span>
<span id="cb494-22"><a href="sec102.html#cb494-22" tabindex="-1"></a>    PMPapm <span class="ot">&lt;-</span> <span class="fu">length</span>(idModm)<span class="sc">/</span>M <span class="co"># PMP using relative frequency in all selected models</span></span>
<span id="cb494-23"><a href="sec102.html#cb494-23" tabindex="-1"></a>    PMPap <span class="ot">&lt;-</span> <span class="fu">c</span>(PMPap, PMPapm)</span>
<span id="cb494-24"><a href="sec102.html#cb494-24" tabindex="-1"></a>    FreqMod <span class="ot">&lt;-</span> <span class="fu">c</span>(FreqMod, <span class="fu">length</span>(idModm))</span>
<span id="cb494-25"><a href="sec102.html#cb494-25" tabindex="-1"></a>}</span>
<span id="cb494-26"><a href="sec102.html#cb494-26" tabindex="-1"></a>PIP <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb494-27"><a href="sec102.html#cb494-27" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K){</span>
<span id="cb494-28"><a href="sec102.html#cb494-28" tabindex="-1"></a>    PIPk <span class="ot">&lt;-</span> <span class="fu">sum</span>(PMP[<span class="fu">which</span>(ModelsUni[,k] <span class="sc">==</span> <span class="dv">1</span>)])</span>
<span id="cb494-29"><a href="sec102.html#cb494-29" tabindex="-1"></a>    PIP <span class="ot">&lt;-</span> <span class="fu">c</span>(PIP, PIPk)</span>
<span id="cb494-30"><a href="sec102.html#cb494-30" tabindex="-1"></a>}</span>
<span id="cb494-31"><a href="sec102.html#cb494-31" tabindex="-1"></a>Means <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb494-32"><a href="sec102.html#cb494-32" tabindex="-1"></a>Vars <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, nModels, K)</span>
<span id="cb494-33"><a href="sec102.html#cb494-33" tabindex="-1"></a><span class="cf">for</span>(m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nModels){</span>
<span id="cb494-34"><a href="sec102.html#cb494-34" tabindex="-1"></a>    idXs <span class="ot">&lt;-</span> <span class="fu">which</span>(ModelsUni[m,] <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb494-35"><a href="sec102.html#cb494-35" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">length</span>(idXs) <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb494-36"><a href="sec102.html#cb494-36" tabindex="-1"></a>        Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb494-37"><a href="sec102.html#cb494-37" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb494-38"><a href="sec102.html#cb494-38" tabindex="-1"></a>        Xm <span class="ot">&lt;-</span> X[, idXs]</span>
<span id="cb494-39"><a href="sec102.html#cb494-39" tabindex="-1"></a>        Regm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> Xm)</span>
<span id="cb494-40"><a href="sec102.html#cb494-40" tabindex="-1"></a>        SumRegm <span class="ot">&lt;-</span> <span class="fu">summary</span>(Regm)</span>
<span id="cb494-41"><a href="sec102.html#cb494-41" tabindex="-1"></a>        Means[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb494-42"><a href="sec102.html#cb494-42" tabindex="-1"></a>        Vars[m, idXs] <span class="ot">&lt;-</span> SumRegm[[<span class="st">&quot;coefficients&quot;</span>]][<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb494-43"><a href="sec102.html#cb494-43" tabindex="-1"></a>    }</span>
<span id="cb494-44"><a href="sec102.html#cb494-44" tabindex="-1"></a>}</span>
<span id="cb494-45"><a href="sec102.html#cb494-45" tabindex="-1"></a>BMAmeans <span class="ot">&lt;-</span> <span class="fu">colSums</span>(Means<span class="sc">*</span>PMP)</span>
<span id="cb494-46"><a href="sec102.html#cb494-46" tabindex="-1"></a>BMAsd <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(PMP<span class="sc">*</span>Vars)  <span class="sc">+</span> <span class="fu">colSums</span>(PMP<span class="sc">*</span>(Means<span class="sc">-</span><span class="fu">matrix</span>(<span class="fu">rep</span>(BMAmeans, <span class="at">each =</span> nModels), nModels, K))<span class="sc">^</span><span class="dv">2</span>))<span class="sc">^</span><span class="fl">0.5</span> </span>
<span id="cb494-47"><a href="sec102.html#cb494-47" tabindex="-1"></a>BMAmeans; BMAsd; BMAmeans<span class="sc">/</span>BMAsd</span></code></pre></div>
<pre><code>##  [1]  1.001787e+00 -1.099674e-05  5.928440e-06  1.237330e-06  4.976289e-01
##  [6] -7.240575e-04  5.317222e-04  4.590382e-05  5.316641e-04 -7.035531e-01</code></pre>
<pre><code>##  [1] 0.0152733978 0.0006149476 0.0005564602 0.0002155678 0.0156674028
##  [6] 0.0046971545 0.0063886626 0.0018639899 0.0062682419 0.0309357255</code></pre>
<pre><code>##  [1]  65.590349277  -0.017882401   0.010653843   0.005739863  31.762056368
##  [6]  -0.154148108   0.083229033   0.024626646   0.084818691 -22.742415397</code></pre>
<p>The second part of the code demonstrates how to perform the MC3 algorithm. While this algorithm is not strictly necessary for this small-dimensional problem, it serves as a useful pedagogical exercise. The starting point is to set <span class="math inline">\(S=100\)</span> random models and order their log marginal likelihoods. The logic of the algorithm is to select the worst model among the <span class="math inline">\(S\)</span> models and propose a candidate model to compete against it. We repeat this process for 1,000 iterations (as shown in the code). Note that 1,000 iterations is fewer than the number of potential models (1024). This is the essence of the MC3 algorithm: performing fewer iterations than the number of models in the space.</p>
<p>In our algorithm, we analyze all model scenarios using different conditionals and reasonably assume the same prior model probability for all models, with the same cardinality for both the actual and candidate models. The posterior model probability (PMP) can be calculated in several ways. One method is to recover the unique models from the final set of <span class="math inline">\(S\)</span> models, calculate the log marginal likelihood for these models, and then standardize by the best model among them. Another method involves calculating the PMP using the complete set of <span class="math inline">\(S\)</span> final models, accounting for the fact that some models may appear multiple times in the set, which requires summing the PMPs of repeated models. A third method is to calculate the PMP based on the relative frequency with which a model appears in the final set of <span class="math inline">\(S\)</span> models. These three methods can yield different PMPs, particularly when the number of MC3 iterations is small. In our example, using 1,000 MC3 iterations, the data-generating process receives the highest PMP across all three methods.</p>
<p>A noteworthy aspect of this algorithm is that we can obtain a single model after significantly increasing the number of iterations (for example, try using 10,000 iterations). This can be advantageous if we require only one model. However, this approach neglects model uncertainty, which could be a desirable characteristic in some cases. As a challenge, we suggest programming an algorithm that yields <span class="math inline">\(S\)</span> different models after completing the MC3 iterations (Exercise 3).</p>
<p>An important issue to account for regressors (model) uncertainty in the identification of causal effects, rather than finding good predictors (association relationships), is endogeneity. Thus, we also implement the instrumental variable approach of Section <a href="sec73.html#sec73">7.3</a> to tackle this issue in BMA. We assume that <span class="math inline">\(\boldsymbol{\gamma}\sim {N}(\boldsymbol{0},\boldsymbol{I})\)</span>, <span class="math inline">\(\boldsymbol{\beta}\sim {N}(\boldsymbol{0},\boldsymbol{I})\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}^{-1} \sim {W}(3,\boldsymbol{I})\)</span> <span class="citation">(<a href="#ref-Karl2012">Karl and Lenkoski 2012</a>)</span>.</p>
<p><span class="citation">Lenkoski, Karl, and Neudecker (<a href="#ref-Lenkoski2013">2013</a>)</span> propose an algorithm based on conditional Bayes factors <span class="citation">(<a href="#ref-Dickey1978">J. M. Dickey and Gunel 1978</a>)</span> that allows embedding MC3 within a Gibbs sampling algorithm. Given the candidate (<span class="math inline">\(M_{c}^{2nd}\)</span>) and actual (<span class="math inline">\(M_{s-1}^{2nd}\)</span>) models for the iteration <span class="math inline">\(s\)</span> in the second stage, the conditional Bayes factor is
<span class="math display">\[\begin{equation*}
    CBF^{2nd}=\frac{p(\boldsymbol{y}|M_{c}^{2nd},\boldsymbol{\gamma},\boldsymbol{\Sigma})}{p(\boldsymbol{y}|M_{s-1}^{2nd},\boldsymbol{\gamma},\boldsymbol{\Sigma})},
\end{equation*}\]</span>
where
<span class="math display">\[\begin{align*}
    p(\boldsymbol{y}|M_{c}^{2nd},\boldsymbol{\gamma},\boldsymbol{\Sigma})&amp;=\int_{\mathcal{M}^{2nd}}p(\boldsymbol{y}|\boldsymbol{\beta},\boldsymbol{\gamma},\boldsymbol{\Sigma})\pi(\boldsymbol{\beta}|M_{c}^{2nd})d\boldsymbol{\beta}\\
    &amp;\propto |\boldsymbol{B}_n|^{-1/2} \exp\left\{\frac{1}{2}{\boldsymbol{\beta}_n}^{\top}\boldsymbol{B}_n^{-1}\boldsymbol{\beta}_n\right\}
    .
\end{align*}\]</span></p>
<p>In the first stage,
<span class="math display">\[\begin{equation*}
    CBF^{1st}=\frac{p(\boldsymbol{y}|M_{c}^{1st},\boldsymbol{\beta},\boldsymbol{\Sigma})}{p(\boldsymbol{y}|M_{s-1}^{1st},\boldsymbol{\beta},\boldsymbol{\Sigma})},
\end{equation*}\]</span>
where <span class="math display">\[\begin{align*}
    p(\boldsymbol{y}|M_{c}^{1st},\boldsymbol{\beta},\boldsymbol{\Sigma})&amp;=\int_{\mathcal{M}^{1st}}p(\boldsymbol{y}|\boldsymbol{\gamma},\boldsymbol{\beta},\boldsymbol{\Sigma})\pi(\boldsymbol{\gamma}|M_{c}^{1st})d\boldsymbol{\gamma}\\
    &amp;\propto |\boldsymbol{G}_n|^{-1/2} \exp\left\{\frac{1}{2}{\boldsymbol{\gamma}_n}^{\top}\boldsymbol{G}_n^{-1}\boldsymbol{\gamma}_n\right\}.
\end{align*}\]</span>
These conditional Bayes factors assume <span class="math inline">\(\pi(M^{1st},M^{2sd})\propto 1\)</span>. See <span class="citation">Lenkoski, Karl, and Neudecker (<a href="#ref-Lenkoski2013">2013</a>)</span> for more details of the instrumental variable BMA algorithm.<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a></p>
<p>We perform instrumental variable BMA in our GUI using the package <em>ivbma</em>. The following Algorithm shows how to perform this in our GUI.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Instrumental Variable Bayesian Model Averaging in Linear Gaussian Models</strong></p>
<ol style="list-style-type: decimal">
<li><p>Select <em>Bayesian Model Averaging</em> on the top panel</p></li>
<li><p>Select <em>Normal data</em> model using the left radio button</p></li>
<li><p>Select <em>Instrumental variable</em> using the right radio button under <strong>Which type do you want to perform?</strong></p></li>
<li><p>Upload the dataset containing the dependent variable, endogenous regressors, and exogenous regressors (including the constant). The user should first select if there is a header in the file and the kind of separator in the <em>csv</em> file (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Upload the dataset containing the instruments. The user should first select if there is a header in the file and the kind of separator in the <em>csv</em> file (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File (Instruments)</strong> legend</p></li>
<li><p>Write down the number of endogenous regressors in the box labeled <strong>Number of Endogenous variables</strong></p></li>
<li><p>Select MCMC iterations and burn-in using the <em>Range slider</em> under the labels <strong>MCMC iterations:</strong> and <strong>Burn-in Sample:</strong></p></li>
<li><p>Click the <em>Go!</em> button</p></li>
<li><p>Analyze results: After a few seconds or minutes, two tables appear showing, for each regressor in the dataset, the PIP (posterior inclusion probability, <strong>p!=0</strong>), and the BMA posterior mean (<strong>EV</strong>). The top table shows the results of the second stage (main equation), and the bottom table shows the results of the first stage (auxiliary equations)</p></li>
<li><p>Download posterior results using the <em>Download results using IV</em> button. Three files are provided:<br />
</p></li>
</ol>
<ul>
<li>The first file contains the posterior inclusion probabilities of each variable and the BMA posterior means of the coefficients in the first stage equations<br />
</li>
<li>The second file contains these results for the second stage (main equation)<br />
</li>
<li>The third file contains the posterior chains of all parameters by iteration</li>
</ul>
</div>
</div>
<p><strong>Example: Simulation exercise</strong></p>
<p>Let’s assume that <span class="math inline">\(y_i = 2 + 0.5x_{i1} - x_{i2} + x_{i3} + \mu_i\)</span>, where <span class="math inline">\(x_{i1} = 4z_{i1} - z_{i2} + 2z_{i3} + \epsilon_{i1}\)</span> and <span class="math inline">\(x_{i2} = -2z_{i1} + 3z_{i2} - z_{i3} + \epsilon_{i2}\)</span>, such that <span class="math inline">\([\epsilon_{i1} \ \epsilon_{i2} \ \mu_i]^{\top} \sim N(\boldsymbol{0}, \boldsymbol{\Sigma})\)</span>, where <span class="math inline">\(\boldsymbol{\Sigma} = \begin{bmatrix} 1 &amp; 0 &amp; 0.8 \\ 0 &amp; 1 &amp; 0.5 \\ 0.8 &amp; 0.5 &amp; 1 \end{bmatrix}\)</span>, for <span class="math inline">\(i = 1, 2, \dots, 1000\)</span>. The endogeneity arises due to the correlation between <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span> through the stochastic errors. In addition, there are three instruments, <span class="math inline">\(z_{il} \sim U(0,1)\)</span>, for <span class="math inline">\(l = 1, 2, 3\)</span>, and another 18 regressors believed to influence <span class="math inline">\(y_i\)</span>, which are distributed according to a standard normal distribution.</p>
<p>The following code shows how to perform IV BMA using the <em>ivbma</em> package. We see from the results that the PIP of <span class="math inline">\(x_{i1}\)</span>, <span class="math inline">\(x_{i2}\)</span>, intercept and <span class="math inline">\(x_{i3}\)</span> are equal to 1, whereas the remaining PIP are close to 0. In addition, the BMA means are also close to the population values. The PIP of the first stage equations, as well as their BMA posterior means, are very close to the populations values. The same happens with the covariance matrix.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="sec102.html#cb498-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb498-2"><a href="sec102.html#cb498-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb498-3"><a href="sec102.html#cb498-3" tabindex="-1"></a>simIV <span class="ot">&lt;-</span> <span class="cf">function</span>(delta1,delta2,beta0,betas1,betas2,beta2,Sigma,n,z) {</span>
<span id="cb498-4"><a href="sec102.html#cb498-4" tabindex="-1"></a>    eps <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">3</span><span class="sc">*</span>n),<span class="at">ncol=</span><span class="dv">3</span>) <span class="sc">%*%</span> <span class="fu">chol</span>(Sigma)</span>
<span id="cb498-5"><a href="sec102.html#cb498-5" tabindex="-1"></a>    xs1 <span class="ot">&lt;-</span> z<span class="sc">%*%</span>delta1 <span class="sc">+</span> eps[,<span class="dv">1</span>]</span>
<span id="cb498-6"><a href="sec102.html#cb498-6" tabindex="-1"></a>    xs2 <span class="ot">&lt;-</span> z<span class="sc">%*%</span>delta2 <span class="sc">+</span> eps[,<span class="dv">2</span>]</span>
<span id="cb498-7"><a href="sec102.html#cb498-7" tabindex="-1"></a>    x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">dim</span>(z)[<span class="dv">1</span>])</span>
<span id="cb498-8"><a href="sec102.html#cb498-8" tabindex="-1"></a>    y <span class="ot">&lt;-</span> beta0<span class="sc">+</span>betas1<span class="sc">*</span>xs1<span class="sc">+</span>betas2<span class="sc">*</span>xs2<span class="sc">+</span>beta2<span class="sc">*</span>x2 <span class="sc">+</span> eps[,<span class="dv">3</span>]</span>
<span id="cb498-9"><a href="sec102.html#cb498-9" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(xs1,xs2,<span class="dv">1</span>,x2)) </span>
<span id="cb498-10"><a href="sec102.html#cb498-10" tabindex="-1"></a>    <span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1en&quot;</span>,<span class="st">&quot;x2en&quot;</span>,<span class="st">&quot;cte&quot;</span>,<span class="st">&quot;xex&quot;</span>)</span>
<span id="cb498-11"><a href="sec102.html#cb498-11" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(y,<span class="fu">dim</span>(z)[<span class="dv">1</span>],<span class="dv">1</span>)</span>
<span id="cb498-12"><a href="sec102.html#cb498-12" tabindex="-1"></a>    <span class="fu">colnames</span>(y) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb498-13"><a href="sec102.html#cb498-13" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">X=</span>X,<span class="at">y=</span>y)</span>
<span id="cb498-14"><a href="sec102.html#cb498-14" tabindex="-1"></a>}</span>
<span id="cb498-15"><a href="sec102.html#cb498-15" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span> ; p <span class="ot">&lt;-</span> <span class="dv">3</span> </span>
<span id="cb498-16"><a href="sec102.html#cb498-16" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p),<span class="at">ncol=</span>p)</span>
<span id="cb498-17"><a href="sec102.html#cb498-17" tabindex="-1"></a>rho31 <span class="ot">&lt;-</span> <span class="fl">0.8</span>; rho32 <span class="ot">&lt;-</span> <span class="fl">0.5</span>;</span>
<span id="cb498-18"><a href="sec102.html#cb498-18" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,rho31,<span class="dv">0</span>,<span class="dv">1</span>,rho32,rho31,rho32,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb498-19"><a href="sec102.html#cb498-19" tabindex="-1"></a>delta1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>); delta2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">3</span>,<span class="sc">-</span><span class="dv">1</span>); betas1 <span class="ot">&lt;-</span> .<span class="dv">5</span>; betas2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span>; beta2 <span class="ot">&lt;-</span> <span class="dv">1</span>; beta0 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb498-20"><a href="sec102.html#cb498-20" tabindex="-1"></a>simiv <span class="ot">&lt;-</span> <span class="fu">simIV</span>(delta1,delta2,beta0,betas1,betas2,beta2,Sigma,n,z)</span>
<span id="cb498-21"><a href="sec102.html#cb498-21" tabindex="-1"></a>nW <span class="ot">&lt;-</span> <span class="dv">18</span></span>
<span id="cb498-22"><a href="sec102.html#cb498-22" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(nW<span class="sc">*</span><span class="fu">dim</span>(z)[<span class="dv">1</span>]),<span class="fu">dim</span>(z)[<span class="dv">1</span>],nW)</span>
<span id="cb498-23"><a href="sec102.html#cb498-23" tabindex="-1"></a>YXW<span class="ot">&lt;-</span><span class="fu">cbind</span>(simiv<span class="sc">$</span>y, simiv<span class="sc">$</span>X, W)</span>
<span id="cb498-24"><a href="sec102.html#cb498-24" tabindex="-1"></a>y <span class="ot">&lt;-</span> YXW[,<span class="dv">1</span>]; X <span class="ot">&lt;-</span> YXW[,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]; W <span class="ot">&lt;-</span> YXW[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)]</span>
<span id="cb498-25"><a href="sec102.html#cb498-25" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">10000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb498-26"><a href="sec102.html#cb498-26" tabindex="-1"></a>regivBMA <span class="ot">&lt;-</span> ivbma<span class="sc">::</span><span class="fu">ivbma</span>(<span class="at">Y =</span> y, <span class="at">X =</span> X, <span class="at">Z =</span> z, <span class="at">W =</span> W, <span class="at">s =</span> S<span class="sc">+</span>burnin, <span class="at">b =</span> burnin, <span class="at">odens =</span> S, <span class="at">print.every =</span> <span class="fu">round</span>(S<span class="sc">/</span><span class="dv">10</span>), <span class="at">run.diagnostics =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Running IVBMA for 11000 iterations 2025-11-30 12:42:59.56848&quot;
## [1] &quot;On Iteration 1000 2025-11-30 12:43:03.933056&quot;
## [1] &quot;On Iteration 2000 2025-11-30 12:43:08.056128&quot;
## [1] &quot;On Iteration 3000 2025-11-30 12:43:12.264072&quot;
## [1] &quot;On Iteration 4000 2025-11-30 12:43:16.303512&quot;
## [1] &quot;On Iteration 5000 2025-11-30 12:43:20.538529&quot;
## [1] &quot;On Iteration 6000 2025-11-30 12:43:24.748863&quot;
## [1] &quot;On Iteration 7000 2025-11-30 12:43:29.160016&quot;
## [1] &quot;On Iteration 8000 2025-11-30 12:43:33.663296&quot;
## [1] &quot;On Iteration 9000 2025-11-30 12:43:37.938781&quot;
## [1] &quot;On Iteration 10000 2025-11-30 12:43:42.019613&quot;
## [1] &quot;On Iteration 11000 2025-11-30 12:43:46.526693&quot;</code></pre>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="sec102.html#cb500-1" tabindex="-1"></a>PIPmain <span class="ot">&lt;-</span> regivBMA[[<span class="st">&quot;L.bar&quot;</span>]] <span class="co"># PIP outcome</span></span>
<span id="cb500-2"><a href="sec102.html#cb500-2" tabindex="-1"></a>PIPmain</span></code></pre></div>
<pre><code>##  [1] 1.0000 1.0000 1.0000 1.0000 0.0140 0.0243 0.0116 0.0309 0.0305 0.0145
## [11] 0.0105 0.0103 0.0233 0.0402 0.0048 0.0349 0.0210 0.0343 0.0044 0.0137
## [21] 0.0175 0.0223</code></pre>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="sec102.html#cb502-1" tabindex="-1"></a>EVmain <span class="ot">&lt;-</span> regivBMA[[<span class="st">&quot;rho.bar&quot;</span>]] <span class="co"># Posterior mean outcome</span></span>
<span id="cb502-2"><a href="sec102.html#cb502-2" tabindex="-1"></a>EVmain</span></code></pre></div>
<pre><code>##  [1]  5.074816e-01 -9.873671e-01  2.004880e+00  1.005181e+00 -1.849026e-04
##  [6] -3.951743e-04 -8.087320e-05  6.401508e-04  6.699469e-04 -1.456550e-04
## [11]  7.932297e-05 -5.582272e-05 -1.913875e-04  6.135038e-04  2.458307e-05
## [16] -5.885881e-04 -2.431515e-05 -1.170588e-04  1.793956e-05  7.936547e-05
## [21] -2.579314e-05 -2.026556e-04</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="sec102.html#cb504-1" tabindex="-1"></a>PIPaux <span class="ot">&lt;-</span> regivBMA[[<span class="st">&quot;M.bar&quot;</span>]] <span class="co"># PIP auxiliary</span></span>
<span id="cb504-2"><a href="sec102.html#cb504-2" tabindex="-1"></a>EVaux <span class="ot">&lt;-</span> regivBMA[[<span class="st">&quot;lambda.bar&quot;</span>]] <span class="co"># Posterior mean auxiliary</span></span>
<span id="cb504-3"><a href="sec102.html#cb504-3" tabindex="-1"></a><span class="fu">plot</span>(EVaux[,<span class="dv">1</span>])</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-53-1.svg" width="672" /></p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="sec102.html#cb505-1" tabindex="-1"></a><span class="fu">plot</span>(EVaux[,<span class="dv">2</span>])</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-53-2.svg" width="672" /></p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="sec102.html#cb506-1" tabindex="-1"></a>EVsigma <span class="ot">&lt;-</span> regivBMA[[<span class="st">&quot;Sigma.bar&quot;</span>]] <span class="co"># Posterior mean variance matrix</span></span>
<span id="cb506-2"><a href="sec102.html#cb506-2" tabindex="-1"></a>EVsigma</span></code></pre></div>
<pre><code>##           [,1]        [,2]        [,3]
## [1,] 1.0345164 0.848730076 0.515301058
## [2,] 0.8487301 1.081309693 0.008180855
## [3,] 0.5153011 0.008180855 1.028434857</code></pre>
<p>Bayesian model averaging has been also extended to state-space models. The point of departure is the univariate random walk state-space model (see Chapter <a href="Chap8.html#Chap8">8</a>) conditional on model <span class="math inline">\(\mathcal{M}_m\)</span>, <span class="math inline">\(m=1,2\dots,M\)</span>.
<span class="math display">\[\begin{align}
    y_t&amp;=\boldsymbol{x}_{mt}^{\top}\boldsymbol{\beta}_{mt}+\mu_{mt}\\
    \boldsymbol{\beta}_{mt}&amp;=\boldsymbol{\beta}_{mt-1}+\boldsymbol{w}_{mt},
\end{align}\]</span>
where <span class="math inline">\(\mu_{mt}\sim N(0,\sigma^2)\)</span> and <span class="math inline">\(\boldsymbol{w}_{mt}\sim N(\boldsymbol{0},\boldsymbol{\Omega}_{mt})\)</span>.</p>
<p>Given <span class="math inline">\(\boldsymbol{\beta}_{mt-1}|\boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{b}_{mt-1},\boldsymbol{B}_{mt-1})\)</span>, then, we know from Chapter <a href="Chap8.html#Chap8">8</a> that <span class="math inline">\(\boldsymbol{\beta}_{mt}|\boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{b}_{mt-1}, \boldsymbol{R}_{mt})\)</span>, <span class="math inline">\(\boldsymbol{R}_{mt}=\boldsymbol{B}_{mt-1}+\boldsymbol{\Omega}_{mt}\)</span>.</p>
<p>Specification of <span class="math inline">\(\boldsymbol{\Omega}_t\)</span> can be highly demanding. Thus, a common approach is to express <span class="math inline">\(\boldsymbol{\Omega}_{mt}=\frac{1-\lambda}{\lambda}\boldsymbol{B}_{mt-1}\)</span>, where <span class="math inline">\(\lambda\)</span> is called the <em>forgetting parameter</em> or <em>discount factor</em>, because it discounts the matrix <span class="math inline">\(\boldsymbol{B}_{mt-1}\)</span> that we would have with a deterministic state evolution into the matrix <span class="math inline">\(\boldsymbol{R}_{mt}\)</span> <span class="citation">(<a href="#ref-petris2009dynamic">Petris, Petrone, and Campagnoli 2009</a>)</span>. This parameter is typically slightly below 1, and implies that <span class="math inline">\(\boldsymbol{R}_{mt}=\lambda^{-1}\boldsymbol{B}_{mt-1}\)</span>. (<span class="math inline">\(\lambda^{-1}&gt;1\)</span>).</p>
<p><span class="citation">Adrian E. Raftery, Kárnỳ, and Ettler (<a href="#ref-raftery2010online">2010</a>)</span> assume that the model changes infrequently, and its evolution is given by the transition matrix <span class="math inline">\(\boldsymbol{T}=[t_{ml}]\)</span>, where <span class="math inline">\(t_{ml}=P(\mathcal{M}_t=\mathcal{M}_m|\mathcal{M}_{t-1}=\mathcal{M}_l)\)</span>.</p>
<p>Then, the aim is to calculate the filtering distribution <span class="math inline">\(p(\boldsymbol{\beta}_{mt},\mathcal{M}_t|y_t)=\sum_{m=1}^Mp(\boldsymbol{\beta}_{mt}|\mathcal{M}_t=\mathcal{M}_m,y_t)p(\mathcal{M}_t=\mathcal{M}_m|y_t)\)</span>. Thus, given the conditional distribution of the state at time <span class="math inline">\(t-1\)</span>, <span class="math inline">\(p(\boldsymbol{\beta}_{mt-1},\mathcal{M}_{t-1}|{y}_{t-1})=\sum_{m=1}^Mp(\boldsymbol{\beta}_{mt-1}|\mathcal{M}_{t-1}=\mathcal{M}_m,{y}_{t-1})p(\mathcal{M}_{t-1}=\mathcal{M}_m|{y}_{t-1})\)</span>, where the conditional distribution of <span class="math inline">\(\boldsymbol{\beta}_{mt-1}\)</span> is approximated by a Gaussian distribution, <span class="math inline">\(\boldsymbol{\beta}_{mt-1}|\mathcal{M}_{t-1}=\mathcal{M}_{m},y_{t-1}\sim N(\boldsymbol{b}_{mt-1},\boldsymbol{B}_{mt-1})\)</span>, then the first step to get the one-step-ahead predictive distribution is getting the prediction of the model indicator,
<span class="math display">\[\begin{align*}
    p(\mathcal{M}_t=\mathcal{M}_l|y_{t-1})&amp;=\sum_{m=1}^M p(\mathcal{M}_{t-1}=\mathcal{M}_m|y_{t-1})\times t_{lm}\\
    &amp;\approx \frac{p(\mathcal{M}_{t-1}=\mathcal{M}_l|y_{t-1})^{\delta}+c}{\sum_{m=1}^M p(\mathcal{M}_{t-1}=\mathcal{M}_m|y_{t-1})^{\delta}+c},  
\end{align*}\]</span>
where the second equality is used to avoid dealing with the <span class="math inline">\(M^2\)</span> elements of the transition matrix <span class="math inline">\(\boldsymbol{T}\)</span> such that the forgetting parameter <span class="math inline">\(\delta\)</span> is used, this parameter is slightly less than 1, and <span class="math inline">\(c=0.001/M\)</span> is introduced to handle a model probability being brought to computational zero by outliers (see <span class="citation">Adrian E. Raftery, Kárnỳ, and Ettler (<a href="#ref-raftery2010online">2010</a>)</span>).</p>
<p>Then, we get the one-step-ahead predictive distribution of the state vector, <span class="math inline">\(\boldsymbol{\beta}_{mt}|\mathcal{M}_{t}=\mathcal{M}_{m},y_{t-1}\sim N(\boldsymbol{b}_{mt-1},\lambda^{-1}\boldsymbol{B}_{mt-1})\)</span></p>
<p>Now, we consider the filtering stage, where the model filtering equation is
<span class="math display">\[\begin{align*}
    p(\mathcal{M}_t=\mathcal{M}_l|y_{t})=\frac{p(\mathcal{M}_t=\mathcal{M}_l|y_{t-1})p_l(y_t|y_{t-1})}{\sum_{m=1}^M p(\mathcal{M}_t=\mathcal{M}_m|y_{t-1})p_m(y_t|y_{t-1})},
\end{align*}\]</span>
where <span class="math inline">\(p_m(y_t|y_{t-1})\)</span> is the one-step-ahead predictive distribution of <span class="math inline">\(y_t|{y}_{t-1}\)</span>, which is <span class="math inline">\(N(f_t,Q_t)\)</span>, where <span class="math inline">\(f_t=\boldsymbol{x}_t^{\top}\boldsymbol{b}_{t-1}\)</span> and <span class="math inline">\(Q_t=\boldsymbol{x}_{mt}^{\top}\lambda^{-1}\boldsymbol{B}_{mt-1}\boldsymbol{x}_{mt}+\sigma^2\)</span> (see Chapter <a href="Chap8.html#Chap8">8</a>).</p>
<p>The states filtering equation is <span class="math inline">\(\boldsymbol{\beta}_{mt}|\mathcal{M}_{t}=\mathcal{M}_{m},y_{t}\sim N(\boldsymbol{b}_{mt},\boldsymbol{B}_{mt})\)</span> where <span class="math inline">\(\boldsymbol{b}_{mt}\)</span> and <span class="math inline">\(\boldsymbol{B}_{mt}\)</span> are given in the Kalman filtering recursion of Chapter <a href="Chap8.html#Chap8">8</a>.</p>
<p><span class="citation">Adrian E. Raftery, Kárnỳ, and Ettler (<a href="#ref-raftery2010online">2010</a>)</span> initiate their algorithm assuming equal prior model probabilities, and <span class="math inline">\(\sigma^2\)</span> is estimated using a recursive method of moments estimator.<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a></p>
<p>We implement dynamic Bayesian model averaging in our GUI using the function <em>dma</em> from the package <em>dma</em>. The next Algorithm shows how to perform inference using our GUI.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Dynamic Bayesian Model Averaging</strong></p>
<ol style="list-style-type: decimal">
<li><p>Select <em>Bayesian Model Averaging</em> on the top panel</p></li>
<li><p>Select <em>Normal data</em> model using the left radio button</p></li>
<li><p>Select <em>Dynamic Bayesian Model Averaging</em> using the right radio button under <strong>Which type do you want to perform?</strong></p></li>
<li><p>Upload the dataset, selecting first whether there is a header in the file and the type of separator in the <em>csv</em> file (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Upload the matrix of models, selecting first whether there is a header in the file and the type of separator in the <em>csv</em> file (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Type the <em>forgetting parameters</em> in the boxes under <strong>Lambda: Number slightly below 1</strong> and <strong>Delta: Number slightly below 1</strong>. This is not necessary, as the default values are 0.99 for both</p></li>
<li><p>Click the <em>Go!</em> button</p></li>
<li><p>Analyze results: After a few seconds or minutes, a figure showing the posterior model probabilities in each period appears</p></li>
<li><p>Download posterior results using the <strong>Download results for Dynamic Model Average</strong>. There are four files: the dynamic Bayesian model averaging prediction, the dynamic Bayesian average filtering recursions for each state, and their standard deviation, and the PMP of each model.</p></li>
</ol>
</div>
</div>
<p><strong>Example: Dynamic Bayesian model averaging</strong></p>
<p>We perform a simulation exercise where there are 8 (<span class="math inline">\(2^3\)</span>) competing models originating from 3 regressors: <span class="math inline">\(x_{tk} \sim N(0.5, 0.8^2)\)</span> for <span class="math inline">\(k = 2, 3, 4\)</span>, with <span class="math inline">\(\beta_1 = 0.5\)</span>. The sequence <span class="math inline">\(\beta_{2t}\)</span> ranges from 1 to 2 in steps of <span class="math inline">\(1/T\)</span>, and <span class="math inline">\(\beta_{3t}\)</span> is given by:
<span class="math display">\[
\beta_{3t} = \begin{cases}
    -1, &amp; 1 &lt; t \leq 0.75T \\
    0, &amp; 0.75T &lt; t \leq T
\end{cases}
\]</span>
and <span class="math inline">\(\beta_4 = 1.2\)</span>.</p>
<p>Then, we have the model:
<span class="math display">\[
y_t = \beta_1 + \beta_{2t} x_{2t} + \beta_{3t} x_{3t} + \beta_4 x_{4t} + \mu_t,
\]</span>
where <span class="math inline">\(\mu_t \sim N(0,1)\)</span> for <span class="math inline">\(t = 1, 2, \dots, 500\)</span>. This setting implies that during the first 75% of the period, the model with all 3 regressors is the data-generating process, while after this, the model with regressors 2 and 4 is the data-generating process.</p>
<p>The following code shows the simulation exercise and the results of the dynamic Bayesian model averaging, setting <span class="math inline">\(\lambda = \delta = 0.99\)</span>.</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="sec102.html#cb508-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb508-2"><a href="sec102.html#cb508-2" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">500</span>; K <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb508-3"><a href="sec102.html#cb508-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(T<span class="sc">*</span>K, <span class="at">mean =</span> <span class="fl">0.5</span>, <span class="at">sd =</span> <span class="fl">0.8</span>), T, K)</span>
<span id="cb508-4"><a href="sec102.html#cb508-4" tabindex="-1"></a>combs <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb508-5"><a href="sec102.html#cb508-5" tabindex="-1"></a>B1 <span class="ot">&lt;-</span> <span class="fl">0.5</span>; B2t <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="at">length.out=</span>T )</span>
<span id="cb508-6"><a href="sec102.html#cb508-6" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">0.75</span>; B3t <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="fu">round</span>(a<span class="sc">*</span>T)), <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">round</span>((<span class="dv">1</span><span class="sc">-</span>a)<span class="sc">*</span>T)))</span>
<span id="cb508-7"><a href="sec102.html#cb508-7" tabindex="-1"></a>B4 <span class="ot">&lt;-</span> <span class="fl">1.2</span>; sigma <span class="ot">&lt;-</span> <span class="dv">1</span>; mu <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(T, <span class="dv">0</span>, sigma)</span>
<span id="cb508-8"><a href="sec102.html#cb508-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> B1 <span class="sc">+</span> X[,<span class="dv">1</span>]<span class="sc">*</span>B2t <span class="sc">+</span> X[,<span class="dv">2</span>]<span class="sc">*</span>B3t <span class="sc">+</span> X[,<span class="dv">3</span>]<span class="sc">*</span>B4 <span class="sc">+</span> mu</span>
<span id="cb508-9"><a href="sec102.html#cb508-9" tabindex="-1"></a>T0 <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb508-10"><a href="sec102.html#cb508-10" tabindex="-1"></a>dma.test <span class="ot">&lt;-</span> dma<span class="sc">::</span><span class="fu">dma</span>(X, y, combs, <span class="at">lambda=</span>.<span class="dv">99</span>, <span class="at">gamma=</span>.<span class="dv">99</span>, <span class="at">initialperiod =</span> T0)</span>
<span id="cb508-11"><a href="sec102.html#cb508-11" tabindex="-1"></a><span class="fu">plot</span>(dma.test[[<span class="st">&quot;pmp&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">8</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Posterior model probability&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;PMP&quot;</span>)</span>
<span id="cb508-12"><a href="sec102.html#cb508-12" tabindex="-1"></a><span class="fu">lines</span>(dma.test[[<span class="st">&quot;pmp&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">6</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb508-13"><a href="sec102.html#cb508-13" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">1</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Model: All regressors&quot;</span>, <span class="st">&quot;Model: Regressors 2 and 4&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-54-1.svg" width="672" /></p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="sec102.html#cb509-1" tabindex="-1"></a><span class="fu">require</span>(latex2exp)</span>
<span id="cb509-2"><a href="sec102.html#cb509-2" tabindex="-1"></a><span class="fu">plot</span>(dma.test[[<span class="st">&quot;thetahat.ma&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Bayesian model average filtering recursion&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{1}$&quot;</span>))</span>
<span id="cb509-3"><a href="sec102.html#cb509-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> B1, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb509-4"><a href="sec102.html#cb509-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="fl">0.4</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;State filtering&quot;</span>, <span class="st">&quot;State population&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-54-2.svg" width="672" /></p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="sec102.html#cb510-1" tabindex="-1"></a><span class="fu">plot</span>(dma.test[[<span class="st">&quot;thetahat.ma&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Bayesian model average filtering recursion&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{2t}$&quot;</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="dv">2</span>))</span>
<span id="cb510-2"><a href="sec102.html#cb510-2" tabindex="-1"></a><span class="fu">lines</span>(B2t[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb510-3"><a href="sec102.html#cb510-3" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="fl">0.8</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;State filtering&quot;</span>, <span class="st">&quot;State population&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-54-3.svg" width="672" /></p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="sec102.html#cb511-1" tabindex="-1"></a><span class="fu">plot</span>(dma.test[[<span class="st">&quot;thetahat.ma&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">3</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Bayesian model average filtering recursion&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{3t}$&quot;</span>))</span>
<span id="cb511-2"><a href="sec102.html#cb511-2" tabindex="-1"></a><span class="fu">lines</span>(B3t[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0)], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb511-3"><a href="sec102.html#cb511-3" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.4</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;State filtering&quot;</span>, <span class="st">&quot;State population&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-54-4.svg" width="672" /></p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="sec102.html#cb512-1" tabindex="-1"></a><span class="fu">plot</span>(dma.test[[<span class="st">&quot;thetahat.ma&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T0),<span class="dv">4</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Bayesian model average filtering recursion&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{4t}$&quot;</span>))</span>
<span id="cb512-2"><a href="sec102.html#cb512-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> B4, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb512-3"><a href="sec102.html#cb512-3" tabindex="-1"></a><span class="fu">legend</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="fl">1.3</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;State filtering&quot;</span>, <span class="st">&quot;State population&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-54-5.svg" width="672" /></p>
<p>The first Figure shows the posterior model probabilities for the model with all the regressors (green line) and the model with regressors 2 and 4 (red line). On one hand, we see that the model with all regressors, which is the data-generating process in the first period (<span class="math inline">\(t \leq 0.75T\)</span>), has a PMP close to 1, and then its PMP decreases. On the other hand, the model with regressors 2 and 4 has a PMP close to 0 in the first part of the period, and then its PMP increases to values higher than 60% on average, when this model becomes the data-generating process. These results suggest that, in this particular simulation exercise, the dynamic Bayesian model averaging method works relatively well in calculating the PMPs.</p>
<p>The following four Figures show a comparison between the Bayesian model averaging filtering recursions of the states (green lines) and their population values (red lines). We observe that the filtering recursions follow the general pattern of the population values. However, the values are not perfectly aligned. This discrepancy arises because the posterior model probabilities (PMPs) of the models that match the data-generating process are not equal to 1, which in turn affects the performance of the filtering recursions.</p>
<p>Dynamic Bayesian model averaging was extended to logit models by <span class="citation">Tyler H. McCormick et al. (<a href="#ref-mccormick2012dynamic">2012</a>)</span>. We ask in Exercise 12 to perform a simulation of this model, and perform BMA using the function <em>logistic.dma</em> from the <em>dma</em> package.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Barbieri2004" class="csl-entry">
Barbieri, M., and J. Berger. 2004. <span>“Optimal Predictive Model Selection.”</span> <em>The Annals of Statistics</em> 32 (3): 870–97.
</div>
<div id="ref-Clyde2004" class="csl-entry">
Clyde, M., and E. George. 2004. <span>“Model Uncertatinty.”</span> <em>Statistical Science</em> 19 (1): 81–94.
</div>
<div id="ref-Dickey1978" class="csl-entry">
Dickey, J. M., and E. Gunel. 1978. <span>“Bayes Factors from Mixed Probabilities.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodology)</em> 40: 43–46.
</div>
<div id="ref-fernandez2001benchmark" class="csl-entry">
Fernandez, Carmen, Eduardo Ley, and Mark FJ Steel. 2001. <span>“Benchmark Priors for Bayesian Model Averaging.”</span> <em>Journal of Econometrics</em> 100 (2): 381–427.
</div>
<div id="ref-prior1991bayesian" class="csl-entry">
Ibrahim, Joseph G., and Purushottam W. Laud. 1991. <span>“On Bayesian Analysis of Generalized Linear Models Using Jeffreys’s Prior.”</span> <em>Journal of the American Statistical Association</em> 86 (416): 981–86.
</div>
<div id="ref-Jetter2015" class="csl-entry">
Jetter, M., and A. Ramírez Hassan. 2015. <span>“Want Export Diversification? <span>E</span>ducate the Kids First.”</span> <em>Economic Inquiry</em> 53 (4): 1765–82.
</div>
<div id="ref-Karl2012" class="csl-entry">
Karl, A., and A. Lenkoski. 2012. <span>“Instrumental Variable <span>B</span>ayesian Model Averaging via Conditional <span>B</span>ayes Factor.”</span> Heidelberg University.
</div>
<div id="ref-koop2003bayesian" class="csl-entry">
Koop, Gary M. 2003. <em>Bayesian Econometrics</em>. John Wiley &amp; Sons Inc.
</div>
<div id="ref-Koop12" class="csl-entry">
Koop, G, R León-Gonzalez, and R Strachan. 2012. <span>“Bayesian Model Averaging in the Instrumental Variable Regression Model.”</span> <em>Journal of Econometrics</em> 171: 237–50.
</div>
<div id="ref-Lenkoski2014" class="csl-entry">
Lenkoski, Alex, Theo S. Eicher, and Adrian Raftery. 2014. <span>“Two-Stage <span>B</span>ayesian Model Averaging in Endogeneous Variable Models.”</span> <em>Econometric Reviews</em> 33.
</div>
<div id="ref-Lenkoski2013" class="csl-entry">
Lenkoski, Alex, Anna Karl, and Andreas Neudecker. 2013. <em>Package <span class="nocase">ivbma</span></em>. <a href="https://CRAN.R-project.org/package=ivbma">https://CRAN.R-project.org/package=ivbma</a>.
</div>
<div id="ref-mccormick2012dynamic" class="csl-entry">
McCormick, Tyler H, Adrian E Raftery, David Madigan, and Randall S Burd. 2012. <span>“Dynamic Logistic Regression and Dynamic Model Averaging for Binary Classification.”</span> <em>Biometrics</em> 68 (1): 23–30.
</div>
<div id="ref-petris2009dynamic" class="csl-entry">
Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. <span>“Dynamic Linear Models.”</span> In <em>Dynamic Linear Models with r</em>, 31–84. Springer.
</div>
<div id="ref-Raftery1995" class="csl-entry">
Raftery, A. 1995. <span>“Bayesian Model Selection in Social Research.”</span> <em>Sociological Methodology</em> 25: 111–63.
</div>
<div id="ref-raftery2010online" class="csl-entry">
Raftery, Adrian E, Miroslav Kárnỳ, and Pavel Ettler. 2010. <span>“Online Prediction Under Model Uncertainty via Dynamic Model Averaging: Application to a Cold Rolling Mill.”</span> <em>Technometrics</em> 52 (1): 52–66.
</div>
<div id="ref-Raftery1997" class="csl-entry">
Raftery, Adrian E., David Madigan, and Jennifer A. Hoeting. 1997. <span>“Bayesian Model Averaging for Linear Regression Models.”</span> <em>Journal of the American Statistical Association</em> 92 (437): 179–91.
</div>
<div id="ref-ramirez2020dynamic" class="csl-entry">
Ramı́rez-Hassan, Andrés. 2020. <span>“Dynamic Variable Selection in Dynamic Logistic Regression: An Application to Internet Subscription.”</span> <em>Empirical Economics</em> 59 (2): 909–32.
</div>
<div id="ref-zellner1986assessing" class="csl-entry">
Zellner, Arnold. 1986. <span>“On Assessing Prior Distributions and Bayesian Regression Analysis with g-Prior Distributions.”</span> <em>Bayesian Inference and Decision Techniques</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="63">
<li id="fn63"><p><span class="citation">G. Koop, León-Gonzalez, and Strachan (<a href="#ref-Koop12">2012</a>)</span> and <span class="citation">Lenkoski, Eicher, and Raftery (<a href="#ref-Lenkoski2014">2014</a>)</span> propose other frameworks for BMA taking into account endogeneity.<a href="sec102.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p><span class="citation">Ramı́rez-Hassan (<a href="#ref-ramirez2020dynamic">2020</a>)</span> extends this approach to Markov chain Monte Carlo model composition<a href="sec102.html#fnref64" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec10_1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec103.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/11-BMA.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
