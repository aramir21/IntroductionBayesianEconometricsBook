<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.4 Exercises: Chapter 1 | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1.4 Exercises: Chapter 1 | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.4 Exercises: Chapter 1 | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="summary-chapter-1.html"/>
<link rel="next" href="Chap3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.3</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.4" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>2</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>2.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="2.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>2.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>2.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="2.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="2.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>2.5</b> Computational examples</a></li>
<li class="chapter" data-level="2.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>2.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="2.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>2.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>3</b> Simulation methods</a></li>
<li class="chapter" data-level="4" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>4</b> Graphical user interface</a></li>
<li class="chapter" data-level="5" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>5</b> Time series</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exercises-chapter-1" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Exercises: Chapter 1<a href="exercises-chapter-1.html#exercises-chapter-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><strong>The court case: the blue or green cap</strong></li>
</ol>
<p>A cab was involved in a hit and run accident at night. There are two cab companies in the town: blue and green. The former has 150 cabs, and the latter 850 cabs. A witness said that a blue cab was involved in the accident; the court tested his/her reliability under the same circumstances, and got that 80% of the times the witness correctly identified the color of the cab. <strong>what is the probability that the color of the cab involved in the accident was blue given that the witness said it was blue?</strong></p>
<!--
**Solution**
Set $WB$ and $WG$ equal to the events that the witness said the cab was blue and green, respectively. Set $B$ and $G$ equal to the events that the cabs are blue and green, respectively. We need to calculate $P(B|WB)$, then:

\begin{align}
  P(B|WB)&=\frac{P(B,WB)}{P(WB)}\\
         &=\frac{P(WB|B)\times P(B)}{P(WB|B)\times P(B)+(1-P(WB|B))\times (1-P(B))}\\
         &=\frac{0.8\times 0.15}{0.8\times 0.15+0.2\times 0.85}\\
         &=0.41
\end{align}
-->
<ol start="2" style="list-style-type: decimal">
<li><strong>The Monty Hall problem</strong></li>
</ol>
<p>What is the probability of winning a car in the <strong>Monty Hall problem</strong> switching the decision if there are four doors, where there are three goats and one car? Solve this problem analytically and computationally. What if there are <span class="math inline">\(n\)</span> doors, <span class="math inline">\(n-1\)</span> goats and one car?</p>
<!--
**Solution**

Let's name $P_i$ the event *contestant picks door No. $i$*, $H_i$ the event *host picks door No. $i$*, and $C_i$ the event *car is behind door No. $i$*. Let's assume that the contestant picked door number 1, and the host picked door number 3, then the contestant is interested in the probability of the event $P(C_i|H_3,P_1), i = 2 \ \text{or} \ 4$. Then, $P(H_3|C_3,P_1)=0$, $P(H_3|C_2,P_1)=P(H_3|C_4,P_1)=1/2$ and $P(H_3|C_1,P_1)=1/3$. Then, using equation \@ref(eq:112) 

\begin{align}
  P(C_i|H_3,P_1)&= \frac{P(C_i,H_3,P_1)}{P(H_3,P_1)}\\
                &= \frac{P(H_3|C_i,P_1)P(C_i|P_1)P(P_1)}{P(H_3|P_1)\times P(P_1)}\\
                &= \frac{P(H_3|C_i,P_1)P(C_i)}{P(H_3|P_1)}\\
                &=\frac{1/2\times 1/4}{1/3}\\
                &=\frac{3}{8},
\end{align}
where the third equation uses the fact that $C_i$ and $P_i$ are independent events, and $P(H_3|P_1)=1/3$ due to this depending just on $P_1$ (not on $C_i$).

Therefore, changing the initial decision increases the probability of getting the car from 1/4 to 3/8!


Let's check the case with $n$ doors, and assume that the contestant picks the door No. $1$, the car is behind the door No. $n$, and the host, who knows what is behind each door, opens any of the remaining $n-2$ doors, where there is a goat. The contestant is interested in the probability of the event:

\begin{align*}
    &   P\left( C_{n} | (H_2 \cup H_3 \cup  \ldots \cup H_{n-1}) \cap P_1 \right)  = 
    \frac{P\left( (H_2 \cup H_3 \cup \ldots \cup H_{n-1}) | C_{n} \cap P_1\right) P(C_{n} | P_1) P(P_1)}{P\left( (H_2 \cup H_3 \cup \ldots \cup H_{n-1}) | P_1 \right) P(P_1)} \\
    & = \frac{\left[ P\left( H_2 | C_{n} \cap P_1\right) + P\left( H_3| C_{n} \cap P_1\right) + \ldots P\left( H_{n-1}| C_{n} \cap P_1\right) \right] P(C_{n})}{P\left( H_2 | P_1 \right) + P\left( H_3 | P_1 \right) + \ldots + P\left( H_{n-1} | P_1 \right)} \\
    & = \frac{1 \times \left( \frac{1}{n} \right) }{\frac{1}{n-1} + \frac{1}{n-1} + \ldots + \frac{1}{n-1}} \\
    & = \left( \frac{1}{n}\right) \left( \frac{n-1}{n-2}\right). 
\end{align*}

In general, the probability of winning the car changing the pick is $\frac{1}{n} \frac{n-1}{n-2}$, while the probability of winning given no change is $\frac{1}{n}$. Given that $\frac{1}{n} \frac{n-1}{n-2} > \frac{1}{n}$ for all $n \geq 3$, where the difference between both probabilities is  $\frac{1}{n(n-2)}$. We observe that as the number of doors increases, the difference between the two probabilities becomes zero.

Let's see a code for the general setting,



```r
set.seed(0101) # Set simulation seed
S <- 100000 # Simulations
Game <- function(opt = 3){
  # opt: number of options. opt > 2, it is 3 in the original game
  opts <- 1:opt 
  car <- sample(opts, 1) # car location
  guess1 <- sample(opts, 1) # Initial guess pick
  
  if(opt == 3 && car != guess1) {
    host <- opts[-c(car, guess1)]
    } else {
    host <- sample(opts[-c(car, guess1)], 1)
    }
  
  win1 <- guess1 == car # Win given no change
  
  if(opt == 3) {
    guess2 <- opts[-c(host, guess1)]
  } else {
    guess2 <- sample(opts[-c(host, guess1)], 1)
  } 
  
  win2 <- guess2 == car # Win given change

  return(c(win1, win2))
}

Prob <- rowMeans(replicate(S, Game(opt = 4))) #Win probabilities
paste("Winning probabilities no changing door is", Prob[1], sep = " ")
```

```
## [1] "Winning probabilities no changing door is 0.25151"
```

```r
paste("Winning probabilities changing door is", Prob[2], sep = " ")
```

```
## [1] "Winning probabilities changing door is 0.37267"
```
-->
<ol start="3" style="list-style-type: decimal">
<li>Solve the health insurance example using a Gamma prior in the rate parametrization, that is, <span class="math inline">\(\pi(\lambda)=\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}\lambda^{\alpha_0-1}\exp\left\{-\lambda\beta_0\right\}\)</span>.</li>
</ol>
<!--
**Solution**

First, we get the posterior distribution,
\begin{align}
    \pi\left(\lambda | \textbf{y}  \right) & = \left( \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \lambda^{\alpha_0 - 1} e^{-\lambda \beta_0} \right) \left(  \prod_{i = 1}^{N} \frac{\lambda^{y_i} e^{-\lambda}}{y_i!} \right)  \\ 
    & = \left( \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \lambda^{\alpha_0 - 1} e^{-\lambda \beta_0} \right) \left( \frac{\lambda^{\sum_{i = 1}^{N} y_i} e^{-N \lambda}}{\prod_{i = 1}^{N} y_i !}\right) \nonumber \\ \nonumber
    & = \left(  \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \frac{1}{\prod_{i = 1}^{N} y_i !} \right) \lambda^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} e^{-\lambda\left( \beta_0 + N \right) } \\ \nonumber
    & \propto \lambda^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} e^{-\lambda\left( \beta_0 + N \right) }. 
\end{align}

The last expression is the kernel of a Gamma distribution with parameters $\alpha_n = \sum_{i = 1}^{N} y_i + \alpha_0$ and $\beta_n = \beta_0 + N$.

Given that $\int_{0}^{\infty}\pi\left(\lambda|\textbf{y}\right)d\lambda=1$, then the constant of proportionality in the last expression is $\Gamma\left(\alpha_n\right)/\beta_n^{\alpha_n}$. Therefore the posterior density function $\pi\left(\lambda|\textbf{y}\right)$ is $G\left(\alpha_n,\beta_n\right)$.

The posterior mean is

\begin{align}
    E\left[ \lambda | \textbf{y}\right] & = \frac{\alpha_n}{\beta_n}  \\ 
    & = \frac{\sum_{i = 1}^{N} y_i + \alpha_0}{\beta_0 + N} \nonumber \\ 
    & = \left( \frac{N}{\beta_0 + N}\right) \bar{y} + \left( \frac{\beta_0}{\beta_0 + N}\right) \frac{\alpha_0}{\beta_0}  \nonumber \\
    & = w \bar{y} + \left( 1 - w \right) E\left[ \lambda \right], \nonumber 
\end{align}

where $w = \frac{N}{\beta_0 + N}$, $\bar{y}$ is the sample mean, and $E\left[ \lambda \right]  = \frac{\alpha_0}{\beta_0}$.

The posterior predictive distribution is given by

\begin{align}
    \pi\left(Y_0 | \textbf{y} \right) & = \int_{0}^{\infty} \frac{\lambda^{y_0} e^{-\lambda}}{y_0!} \pi\left(\lambda | \textbf{y}  \right) d \lambda\\
    & = \int_{0}^{\infty} \left( \frac{\lambda^{y_0} e^{-\lambda}}{y_0!} \right) \left( \frac{\beta_n^{\alpha_n}}{\Gamma(\alpha_n)} \lambda^{\alpha_n - 1} e^{-\lambda \beta_n} \right) d\lambda \nonumber \\
    & = \frac{\beta_n^{\alpha_n}}{\Gamma\left( \alpha_n\right) y_0 !} \int_{0}^{\infty} \lambda^{y_0 + \alpha_n - 1} e^{-\lambda \left( 1 + \beta_n\right)} d\lambda \nonumber \\
    & = \frac{\beta_n^{\alpha_n}}{\Gamma\left( \alpha_n\right) y_0 !} \frac{\Gamma\left( y_0 + \alpha_n \right) }{\left( 1 + \beta_n \right)^{y_0 + \alpha_n} } \nonumber \\
    & = \frac{\Gamma\left( y_0 + \alpha_n \right)}{\Gamma\left( \alpha_n \right) y_0 !} \left( \frac{1}{1 + \beta_n}\right)^{y_0} \left(\frac{\beta_n}{1 + \beta_n} \right)^{\alpha_n} \nonumber \\
    & = \frac{\left( y_0 + \alpha_n - 1\right)!}{\left( \alpha_n - 1\right)! y_0 ! } \left( \frac{1}{1 + \beta_n}\right)^{y_0} \left(\frac{\beta_n}{1 + \beta_n} \right)^{\alpha_n} \nonumber \\    
    & = \binom{y_0 + \alpha_n - 1}{y_0} \left( \frac{1}{1 + \beta_n}\right)^{y_0} \left(\frac{\beta_n}{1 + \beta_n} \right)^{\alpha_n}. \nonumber 
\end{align}

Therefore $Y_0|y\sim NB(\alpha_n,p_n) $ where $p_n = \frac{1}{1 + \beta_n}$.

To use empirical Bayes, we have the following setting

\begin{equation*}
    \left[ \hat{\alpha_0} \hat{\beta_0}\right] = \arg \max_{\alpha_0, \beta_0} \ln (p(\textbf{y})), 
\end{equation*}
where
\begin{align}
    p(y) & = \int_{0}^{\infty} \left( \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \lambda^{\alpha_0 - 1} e^{-\lambda \beta_0} \right) \left(  \prod_{i = 1}^{N} \frac{\lambda^{y_0} e^{-\lambda}}{y_0!} \right)  d\lambda \\
    & = \left(  \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0) \prod_{i = 1}^{N} y_i !} \right) \int_{0}^{\infty}\lambda^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} e^{-\lambda\left( \beta_0 + N \right)} d\lambda \nonumber \\
    & = \left(  \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0) \prod_{i = 1}^{N} y_i !} \right) \left(   \frac{\Gamma\left( \sum_{i=1}^{N} y_i + \alpha_0 \right) }{\left(\beta_0 + N\right)^{\sum_{i=1}^{N} y_i + \alpha_0} }  \right) \nonumber \\
    & = \frac{\Gamma\left( \sum_{i=1}^{N} y_i + \alpha_0 \right)}{\Gamma(\alpha_0) \prod_{i = 1}^{N} y_i !} \left( \frac{1}{\beta_0 + N}\right)^{\sum_{i=1}^{N} y_i} \left(\frac{\beta_0}{\beta_0 + N} \right)^{\alpha_0}.\nonumber   
\end{align}



```r
set.seed(010101)
y <- c(0, 3, 2, 1, 0) # Data
N <- length(y)

paste("The sample mean is", mean(y), sep = " ")
```

```
## [1] "The sample mean is 1.2"
```

```r
paste("The sample variance is", var(y), sep = " ")
```

```
## [1] "The sample variance is 1.7"
```

```r
# Predictive distribution
ProbBo <- function(y, a0, b0){
  N <- length(y) #sample size
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N # Posterior scale parameter
  p <- 1 / (bN + 1) # Probability negative binomial density
  Pr <- 1 - pnbinom(0, size = aN, prob = (1 - p)) # Probability of visiting the Doctor at least once next year
  # Observe that in R there is a slightly different parametrization.
  return(Pr)
} 

# Using a vague prior:
a0 <- 0.001 # Prior shape parameter
b0 <- 0.001 # Prior scale parameter
PriMeanV <- a0 / b0 # Prior mean
PriVarV <- a0 / b0^2 # Prior variance
paste("Prior mean and prior variance using vague information are", PriMeanV, "and", PriVarV, "respectively", sep = " ")
```

```
## [1] "Prior mean and prior variance using vague information are 1 and 1000 respectively"
```

```r
Pp <- ProbBo(y, a0 = 0.001, b0 = 0.001) # This setting is defining vague prior information.
paste("The probability of visiting the Doctor at least once next year using a vague prior is", Pp, sep = " ")
```

```
## [1] "The probability of visiting the Doctor at least once next year using a vague prior is 0.665096103908558"
```

```r
# Using Emprirical Bayes
LogMgLik <- function(theta, y){
  N <- length(y) #sample size
  a0 <- theta[1] # prior shape hyperparameter
  b0 <- theta[2] # prior scale hyperparameter
  aN <- sum(y) + a0 # posterior shape parameter
  if(a0 <= 0 || b0 <= 0){ #Avoiding negative values
    lnp <- -Inf
  }else{lnp <- lgamma(aN) - sum(y)*log(b0+N) + a0*log(b0/(b0+N)) - lgamma(a0)} # log marginal likelihood
  return(-lnp)
}

theta0 <- c(0.01, 0.01) # Initial values
control <- list(maxit = 1000) # Number of iterations in optimization
EmpBay <- optim(theta0, LogMgLik, method = "BFGS", control = control, hessian = TRUE, y = y) # Optimization
EmpBay$convergence # Checking convergence
```

```
## [1] 0
```

```r
EmpBay$value # Maximum
```

```
## [1] 4.928946
```

```r
a0EB <- EmpBay$par[1] # Prior shape using empirical Bayes
b0EB <- EmpBay$par[2] # Prior scale using empirical Bayes
paste("The prior shape and scale parameters are", a0EB, "and", b0EB, "respectively", sep = " ")
```

```
## [1] "The prior shape and scale parameters are 128.383878297508 and 106.801156225464 respectively"
```

```r
PriMeanEB <- a0EB / b0EB # Prior mean
PriVarEB <- a0EB / b0EB^2 # Prior variance
paste("Prior mean and variance using empirical Bayes are", PriMeanEB, "and", PriVarEB, "respectively", sep = " ")
```

```
## [1] "Prior mean and variance using empirical Bayes are 1.2020832248902 and 0.011255339055998 respectively"
```

```r
PpEB <- ProbBo(y, a0 = a0EB, b0 = b0EB) # This setting is using emprical Bayes.
paste("The probability of visiting the Doctor at least once next year using empirical Bayes is", PpEB, sep = " ")
```

```
## [1] "The probability of visiting the Doctor at least once next year using empirical Bayes is 0.697793985946527"
```

```r
# Density figures
lambda <- seq(0.01, 10, 0.01) # Values of lambda
VaguePrior <- dgamma(lambda, shape = a0, scale = 1/b0)
EBPrior <- dgamma(lambda, shape = a0EB, scale = 1/b0EB)
PosteriorV <- dgamma(lambda, shape = a0 + sum(y), scale = 1/(b0 +N)) 
PosteriorEB <- dgamma(lambda, shape = a0EB + sum(y), scale = 1/(b0EB +N))

# Likelihood function
Likelihood <- function(theta, y){
  LogL <- dpois(y, theta, log = TRUE)
  Lik <- prod(exp(LogL))
  return(Lik)
}
Liks <- sapply(lambda, function(par) {Likelihood(par, y = y)})
Sc <- max(PosteriorEB)/max(Liks) #Scale for displaying in figure
LiksScale <- Liks * Sc

data <- data.frame(cbind(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) #Data frame

require(ggplot2) # Cool figures
require(latex2exp) # LaTeX equations in figures
require(ggpubr) # Multiple figures in one page

fig1 <- ggplot(data = data, aes(lambda, VaguePrior)) + 
  geom_line() +  
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Prior: Vague Gamma") 

fig2 <- ggplot(data = data, aes(lambda, EBPrior)) + 
  geom_line() +  
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Prior: Empirical Bayes Gamma")

fig3 <- ggplot(data = data, aes(lambda, PosteriorV)) + 
  geom_line() +  
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Posterior: Vague Gamma")

fig4 <- ggplot(data = data, aes(lambda, PosteriorEB)) + 
  geom_line() +  
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Posterior: Empirical Bayes Gamma")

FIG <- ggarrange(fig1, fig2, fig3, fig4,
                 ncol = 2, nrow = 2)

annotate_figure(FIG,
                top = text_grob("Vague versus Empirical Bayes: Poisson-Gamma model", color = "black", face = "bold", size = 14))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-1.svg" width="672" />

```r
# Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model
dataNew <- data.frame(cbind(rep(lambda, 3), c(EBPrior, PosteriorEB, LiksScale),
                            rep(1:3, each = 1000))) #Data frame

colnames(dataNew) <- c("Lambda", "Density", "Factor")
dataNew$Factor <- factor(dataNew$Factor, levels=c("1", "3", "2"), 
                         labels=c("Prior", "Likelihood", "Posterior"))

ggplot(data = dataNew, aes_string(x = "Lambda", y = "Density", group = "Factor")) + 
  geom_line(aes(color = Factor)) +
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model") +
  guides(color=guide_legend(title="Information")) +
  scale_color_manual(values = c("red", "yellow", "blue"))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-2.svg" width="672" />

```r
# Predictive distributions
PredDen <- function(y, y0, a0, b0){
  N <- length(y)
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N # Posterior scale parameter
  p <- 1 / (bN + 1) # Probability negative binomial density
  Pr <- dnbinom(y0, size = aN, prob = (1 - p)) # Predictive density
  # Observe that in R there is a slightly different parametrization.
  return(Pr)
}
y0 <- 0:10
PredVague <- PredDen(y = y, y0 = y0, a0 = a0, b0 = b0)
PredEB <- PredDen(y = y, y0 = y0, a0 = a0EB, b0 = b0EB)
dataPred <- as.data.frame(cbind(y0, PredVague, PredEB))
colnames(dataPred) <- c("y0", "PredictiveVague", "PredictiveEB")

ggplot(data = dataPred) + 
  geom_point(aes(y0, PredictiveVague, color = "red")) +  
  xlab(TeX("$y_0$")) + ylab("Density") +
  ggtitle("Predictive density: Vague and Empirical Bayes priors") +
  geom_point(aes(y0, PredictiveEB, color = "yellow")) +
  guides(color = guide_legend(title="Prior")) +
  scale_color_manual(labels = c("Vague", "Empirical Bayes"), values = c("red", "yellow")) +
  scale_x_continuous(breaks=seq(0,10,by=1))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-3.svg" width="672" />

```r
# Posterior odds: Vague vs Empirical Bayes

PO12 <- exp(-LogMgLik(c(a0EB, b0EB), y = y))/exp(-LogMgLik(c(a0, b0), y = y))
paste("The posterior odds: Empirical Bayes vs Vague prior prior is", PO12, sep = " ")
```

```
## [1] "The posterior odds: Empirical Bayes vs Vague prior prior is 948.972284725731"
```

```r
PostProMEM <- PO12/(1 + PO12) # Posterior model probability Empirical Bayes
PostProbMV <- 1 - PostProMEM # Posterior model probability vague prior

paste("These are the posterior model probabilities", PostProMEM, PostProbMV, "for the Empirical Bayes and vague priors, respectively")
```

```
## [1] "These are the posterior model probabilities 0.998947337710712 0.00105266228928846 for the Empirical Bayes and vague priors, respectively"
```

```r
# Bayesian model average (BMA)
PostMeanEB <- (a0EB + sum(y)) / (b0EB + N) # Posterior mean Empirical Bayes 
PostMeanV <- (a0 + sum(y)) / (b0 + N ) # Posterior mean vague priors
BMAmean <- PostProMEM * PostMeanEB + PostProbMV * PostMeanV  # BMA posterior mean

PostVarEB <- (a0EB + sum(y)) / (b0EB + N + 1)^2 # Posterior variance Empirical Bayes
PostVarV <- (a0 + sum(y)) / (b0 + N)^2 # Posterior variance vague prior 

BMAVar <- PostProMEM * PostVarEB + PostProbMV * PostVarV + PostProMEM * (PostMeanEB - BMAmean)^2 + PostProbMV * (PostMeanV - BMAmean)^2# BMA posterior variance   

paste("The BMA posterior mean and variance are", BMAmean, "and", BMAVar, "respectively", sep = " ")
```

```
## [1] "The BMA posterior mean and variance are 1.20198792141458 and 0.0108028321867619 respectively"
```

```r
# BMA: Predictive
BMAPred <- PostProMEM * PredEB + PostProbMV * PredVague    
dataPredBMA <- as.data.frame(cbind(y0, BMAPred))
colnames(dataPredBMA) <- c("y0", "PredictiveBMA")

ggplot(data = dataPredBMA) + 
  geom_point(aes(y0, PredictiveBMA, color = "red")) +  
  xlab(TeX("$y_0$")) + ylab("Density") +
  ggtitle("Predictive density: BMA") +
  guides(color = guide_legend(title="BMA")) +
  scale_color_manual(labels = c("Probability"), values = c("red")) +
  scale_x_continuous(breaks=seq(0,10,by=1))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-4.svg" width="672" />

```r
# Bayesian updating
BayUp <- function(y, lambda, a0, b0){
  N <- length(y)
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N      # Posterior scale parameter
  p <- dgamma(lambda, shape = aN, scale = 1/bN) # Posterior density
  return(list(Post = p, a0New = aN, b0New = bN))
}

PostUp <- NULL
for(i in 1:N){
  if(i == 1){
    PostUpi <- BayUp(y[i], lambda, a0 = 0.001, b0 = 0.001)}
  else{
    PostUpi <- BayUp(y[i], lambda, a0 = PostUpi$a0New, b0 = PostUpi$b0New)
  }
  PostUp <- cbind(PostUp, PostUpi$Post)
}

DataUp <- data.frame(cbind(rep(lambda, 5), c(PostUp),
                           rep(1:5, each = 1000))) #Data frame

colnames(DataUp) <- c("Lambda", "Density", "Factor")

DataUp$Factor <- factor(DataUp$Factor, levels=c("1", "2", "3", "4", "5"), 
                        labels=c("Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5"))

ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) + 
  geom_line(aes(color = Factor)) +
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Bayesian updating: Poisson-Gamma model with vague prior") +
  guides(color=guide_legend(title="Update")) +
  scale_color_manual(values = c("red", "purple", "blue", "yellow", "black"))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-9-5.svg" width="672" />

```r
S <- 100000 # Posterior draws
PostMeanLambdaUps <- sapply(1:N, function(i) {mean(sample(lambda, S, replace = TRUE, prob = PostUp[ , i]))}) #Posterior mean update i
paste("Posterior means using all information and sequential updating are:", round(PostMeanV, 2), "and", round(PostMeanLambdaUps[5], 2), sep = " ") 
```

```
## [1] "Posterior means using all information and sequential updating are: 1.2 and 1.2"
```

```r
PostVarLambdaUps <- sapply(1:N, function(i) {var(sample(lambda, S, replace = TRUE, prob = PostUp[ , i]))}) #Posterior variance update i
paste("Posterior variances using all information and sequential updating are:", round(PostVarV, 2), "and", round(PostVarLambdaUps[5], 2), sep = " ")
```

```
## [1] "Posterior variances using all information and sequential updating are: 0.24 and 0.24"
```
-->
<ol start="4" style="list-style-type: decimal">
<li>Suppose that you are analyzing to buy a car insurance next year. To make a better decision you want to know <strong>what is the probability that you have a car claim next year?</strong> You have the records of your car claims in the last 15 years, <span class="math inline">\(\mathbf{y}=\left\{0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0\right\}\)</span>.</li>
</ol>
<p>Assume that this is a random sample from a data generating process (statistical model) that is binomial, <span class="math inline">\(Y_i\sim Beroulli(p)\)</span>, and your probabilistic prior beliefs about <span class="math inline">\(p\)</span> are well described by a beta distribution with parameters <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(p\sim B(\alpha_0, \beta_0)\)</span>, then, you are interested in calculating the probability of a claim the next year <span class="math inline">\(P(Y_0 = 1|\mathbf{y})\)</span>.</p>
<p>Solve this using an empirical Bayes approach and a non-informative approach where <span class="math inline">\(\alpha_0=\beta_0=1\)</span> (uniform distribution).</p>
<!--
**Solution**

The posterior distribution is given by

\begin{align}
        \pi\left( p | \textbf{y}\right) & = \left[ \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right) } p^{\alpha_0 - 1} \left( 1 - p \right)^{\beta_0 - 1} \right] \left[ \prod_{i = 1}^{N} p^{y_i} \left( 1 - p\right)^{1 - y_i} \right]  \\
        & = \left[ \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right) } p^{\alpha_0 - 1} \left( 1 - p \right)^{\beta_0 - 1} \right] \left[ p^{\sum_{i = 1}^{N} y_i}\left( 1 - p\right)^{N - \sum_{i = 1}^{N} y_i} \right] \nonumber \\
        & = \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right) } p^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} \left( 1 - p\right)^{\beta_0 + N - \sum_{i = 1}^{N} y_i - 1} \nonumber \\
        & \propto  p^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} \left( 1 - p\right)^{\beta_0 + N - \sum_{i = 1}^{N} y_i - 1}. \nonumber
\end{align}
    
The last expression is the kernel of a Beta distribution with parameters $\alpha_n = \sum_{i = 1}^{N} y_i + \alpha_0$ and $\beta_n = \beta_0 + N - \sum_{i = 1}^{N} y_i$. Thus, the posterior mean is 
    
\begin{align}
        E\left[ p | \textbf{y}\right] & = \frac{\alpha_n}{\alpha_n + \beta_n} \nonumber \\
        & = \frac{\sum_{i = 1}^{N} y_i + \alpha_0}{\alpha_0 + \beta_0 + N} \\
        & = \frac{N \bar{y}}{\alpha_0 + \beta_0 + N} + \frac{\alpha_0}{\alpha_0 + \beta_0 + N} \nonumber \\
        & = \frac{N}{\alpha_0 + \beta_0 + N} \left( \bar{y} \right) + \frac{\alpha_0 + \beta_0}{\alpha_0 + \beta_0 + N} \left( \frac{\alpha_0}{\alpha_0 + \beta_0}\right) \nonumber \\
        & = w \left( \bar{y}\right) + (1 - w) E\left[ p \right], \nonumber  
\end{align}
    
where $w = \frac{N}{\alpha_0 + \beta_0 + N}$, $\bar{y}$ is the sample mean, and $E\left[ p \right]  = \frac{\alpha_0}{\alpha_0 + \beta_0}$ is the prior mean.
    
The posterior predictive distribution of claim the next year is given by
    
\begin{align}
        \pi(Y_0 = 1| \textbf{y}) & = \int_{0}^{1}  P(Y_0 = 1| \textbf{y}, p) \pi\left( p | \textbf{y}\right) dp \nonumber \\
        & = \int_{0}^{1}  p  \times \pi\left( p | \textbf{y}\right) dp \nonumber \\
        & =  \mathbb{E}\left[ p | \textbf{y} \right]   \nonumber \\
        & =  \frac{\alpha_n}{\alpha_n + \beta_n}.
\end{align}
    
To use empirical Bayes, we have the following setting
    
\begin{equation*}
        \left[ \hat{\alpha_0} \hat{\beta_0}\right] = \arg \max_{\alpha_0, \beta_0} \ln (p(\textbf{y})), 
\end{equation*}

where
    
\begin{align}
        p(\textbf{y}) & = \int_{0}^{1} \left[ \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right) } p^{\alpha_0 - 1} \left( 1 - p\right)^{\beta_0 - 1} \right]  \left[ \prod_{i = 1}^{N}  \left( 1 - p\right)^{1 - y_i}  \right] dp \\
        & =  \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right) }  \int_{0}^{1} p^{\sum_{i = 1}^{N} y_i + \alpha_0 - 1} \left( 1 - p \right)^{\beta_0 + N - \sum_{i = 1}^{N} y_i -1} dp \nonumber \\
        & =  \frac{\Gamma\left( \alpha_0 + \beta_0\right) }{\Gamma\left( \alpha_0\right) \Gamma\left( \beta_0\right)} \frac{\Gamma\left(\sum_{i = 1}^{N} y_i + \alpha_0 \right) \Gamma\left(\beta_0 + N - \sum_{i = 1}^{N} y_i \right) }{\Gamma\left( \alpha_0 + \beta_0 + N\right) }.\nonumber 
\end{align}


```r
set.seed(010101)
y <- c(0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0) # Data
N <- length(y)

paste("The sample mean is", mean(y), sep = " ")
```

```
## [1] "The sample mean is 0.466666666666667"
```

```r
paste("The sample variance is", var(y), sep = " ")
```

```
## [1] "The sample variance is 0.266666666666667"
```

```r
#require(TailRank)

# Predictive distribution
ProbBo <- function(y, a0, b0){
  N <- length(y) #sample size
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N - sum(y) # Posterior scale parameter
  pr <- aN / (aN + bN) # Probability of a claim the next year
  return(pr)
} 

# Using a vague prior:
a0 <- 1 # Prior shape parameter
b0 <- 1 # Prior scale parameter
PriMeanV <- a0 / (a0 + b0) # Prior mean
PriVarV <- (a0*b0) / (((a0+b0)^2)*(a0+b0+1)) # Prior variance
paste("Prior mean and prior variance using vague information are", PriMeanV, "and", PriVarV, "respectively", sep = " ")
```

```
## [1] "Prior mean and prior variance using vague information are 0.5 and 0.0833333333333333 respectively"
```

```r
Pp <- ProbBo(y, a0 = 1, b0 = 1) # This setting is defining vague prior information.
paste("The probability of a claim the next year using a vague prior is", Pp, sep = " ")
```

```
## [1] "The probability of a claim the next year using a vague prior is 0.470588235294118"
```

```r
# Using Emprirical Bayes
LogMgLik <- function(theta, y){
  N <- length(y) #sample size
  a0 <- theta[1] # prior shape hyperparameter
  b0 <- theta[2] # prior scale hyperparameter
  aN <- sum(y) + a0 # posterior shape parameter
  if(a0 <= 0 || b0 <= 0){ #Avoiding negative values
    lnp <- -Inf
  }else{lnp <- lgamma(a0+b0) + lgamma(aN) + lgamma(b0+N-sum(y)) -lgamma(a0) -lgamma(b0) - lgamma(a0+b0+N)} # log marginal likelihood
  return(-lnp)
}

theta0 <- c(0.1, 0.1) # Initial values
control <- list(maxit = 1000) # Number of iterations in optimization
EmpBay <- optim(theta0, LogMgLik, method = "BFGS", control = control, hessian = TRUE, y = y) # Optimization
EmpBay$convergence # Checking convergence
```

```
## [1] 0
```

```r
EmpBay$value # Maximum
```

```
## [1] 10.40218
```

```r
a0EB <- EmpBay$par[1] # Prior shape using empirical Bayes
b0EB <- EmpBay$par[2] # Prior scale using empirical Bayes
paste("The prior shape and scale parameters are", a0EB, "and", b0EB, "respectively", sep = " ")
```

```
## [1] "The prior shape and scale parameters are 88.5027336812524 and 100.516404334558 respectively"
```

```r
PriMeanEB <- a0EB /(a0EB + b0EB)  # Prior mean
PriVarEB <- (a0EB*b0EB) / (((a0EB+b0EB)^2)*(a0EB+b0EB+1)) # Prior variance
paste("Prior mean and variance using empirical Bayes are", PriMeanEB, "and", PriVarEB, "respectively", sep = " ")
```

```
## [1] "Prior mean and variance using empirical Bayes are 0.468221020423072 and 0.00131034220582735 respectively"
```

```r
PpEB <- ProbBo(y, a0 = a0EB, b0 = b0EB) # This setting is using empirical Bayes.
paste("The probability of visiting the Doctor at least once next year using empirical Bayes is", PpEB, sep = " ")
```

```
## [1] "The probability of visiting the Doctor at least once next year using empirical Bayes is 0.46810674042673"
```

```r
# Density figures
lambda <- seq(0.001, 1, 0.001) # Values of lambda
VaguePrior <- dbeta(lambda, shape1 = a0, shape2 = b0)
EBPrior <- dbeta(lambda, shape1 = a0EB, shape2 = b0EB)
PosteriorV <- dbeta(lambda, shape1 = a0 + sum(y), shape2 = b0 + N - sum(y))
PosteriorEB <- dbeta(lambda, shape1 = a0EB + sum(y), shape2 = b0EB + N - sum(y))

# Likelihood function
Likelihood <- function(theta, y){
LogL <- dbinom(y, 1, theta, log = TRUE)
#  LogL <- dbern(y, theta)
  Lik <- prod(exp(LogL))
  return(Lik)
}


Liks <- sapply(lambda, function(par) {Likelihood(par, y = y)})
Sc <- max(PosteriorEB)/max(Liks) #Scale for displaying in figure
LiksScale <- Liks * Sc

data <- data.frame(cbind(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) #Data frame

require(ggplot2) # Cool figures
require(latex2exp) # LaTeX equations in figures
require(ggpubr) # Multiple figures in one page

fig1 <- ggplot(data = data, aes(lambda, VaguePrior)) + 
  geom_line() +  
  xlab(TeX("$p$")) + ylab("Density") +
  ggtitle("Prior: Vague Beta") 

fig2 <- ggplot(data = data, aes(lambda, EBPrior)) + 
  geom_line() +  
  xlab(TeX("$p$")) + ylab("Density") +
  ggtitle("Prior: Empirical Bayes Beta")

fig3 <- ggplot(data = data, aes(lambda, PosteriorV)) + 
  geom_line() +  
  xlab(TeX("$p$")) + ylab("Density") +
  ggtitle("Posterior: Vague Beta")

fig4 <- ggplot(data = data, aes(lambda, PosteriorEB)) + 
  geom_line() +  
  xlab(TeX("$p$")) + ylab("Density") +
  ggtitle("Posterior: Empirical Bayes Beta")

FIG <- ggarrange(fig1, fig2, fig3, fig4,
                 ncol = 2, nrow = 2)

annotate_figure(FIG,
                top = text_grob("Vague versus Empirical Bayes: Beta-Bernoulli model", color = "black", face = "bold", size = 14))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-1.svg" width="672" />

```r
# Prior, likelihood and posterior: Empirical Bayes Binonial-Beta model
dataNew <- data.frame(cbind(rep(lambda, 3), c(EBPrior, PosteriorEB, LiksScale),
                            rep(1:3, each = 1000))) #Data frame

colnames(dataNew) <- c("Lambda", "Density", "Factor")
dataNew$Factor <- factor(dataNew$Factor, levels=c("1", "3", "2"), 
                         labels=c("Prior", "Likelihood", "Posterior"))

ggplot(data = dataNew, aes_string(x = "Lambda", y = "Density", group = "Factor")) + 
  geom_line(aes(color = Factor)) +
  xlab(TeX("$\\lambda$")) + ylab("Density") +
  ggtitle("Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model") +
  guides(color=guide_legend(title="Information")) +
  scale_color_manual(values = c("red", "yellow", "blue"))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-2.svg" width="672" />

```r
# Predictive distributions
require(TailRank)
```

```
## Loading required package: TailRank
```

```
## Warning: package 'TailRank' was built under R version 4.3.3
```

```
## Loading required package: oompaBase
```

```
## Warning: package 'oompaBase' was built under R version 4.3.3
```

```
## Error: package or namespace load failed for 'TailRank' in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
##  there is no package called 'Biobase'
```

```r
PredDen <- function(y, y0, a0, b0){
  N <- length(y)
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N - sum(y) # Posterior scale parameter
  Pr <- aN/(aN+bN)
  Probs <- dbinom(y0, 1, prob = Pr)
  return(Probs)
}
y0 <- 0:1
PredVague <- PredDen(y = y, y0 = y0, a0 = a0, b0 = b0)
PredEB <- PredDen(y = y, y0 = y0, a0 = a0EB, b0 = b0EB)
dataPred <- as.data.frame(cbind(y0, PredVague, PredEB))
colnames(dataPred) <- c("y0", "PredictiveVague", "PredictiveEB")

ggplot(data = dataPred) + 
  geom_point(aes(y0, PredictiveVague, color = "red")) +  
  xlab(TeX("$y_0$")) + ylab("Density") +
  ggtitle("Predictive density: Vague and Empirical Bayes priors") +
  geom_point(aes(y0, PredictiveEB, color = "yellow")) +
  guides(color = guide_legend(title="Prior")) +
  scale_color_manual(labels = c("Vague", "Empirical Bayes"), values = c("red", "yellow")) +
  scale_x_continuous(breaks=seq(0,1,by=1))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-3.svg" width="672" />

```r
# Posterior odds: Vague vs Empirical Bayes
PO12 <- exp(-LogMgLik(c(a0EB, b0EB), y = y))/exp(-LogMgLik(c(a0, b0), y = y))
paste("The posterior odds: Empirical Bayes vs Vague prior prior is", PO12, sep = " ")
```

```
## [1] "The posterior odds: Empirical Bayes vs Vague prior prior is 3.12649602101054"
```

```r
PostProMEM <- PO12/(1 + PO12) # Posterior model probability Empirical Bayes
PostProbMV <- 1 - PostProMEM # Posterior model probability vague prior

paste("These are the posterior model probabilities", PostProMEM, PostProbMV, "for the Empirical Bayes and vague priors, respectively")
```

```
## [1] "These are the posterior model probabilities 0.75766364612776 0.24233635387224 for the Empirical Bayes and vague priors, respectively"
```

```r
# Bayesian model average (BMA)
PostMeanEB <- (a0EB + sum(y)) / (a0EB + b0EB + N) # Posterior mean Empirical Bayes 
PostMeanV <- (a0 + sum(y)) / (a0 + b0 + N) # Posterior mean vague priors
BMAmean <- PostProMEM * PostMeanEB + PostProbMV * PostMeanV  # BMA posterior mean

PostVarEB <- (a0EB + sum(y))*(b0EB + N - sum(y)) / ((a0EB + b0EB + N)^2)*(a0EB + b0EB + N -1) # Posterior variance Empirical Bayes
PostVarV <- (a0 + sum(y))*(b0 + N - sum(y)) / ((a0 + b0 + N)^2)*(a0 + b0 + N -1) # Posterior variance vague prior 

BMAVar <- PostProMEM * PostVarEB + PostProbMV * PostVarV + PostProMEM * (PostMeanEB - BMAmean)^2 + PostProbMV * (PostMeanV - BMAmean)^2# BMA posterior variance   

paste("The BMA posterior mean and variance are", BMAmean, "and", BMAVar, "respectively", sep = " ")
```

```
## [1] "The BMA posterior mean and variance are 0.468708096845045 and 39.2645846427469 respectively"
```

```r
# BMA: Predictive
BMAPred <- PostProMEM * PredEB + PostProbMV * PredVague    
dataPredBMA <- as.data.frame(cbind(y0, BMAPred))
colnames(dataPredBMA) <- c("y0", "PredictiveBMA")

ggplot(data = dataPredBMA) + 
  geom_point(aes(y0, PredictiveBMA, color = "red")) +  
  xlab(TeX("$y_0$")) + ylab("Density") +
  ggtitle("Predictive density: BMA") +
  guides(color = guide_legend(title="BMA")) +
  scale_color_manual(labels = c("Probability"), values = c("red")) +
  scale_x_continuous(breaks=seq(0,1,by=1))
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-4.svg" width="672" />

```r
# Bayesian updating
BayUp <- function(y, lambda, a0, b0){
  N <- length(y)
  aN <- a0 + sum(y) # Posterior shape parameter
  bN <- b0 + N - sum(y)    # Posterior scale parameter
  p <- dbeta(lambda, shape1 = aN, shape2 = bN) # Posterior density
  return(list(Post = p, a0New = aN, b0New = bN))
}

PostUp <- NULL
for(i in 1:N){
  if(i == 1){
    PostUpi <- BayUp(y[i], lambda, a0 = 1, b0 = 1)}
  else{
    PostUpi <- BayUp(y[i], lambda, a0 = PostUpi$a0New, b0 = PostUpi$b0New)
  }
  PostUp <- cbind(PostUp, PostUpi$Post)
}

DataUp <- data.frame(cbind(rep(lambda, 15), c(PostUp),
                           rep(1:15, each = 1000))) #Data frame

colnames(DataUp) <- c("Lambda", "Density", "Factor")

DataUp$Factor <- factor(DataUp$Factor, levels=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"), 
                        labels=c("Iter_1","Iter_2","Iter_3","Iter_4","Iter_5","Iter_6","Iter_7","Iter_8","Iter_9","Iter_10","Iter_11","Iter_12","Iter_13","Iter_14","Iter_15"))

ggplot(data = DataUp, aes_string(x = "Lambda", y = "Density", group = "Factor")) + 
  geom_line(aes(color = Factor)) +
  xlab(TeX("$p$")) + ylab("Density") +
  ggtitle("Bayesian updating: Beta-Binomial model with vague prior") +
  guides(color=guide_legend(title="Update")) 
```

<img src="BayesianEconometrics_files/figure-html/unnamed-chunk-10-5.svg" width="672" />

-->
<ol start="5" style="list-style-type: decimal">
<li>Show that given the loss function, <span class="math inline">\(L({\theta},a)=|{\theta}-a|\)</span>, then <span class="math inline">\({\delta}(\mathbf{y})\)</span> is the median.</li>
</ol>
<!--
**Solution**

$\int_{{\Theta}} |{\theta}-a|\pi({\theta}|\mathbf{y})d{\theta}=\int_{-\infty}^a (a-{\theta})\pi({\theta}|\mathbf{y})d{\theta}+\int_{a}^{\infty} ({\theta}-a)\pi({\theta}|\mathbf{y})d{\theta}$. Differentiating with respect to $a$, and equaliting to zero,

\begin{equation}
  \int_{-\infty}^a \pi({\theta}|\mathbf{y})d{\theta}=\int_{a}^{\infty} \pi({\theta}|\mathbf{y})d{\theta},  
\end{equation}

then,

\begin{equation}
  2\int_{-\infty}^a \pi({\theta}|\mathbf{y})d{\theta}=\int_{-\infty}^{\infty} \pi({\theta}|\mathbf{y})d{\theta}=1,  
\end{equation}

that is, $a^*(\mathbf{y})$ is the median.
-->
<p>–&gt;</p>

</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="summary-chapter-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Chap3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
