<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.1 State-space representation | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="8.1 State-space representation | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.1 State-space representation | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Chap8.html"/>
<link rel="next" href="sec82.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Dirichlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Non-parametric generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec81" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> State-space representation<a href="sec81.html#sec81" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <em>state-space model</em> consists of an <em>unobservable state vector</em> <span class="math inline">\(\boldsymbol{\beta}_t \in \mathbb{R}^K\)</span> and an <em>observed</em> measure <span class="math inline">\(\boldsymbol{Y}_t \in \mathbb{R}^M\)</span>, for <span class="math inline">\(t=1,2,\dots\)</span>. These components satisfy two key properties:<br />
(i) <span class="math inline">\(\boldsymbol{\beta}_t\)</span> follows a <em>Markov process</em>, meaning that <span class="math inline">\(\pi(\boldsymbol{\beta}_t\mid \boldsymbol{\beta}_{1:t-1})=\pi(\boldsymbol{\beta}_t\mid \boldsymbol{\beta}_{t-1})\)</span>. In other words, all information about <span class="math inline">\(\boldsymbol{\beta}_t\)</span> is carried by <span class="math inline">\(\boldsymbol{\beta}_{t-1}\)</span>, and<br />
(ii) <span class="math inline">\(\boldsymbol{Y}_t\)</span> is independent of <span class="math inline">\(\boldsymbol{Y}_s\)</span> given <span class="math inline">\(\boldsymbol{\beta}_t\)</span> for all <span class="math inline">\(s &lt; t\)</span> <span class="citation">(<a href="#ref-petris2009dynamic">Petris, Petrone, and Campagnoli 2009, chap. 2</a>)</span>.</p>
<p>These assumptions imply that<br />
<span class="math display">\[
\pi(\boldsymbol{\beta}_{0:t},\boldsymbol{Y}_{1:t})=\pi(\boldsymbol{\beta}_0)\prod_{s=1}^{t}\pi(\boldsymbol{\beta}_s\mid \boldsymbol{\beta}_{s-1})\pi(\boldsymbol{Y}_s\mid \boldsymbol{\beta}_s).
\]</span><br />
A <em>state-space model</em> where the states are discrete random variables is called a <em>hidden Markov model</em>.</p>
<p>There are three key aims in <em>state-space models</em>: <em>filtering</em>, <em>smoothing</em>, and <em>forecasting</em>.<br />
- In <em>filtering</em>, we estimate the current state given observations up to time <span class="math inline">\(t\)</span>, obtaining the density <span class="math inline">\(\pi(\boldsymbol{\beta}_{s}\mid \boldsymbol{y}_{1:t})\)</span> for <span class="math inline">\(s = t\)</span>.<br />
- In <em>smoothing</em>, we analyze past states, obtaining <span class="math inline">\(\pi(\boldsymbol{\beta}_{s}\mid \boldsymbol{y}_{1:t})\)</span> for <span class="math inline">\(s &lt; t\)</span>.<br />
- In <em>forecasting</em>, we predict future observations by first computing <span class="math inline">\(\pi(\boldsymbol{\beta}_{s}\mid \boldsymbol{y}_{1:t})\)</span> as an intermediate step to obtain <span class="math inline">\(\pi(\boldsymbol{Y}_{s}\mid \boldsymbol{y}_{1:t})\)</span> for <span class="math inline">\(s &gt; t\)</span>.</p>
<p>A key advantage of these methods is that all these densities can be calculated recursively. <span class="citation">Petris, Petrone, and Campagnoli (<a href="#ref-petris2009dynamic">2009</a>)</span> provide the recursive equations in Propositions 2.1 (filtering), 2.3 (smoothing), and 2.5 (forecasting).</p>
<div id="gaussian-linear-state-space-models" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Gaussian linear state-space models<a href="sec81.html#gaussian-linear-state-space-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An important class of <em>state-space models</em> is the <em>Gaussian linear state-space model</em>, also known as a <em>dynamic linear model</em>:</p>
<p><span class="math display">\[\begin{align}
    \boldsymbol{Y}_t &amp;= \boldsymbol{X}_t\boldsymbol{\beta}_t+\boldsymbol{\mu}_t &amp; \text{(Observation equations)} \\
    \boldsymbol{\beta}_t &amp;= \boldsymbol{G}_t\boldsymbol{\beta}_{t-1}+\boldsymbol{w}_t &amp; \text{(State equations)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\beta}_0\sim N(\boldsymbol{b}_0,\boldsymbol{B}_0)\)</span>, <span class="math inline">\(\boldsymbol{\mu}_t\sim N(\boldsymbol{0}, \boldsymbol{\Sigma}_t)\)</span>, and <span class="math inline">\(\boldsymbol{w}_t\sim N(\boldsymbol{0}, \boldsymbol{\Omega}_t)\)</span>. The terms <span class="math inline">\(\boldsymbol{\beta}_0\)</span>, <span class="math inline">\(\boldsymbol{\mu}_t\)</span>, and <span class="math inline">\(\boldsymbol{w}_t\)</span> are independent, while <span class="math inline">\(\boldsymbol{X}_t\)</span> and <span class="math inline">\(\boldsymbol{G}_t\)</span> are known matrices of dimensions <span class="math inline">\(M\times K\)</span> and <span class="math inline">\(K\times K\)</span>, respectively.</p>
<p>These assumptions imply that<br />
<span class="math display">\[
\boldsymbol{Y}_t\mid \boldsymbol{\beta}_t \sim N(\boldsymbol{X}_t\boldsymbol{\beta}_t, \boldsymbol{\Sigma}_t), \quad
\boldsymbol{\beta}_t\mid \boldsymbol{\beta}_{t-1} \sim N(\boldsymbol{G}_t\boldsymbol{\beta}_{t-1}, \boldsymbol{\Omega}_t).
\]</span><br />
A general <em>state-space model</em> is defined as <span class="math inline">\(\boldsymbol{Y}_t = \boldsymbol{f}_t(\boldsymbol{\beta}_t, \boldsymbol{\mu}_t)\)</span> and <span class="math inline">\(\boldsymbol{\beta}_t = \boldsymbol{m}_t(\boldsymbol{\beta}_{t-1}, \boldsymbol{w}_t)\)</span>, where <span class="math inline">\(\boldsymbol{f}_t\)</span> and <span class="math inline">\(\boldsymbol{m}_t\)</span> are arbitrary functions with corresponding distributions for <span class="math inline">\(\boldsymbol{\mu}_t\)</span> and <span class="math inline">\(\boldsymbol{w}_t\)</span>, and a prior for <span class="math inline">\(\boldsymbol{\beta}_0\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\beta}_{t-1}\mid \boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{b}_{t-1},\boldsymbol{B}_{t-1})\)</span>, then, we can get the <em>Kalman filter</em> by obtaining:</p>
<ol style="list-style-type: decimal">
<li><p>The one-step-ahead predictive distribution of <span class="math inline">\(\boldsymbol{\beta}_t\)</span> given <span class="math inline">\(\boldsymbol{y}_{1:t-1}\)</span> is <span class="math inline">\(\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{a}_t, \boldsymbol{R}_t)\)</span>, where
<span class="math display">\[\boldsymbol{a}_t=\boldsymbol{G}_t\boldsymbol{b}_{t-1}, \quad \boldsymbol{R}_t=\boldsymbol{G}_t\boldsymbol{B}_{t-1}\boldsymbol{G}_t^{\top}+\boldsymbol{\Omega}_t.\]</span></p></li>
<li><p>The one-step-ahead predictive distribution of <span class="math inline">\(\boldsymbol{Y}_t\)</span> given <span class="math inline">\(\boldsymbol{y}_{1:t-1}\)</span> is <span class="math inline">\(\boldsymbol{Y}_t\mid \boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{f}_t, \boldsymbol{Q}_t)\)</span>, where
<span class="math display">\[\boldsymbol{f}_t=\boldsymbol{X}_t\boldsymbol{a}_t, \quad \boldsymbol{Q}_t=\boldsymbol{X}_t\boldsymbol{R}_t\boldsymbol{X}_t^{\top}+\boldsymbol{\Sigma}_t.\]</span></p></li>
<li><p>The distribution of the one-step-ahead prediction error <span class="math inline">\(\boldsymbol{e}_t=\boldsymbol{Y}_t-\mathbb{E}[\boldsymbol{Y}_t\mid \boldsymbol{y}_{1:t-1}]=\boldsymbol{Y}_t-\boldsymbol{f}_t\)</span> is <span class="math inline">\(N(\boldsymbol{0}, \boldsymbol{Q}_t)\)</span> <span class="citation">Shumway and Stoffer (<a href="#ref-shumway2017time">2017</a>)</span>, Chap. 6.</p></li>
<li><p>The filtering distribution of <span class="math inline">\(\boldsymbol{\beta}_t\)</span> given <span class="math inline">\(\boldsymbol{y}_{1:t}\)</span> is <span class="math inline">\(\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t}\sim N(\boldsymbol{b}_t, \boldsymbol{B}_t)\)</span>, where
<span class="math display">\[\boldsymbol{b}_t=\boldsymbol{a}_t+\boldsymbol{K}_t\boldsymbol{e}_t, \quad \boldsymbol{K}_t=\boldsymbol{R}_t\boldsymbol{X}_t^{\top}\boldsymbol{Q}_t^{-1}\]</span><br />
is the <em>Kalman gain</em>, and
<span class="math display">\[\boldsymbol{B}_t=\boldsymbol{R}_t-\boldsymbol{R}_t\boldsymbol{X}_t^{\top}\boldsymbol{Q}_t^{-1}\boldsymbol{X}_t\boldsymbol{R}_t.\]</span></p></li>
</ol>
<p>The formal proofs of these results can be found in <span class="citation">Petris, Petrone, and Campagnoli (<a href="#ref-petris2009dynamic">2009</a>)</span>, Chap. 2. Just take into account that the logic of these results follows the Seemingly Unrelated Regression (SUR) model in <a href="sec72.html#sec72">7.2</a> for a particular time period. In addition, we know that the posterior distribution using information up to <span class="math inline">\(t-1\)</span> becomes the prior in <span class="math inline">\(t\)</span>,</p>
<p><span class="math display">\[\pi(\boldsymbol{\theta}\mid \mathbf{y}_{1:t})\propto p(y_{t}\mid \boldsymbol{y}_{1:t-1},\boldsymbol{\theta})\times \pi(\boldsymbol{\theta}\mid \boldsymbol{y}_{1:t-1}).\]</span></p>
<p>This is the updating process from <span class="math inline">\(\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t-1}\sim N(\boldsymbol{a}_t, \boldsymbol{R}_t)\)</span> to <span class="math inline">\(\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t}\sim N(\boldsymbol{b}_t, \boldsymbol{B}_t)\)</span>. Moreover, the posterior mean and variance of the SUR model with independent conjugate priors for a particular time period can be written as</p>
<p><span class="math display">\[\boldsymbol{a}_{t}+\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}+ \boldsymbol{\Sigma}_t)^{-1}(\boldsymbol{y}_t-\boldsymbol{X}_t\boldsymbol{a}_{t})\]</span></p>
<p>and</p>
<p><span class="math display">\[\boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}+\boldsymbol{\Sigma}_t)^{-1} \boldsymbol{X}_t\boldsymbol{R}_{t}^{\top},\]</span></p>
<p>respectively. Let’s see this, we know from <a href="sec72.html#sec72">7.2</a> that</p>
<p><span class="math display">\[\boldsymbol{B}_t=(\boldsymbol{R}_t^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{X}_t)^{-1}\]</span></p>
<p>and</p>
<p><span class="math display">\[\boldsymbol{\beta}_t=\boldsymbol{B}_t(\boldsymbol{R}_t^{-1}\boldsymbol{a}_t+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{y}_t).\]</span></p>
<p>Thus, let’s show that both conditional posterior distributions are the same. In particular, the posterior mean in the <em>state-space representation</em> is</p>
<p><span class="math display">\[[\boldsymbol{I}_K-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}+ \boldsymbol{\Sigma}_t)^{-1}\boldsymbol{X}_t]\boldsymbol{a}_{t}+\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}+ \boldsymbol{\Sigma}_t)^{-1}\boldsymbol{y}_t,\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align*}
    \boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}+ \boldsymbol{\Sigma}_t)^{-1}
    &amp;=\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}[\boldsymbol{\Sigma}_t^{-1}-\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_t^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}]\\
    &amp;=\boldsymbol{R}_{t}[\boldsymbol{I}_K-\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_t^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}]\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\\
    &amp;=\boldsymbol{R}_{t}(\boldsymbol{I}_K-[\boldsymbol{I}_K-\boldsymbol{R}_t^{-1}(\boldsymbol{R}_t^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}])\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\\
    &amp;=(\boldsymbol{R}_t^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1},
\end{align*}\]</span></p>
<p>where the first equality uses the Woodbury matrix identity (matrix inversion lemma), and the third equality uses <span class="math inline">\(\boldsymbol{D}(\boldsymbol{D}+\boldsymbol{E})^{-1}=\boldsymbol{I}-\boldsymbol{E}(\boldsymbol{D}+\boldsymbol{E})^{-1}\)</span>.</p>
<p>Thus, we have the following expression:</p>
<p><span class="math display">\[\begin{align*}
    &amp;[\mathbf{I}_K - \mathbf{R}_t \mathbf{X}_t^{\top} (\mathbf{X}_t \mathbf{R}_t \mathbf{X}_t^{\top} + \boldsymbol{\Sigma}_t)^{-1} \mathbf{X}_t] \mathbf{a}_t + \mathbf{R}_t \mathbf{X}_t^{\top} (\mathbf{X}_t \mathbf{R}_t \mathbf{X}_t^{\top} + \boldsymbol{\Sigma}_t)^{-1} \mathbf{y}_t \\  
    &amp;= [\mathbf{I}_K - (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t] \mathbf{a}_t + (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{y}_t \\  
    &amp;= (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} \mathbf{R}_t^{-1} \mathbf{a}_t + (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{y}_t \\  
    &amp;= (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} (\mathbf{R}_t^{-1} \mathbf{a}_t + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{y}_t) \\  
    &amp;= (\mathbf{R}_t^{-1} + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t)^{-1} (\mathbf{R}_t^{-1} \mathbf{a}_t + \mathbf{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \mathbf{X}_t \hat{\boldsymbol{\beta}}_t),
\end{align*}\]</span></p>
<p>where the second equality uses the identity:</p>
<p><span class="math display">\[
\boldsymbol{I} - (\boldsymbol{D} + \boldsymbol{E})^{-1} \boldsymbol{D} = (\boldsymbol{D} + \boldsymbol{E})^{-1} \boldsymbol{E},
\]</span></p>
<p>and the estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}_t\)</span> is defined as:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_t = (\boldsymbol{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \boldsymbol{X}_t)^{-1} \boldsymbol{X}_t^{\top} \boldsymbol{\Sigma}_t^{-1} \boldsymbol{y}_t.
\]</span></p>
<p>This shows that the posterior mean is a weighted average of the prior mean and the maximum likelihood estimator (which is the generalized least squares estimator).</p>
<p>The weights are linked to the signal-to-noise ratio, that is, the proportion of the total variability (<span class="math inline">\(\boldsymbol{\Omega}_t+\boldsymbol{\Sigma}_t\)</span>) due to the signal (<span class="math inline">\(\boldsymbol{\Omega}_t\)</span>) versus the noise (<span class="math inline">\(\boldsymbol{\Sigma}_t\)</span>). Note that in the simplest case where <span class="math inline">\(M=K=1\)</span>, and <span class="math inline">\(\boldsymbol{X}_t=\boldsymbol{G}_t=1\)</span>, then <span class="math inline">\(\boldsymbol{K}_t=\boldsymbol{R}_t\boldsymbol{Q}_t^{-1}=(B_{t-1}+\Omega_t)/(B_{t-1}+\Omega_t+\Sigma_t)\)</span>. Thus, the weight associated with the observations is equal to 1 if <span class="math inline">\(\Sigma_t=0\)</span>, that is, the posterior mean is equal to the actual observation. On the other hand, if <span class="math inline">\(\Sigma_t\)</span> increases compare to <span class="math inline">\(\Omega_t\)</span>, there is more weight to the prior information, and consequently, the posterior mean is smoother as it heavily dependents on the history. We ask in Exercise 1 to perform simulations with different signal-to-noise ratios to see the effects on the system.</p>
<p>The equality of variances of both approaches is as follows:
<span class="math display">\[\begin{align*}
    Var[\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t}]&amp;
    = \boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{X}_t\boldsymbol{R}_{t}\boldsymbol{X}_t^\top+\boldsymbol{\Sigma}_t)^{-1} \boldsymbol{X}_t\boldsymbol{R}_{t}\\
    &amp;=\boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}(\boldsymbol{\Sigma}_t^{-1}- \boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1})\boldsymbol{X}_t\boldsymbol{R}_{t}\\
    &amp;=\boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t\boldsymbol{R}_{t}+ \boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t\boldsymbol{R}_{t}\\
    &amp;=\boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t\boldsymbol{R}_{t}+ \boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t[\boldsymbol{I}_K-(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\boldsymbol{R}_{t}^{-1}]\boldsymbol{R}_{t}\\
    &amp;=\boldsymbol{R}_{t}-\boldsymbol{R}_{t}\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}\\
    &amp;=\boldsymbol{R}_t[\boldsymbol{I}_K-\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1}]\\
    &amp;=\boldsymbol{R}_{t}[\boldsymbol{I}_K-(\boldsymbol{I}_K-\boldsymbol{R}_{t}^{-1}(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1})]\\
    &amp;=(\boldsymbol{R}_{t}^{-1}+\boldsymbol{X}_t^{\top}\boldsymbol{\Sigma}_t^{-1}\boldsymbol{X}_t)^{-1},
\end{align*}\]</span>
where the second equality uses the Woodbury matrix identity, the fourth equality uses <span class="math inline">\((\boldsymbol{D}+\boldsymbol{E})^{-1}\boldsymbol{D}=\boldsymbol{I}-(\boldsymbol{D}+\boldsymbol{E})^{-1}\boldsymbol{E}\)</span>, and the seventh equality uses <span class="math inline">\(\boldsymbol{D}(\boldsymbol{D}+\boldsymbol{E})^{-1}=\boldsymbol{I}-\boldsymbol{E}(\boldsymbol{D}+\boldsymbol{E})^{-1}\)</span>.</p>
<p>The <em>Kalman filter</em> allows calculating recursively in a forward way <span class="math inline">\(\pi(\boldsymbol{\beta}_t\mid \boldsymbol{y}_{1:t})\)</span> from <span class="math inline">\(\pi(\boldsymbol{\beta}_{t-1}\mid \boldsymbol{y}_{1:t-1})\)</span> starting from <span class="math inline">\(\pi(\boldsymbol{\beta}_0)\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{\beta}_{t+1} \mid \mathbf{y}_{1:T} \sim N(\boldsymbol{s}_{t+1}, \mathbf{S}_{t+1})\)</span>, then we can get the <em>Kalman smoother</em> by<br />
<span class="math inline">\(\boldsymbol{\beta}_{t} \mid \mathbf{y}_{1:T} \sim N(\boldsymbol{s}_{t}, \mathbf{S}_{t})\)</span>, where</p>
<p><span class="math display">\[
\boldsymbol{s}_t = \mathbf{b}_t + \mathbf{B}_t \mathbf{G}_{t+1}^{\top} \mathbf{R}_{t+1}^{-1} (\boldsymbol{s}_{t+1} - \mathbf{a}_{t+1})
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbf{S}_t = \mathbf{B}_t - \mathbf{B}_t \mathbf{G}_{t+1}^{\top} \mathbf{R}_{t+1}^{-1} (\mathbf{R}_{t+1} - \mathbf{S}_{t+1}) \mathbf{R}_{t+1}^{-1} \mathbf{G}_{t+1} \mathbf{B}_{t}.
\]</span></p>
<p>The proof can be found in <span class="citation">Petris, Petrone, and Campagnoli (<a href="#ref-petris2009dynamic">2009</a>)</span>, Chap. 2.</p>
<p>Thus, we can calculate the <em>Kalman smoother</em> starting from <span class="math inline">\(t = T-1\)</span>, that is,<br />
<span class="math inline">\(\boldsymbol{\beta}_{T} \mid \mathbf{y}_{1:T} \sim N(\boldsymbol{s}_{T}, \mathbf{S}_{T})\)</span>. However, this is the filtering distribution at <span class="math inline">\(T\)</span>, which means<br />
<span class="math inline">\(\boldsymbol{s}_{T} = \mathbf{b}_{T}\)</span> and <span class="math inline">\(\mathbf{S}_{T} = \mathbf{B}_{T}\)</span>, and then, we should proceed recursively in a backward way.</p>
<p>Finally, the forecasting recursion in the <em>dynamic linear model</em>, given <span class="math inline">\(\mathbf{a}_t(0) = \mathbf{b}_t\)</span> and <span class="math inline">\(\mathbf{R}_t(0) = \mathbf{B}_t\)</span>, <span class="math inline">\(h \geq 1\)</span>, is given by</p>
<ol style="list-style-type: decimal">
<li><p>The forecasting distribution of <span class="math inline">\(\boldsymbol{\beta}_{t+h} \mid \mathbf{y}_{1:t}\)</span> is <span class="math inline">\(N(\mathbf{a}_t(h), \mathbf{R}_t(h))\)</span>, where<br />
<span class="math display">\[
\mathbf{a}_t(h) = \mathbf{G}_{t+h} \mathbf{a}_{t}(h-1), \quad
\mathbf{R}_t(h) = \mathbf{G}_{t+h} \mathbf{R}_t(h-1) \mathbf{G}_{t+h}^{\top} + \boldsymbol{\Omega}_{t+h}.
\]</span></p></li>
<li><p>The forecasting distribution <span class="math inline">\(\mathbf{Y}_{t+h} \mid \mathbf{y}_{1:t}\)</span> is <span class="math inline">\(N(\mathbf{f}_t(h), \mathbf{Q}_t(h))\)</span>, where<br />
<span class="math display">\[
\mathbf{f}_t(h) = \mathbf{X}_{t+h} \mathbf{a}_t(h), \quad
\mathbf{Q}_t(h) = \mathbf{X}_{t+h} \mathbf{R}_t(h) \mathbf{X}_{t+h}^{\top} + \boldsymbol{\Sigma}_{t+h}.
\]</span></p></li>
</ol>
<p>The proof can be found in <span class="citation">Petris, Petrone, and Campagnoli (<a href="#ref-petris2009dynamic">2009</a>)</span>, Chap. 2.</p>
<p>These recursive equations allow us to perform probabilistic forecasting <span class="math inline">\(h\)</span>-steps-ahead for the state and observation equations.</p>
<p>These results demonstrate how to use these recursive equations for filtering, smoothing, and forecasting in <em>dynamic linear models</em> (<em>Gaussian linear state-space models</em>). Although these algorithms appear simple, they suffer from numerical instability, which can lead to non-symmetric and negative-definite covariance matrices. Thus, special care must be taken when working with them.</p>
<p>In addition, this setup assumes that <span class="math inline">\(\boldsymbol{\Sigma}_t\)</span> and <span class="math inline">\(\boldsymbol{\Omega}_t\)</span> are known. However, this is rarely the case in most situations. Therefore, we need to estimate them. One option is to perform maximum likelihood estimation. However, this approach does not account for the uncertainty associated with the fact that <span class="math inline">\(\boldsymbol{\Sigma}_t\)</span> and <span class="math inline">\(\boldsymbol{\Omega}_t\)</span> are unknown when their estimates are <em>plugged into</em> the <em>state space</em> recursions. On the other hand, we can use a Bayesian approach and perform the recursions associated with each posterior draw of the unknown parameters, thus taking their uncertainty into account.</p>
<p>The point of departure is the posterior distribution, such that</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}, \boldsymbol{\beta}_0, \dots, \boldsymbol{\beta}_T \mid \mathbf{y}, \mathbf{X}, \mathbf{G}) \propto \pi(\boldsymbol{\beta}_0 \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) \prod_{t=1}^{T} \pi(\boldsymbol{\beta}_t \mid \boldsymbol{\beta}_{t-1}, \boldsymbol{\theta}) \pi(\mathbf{y}_t \mid \boldsymbol{\beta}_t, \boldsymbol{\theta}),
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\theta}\)</span> is the vector of unknown parameters.</p>
<p>We can compute<br />
<span class="math display">\[\pi(\boldsymbol{\beta}_s, \boldsymbol{\theta} \mid \mathbf{y}_{1:t}) = \pi(\boldsymbol{\beta}_s \mid \mathbf{y}_{1:t}, \boldsymbol{\theta}) \pi(\boldsymbol{\theta} \mid \mathbf{y}_{1:t}),\]</span><br />
for <span class="math inline">\(s=t\)</span> (<em>filtering</em>), <span class="math inline">\(s&lt;t\)</span> (<em>smoothing</em>), and <span class="math inline">\(s&gt;t\)</span> (<em>forecasting</em>). The marginal posterior distribution of the states is</p>
<p><span class="math display">\[
\pi(\boldsymbol{\beta}_s \mid \mathbf{y}_{1:t}) = \int_{\boldsymbol{\Theta}} \pi(\boldsymbol{\beta}_s \mid \mathbf{y}_{1:t}, \boldsymbol{\theta}) \pi(\boldsymbol{\theta} \mid \mathbf{y}_{1:t}) d\boldsymbol{\theta}.
\]</span></p>
<p>We can use the Gibbs sampling algorithm to get the posterior draws in the <em>dynamic linear model</em> assuming conjugate families. In particular, let’s see the univariate case with <em>random walk states</em>,
<span class="math display" id="eq:eq1State" id="eq:eq1Obs">\[\begin{align}
    y_t&amp;=\boldsymbol{x}_t^{\top}\boldsymbol{\beta}_t+\mu_t \tag{8.1}\\
    \boldsymbol{\beta}_t&amp;=\boldsymbol{\beta}_{t-1}+\boldsymbol{w}_t, \tag{8.2}
\end{align}\]</span>
where <span class="math inline">\(\mu_t\sim N(0,\sigma^2)\)</span> and <span class="math inline">\(\boldsymbol{w}_t\sim N(\boldsymbol{0},\text{diag}\left\{\omega_1^2,\dots,\omega_K^2\right\})\)</span>. We assume that <span class="math inline">\(\pi(\sigma^2,\omega_1^2,\dots,\omega_K^2,\boldsymbol{\beta}_0)=\pi(\sigma^2)\pi(\omega_1^2),\dots,\pi(\omega_K^2)\pi(\boldsymbol{\beta}_0)\)</span> where <span class="math inline">\(\sigma^2\sim IG(\alpha_0/2,\delta_0/2)\)</span>, <span class="math inline">\(\omega_k^2\sim IG(\alpha_{k0}/2,\delta_{k0}/2)\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, and <span class="math inline">\(\boldsymbol{\beta}_0\sim N(\boldsymbol{b}_0,\boldsymbol{B}_0)\)</span>. Thus, the conditional posterior distributions are <span class="math inline">\(\sigma^2\mid \boldsymbol{y},\boldsymbol{X},\boldsymbol{\beta}_{1:T}\sim IG(\alpha_{n}/2,\delta_n/2)\)</span>, where <span class="math inline">\(\alpha_{n}=T+\alpha_0\)</span> and <span class="math inline">\(\delta_n=\sum_{t=1}^T(y_t-\boldsymbol{x}_t^{\top}\boldsymbol{\beta}_t)^2+\delta_0\)</span>, and <span class="math inline">\(\omega_k^2\mid \boldsymbol{y},\boldsymbol{X},\boldsymbol{\beta}_{0:T}\sim IG(\alpha_{kn}/2,\delta_{kn}/2)\)</span>, where <span class="math inline">\(\alpha_{kn}=T+\alpha_{k0}\)</span> and <span class="math inline">\(\delta_{kn}=\sum_{t=1}^T(\boldsymbol{\beta}_{t,k}-\boldsymbol{\beta}_{t-1,k})^2+\delta_{k0}\)</span>. The vector of the dependent variable is <span class="math inline">\(\boldsymbol{y}\)</span>, and all regressors are in <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
<p>We also need to sample the states from <span class="math inline">\(\pi(\boldsymbol{\beta}_{1:T}\mid \boldsymbol{y},\boldsymbol{X},\sigma^2,\omega_1^2,\dots,\omega_K^2)\)</span>. This can be done using the <em>forward filtering backward sampling</em> (FFBS) algorithm <span class="citation">(<a href="#ref-carter1994gibbs">Carter and Kohn 1994</a>; <a href="#ref-fruhwirth1994data">Frühwirth-Schnatter 1994</a>; <a href="#ref-shephard1994partial">Shephard 1994</a>)</span>. This algorithm is basically a simulation version of the <em>smoothing</em> recursion, which allows getting draws of the states, even if we do not have analytical solutions, for instance, in non-linear settings. See below and <span class="citation">Petris, Petrone, and Campagnoli (<a href="#ref-petris2009dynamic">2009</a>)</span> Chap. 3 for details. A word of caution here, users should be careful to set non-informative priors in this setting, and in general, settings where there are a large number of parameters (see <span class="citation">G. M. Koop (<a href="#ref-koop2003bayesian">2003</a>)</span> Chap. 8 for details). Thus, it is useful to use empirical Bayes methods focusing on relevant hyperparameters, for instance, the hyperparameters of the inverse-gamma distributions which define the signal-to-noise ratio.</p>
<p>We use the command <em>dlmGibbsDIG</em> from the <em>dlm</em> package in our GUI to perform Bayesian inference in the univariate <em>dynamic linear model</em> with <em>random walk states</em>. This function uses the FFBS algorithm, and assumes independent gamma priors for the precision (inverse of variance) parameters. In addition, this package uses the singular value decomposition to calculate the covariance matrices to avoid numerical instability.</p>
<p>The following Algorithm shows how to perform inference in the univariate <em>dynamic linear model</em> with random walk states in our GUI. See also Chapter <a href="Chap5.html#Chap5">5</a> for details regarding the dataset structure.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Dynamic Linear Models</strong></p>
<ol style="list-style-type: decimal">
<li><p>Select <em>Time series Model</em> on the top panel</p></li>
<li><p>Select <em>Dynamic linear model</em> using the left radio button</p></li>
<li><p>Upload the dataset selecting first if there is a header in the file, and the kind of separator in the <em>csv</em> file of the dataset (comma, semicolon, or tab). Then, use the <em>Browse</em> button under the <strong>Choose File</strong> legend</p></li>
<li><p>Select MCMC iterations, burn-in, and thinning parameters using the <em>Range sliders</em></p></li>
<li><p>Set the hyperparameters of the <em>precision of the observation equation</em>: prior mean and variance</p></li>
<li><p>Set the hyperparameters of the <em>precision of the state equations</em>: just one set of prior mean and variance parameters</p></li>
<li><p>Click the <em>Go!</em> button</p></li>
<li><p>Analyze results</p></li>
<li><p>Download posterior chains of variances of observation and state equations, and posterior chains of states using the <em>Download Results</em> button</p></li>
</ol>
</div>
</div>
<p><strong>Example: Simulation exercise of the dynamic linear model</strong></p>
<p>We simulate the process <span class="math inline">\(y_t = \beta_{t1} + x_t \beta_{t2} + \mu_t\)</span> and <span class="math inline">\(\boldsymbol{\beta}_t = \boldsymbol{\beta}_{t-1} + \boldsymbol{w}_t\)</span>, <span class="math inline">\(t = 1, 2, \dots, 200\)</span>, where <span class="math inline">\(\boldsymbol{\beta}_t = [\beta_{t1} \ \beta_{t2}]^{\top}\)</span>, <span class="math inline">\(\mu_t \sim N(0, 0.5^2)\)</span>, <span class="math inline">\(\boldsymbol{w}_t \sim N(\boldsymbol{0}, \text{diag}\{0.2, 0.1\})\)</span>, <span class="math inline">\(x_t \sim N(1, 1)\)</span>, <span class="math inline">\(\boldsymbol{\beta}_0\)</span> and <span class="math inline">\(\boldsymbol{B}_0\)</span> are the OLS estimates and variance of the recursive OLS estimates (see below), respectively.</p>
<p>The following algorithm demonstrates how to perform inference using <em>dlmGibbsDIG</em> and compares the results to those of the maximum likelihood estimator, which is based on the <em>dlmMLE</em> function. We also use the <em>dlmSvd2var</em> function, which is based on singular value decomposition, to calculate the variance of the smoothing states. All these functions are from the <em>dlm</em> package in <strong>R</strong>.</p>
<p>Users can observe that we employ a straightforward strategy for setting the hyperparameters. First, we recursively estimate the model using ordinary least squares (OLS), progressively increasing the sample size, and save the location parameters. Next, we compute the covariance matrix of this sequence and use it to set the priors: the prior mean of the precision of the state vector is set equal to the inverse of the maximum element of the main diagonal of this covariance matrix (<em>a.theta</em>), and the prior variance is set equal to ten times this value (<em>b.theta</em>). For the observation equation, the prior mean of the precision is set equal to the inverse of the OLS variance estimate (<em>a.y</em>), and the prior variance is set equal to ten times this value (<em>b.y</em>). We perform some sensitivity analysis of the results regarding the hyperparameters, and it seems that the results are robust. However, we encourage giving more consideration to empirical Bayes methods for setting hyperparameters in <em>state-space models</em>.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="sec81.html#cb306-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb306-2"><a href="sec81.html#cb306-2" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">200</span>; sig2 <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb306-3"><a href="sec81.html#cb306-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(T, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>) </span>
<span id="cb306-4"><a href="sec81.html#cb306-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x); B0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb306-5"><a href="sec81.html#cb306-5" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">length</span>(B0)</span>
<span id="cb306-6"><a href="sec81.html#cb306-6" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(T, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sig2<span class="sc">^</span><span class="fl">0.5</span>)</span>
<span id="cb306-7"><a href="sec81.html#cb306-7" tabindex="-1"></a>Omega <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.1</span>))</span>
<span id="cb306-8"><a href="sec81.html#cb306-8" tabindex="-1"></a>w <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(T, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), Omega)</span>
<span id="cb306-9"><a href="sec81.html#cb306-9" tabindex="-1"></a>Bt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, T, K); Bt[<span class="dv">1</span>,] <span class="ot">&lt;-</span> B0</span>
<span id="cb306-10"><a href="sec81.html#cb306-10" tabindex="-1"></a>yt <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, T) </span>
<span id="cb306-11"><a href="sec81.html#cb306-11" tabindex="-1"></a>yt[<span class="dv">1</span>] <span class="ot">&lt;-</span> X[<span class="dv">1</span>,]<span class="sc">%*%</span>B0 <span class="sc">+</span> e[<span class="dv">1</span>]</span>
<span id="cb306-12"><a href="sec81.html#cb306-12" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb306-13"><a href="sec81.html#cb306-13" tabindex="-1"></a>    <span class="cf">if</span>(t <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb306-14"><a href="sec81.html#cb306-14" tabindex="-1"></a>        Bt[t,] <span class="ot">&lt;-</span> w[t,]</span>
<span id="cb306-15"><a href="sec81.html#cb306-15" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb306-16"><a href="sec81.html#cb306-16" tabindex="-1"></a>        Bt[t,] <span class="ot">&lt;-</span> Bt[t<span class="dv">-1</span>,] <span class="sc">+</span> w[t,]</span>
<span id="cb306-17"><a href="sec81.html#cb306-17" tabindex="-1"></a>    }</span>
<span id="cb306-18"><a href="sec81.html#cb306-18" tabindex="-1"></a>    yt[t] <span class="ot">&lt;-</span> X[t,]<span class="sc">%*%</span>Bt[t,] <span class="sc">+</span> e[t]</span>
<span id="cb306-19"><a href="sec81.html#cb306-19" tabindex="-1"></a>}</span>
<span id="cb306-20"><a href="sec81.html#cb306-20" tabindex="-1"></a>RegLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(yt <span class="sc">~</span> x)</span>
<span id="cb306-21"><a href="sec81.html#cb306-21" tabindex="-1"></a>SumRegLS <span class="ot">&lt;-</span> <span class="fu">summary</span>(RegLS)</span>
<span id="cb306-22"><a href="sec81.html#cb306-22" tabindex="-1"></a>SumRegLS; SumRegLS<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yt ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.247  -1.293   0.699   2.632   9.560 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.4941     0.4215   10.66   &lt;2e-16 ***
## x             4.2508     0.2858   14.87   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.274 on 198 degrees of freedom
## Multiple R-squared:  0.5277, Adjusted R-squared:  0.5253 
## F-statistic: 221.2 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## [1] 18.27024</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="sec81.html#cb309-1" tabindex="-1"></a>Bp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(RegLS<span class="sc">$</span>coefficients, T, K, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb309-2"><a href="sec81.html#cb309-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb309-3"><a href="sec81.html#cb309-3" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> S<span class="sc">:</span>T){</span>
<span id="cb309-4"><a href="sec81.html#cb309-4" tabindex="-1"></a>    RegLSt <span class="ot">&lt;-</span> <span class="fu">lm</span>(yt[<span class="dv">1</span><span class="sc">:</span>t] <span class="sc">~</span> x[<span class="dv">1</span><span class="sc">:</span>t])</span>
<span id="cb309-5"><a href="sec81.html#cb309-5" tabindex="-1"></a>    Bp[t,] <span class="ot">&lt;-</span> RegLSt<span class="sc">$</span>coefficients </span>
<span id="cb309-6"><a href="sec81.html#cb309-6" tabindex="-1"></a>}</span>
<span id="cb309-7"><a href="sec81.html#cb309-7" tabindex="-1"></a><span class="co"># plot(Bp[S:T,2], type = &quot;l&quot;)</span></span>
<span id="cb309-8"><a href="sec81.html#cb309-8" tabindex="-1"></a>VarBp <span class="ot">&lt;-</span> <span class="fu">var</span>(Bp)</span>
<span id="cb309-9"><a href="sec81.html#cb309-9" tabindex="-1"></a><span class="co"># State space model</span></span>
<span id="cb309-10"><a href="sec81.html#cb309-10" tabindex="-1"></a>ModelReg <span class="ot">&lt;-</span> <span class="cf">function</span>(par){</span>
<span id="cb309-11"><a href="sec81.html#cb309-11" tabindex="-1"></a>    Mod <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmModReg</span>(x, <span class="at">dV =</span> <span class="fu">exp</span>(par[<span class="dv">1</span>]), <span class="at">dW =</span> <span class="fu">exp</span>(par[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]), <span class="at">m0 =</span> RegLS<span class="sc">$</span>coefficients, <span class="at">C0 =</span> VarBp)</span>
<span id="cb309-12"><a href="sec81.html#cb309-12" tabindex="-1"></a>    <span class="fu">return</span>(Mod)</span>
<span id="cb309-13"><a href="sec81.html#cb309-13" tabindex="-1"></a>}</span>
<span id="cb309-14"><a href="sec81.html#cb309-14" tabindex="-1"></a>outMLEReg <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmMLE</span>(yt, <span class="at">parm =</span> <span class="fu">rep</span>(<span class="dv">0</span>, K<span class="sc">+</span><span class="dv">1</span>), ModelReg)</span>
<span id="cb309-15"><a href="sec81.html#cb309-15" tabindex="-1"></a><span class="fu">exp</span>(outMLEReg<span class="sc">$</span>par)</span></code></pre></div>
<pre><code>## [1] 0.1880010 0.2773561 0.0785071</code></pre>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="sec81.html#cb311-1" tabindex="-1"></a>RegFilter <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmFilter</span>(yt, <span class="fu">ModelReg</span>(outMLEReg<span class="sc">$</span>par))</span>
<span id="cb311-2"><a href="sec81.html#cb311-2" tabindex="-1"></a>RegSmoth <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmSmooth</span>(yt, <span class="fu">ModelReg</span>(outMLEReg<span class="sc">$</span>par))</span>
<span id="cb311-3"><a href="sec81.html#cb311-3" tabindex="-1"></a>SmoothB2 <span class="ot">&lt;-</span> RegSmoth<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb311-4"><a href="sec81.html#cb311-4" tabindex="-1"></a>VarSmooth <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmSvd2var</span>(<span class="at">u =</span> RegSmoth[[<span class="st">&quot;U.S&quot;</span>]], RegSmoth[[<span class="st">&quot;D.S&quot;</span>]])</span>
<span id="cb311-5"><a href="sec81.html#cb311-5" tabindex="-1"></a>SDVarSmoothB2 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">2</span><span class="sc">:</span>(T<span class="sc">+</span><span class="dv">1</span>), <span class="cf">function</span>(t){VarSmooth[[t]][K,K]<span class="sc">^</span><span class="fl">0.5</span>}) </span>
<span id="cb311-6"><a href="sec81.html#cb311-6" tabindex="-1"></a>LimInfB2 <span class="ot">&lt;-</span> SmoothB2 <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>SDVarSmoothB2</span>
<span id="cb311-7"><a href="sec81.html#cb311-7" tabindex="-1"></a>LimSupB2 <span class="ot">&lt;-</span> SmoothB2 <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>SDVarSmoothB2</span>
<span id="cb311-8"><a href="sec81.html#cb311-8" tabindex="-1"></a><span class="co"># Gibbs</span></span>
<span id="cb311-9"><a href="sec81.html#cb311-9" tabindex="-1"></a>MCMC <span class="ot">&lt;-</span> <span class="dv">2000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb311-10"><a href="sec81.html#cb311-10" tabindex="-1"></a>a.y <span class="ot">&lt;-</span> (SumRegLS<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>); b.y <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span>a.y; a.theta <span class="ot">&lt;-</span> (<span class="fu">max</span>(<span class="fu">diag</span>(VarBp)))<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>); b.theta <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span>a.theta </span>
<span id="cb311-11"><a href="sec81.html#cb311-11" tabindex="-1"></a>gibbsOut <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmGibbsDIG</span>(yt, <span class="at">mod =</span> dlm<span class="sc">::</span><span class="fu">dlmModReg</span>(x), <span class="at">a.y =</span> a.y, <span class="at">b.y =</span> b.y, <span class="at">a.theta =</span> a.theta, <span class="at">b.theta =</span> b.theta, <span class="at">n.sample =</span> MCMC, <span class="at">thin =</span> <span class="dv">5</span>, <span class="at">save.states =</span> <span class="cn">TRUE</span>)</span>
<span id="cb311-12"><a href="sec81.html#cb311-12" tabindex="-1"></a>B2t <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, MCMC <span class="sc">-</span> burnin, T <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb311-13"><a href="sec81.html#cb311-13" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(T<span class="sc">+</span><span class="dv">1</span>)){</span>
<span id="cb311-14"><a href="sec81.html#cb311-14" tabindex="-1"></a>    B2t[,t] <span class="ot">&lt;-</span> gibbsOut[[<span class="st">&quot;theta&quot;</span>]][t,<span class="dv">2</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>burnin)] </span>
<span id="cb311-15"><a href="sec81.html#cb311-15" tabindex="-1"></a>}</span>
<span id="cb311-16"><a href="sec81.html#cb311-16" tabindex="-1"></a>Lims <span class="ot">&lt;-</span> <span class="fu">apply</span>(B2t, <span class="dv">2</span>, <span class="cf">function</span>(x){<span class="fu">quantile</span>(x, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))})</span>
<span id="cb311-17"><a href="sec81.html#cb311-17" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(gibbsOut[[<span class="st">&quot;dV&quot;</span>]]))</span></code></pre></div>
<pre><code>## 
## Iterations = 1:2000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 2000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       27.06996        2.95137        0.06599        0.06599 
## 
## 2. Quantiles for each variable:
## 
##  2.5%   25%   50%   75% 97.5% 
## 21.94 24.92 26.81 29.02 33.26</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="sec81.html#cb313-1" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(gibbsOut[[<span class="st">&quot;dW&quot;</span>]]))</span></code></pre></div>
<pre><code>## 
## Iterations = 1:2000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 2000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##       Mean      SD Naive SE Time-series SE
## W.1 0.1272 0.07831 0.001751       0.005149
## W.2 0.1185 0.05903 0.001320       0.003612
## 
## 2. Quantiles for each variable:
## 
##        2.5%     25%    50%    75%  97.5%
## W.1 0.03489 0.07248 0.1072 0.1605 0.3397
## W.2 0.03917 0.07601 0.1052 0.1485 0.2625</code></pre>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="sec81.html#cb315-1" tabindex="-1"></a><span class="co"># Figure</span></span>
<span id="cb315-2"><a href="sec81.html#cb315-2" tabindex="-1"></a><span class="fu">require</span>(latex2exp) <span class="co"># LaTeX equations in figures</span></span>
<span id="cb315-3"><a href="sec81.html#cb315-3" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>(T<span class="sc">+</span><span class="dv">1</span>), (T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb315-4"><a href="sec81.html#cb315-4" tabindex="-1"></a>yy <span class="ot">&lt;-</span> <span class="fu">c</span>(Lims[<span class="dv">1</span>,], <span class="fu">rev</span>(Lims[<span class="dv">2</span>,]))</span>
<span id="cb315-5"><a href="sec81.html#cb315-5" tabindex="-1"></a><span class="fu">plot</span>   (xx, yy, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{t2}$&quot;</span>))</span>
<span id="cb315-6"><a href="sec81.html#cb315-6" tabindex="-1"></a><span class="fu">polygon</span>(xx, yy, <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">border =</span> <span class="st">&quot;lightblue&quot;</span>)</span>
<span id="cb315-7"><a href="sec81.html#cb315-7" tabindex="-1"></a>xxML <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>T, T<span class="sc">:</span><span class="dv">1</span>)</span>
<span id="cb315-8"><a href="sec81.html#cb315-8" tabindex="-1"></a>yyML <span class="ot">&lt;-</span> <span class="fu">c</span>(LimInfB2, <span class="fu">rev</span>(LimSupB2))</span>
<span id="cb315-9"><a href="sec81.html#cb315-9" tabindex="-1"></a><span class="fu">polygon</span>(xxML, yyML, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">border =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb315-10"><a href="sec81.html#cb315-10" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">colMeans</span>(B2t), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lw =</span> <span class="dv">2</span>)</span>
<span id="cb315-11"><a href="sec81.html#cb315-11" tabindex="-1"></a><span class="fu">lines</span>(Bt[,<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lw =</span> <span class="dv">2</span>)</span>
<span id="cb315-12"><a href="sec81.html#cb315-12" tabindex="-1"></a><span class="fu">lines</span>(SmoothB2, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lw =</span> <span class="dv">2</span>)</span>
<span id="cb315-13"><a href="sec81.html#cb315-13" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;State vector: Slope parameter&quot;</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-39-1.svg" width="672" /></p>
<p>The Figure shows the comparison between maximum likelihood (ML) and Bayesian inference. The light blue (Bayesian) and dark blue (maximum likelihood) shadows show the credible and confidence intervals at 95% for the state slope parameter (<span class="math inline">\(\beta_{t2}\)</span>). We see that the Bayesian interval encompass the ML interval. This is a reflection of the extra uncertainty of the unknown variances. The black line is the actual trajectory of <span class="math inline">\(\beta_{t2}\)</span>, the green and red lines are the <em>smoothing</em> recursions using the ML and Bayesian estimates (posterior mean), respectively.</p>
<p><strong>Example: Effects of inflation on interest rate I</strong></p>
<p>We use the dataset <em>16INTDEF.csv</em> provided by <span class="citation">Jeffrey M. Wooldridge (<a href="#ref-wooldridge2016introductory">2016</a>)</span> to study the effects of inflation on the interest rate. The specification is
<span class="math display">\[
\Delta i_t = \beta_{t1} + \beta_{t2} \Delta \text{inf}_t + \beta_{t3} \Delta \text{def}_t + \mu_t
\]</span>
and
<span class="math display">\[
\boldsymbol{\beta}_t = \boldsymbol{\beta}_{t-1} + \boldsymbol{w}_t,
\]</span>
where <span class="math inline">\(\Delta z_t = z_t - z_{t-1}\)</span> is the difference operator, <span class="math inline">\(i_t\)</span> is the three-month T-bill rate, <span class="math inline">\(\text{inf}_t\)</span> is the annual inflation rate based on the consumer price index (CPI), and <span class="math inline">\(\text{def}_t\)</span> is the federal budget deficit as a percentage of gross domestic product (GDP) from 1948 to 2003 in the USA. In addition, <span class="math inline">\(\mu_t \sim N(0, \sigma^2)\)</span> and <span class="math inline">\(\boldsymbol{w}_t \sim N(\boldsymbol{0}, \text{diag}\{\omega_1^2, \omega_2^2\})\)</span>. We assume inverse-gamma distributions for the priors of the scale parameters and set 12,000 MCMC iterations, 2,000 as burn-in, and 10 as the thinning parameter.</p>
<p>The following code shows how to perform this application. We use the variance of the recursive estimation of OLS to set the hyperparameters of the inverse-gamma distribution for the variances of <span class="math inline">\(\boldsymbol{w}_t\)</span>, and the OLS estimate of the variance of the model to set the hyperparameters of the distribution of <span class="math inline">\(\sigma^2\)</span>. Note that, as we are using the function <em>dlmGibbsDIG</em> from the <em>dlm</em> package, the hyperparameters are set in terms of precision parameters.</p>
<p>The Figure shows the posterior results of the effect of inflation on the interest rate. This is a fan chart indicating deciles from 10% to 90%. The red shaded area shows the range around the median value, and the black line represents the mean value of the state associated with the annual change in inflation. We see that the annual changes in interest rates are weakly positively related to annual changes in inflation.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="sec81.html#cb316-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb316-2"><a href="sec81.html#cb316-2" tabindex="-1"></a>DataIntRate <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/16INTDEF.csv&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb316-3"><a href="sec81.html#cb316-3" tabindex="-1"></a><span class="fu">attach</span>(DataIntRate); Xt <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">diff</span>(inf), <span class="fu">diff</span>(def))</span>
<span id="cb316-4"><a href="sec81.html#cb316-4" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">dim</span>(Xt)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">1</span>; yt <span class="ot">&lt;-</span> <span class="fu">diff</span>(i3)</span>
<span id="cb316-5"><a href="sec81.html#cb316-5" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="fu">length</span>(yt); RegLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(yt <span class="sc">~</span> Xt)</span>
<span id="cb316-6"><a href="sec81.html#cb316-6" tabindex="-1"></a>SumRegLS <span class="ot">&lt;-</span> <span class="fu">summary</span>(RegLS); SumRegLS; SumRegLS<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = yt ~ Xt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.60299 -0.82620  0.02311  0.91651  2.92275 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   0.1145     0.1874   0.611   0.5443  
## Xt1           0.1683     0.1002   1.680   0.0999 .
## Xt2          -0.1075     0.1719  -0.625   0.5349  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.293 on 45 degrees of freedom
## Multiple R-squared:  0.161,  Adjusted R-squared:  0.1237 
## F-statistic: 4.316 on 2 and 45 DF,  p-value: 0.01928</code></pre>
<pre><code>## [1] 1.671989</code></pre>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="sec81.html#cb319-1" tabindex="-1"></a><span class="co"># Recursive OLS</span></span>
<span id="cb319-2"><a href="sec81.html#cb319-2" tabindex="-1"></a>Bp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(RegLS<span class="sc">$</span>coefficients, T, K, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb319-3"><a href="sec81.html#cb319-3" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb319-4"><a href="sec81.html#cb319-4" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> S<span class="sc">:</span>T){</span>
<span id="cb319-5"><a href="sec81.html#cb319-5" tabindex="-1"></a>    RegLSt <span class="ot">&lt;-</span> <span class="fu">lm</span>(yt[<span class="dv">1</span><span class="sc">:</span>t] <span class="sc">~</span> Xt[<span class="dv">1</span><span class="sc">:</span>t,])</span>
<span id="cb319-6"><a href="sec81.html#cb319-6" tabindex="-1"></a>    Bp[t,] <span class="ot">&lt;-</span> RegLSt<span class="sc">$</span>coefficients </span>
<span id="cb319-7"><a href="sec81.html#cb319-7" tabindex="-1"></a>}</span>
<span id="cb319-8"><a href="sec81.html#cb319-8" tabindex="-1"></a>VarBp <span class="ot">&lt;-</span> <span class="fu">var</span>(Bp)</span>
<span id="cb319-9"><a href="sec81.html#cb319-9" tabindex="-1"></a><span class="co"># State space model</span></span>
<span id="cb319-10"><a href="sec81.html#cb319-10" tabindex="-1"></a>ModelReg <span class="ot">&lt;-</span> <span class="cf">function</span>(par){</span>
<span id="cb319-11"><a href="sec81.html#cb319-11" tabindex="-1"></a>    Mod <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmModReg</span>(Xt, <span class="at">dV =</span> <span class="fu">exp</span>(par[<span class="dv">1</span>]), <span class="at">dW =</span> <span class="fu">exp</span>(par[<span class="dv">2</span><span class="sc">:</span>(K<span class="sc">+</span><span class="dv">1</span>)]), <span class="at">m0 =</span> RegLS<span class="sc">$</span>coefficients,</span>
<span id="cb319-12"><a href="sec81.html#cb319-12" tabindex="-1"></a>    <span class="at">C0 =</span> <span class="fu">diag</span>(VarBp))</span>
<span id="cb319-13"><a href="sec81.html#cb319-13" tabindex="-1"></a>    <span class="fu">return</span>(Mod)</span>
<span id="cb319-14"><a href="sec81.html#cb319-14" tabindex="-1"></a>}</span>
<span id="cb319-15"><a href="sec81.html#cb319-15" tabindex="-1"></a>MCMC <span class="ot">&lt;-</span> <span class="dv">12000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">2000</span>; thin <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb319-16"><a href="sec81.html#cb319-16" tabindex="-1"></a>a.y <span class="ot">&lt;-</span> (SumRegLS<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>); b.y <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span>a.y; a.theta <span class="ot">&lt;-</span> (<span class="fu">max</span>(<span class="fu">diag</span>(VarBp)))<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>); b.theta <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span>a.theta </span>
<span id="cb319-17"><a href="sec81.html#cb319-17" tabindex="-1"></a>gibbsOut <span class="ot">&lt;-</span> dlm<span class="sc">::</span><span class="fu">dlmGibbsDIG</span>(yt, <span class="at">mod =</span> dlm<span class="sc">::</span><span class="fu">dlmModReg</span>(Xt), <span class="at">a.y =</span> a.y, <span class="at">b.y =</span> b.y, <span class="at">a.theta =</span> a.theta, <span class="at">b.theta =</span> b.theta, <span class="at">n.sample =</span> MCMC, <span class="at">thin =</span> <span class="dv">5</span>, <span class="at">save.states =</span> <span class="cn">TRUE</span>)</span>
<span id="cb319-18"><a href="sec81.html#cb319-18" tabindex="-1"></a>B2t <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, MCMC <span class="sc">-</span> burnin, T <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb319-19"><a href="sec81.html#cb319-19" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(T<span class="sc">+</span><span class="dv">1</span>)){</span>
<span id="cb319-20"><a href="sec81.html#cb319-20" tabindex="-1"></a>    B2t[,t] <span class="ot">&lt;-</span> gibbsOut[[<span class="st">&quot;theta&quot;</span>]][t,<span class="dv">2</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>burnin)] </span>
<span id="cb319-21"><a href="sec81.html#cb319-21" tabindex="-1"></a>}</span>
<span id="cb319-22"><a href="sec81.html#cb319-22" tabindex="-1"></a>dV <span class="ot">&lt;-</span> coda<span class="sc">::</span><span class="fu">mcmc</span>(gibbsOut[[<span class="st">&quot;dV&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>burnin)])</span>
<span id="cb319-23"><a href="sec81.html#cb319-23" tabindex="-1"></a>dW <span class="ot">&lt;-</span> coda<span class="sc">::</span><span class="fu">mcmc</span>(gibbsOut[[<span class="st">&quot;dW&quot;</span>]][<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>burnin),])</span>
<span id="cb319-24"><a href="sec81.html#cb319-24" tabindex="-1"></a><span class="fu">summary</span>(dV); <span class="fu">summary</span>(dW)</span></code></pre></div>
<pre><code>## 
## Iterations = 1:10000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##        2.19167        0.53702        0.00537        0.00537 
## 
## 2. Quantiles for each variable:
## 
##  2.5%   25%   50%   75% 97.5% 
## 1.401 1.814 2.104 2.476 3.517</code></pre>
<pre><code>## 
## Iterations = 1:10000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##         Mean       SD  Naive SE Time-series SE
## W.1 0.005957 0.001504 1.504e-05      1.544e-05
## W.2 0.005899 0.001411 1.411e-05      1.480e-05
## W.3 0.006022 0.001518 1.518e-05      1.515e-05
## 
## 2. Quantiles for each variable:
## 
##         2.5%      25%      50%      75%    97.5%
## W.1 0.003733 0.004895 0.005739 0.006776 0.009440
## W.2 0.003710 0.004898 0.005714 0.006686 0.009139
## W.3 0.003761 0.004953 0.005788 0.006830 0.009646</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="sec81.html#cb322-1" tabindex="-1"></a><span class="fu">plot</span>(dV); <span class="fu">plot</span>(dW)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-40-1.svg" width="672" /><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-40-2.svg" width="672" /></p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="sec81.html#cb323-1" tabindex="-1"></a><span class="fu">library</span>(fanplot); <span class="fu">library</span>(latex2exp)</span></code></pre></div>
<pre><code>## Warning: package &#39;fanplot&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="sec81.html#cb325-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(B2t)</span>
<span id="cb325-2"><a href="sec81.html#cb325-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">main=</span><span class="st">&quot;Percentiles&quot;</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">1</span>, T<span class="sc">+</span><span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>), <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">ylab =</span> <span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">beta_{t1}$&quot;</span>))</span>
<span id="cb325-3"><a href="sec81.html#cb325-3" tabindex="-1"></a><span class="fu">fan</span>(<span class="at">data =</span> df); <span class="fu">lines</span>(<span class="fu">colMeans</span>(B2t), <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lw =</span> <span class="dv">2</span>)</span>
<span id="cb325-4"><a href="sec81.html#cb325-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-40-3.svg" width="672" /></p>
<p>We can extend the <em>dynamic linear model</em> with <em>random walk states</em> to take into account time-invariant location parameters. In particular, we follow <span class="citation">De Jong and Shephard (<a href="#ref-de1995simulation">1995</a>)</span>, who propose the <em>simulation smoother</em>. This algorithm overcomes some shortcomings of the FFBS algorithm, such as slow convergence and computational overhead. We focus on the case <span class="math inline">\(M = 1\)</span>,</p>
<p><span class="math display" id="eq:DeJongSt" id="eq:DeJongObs">\[\begin{align}
y_t &amp;= \boldsymbol{z}_t^{\top} \boldsymbol{\alpha} + \boldsymbol{x}_t^{\top} \boldsymbol{\beta}_t + \boldsymbol{h}_t^{\top} \boldsymbol{\epsilon}_t, &amp; t = 1, 2, \dots, T. &amp; \text{   (Observation equation)} \tag{8.3} \\
\boldsymbol{\beta}_t &amp;= \boldsymbol{\beta}_{t-1} + \boldsymbol{H}_t \boldsymbol{\epsilon}_t, &amp; t = 1, 2, \dots, T. &amp; \text{   (States equation)} \tag{8.4}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{z}_t\)</span> and <span class="math inline">\(\boldsymbol{x}_t\)</span> are <span class="math inline">\(L\)</span>-dimensional and <span class="math inline">\(K\)</span>-dimensional vectors of regressors associated with time-invariant and time-varying parameters, respectively, <span class="math inline">\(\boldsymbol{h}_t\)</span> is a vector of dimension <span class="math inline">\(1+K\)</span>, <span class="math inline">\(\boldsymbol{H}_t\)</span> is a matrix of dimension <span class="math inline">\(K \times (1+K)\)</span>, <span class="math inline">\(\boldsymbol{\beta}_0 = \boldsymbol{0}\)</span>, and <span class="math inline">\(\boldsymbol{\epsilon}_t \sim N(\boldsymbol{0}_{1+K}, \sigma^2 \boldsymbol{I}_{1+K})\)</span>.</p>
<p>Observe that this specification encompasses Equations <a href="sec81.html#eq:eq1Obs">(8.1)</a> and <a href="sec81.html#eq:eq1State">(8.2)</a> setting <span class="math inline">\(\boldsymbol{\epsilon}_t = [\mu_t \ \boldsymbol{w}_t^{\top}]^{\top}\)</span>, <span class="math inline">\(\boldsymbol{h}_t = [1 \ 0 \ \dots \ 0]\)</span>, <span class="math inline">\(\boldsymbol{H}_t = [\boldsymbol{0}_K \ \boldsymbol{U}_{K \times K}]\)</span> such that <span class="math inline">\(\text{diag}\{\omega_1^2 \ \dots \ \omega_K^2\} = \sigma^2 \boldsymbol{U} \boldsymbol{U}^{\top}\)</span>, <span class="math inline">\(\boldsymbol{\alpha} = \boldsymbol{0}\)</span>, and <span class="math inline">\(\boldsymbol{h}_t \boldsymbol{H}_t^{\top} = \boldsymbol{0}_K\)</span>.</p>
<p>The nice idea of <span class="citation">De Jong and Shephard (<a href="#ref-de1995simulation">1995</a>)</span> was to propose an efficient algorithm to get draws from <span class="math inline">\(\boldsymbol{\eta}_t = \boldsymbol{F}_t \boldsymbol{\epsilon}_t\)</span>, where the most common choice is <span class="math inline">\(\boldsymbol{F}_t = \boldsymbol{H}_t\)</span>, which means drawing samples from the perturbations of the states, and then, recovering the states from Equation <a href="sec81.html#eq:DeJongSt">(8.4)</a> with <span class="math inline">\(\boldsymbol{\beta}_0 = \boldsymbol{0}\)</span>. <span class="citation">De Jong and Shephard (<a href="#ref-de1995simulation">1995</a>)</span> present a more general version of the <em>state space model</em> than the one presented here.</p>
<p>Using the system given by Equations <a href="sec81.html#eq:DeJongObs">(8.3)</a> and <a href="sec81.html#eq:DeJongSt">(8.4)</a>, <span class="math inline">\(\boldsymbol{F}_t=\boldsymbol{H}_t\)</span> and <span class="math inline">\(\boldsymbol{h}_t\boldsymbol{H}_t^{\top}=\boldsymbol{0}_{K}\)</span>, the <em>filtering</em> recursions are given by <span class="math inline">\(e_t=Y_t-\boldsymbol{z}_t^{\top}\boldsymbol{\alpha}-\boldsymbol{x}_t^{\top}\boldsymbol{b}_{t-1}\)</span>, <span class="math inline">\({q}_t=\boldsymbol{x}_t^{\top}\boldsymbol{B}_{t-1}\boldsymbol{x}_t+\boldsymbol{h}_t^{\top}\boldsymbol{h}_t\)</span>, <span class="math inline">\(\boldsymbol{K}_t=\boldsymbol{B}_{t-1}\boldsymbol{x}_tq_t^{-1}\)</span>, <span class="math inline">\(\boldsymbol{b}_t=\boldsymbol{b}_{t-1}+\boldsymbol{K}_t e_t\)</span>, and <span class="math inline">\(\boldsymbol{B}_t=\boldsymbol{B}_{t-1}-\boldsymbol{B}_{t-1}\boldsymbol{x}_t\boldsymbol{K}_t^{\top}+\boldsymbol{H}_t\boldsymbol{H}_t^{\top}\)</span>, where <span class="math inline">\(\boldsymbol{b}_0=\boldsymbol{0}\)</span> and <span class="math inline">\(\boldsymbol{B}_0=\boldsymbol{H}_0\boldsymbol{H}_0^{\top}\)</span>. See system 2 in <span class="citation">De Jong and Shephard (<a href="#ref-de1995simulation">1995</a>)</span> for a more general case. We should save <span class="math inline">\(e_t\)</span> (innovation vector), <span class="math inline">\(q_t\)</span> (scale innovation variance) and <span class="math inline">\(\boldsymbol{K}_t\)</span> (<em>Kalman gain</em>) from this recursion.</p>
<p>Then, setting <span class="math inline">\(\boldsymbol{r}_T=0\)</span> and <span class="math inline">\(\boldsymbol{M}_T=\boldsymbol{0}\)</span>, we run backwards from <span class="math inline">\(t=T-1, T-2, \dots, 1\)</span>, the following recursions: <span class="math inline">\(\boldsymbol{\Lambda}_{t+1}=\boldsymbol{H}_{t+1}\boldsymbol{H}_{t+1}^{\top}\)</span>, <span class="math inline">\(\boldsymbol{C}_{t+1}=\boldsymbol{\Lambda}_{t+1}-\boldsymbol{\Lambda}_{t+1}\boldsymbol{M}_{t+1}\boldsymbol{\Lambda}_{t+1}^{\top}\)</span>, <span class="math inline">\(\boldsymbol{\xi}_{t+1}\sim N(\boldsymbol{0}_K,\sigma^2\boldsymbol{C}_{t+1})\)</span>, <span class="math inline">\(\boldsymbol{L}_{t+1}=\boldsymbol{I}_K-\boldsymbol{K}_{t+1}\boldsymbol{x}_{t+1}^{\top}\)</span>, <span class="math inline">\(\boldsymbol{V}_{t+1}=\boldsymbol{\Lambda}_{t+1}\boldsymbol{M}_{t+1}\boldsymbol{L}_{t+1}\)</span>, <span class="math inline">\(\boldsymbol{r}_{t}=\boldsymbol{x}_{t+1} e_{t+1}/q_{t+1} + \boldsymbol{L}_{t+1}^{\top}\boldsymbol{r}_{t+1}-\boldsymbol{V}_{t+1}^{\top}\boldsymbol{C}_{t+1}^{-1}\boldsymbol{\xi}_{t+1}\)</span>, <span class="math inline">\(\boldsymbol{M}_{t}=\boldsymbol{x}_{t+1}\boldsymbol{x}_{t+1}^{\top}/q_{t+1}+\boldsymbol{L}_{t+1}^{\top}\boldsymbol{M}_{t+1}\boldsymbol{L}_{t+1}+\boldsymbol{V}_{t+1}^{\top}\boldsymbol{C}_{t+1}^{-1}\boldsymbol{V}_{t+1}\)</span>, and <span class="math inline">\(\boldsymbol{\eta}_{t+1}=\boldsymbol{\Lambda}_{t+1}\boldsymbol{r}_{t+1}+\boldsymbol{\xi}_{t+1}\)</span>. <span class="citation">De Jong and Shephard (<a href="#ref-de1995simulation">1995</a>)</span> show that <span class="math inline">\(\boldsymbol{\eta}=[\boldsymbol{\eta}_1^{\top} \ \dots \ \boldsymbol{\eta}_T^{\top}]^{\top}\)</span> is drawn from <span class="math inline">\(p(\boldsymbol{H}_t\boldsymbol{\epsilon}_t\mid y_t,\boldsymbol{x}_t,\boldsymbol{z}_t,\boldsymbol{h}_t,\boldsymbol{H}_t,\boldsymbol{\alpha},\sigma^2, t=1,2,\dots,T)\)</span>. Thus, we can recover <span class="math inline">\(\boldsymbol{\beta}_t\)</span> using <a href="sec81.html#eq:DeJongSt">(8.4)</a> and <span class="math inline">\(\boldsymbol{\beta}_0=\boldsymbol{0}_K\)</span>.</p>
<p>We assume in the model given by Equations <a href="sec81.html#eq:DeJongObs">(8.3)</a> and <a href="sec81.html#eq:DeJongSt">(8.4)</a> that <span class="math inline">\(\boldsymbol{h}_t=[1 \ 0 \ \dots \ 0]^{ \top}\)</span> and <span class="math inline">\(\boldsymbol{H}_t=[\boldsymbol{0}_K \ \text{diag}\left\{1/\tau_1\dots1/\tau_K\right\}]\)</span>, and then perform Bayesian inference assuming independent priors, that is, <span class="math inline">\(\pi(\boldsymbol{\beta}_0,\boldsymbol{\alpha},\sigma^2,\boldsymbol{\tau})=\pi(\boldsymbol{\beta}_0)\pi(\boldsymbol{\alpha})\pi(\sigma^2)\prod_{k=1}^K\pi(\tau_k^2)\)</span> where <span class="math inline">\(\sigma^2\sim IG(\alpha_0/2,\delta_0/2)\)</span>, <span class="math inline">\(\tau_k^2\sim G(v_{0}/2,v_{0}/2)\)</span>, <span class="math inline">\(k=1,\dots,K\)</span>, <span class="math inline">\(\boldsymbol{\alpha}\sim N(\boldsymbol{a}_0,\boldsymbol{A}_0)\)</span> and <span class="math inline">\(\boldsymbol{\beta}_0\sim N(\boldsymbol{b}_0,\boldsymbol{B}_0)\)</span>. The conditional posterior distributions are <span class="math inline">\(\sigma^2\mid \boldsymbol{y},\boldsymbol{X},\boldsymbol{Z},\boldsymbol{\beta}_{0:T},\boldsymbol{\alpha},\boldsymbol{\tau}\sim IG(\alpha_{n}/2,\delta_n/2)\)</span>, where <span class="math inline">\(\delta_n=\sum_{t=1}^T\left[(\boldsymbol{\beta}_t-\boldsymbol{\beta}_{t-1})^{\top}\boldsymbol{\Psi}(\boldsymbol{\beta}_t-\boldsymbol{\beta}_{t-1})+(y_t-\boldsymbol{z}_t^{\top}\boldsymbol{\alpha}-\boldsymbol{x}_t^{\top}\boldsymbol{\beta}_t)^{\top}(y_t-\boldsymbol{z}_t^{\top}\boldsymbol{\alpha}-\boldsymbol{x}_t^{\top}\boldsymbol{\beta}_t)\right]+\delta_0\)</span> and <span class="math inline">\(\alpha_{n}=T(K+1)+\alpha_0\)</span>, <span class="math inline">\(\boldsymbol{\tau}=[\tau_1 \ \dots \ \tau_K]\)</span>, <span class="math inline">\(\boldsymbol{\Psi}=\text{diag}\left\{\tau_1^2,\dots,\tau_K^2\right\}\)</span>, and <span class="math inline">\(\tau_k^2\mid \boldsymbol{y},\boldsymbol{X},\boldsymbol{Z},\boldsymbol{\beta}_{0:T},\sigma^2\sim G(v_{1n}/2,v_{2kn}/2)\)</span>, where <span class="math inline">\(v_{1n}=T+v_{0}\)</span> and <span class="math inline">\(v_{2kn}=\sigma^{-2}\sum_{t=1}^T(\boldsymbol{\beta}_{t,k}-\boldsymbol{\beta}_{t-1,k})^2+v_{0}\)</span>, and <span class="math inline">\(\boldsymbol{\alpha}\mid \boldsymbol{y},\boldsymbol{X},\boldsymbol{Z},\sigma^2,\boldsymbol{\beta}_{1:T},\boldsymbol{\tau}\sim N(\boldsymbol{a}_n,\boldsymbol{A}_n)\)</span>, where <span class="math inline">\(\boldsymbol{A}_n=(\boldsymbol{A}_0^{-1}+\sigma^{-2}\sum_{t=1}^T\boldsymbol{z}_t\boldsymbol{z}_t^{\top})^{-1}\)</span> and <span class="math inline">\(\boldsymbol{a}_n=\boldsymbol{A}_n(\boldsymbol{A}_0^{-1}\boldsymbol{a}_0+\sigma^{-2}\sum_{t=1}^T\boldsymbol{z}_t(y_t-\boldsymbol{x}_t^{\top}\boldsymbol{\beta}_t))\)</span>. The vector of the dependent variable is <span class="math inline">\(\boldsymbol{y}\)</span>, and all regressors are in <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Z}\)</span>.</p>
<p>We can see that all the previous posterior distributions are conditional on the state vector <span class="math inline">\(\boldsymbol{\beta}_{0:T}\)</span>, which can be sampled using the <em>simulation smoother</em> algorithm, conditional on draws of the time-invariant parameters. Thus, the <em>state space model</em> provides an excellent illustration of the modular nature of the Bayesian framework, where performing inference on more complex models often simply involves adding new blocks to an MCMC algorithm. This means we can break down a complex inferential problem into smaller, more manageable parts, which is a “divide and conquer” approach. This is possible due to the structure of the conditional posterior distributions. Exercise 3 asks you to perform a simulation of the model given by Equations <a href="sec81.html#eq:DeJongObs">(8.3)</a> and <a href="sec81.html#eq:DeJongSt">(8.4)</a>, and to program the MCMC algorithm, including the <em>simulation smoother</em>.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-carter1994gibbs" class="csl-entry">
Carter, Chris K, and Robert Kohn. 1994. <span>“On Gibbs Sampling for State Space Models.”</span> <em>Biometrika</em> 81 (3): 541–53.
</div>
<div id="ref-de1995simulation" class="csl-entry">
De Jong, Piet, and Neil Shephard. 1995. <span>“The Simulation Smoother for Time Series Models.”</span> <em>Biometrika</em> 82 (2): 339–50.
</div>
<div id="ref-fruhwirth1994data" class="csl-entry">
Frühwirth-Schnatter, Sylvia. 1994. <span>“Data Augmentation and Dynamic Linear Models.”</span> <em>Journal of Time Series Analysis</em> 15 (2): 183–202.
</div>
<div id="ref-koop2003bayesian" class="csl-entry">
Koop, Gary M. 2003. <em>Bayesian Econometrics</em>. John Wiley &amp; Sons Inc.
</div>
<div id="ref-petris2009dynamic" class="csl-entry">
Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. <span>“Dynamic Linear Models.”</span> In <em>Dynamic Linear Models with r</em>, 31–84. Springer.
</div>
<div id="ref-shephard1994partial" class="csl-entry">
Shephard, Neil. 1994. <span>“Partial Non-Gaussian State Space.”</span> <em>Biometrika</em> 81 (1): 115–31.
</div>
<div id="ref-shumway2017time" class="csl-entry">
Shumway, Robert H., and David S. Stoffer. 2017. <em>Time Series Analysis and Its Applications: With r Examples</em>. 4th ed. Cham, Switzerland: Springer. <a href="https://doi.org/10.1007/978-3-319-52452-8">https://doi.org/10.1007/978-3-319-52452-8</a>.
</div>
<div id="ref-wooldridge2016introductory" class="csl-entry">
———. 2016. <em>Introductory Econometrics: A Modern Approach</em>. 6th ed. Boston, MA: Cengage Learning.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Chap8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec82.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/08-Timeseries.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
