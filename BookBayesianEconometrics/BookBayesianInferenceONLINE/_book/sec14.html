<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-03-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12.html"/>
<link rel="next" href="summary.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec14" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Bayesian reports: Decision theory under uncertainty<a href="sec14.html#sec14" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Bayesian framework allows reporting the full posterior distributions. However, some situations require reporting a specific value of the posterior distribution (point estimate), an informative interval (set), point or interval predictions, and/or selecting a specific model. Decision theory offers an elegant framework to make decisions regarding the optimal posterior values to report <span class="citation">(<a href="#ref-berger2013statistical">Berger 2013</a>)</span>.</p>
<p>The starting point is a <em>loss function</em>, which is a non-negative real-valued function whose arguments are the unknown <em>state of nature</em> (<span class="math inline">\(\mathbf{\Theta}\)</span>), and a set of <em>actions</em> to be taken (<span class="math inline">\(\mathcal{A}\)</span>), that is,
<span class="math display">\[\begin{equation*}
L(\mathbf{\theta}, a):\mathbf{\Theta}\times \mathcal{A}\rightarrow \mathcal{R}^+.
\end{equation*}\]</span></p>
<p>This function is a mathematical representation of the loss incurred from making mistakes. In particular, selecting action <span class="math inline">\(a\in\mathcal{A}\)</span> when <span class="math inline">\(\mathbf{\theta}\in\mathbf{\Theta}\)</span> is the true state. In our case, the unknown state of nature can refer to parameters, functions of them, future or unknown realizations, models, etc.</p>
<p>From a Bayesian perspective, we should choose the action that minimizes the posterior expected loss (<span class="math inline">\(a^*(\mathbf{y})\)</span>), that is, the <em>posterior risk function</em> (<span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)\mid \mathbf{y}]\)</span>),
<span class="math display">\[\begin{equation*}
a^*(\mathbf{y})=\underset{a \in \mathcal{A}}{\mathrm{argmin}} \  \mathbb{E}[L(\mathbf{\theta}, a)\mid \mathbf{y}],
\end{equation*}\]</span>
where <span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)\mid \mathbf{y}] = \int_{\mathbf{\Theta}} L(\mathbf{\theta}, a)\pi(\mathbf{\theta}\mid \mathbf{y})d\mathbf{\theta}\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<p>Different loss functions imply different optimal decisions. We illustrate this assuming <span class="math inline">\(\theta \in \mathcal{R}\)</span>.</p>
<ul>
<li>The quadratic loss function, <span class="math inline">\(L(\theta,a)=[\theta-a]^2\)</span>, gives as the optimal decision the posterior mean, <span class="math inline">\(a^*(\mathbf{y})=\mathbb{E}[\theta \mid \mathbf{y}]\)</span>, that is:</li>
</ul>
<p><span class="math display">\[\begin{equation*}
\mathbb{E}[\theta \mid \mathbf{y}] = \underset{a \in \mathcal{A}}{\mathrm{argmin}} \  \int_{\Theta} [\theta - a]^2 \pi(\theta \mid \mathbf{y}) \, d\theta.
\end{equation*}\]</span></p>
<p>To obtain this result, let’s use the first-order condition, differentiate the risk function with respect to <span class="math inline">\(a\)</span>, interchange the differential and integral order, and set the result equal to zero:</p>
<p><span class="math display">\[
-2 \int_{\Theta} [\theta - a^*] \pi(\theta \mid \mathbf{y}) \, d\theta = 0.
\]</span></p>
<p>This implies that</p>
<p><span class="math display">\[
a^* \int_{\Theta} \pi(\theta \mid \mathbf{y}) \, d\theta = a^*(\mathbf{y}) = \int_{\Theta} \theta \pi(\theta \mid \mathbf{y}) \, d\theta = \mathbb{E}[\theta \mid \mathbf{y}],
\]</span></p>
<p>that is, the posterior mean is the Bayesian optimal action. This means that we should report the posterior mean as a point estimate of <span class="math inline">\(\theta\)</span> when facing the quadratic loss function.</p>
<ul>
<li>The generalized quadratic loss function, <span class="math inline">\(L(\theta,a) = w(\theta) [\theta - a]^2\)</span>, where <span class="math inline">\(w(\theta) &gt; 0\)</span> is a weighting function, gives as the optimal decision rule the weighted mean. We should follow the same steps as the previous result to obtain</li>
</ul>
<p><span class="math display">\[
a^*(\mathbf{y}) = \frac{\mathbb{E}[w(\theta) \times \theta \mid \mathbf{y}]}{\mathbb{E}[w(\theta) \mid \mathbf{y}]}.
\]</span></p>
<p>Observe that the weighted average is driven by the weighting function <span class="math inline">\(w(\theta)\)</span>.</p>
<ul>
<li><p>The absolute error loss function, <span class="math inline">\(L(\theta,a) = |\theta - a|\)</span>, gives as the optimal action the posterior median (Exercise 5).</p></li>
<li><p>The generalized absolute error function,</p></li>
</ul>
<p><span class="math display">\[
L(\theta,a) =
\begin{cases}
K_0 (\theta - a), &amp; \text{if } \theta - a \geq 0, \\
K_1 (a - \theta), &amp; \text{if } \theta - a &lt; 0,
\end{cases} \quad K_0, K_1 &gt; 0,
\]</span></p>
<p>implies the following risk function:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[L(\theta, a) \mid \mathbf{y}] &amp;= \int_{-\infty}^{a} K_1(a - \theta) \pi(\theta \mid \mathbf{y}) \, d\theta + \int_{a}^{\infty} K_0 (\theta - a) \pi(\theta \mid \mathbf{y}) \, d\theta.
\end{align*}\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(a\)</span>, interchanging differentials and integrals, and equating to zero, we get:</p>
<p><span class="math display">\[\begin{align*}
K_1 \int_{-\infty}^{a^*} \pi(\theta \mid \mathbf{y}) \, d\theta - K_0 \int_{a^*}^{\infty} \pi(\theta \mid \mathbf{y}) \, d\theta &amp;= 0.
\end{align*}\]</span></p>
<p>Thus, we have</p>
<p><span class="math display">\[
\int_{-\infty}^{a^*} \pi(\theta \mid \mathbf{y}) \, d\theta = \frac{K_0}{K_0 + K_1},
\]</span></p>
<p>that is, any <span class="math inline">\(\frac{K_0}{K_0 + K_1}\)</span>-percentile of <span class="math inline">\(\pi(\theta \mid \mathbf{y})\)</span> is an optimal Bayesian estimate of <span class="math inline">\(\theta\)</span>.</p>
<p>We can also use decision theory under uncertainty in hypothesis testing. In particular, testing <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> versus <span class="math inline">\(H_1: \theta \in \Theta_1\)</span>, where <span class="math inline">\(\Theta = \Theta_0 \cup \Theta_1\)</span> and <span class="math inline">\(\emptyset = \Theta_0 \cap \Theta_1\)</span>, there are two actions of interest, <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span>, where <span class="math inline">\(a_j\)</span> denotes not rejecting <span class="math inline">\(H_j\)</span>, for <span class="math inline">\(j = \{0,1\}\)</span>.</p>
<p>Given the <span class="math inline">\(0-K_j\)</span> loss function:</p>
<p><span class="math display">\[\begin{equation*}
L(\theta,a_j) =
\begin{cases}
0, &amp; \text{if } \theta \in \Theta_j, \\
K_j, &amp; \text{if } \theta \in \Theta_i, j \neq i,
\end{cases}
\end{equation*}\]</span></p>
<p>where there is no loss if the right decision is made, for instance, not rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(\theta \in \Theta_0\)</span>, and the loss is <span class="math inline">\(K_j\)</span> when an error is made. For example, a type I error occurs when rejecting the null hypothesis (<span class="math inline">\(H_0\)</span>) when it is true (<span class="math inline">\(\theta \in \Theta_0\)</span>), which results in a loss of <span class="math inline">\(K_1\)</span> due to choosing action <span class="math inline">\(a_1\)</span>, not rejecting <span class="math inline">\(H_1\)</span>.</p>
<p>The posterior expected loss associated with decision <span class="math inline">\(a_j\)</span>, i.e., not rejecting <span class="math inline">\(H_j\)</span>, is:</p>
<p><span class="math display">\[
\mathbb{E}[L(\theta,a_j) \mid \mathbf{y}] = 0 \times P(\Theta_j \mid \mathbf{y}) + K_j P(\Theta_i \mid \mathbf{y}) = K_j P(\Theta_i \mid \mathbf{y}), \quad j \neq i.
\]</span></p>
<p>Therefore, the Bayes optimal decision is the one that minimizes the posterior expected loss. That is, the null hypothesis is rejected (<span class="math inline">\(a_1\)</span> is not rejected) when</p>
<p><span class="math display">\[
K_0 P(\Theta_1 \mid \mathbf{y}) &gt; K_1 P(\Theta_0 \mid \mathbf{y}).
\]</span></p>
<p>Given our framework, <span class="math inline">\(\Theta = \Theta_0 \cup \Theta_1\)</span> and <span class="math inline">\(\emptyset = \Theta_0 \cap \Theta_1\)</span>, we have <span class="math inline">\(P(\Theta_0 \mid \mathbf{y}) = 1 - P(\Theta_1 \mid \mathbf{y})\)</span>. As a result, the rejection region of the Bayesian test is:</p>
<p><span class="math display">\[
R = \left\{ \mathbf{y} : P(\Theta_1 \mid \mathbf{y}) &gt; \frac{K_1}{K_1 + K_0} \right\}.
\]</span></p>
<p>Decision theory also helps to construct interval (region) estimates. Let <span class="math inline">\(\Theta_{C(\mathbf{y})} \subset \Theta\)</span> be a <em>credible set</em> for <span class="math inline">\(\theta\)</span>, and let the loss function be defined as:</p>
<p><span class="math display">\[
L(\theta, \Theta_{C(\mathbf{y})}) = 1 - \mathbf{1}\left\{\theta \in \Theta_{C(\mathbf{y})}\right\},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{1}\left\{\theta \in \Theta_{C(\mathbf{y})}\right\} =
\begin{cases}
1, &amp; \text{if } \theta \in \Theta_{C(\mathbf{y})}, \\
0, &amp; \text{if } \theta \notin \Theta_{C(\mathbf{y})}.
\end{cases}
\]</span></p>
<p>Thus, the loss function becomes:</p>
<p><span class="math display">\[
L(\theta, \Theta_{C(\mathbf{y})}) =
\begin{cases}
0, &amp; \text{if } \theta \in \Theta_{C(\mathbf{y})}, \\
1, &amp; \text{if } \theta \notin \Theta_{C(\mathbf{y})}.
\end{cases}
\]</span></p>
<p>This is a 0-1 loss function, which equals zero when <span class="math inline">\(\theta \in \Theta_{C(\mathbf{y})}\)</span> and equals one when <span class="math inline">\(\theta \notin \Theta_{C(\mathbf{y})}\)</span>. Consequently, the risk function is:</p>
<p><span class="math display">\[
1 - P(\theta \in \Theta_{C(\mathbf{y})}).
\]</span></p>
<p>Given a <em>measure of credibility</em> <span class="math inline">\(\alpha(\mathbf{y})\)</span> that defines the level of trust that <span class="math inline">\(\theta \in \Theta_{C(\mathbf{y})}\)</span>, we can measure the accuracy of the report by the loss function:</p>
<p><span class="math display">\[
L(\theta, \alpha(\mathbf{y})) = \left[\mathbf{1}\left\{\theta \in \Theta_{C(\mathbf{y})}\right\} - \alpha(\mathbf{y})\right]^2.
\]</span></p>
<p>This loss function could be used to suggest a choice of the report <span class="math inline">\(\alpha(\mathbf{y})\)</span>. Given that this is a quadratic loss function, the optimal action is the posterior mean, that is,</p>
<p><span class="math display">\[
\mathbb{E}[\mathbf{1}\left\{\theta \in \Theta_{C(\mathbf{y})}\right\} \mid \mathbf{y}] = P(\theta \in \Theta_{C(\mathbf{y})} \mid \mathbf{y}).
\]</span></p>
<p>This probability can be calculated given the posterior distribution as</p>
<p><span class="math display">\[
P(\theta \in \Theta_{C(\mathbf{y})} \mid \mathbf{y}) = \int_{\Theta_{C(\mathbf{y})}} \pi(\theta \mid \mathbf{y}) \, d\theta.
\]</span></p>
<p>This represents a measure of the belief that <span class="math inline">\(\theta \in \Theta_{C(\mathbf{y})}\)</span> given the prior beliefs and sample information.</p>
<p>The set <span class="math inline">\(\Theta_{C(\mathbf{y})} \subset \Theta\)</span> is a <span class="math inline">\(100(1 - \alpha)\%\)</span> credible set with respect to <span class="math inline">\(\pi(\theta \mid \mathbf{y})\)</span> if</p>
<p><span class="math display">\[
P(\theta \in \Theta_{C(\mathbf{y})} \mid \mathbf{y}) = \int_{\Theta_{C(\mathbf{y})}} \pi(\theta \mid \mathbf{y}) \, d\theta = 1 - \alpha.
\]</span></p>
<p>Two alternatives for reporting credible sets are the <em>symmetric credible set</em> and the <em>highest posterior density set</em> (HPD). The former is based on the <span class="math inline">\(\frac{\alpha}{2}\%\)</span> and <span class="math inline">\((1 - \frac{\alpha}{2})\%\)</span> percentiles of the posterior distribution, and the latter is a <span class="math inline">\(100(1 - \alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span> with the property that it has the smallest distance compared to any other <span class="math inline">\(100(1 - \alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span> based on the posterior distribution. Specifically,</p>
<p><span class="math display">\[
C(\mathbf{y}) = \left\{ \theta : \pi(\theta \mid \mathbf{y}) \geq k(\alpha) \right\},
\]</span></p>
<p>where <span class="math inline">\(k(\alpha)\)</span> is the largest number such that</p>
<p><span class="math display">\[
\int_{\theta : \pi(\theta \mid \mathbf{y}) \geq k(\alpha)} \pi(\theta \mid \mathbf{y}) d\theta = 1 - \alpha.
\]</span></p>
<p>The HPD set can be a collection of disjoint intervals when working with multimodal posterior densities. Additionally, HPD sets have the limitation of not necessarily being invariant under transformations.</p>
<p>Decision theory can also be used to perform prediction (point, sets, or probabilistic). Suppose that there is a loss function <span class="math inline">\(L(Y_0, a)\)</span> involving the prediction of <span class="math inline">\(Y_0\)</span>. Then, the expected loss is</p>
<p><span class="math display">\[
\mathbb{E}_{Y_0}[L(Y_0, a)] = \int_{\mathcal{Y}_0} L(y_0, a) \pi(y_0 \mid \mathbf{y}) \, dy_0,
\]</span></p>
<p>where <span class="math inline">\(\pi(y_0 \mid \mathbf{y})\)</span> is the predictive density function. Thus, we make an optimal choice for prediction that minimizes the risk function given a specific loss function.</p>
<p>Although Bayesian Model Averaging (BMA) allows for incorporating model uncertainty in a regression framework, sometimes it is desirable to select just one model. A compelling alternative is to choose the model with the highest posterior model probability. This model is the best alternative for prediction in the case of a 0-1 loss function <span class="citation">(<a href="#ref-Clyde2004">Clyde and George 2004</a>)</span>.</p>
<p><strong>Example: Health insurance continues</strong></p>
<p>We show some optimal rules in the health insurance example, specifically the best point estimates of <span class="math inline">\(\lambda\)</span> under the quadratic, absolute, and generalized absolute loss functions. For the generalized absolute loss function, we assume that underestimating <span class="math inline">\(\lambda\)</span> is twice as costly as overestimating it, i.e., <span class="math inline">\(K_0 = 2\)</span> and <span class="math inline">\(K_1 = 1\)</span>.</p>
<p>Given that the posterior distribution of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(G(\alpha_0 + \sum_{i=1}^N y_i, \frac{\beta_0}{\beta_0 N + 1})\)</span>, and using the hyperparameters from empirical Bayes, we obtain the following optimal point estimates:</p>
<ul>
<li>The posterior mean: <span class="math inline">\(\mathbb{E}[\lambda \mid \mathbf{y}] = \alpha_n \beta_n = 1.2\)</span>,</li>
<li>The posterior median: 1.19,</li>
<li>The 2/3-th quantile: 1.26.</li>
</ul>
<p>These are the optimal point estimates for the quadratic, absolute, and generalized absolute loss functions, respectively.</p>
<p>In addition, we test the null hypothesis <span class="math inline">\(H_0: \lambda \in [0, 1)\)</span> versus the alternative hypothesis <span class="math inline">\(H_1: \lambda \in [1, \infty)\)</span>, setting <span class="math inline">\(K_0 = K_1 = 1\)</span>. We should reject the null hypothesis since <span class="math inline">\(P(\lambda \in [0, 1)) = 0.9 &gt; \frac{K_1}{K_0 + K_1} = 0.5\)</span>.</p>
<p>The 95% symmetric credible interval is <span class="math inline">\((0.91, 1.53)\)</span>, and the highest posterior density (HPD) interval is <span class="math inline">\((0.90, 1.51)\)</span>. Finally, the optimal point prediction under the quadratic loss function is 1.2, which is the mean value of the posterior predictive distribution. The optimal model, assuming a 0-1 loss function, is the model using the hyperparameters from the empirical Bayes procedure, since the posterior model probability of this model is approximately 1, whereas the posterior model probability of the model using vague hyperparameters is approximately 0.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="sec14.html#cb40-1" tabindex="-1"></a>an <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">+</span> a0EB </span>
<span id="cb40-2"><a href="sec14.html#cb40-2" tabindex="-1"></a><span class="co"># Posterior shape parameter</span></span>
<span id="cb40-3"><a href="sec14.html#cb40-3" tabindex="-1"></a>bn <span class="ot">&lt;-</span> b0EB <span class="sc">/</span> (N<span class="sc">*</span>b0EB <span class="sc">+</span> <span class="dv">1</span>) </span>
<span id="cb40-4"><a href="sec14.html#cb40-4" tabindex="-1"></a><span class="co"># Posterior scale parameter</span></span>
<span id="cb40-5"><a href="sec14.html#cb40-5" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000000</span> </span>
<span id="cb40-6"><a href="sec14.html#cb40-6" tabindex="-1"></a><span class="co"># Number of posterior draws</span></span>
<span id="cb40-7"><a href="sec14.html#cb40-7" tabindex="-1"></a>Draws <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1000000</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) </span>
<span id="cb40-8"><a href="sec14.html#cb40-8" tabindex="-1"></a><span class="co"># Posterior draws</span></span>
<span id="cb40-9"><a href="sec14.html#cb40-9" tabindex="-1"></a><span class="do">###### Point estimation ########</span></span>
<span id="cb40-10"><a href="sec14.html#cb40-10" tabindex="-1"></a>OptQua <span class="ot">&lt;-</span> an<span class="sc">*</span>bn </span>
<span id="cb40-11"><a href="sec14.html#cb40-11" tabindex="-1"></a><span class="co"># Mean: Optimal choice quadratic loss function</span></span>
<span id="cb40-12"><a href="sec14.html#cb40-12" tabindex="-1"></a>OptQua</span></code></pre></div>
<pre><code>## [1] 1.200952</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="sec14.html#cb42-1" tabindex="-1"></a>OptAbs <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.5</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) </span>
<span id="cb42-2"><a href="sec14.html#cb42-2" tabindex="-1"></a><span class="co"># Median: Optimal choice absolute loss function</span></span>
<span id="cb42-3"><a href="sec14.html#cb42-3" tabindex="-1"></a>OptAbs</span></code></pre></div>
<pre><code>## [1] 1.194034</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="sec14.html#cb44-1" tabindex="-1"></a><span class="co"># Setting K0 = 2 and K1 = 1, that is, to underestimate lambda is twice as costly as to overestimate it.</span></span>
<span id="cb44-2"><a href="sec14.html#cb44-2" tabindex="-1"></a>K0 <span class="ot">&lt;-</span> <span class="dv">2</span>; K1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb44-3"><a href="sec14.html#cb44-3" tabindex="-1"></a>OptGenAbs <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Draws, K0<span class="sc">/</span>(K0 <span class="sc">+</span> K1)) </span>
<span id="cb44-4"><a href="sec14.html#cb44-4" tabindex="-1"></a><span class="co"># Median: Optimal choice generalized absolute loss function</span></span>
<span id="cb44-5"><a href="sec14.html#cb44-5" tabindex="-1"></a>OptGenAbs</span></code></pre></div>
<pre><code>## 66.66667% 
##  1.263182</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="sec14.html#cb46-1" tabindex="-1"></a><span class="do">###### Hypothesis test ########</span></span>
<span id="cb46-2"><a href="sec14.html#cb46-2" tabindex="-1"></a><span class="co"># H0: lambda in [0,1) vs H1: lambda in [1, Inf]</span></span>
<span id="cb46-3"><a href="sec14.html#cb46-3" tabindex="-1"></a>K0 <span class="ot">&lt;-</span> <span class="dv">1</span>; K1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb46-4"><a href="sec14.html#cb46-4" tabindex="-1"></a>ProbH0 <span class="ot">&lt;-</span> <span class="fu">pgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) </span>
<span id="cb46-5"><a href="sec14.html#cb46-5" tabindex="-1"></a>ProbH0 <span class="co"># Posterior  probability H0</span></span></code></pre></div>
<pre><code>## [1] 0.09569011</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="sec14.html#cb48-1" tabindex="-1"></a>ProbH1 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span>ProbH0</span>
<span id="cb48-2"><a href="sec14.html#cb48-2" tabindex="-1"></a>ProbH1 <span class="co"># Posterior  probability H1</span></span></code></pre></div>
<pre><code>## [1] 0.9043099</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="sec14.html#cb50-1" tabindex="-1"></a><span class="co"># We should reject H0 given ProbH1 &gt; K1 / (K0 + K1) </span></span>
<span id="cb50-2"><a href="sec14.html#cb50-2" tabindex="-1"></a><span class="do">###### Credible intervals ########</span></span>
<span id="cb50-3"><a href="sec14.html#cb50-3" tabindex="-1"></a>LimInf <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.025</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Lower bound</span></span>
<span id="cb50-4"><a href="sec14.html#cb50-4" tabindex="-1"></a>LimInf</span></code></pre></div>
<pre><code>## [1] 0.9114851</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="sec14.html#cb52-1" tabindex="-1"></a>LimSup <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.975</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Upper bound</span></span>
<span id="cb52-2"><a href="sec14.html#cb52-2" tabindex="-1"></a>LimSup</span></code></pre></div>
<pre><code>## [1] 1.529724</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="sec14.html#cb54-1" tabindex="-1"></a>HDI <span class="ot">&lt;-</span> HDInterval<span class="sc">::</span><span class="fu">hdi</span>(Draws, <span class="at">credMass =</span> <span class="fl">0.95</span>) <span class="co"># Highest posterior density credible interval</span></span>
<span id="cb54-2"><a href="sec14.html#cb54-2" tabindex="-1"></a>HDI</span></code></pre></div>
<pre><code>##     lower     upper 
## 0.9007934 1.5163109 
## attr(,&quot;credMass&quot;)
## [1] 0.95</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="sec14.html#cb56-1" tabindex="-1"></a><span class="do">###### Predictive optimal choices ########</span></span>
<span id="cb56-2"><a href="sec14.html#cb56-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> bn <span class="sc">/</span> (bn <span class="sc">+</span> <span class="dv">1</span>) <span class="co"># Probability negative binomial density</span></span>
<span id="cb56-3"><a href="sec14.html#cb56-3" tabindex="-1"></a>OptPred <span class="ot">&lt;-</span> p<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">*</span>an <span class="co"># Optimal point prediction given a quadratic loss function in prediction</span></span>
<span id="cb56-4"><a href="sec14.html#cb56-4" tabindex="-1"></a>OptPred</span></code></pre></div>
<pre><code>## [1] 1.200952</code></pre>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berger2013statistical" class="csl-entry">
Berger, James O. 2013. <em>Statistical Decision Theory and Bayesian Analysis</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-Chernozhukov2003" class="csl-entry">
Chernozhukov, V., and H. Hong. 2003. <span>“An <span>MCMC</span> Approach to Classical Estimation.”</span> <em>Journal of Econometrics</em> 115: 293–346.
</div>
<div id="ref-Clyde2004" class="csl-entry">
Clyde, M., and E. George. 2004. <span>“Model Uncertatinty.”</span> <em>Statistical Science</em> 19 (1): 81–94.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p><span class="citation">Chernozhukov and Hong (<a href="#ref-Chernozhukov2003">2003</a>)</span> propose Laplace-type estimators (LTE) based on the <em>quasi-posterior</em>, <span class="math inline">\(p(\mathbf{\theta})=\frac{\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})}{\int_{\mathbf{\Theta}}\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})d\theta}\)</span>, where <span class="math inline">\(L_n(\mathbf{\theta})\)</span> is not necessarily a log-likelihood function. The LTE minimizes the <em>quasi-posterior risk</em>.<a href="sec14.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
