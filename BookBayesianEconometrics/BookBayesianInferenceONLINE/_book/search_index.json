[["Chap7.html", "Chapter 7 Multivariate regression", " Chapter 7 Multivariate regression We describe how to perform Bayesian inference in multivariate response models, including multivariate regression, seemingly unrelated regression, instrumental variables, and the multivariate probit model. In particular, we present the posterior distributions of the parameters and demonstrate several applications and simulations. Additionally, we show how to perform inference in these models using three levels of programming skills: GUI, packages, and programming the algorithms from scratch. Finally, we provide some mathematical and computational exercises. Remember that we can run our GUI typing shiny::runGitHub(\"besmarter/BSTApp\", launch.browser=T) in the R console or any R code editor and execute it. However, users should see Chapter 5 for details. "],["sec71.html", "7.1 Multivariate regression", " 7.1 Multivariate regression A complete presentation of this model is given in Section 3.4. We show here the setting, and the posterior distributions for facility in exposition. In particular, there are \\(M\\) multiply dependent variables which share the same set of regressors, and their stochastic errors are contemporaneously correlated. In particular, \\(\\boldsymbol{Y} = \\left[ \\boldsymbol{y_{1}} \\ \\boldsymbol{y_{2}} \\ \\ldots \\ \\boldsymbol{y_{M}} \\right]\\) is an \\(N \\times M\\) matrix that is generated by \\(\\boldsymbol{Y} = \\boldsymbol{X} \\boldsymbol{B} + \\boldsymbol{U}\\) where \\(\\boldsymbol{X}\\) is an \\(N \\times K\\) matrix of regressors, \\(\\boldsymbol{B} = \\left[ \\boldsymbol{\\beta}_{1} \\ \\boldsymbol{\\beta}_{2} \\ldots \\boldsymbol{\\beta}_{M} \\right]\\) is a \\(K \\times M\\) matrix of parameters, and \\(\\boldsymbol{U} = \\left[ \\boldsymbol{\\mu}_{1} \\ \\boldsymbol{\\mu}_{2} \\ldots \\boldsymbol{\\mu}_{M} \\right]\\) is a matrix of stochastic random errors such that \\(\\boldsymbol{\\mu}_i \\sim N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\), for \\(i = 1, 2, \\dots, N\\), each row of \\(\\boldsymbol{U}\\). The prior is given by \\(\\boldsymbol{B} \\mid \\boldsymbol{\\Sigma} \\sim N(\\boldsymbol{B}_0, \\boldsymbol{V}_0, \\boldsymbol{\\Sigma})\\) and \\(\\boldsymbol{\\Sigma} \\sim IW(\\boldsymbol{\\Psi}_0, \\alpha_0)\\). Therefore, the conditional posterior distributions are: \\[ \\boldsymbol{B} \\mid \\boldsymbol{\\Sigma}, \\boldsymbol{Y}, \\boldsymbol{X} \\sim N(\\boldsymbol{B}_n, \\boldsymbol{V}_n, \\boldsymbol{\\Sigma}), \\] \\[ \\boldsymbol{\\Sigma} \\mid \\boldsymbol{Y}, \\boldsymbol{X} \\sim IW(\\boldsymbol{\\Psi}_n, \\alpha_n), \\] where \\[ \\boldsymbol{V}_n = (\\boldsymbol{X}^{\\top} \\boldsymbol{X} + \\boldsymbol{V}_0^{-1})^{-1}, \\quad \\boldsymbol{B}_n = \\boldsymbol{V}_n (\\boldsymbol{V}_0^{-1} \\boldsymbol{B}_0 + \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\hat{\\boldsymbol{B}}), \\] \\[ \\hat{\\boldsymbol{B}} = (\\boldsymbol{X}^{\\top} \\boldsymbol{X})^{-1} \\boldsymbol{X}^{\\top} \\boldsymbol{Y}, \\quad \\boldsymbol{S} = (\\boldsymbol{Y} - \\boldsymbol{X}\\hat{\\boldsymbol{B}})^{\\top}(\\boldsymbol{Y} - \\boldsymbol{X}\\hat{\\boldsymbol{B}}),\\] \\[ \\boldsymbol{\\Psi}_n = \\boldsymbol{\\Psi}_{0} + \\boldsymbol{S} + \\boldsymbol{B}_{0}^{\\top} \\boldsymbol{V}_{0}^{-1} \\boldsymbol{B}_{0} + \\hat{\\boldsymbol{B}}^{\\top} \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\hat{\\boldsymbol{B}} - \\boldsymbol{B}_n^{\\top} \\boldsymbol{V}_n^{-1} \\boldsymbol{B}_n, \\] and \\[ \\alpha_n = \\alpha_0 + N. \\] We can use a Gibbs sampling algorithm in this model since the conditional posterior distributions are standard. Example: The effect of institutions on per capita gross domestic product To illustrate multivariate regression models, we use the dataset provided by Acemoglu, Johnson, and Robinson (2001), who analyzed the effect of property rights on economic growth. We begin with the following simultaneous structural economic model:1 \\[\\begin{align} \\log(\\text{pcGDP95}_i) &amp;= \\beta_1 + \\beta_2 \\text{PAER}_i + \\beta_3 \\text{Africa} + \\beta_4 \\text{Asia} + \\beta_5 \\text{Other} + u_{1i}, \\tag{7.1} \\end{align}\\] \\[\\begin{align} \\text{PAER}_i &amp;= \\alpha_1 + \\alpha_2 \\log(\\text{pcGDP95}_i) + \\alpha_3 \\log(\\text{Mort}_i) + u_{2i}, \\tag{7.2} \\end{align}\\] where pcGDP95, PAER, and Mort represent the per capita gross domestic product (GDP) in 1995, the average index of protection against expropriation between 1985 and 1995, and the settler mortality rate during the period of colonization, respectively. Africa, Asia, and Other are indicator variables for continents, with America serving as the baseline group. In this model, there is reverse (simultaneous) causality due to the contemporaneous effect of GDP on PAER, and vice versa. Therefore, estimating Equations (7.1) and (7.2) without accounting for this phenomenon results in posterior mean estimates that are biased and inconsistent from a sampling (frequentist) perspective.2 A potential strategy to address this issue is to estimate the reduced-form model, i.e., a model without simultaneous causality, where all endogenous variables are functions of exogenous variables. The former are determined within the model (e.g., \\(\\log(\\text{pcGDP95}_i)\\) and PAER in this example), while the latter are determined outside the model (e.g., \\(\\log(\\text{Mort}_i)\\), Africa, Asia, and Other in this example). Replacing Equation (7.2) into Equation (7.1), and solving for \\(\\log(\\textit{pcGDP95})\\), \\[\\begin{align} \\log(\\text{pcGDP95}_i)=\\pi_1+\\pi_2\\log(\\text{Mort}_i)+\\pi_3 \\text{Africa}+\\pi_4 \\text{Asia}+\\pi_5 \\text{Other}+e_{1i}. \\tag{7.3} \\end{align}\\] Then, by substituting Equation (7.3) into Equation (7.2), and solving for PAER, we obtain \\[\\begin{align} \\text{PAER}_i = \\gamma_1 + \\gamma_2 \\log(\\text{Mort}_i) + \\gamma_3 \\text{Africa} + \\gamma_4 \\text{Asia} + \\gamma_5 \\text{Other} + e_{2i}, \\tag{7.4} \\end{align}\\] where \\(\\pi_2 = \\frac{\\beta_2\\alpha_3}{1 - \\beta_2\\alpha_2}\\) and \\(\\gamma_2 = \\frac{\\alpha_3}{1 - \\beta_2\\alpha_2}\\), given that \\(\\beta_2 \\alpha_2 \\neq 1\\), i.e., independent equations (see Exercise 2). Observe that Equations (7.3) and (7.4) have the form of a multivariate regression model, where the common set of regressors is \\[ \\boldsymbol{X} = \\left[\\log(\\text{Mort}) \\ \\text{Africa} \\ \\text{Asia} \\ \\text{Other}\\right] \\] and the common set of dependent variables is \\[ \\boldsymbol{Y} = \\left[\\log(\\text{pcGDP95}) \\ \\text{PAER}\\right]. \\] Therefore, we can estimate this model using the setup outlined in this section. In the first stage, we estimate the parameters of the reduced-form model (Equations (7.3) and (7.4)), but the main interest lies in estimating the parameters of the structural model (Equations (7.1) and (7.2)). A valid question is whether we can recover (identify) the structural parameters from the reduced-form parameters. There are two criteria to answer this question: the order condition, which is necessary, and the rank condition, which is both necessary and sufficient. The order condition Given a system of equations with \\(M\\) endogenous variables, and \\(K\\) exogenous variables (including the intercept), there are two ways to assess the order condition: The parameters of an equation in the system are identified if there are at least \\(M-1\\) variables excluded from the equation (exclusion restrictions). The equation is exactly identified if the number of excluded variables is \\(M-1\\), and is over identified if the number of excluded variables is greater than \\(M-1\\). The parameters of equation \\(m\\) in the system are identified if \\(K-K_m\\geq M_m-1\\), where \\(K_m\\) and \\(M_m\\) are the number of exogenous and endogenous variables in equation \\(m\\), respectively. The \\(m\\)-th equation is exactly identified if \\(K-K_m = M_m-1\\), and over identified if \\(K-K_m &gt; M_m-1\\). We can see from Equations (7.1) and (7.2) in this example that \\(K=5\\), \\(M=2\\), \\(K_1=4\\), \\(K_2=2\\), \\(M_1=2\\), and \\(M_2=2\\). This means that \\(K-K_1=1=M-1\\) and \\(K-K_2=3&gt;M-1=1\\), that is, the order condition says that both equations satisfy the necessary condition of identification, the first equation would be exactly identified, and the second equation would be over identified. Observe that there is one excluded variable from the first equation, and there are three excluded variables from the second equation. The rank condition The rank condition (necessary and sufficient) says that given a structural model with \\(M\\) equations (\\(M\\) endogenous variables), an equation is identified if and only if there is at least one determinant different from zero from a \\((M-1)\\times(M-1)\\) matrix built using the excluded variables in the analyzed equation, but included in any other equation of the system. It is useful to build the identification matrix to implement the rank condition. The next Table shows this matrix in this example. Table 7.1: Example: Rank condition \\(\\log(\\text{pcGDP95})\\) \\(\\text{PAER}\\) Constant \\(\\log(\\text{Mort})\\) Africa Asia Other 1 \\(-\\beta_2\\) \\(-\\beta_1\\) 0 \\(-\\beta_3\\) \\(-\\beta_4\\) \\(-\\beta_5\\) \\(-\\alpha_2\\) 1 \\(-\\alpha_1\\) \\(-\\alpha_3\\) 0 0 0 The only excluded variable in the \\(\\log(\\text{pcGDP95})\\) equation is \\(\\log(\\text{Mort})\\). Therefore, there is only one matrix that can be constructed using the excluded variables from this equation, which is \\([-\\alpha_3]\\) (see column 4 in the Table). The determinant of this matrix is \\(-\\alpha_3\\), and as long as this coefficient is nonzero (i.e., \\(\\alpha_3 \\neq 0\\)), meaning that the mortality rate is relevant in the PAER equation, the coefficients in the \\(\\log(\\text{pcGDP95})\\) equation are exactly identified. For example, \\(\\beta_2 = \\frac{\\pi_2}{\\gamma_2}\\), which represents the effect of property rights on GDP, is exactly identified. It is crucial to observe the importance of excluding \\(\\log(\\text{Mort})\\) from the \\(\\log(\\text{pcGDP95})\\) equation, while including \\(\\log(\\text{Mort})\\) in the PAER equation. This is known as the exclusion restriction, which requires the presence of an exogenous source of variability in the PAER equation to help identify the \\(\\log(\\text{pcGDP95})\\) equation. The presence of relevant exogenous sources of variability is an essential factor in the identification, estimation, and inference of structural parameters. As for the identification of the structural parameters in the PAER equation, there are three potential matrices that can be constructed: \\([-\\beta_3]\\), \\([-\\beta_4]\\), and \\([-\\beta_5]\\) (see columns 5, 6, and 7 in the Table). As long as any of these parameters are relevant in the \\(\\log(\\text{pcGDP95})\\) equation, the PAER equation is identified. In this case, the PAER equation is over-identified, meaning there are multiple ways to estimate the parameters in this equation. For example, \\(\\alpha_2 = \\gamma_3/\\pi_3 = \\gamma_4/\\pi_4 = \\gamma_5/\\pi_5\\) (see Exercise 2). In general, recovering the structural parameters from the reduced-form parameters can be challenging due to the need for relevant identification restrictions, which can be difficult to find in some applications.3 For this example, we set non-informative priors: \\(\\boldsymbol{B}_0 = \\left[\\boldsymbol{0}_5 \\ \\boldsymbol{0}_5\\right]\\), \\(\\boldsymbol{V}_0 = 100 \\boldsymbol{I}_K\\), \\(\\boldsymbol{\\Psi}_0 = 5 \\boldsymbol{I}_2\\), and \\(\\alpha_0 = 5\\).4 Once our GUI is displayed (see the beginning of this chapter), we should follow the next Algorithm to run multivariate linear models in the GUI (see Chapter 5 for details, particularly on how to set the data set). Algorithm: Multivariate Linear Model Select Multivariate Models on the top panel Select Simple Multivariate model using the left radio button Upload the dataset selecting first if there is header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select the number of dependent variables in the box Number of endogenous variables: m Select the number of independent variables (including the intercept) in the box Number of exogenous variables: k Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following R code shows how to perform the Gibss sampling algorithm in this example using the dataset 4Institutions.csv. We ask to run this example using the rmultireg command from the bayesm package as an exercise. We find that the posterior mean structural effect of property rights on GDP is 0.98, and the 95% credible interval is (0.56, 2.87). This means that there is evidence supporting a positive effect of property rights on gross domestic product. rm(list = ls()) set.seed(12345) DataInst &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/4Institutions.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(DataInst) Y &lt;- cbind(logpcGDP95, PAER) X &lt;- cbind(1, logMort, Africa, Asia, Other) M &lt;- dim(Y)[2] K &lt;- dim(X)[2] N &lt;- dim(Y)[1] # Hyperparameters B0 &lt;- matrix(0, K, M) c0 &lt;- 100 V0 &lt;- c0*diag(K) Psi0 &lt;- 5*diag(M) a0 &lt;- 5 # Posterior parameters Bhat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y S &lt;- t(Y - X%*%Bhat)%*%(Y - X%*%Bhat) Vn &lt;- solve(solve(V0) + t(X)%*%X) Bn &lt;- Vn%*%(solve(V0)%*%B0 + t(X)%*%X%*%Bhat) Psin &lt;- Psi0 + S + t(B0)%*%solve(V0)%*%B0 + t(Bhat)%*%t(X)%*%X%*%Bhat - t(Bn)%*%solve(Vn)%*%Bn an &lt;- a0 + N #Posterior draws s &lt;- 10000 #Number of posterior draws SIGs &lt;- replicate(s, LaplacesDemon::rinvwishart(an, Psin)) BsCond &lt;- sapply(1:s, function(s) {MixMatrix::rmatrixnorm(n = 1, mean=Bn, U = Vn,V = SIGs[,,s])}) summary(coda::mcmc(t(BsCond))) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 10.3789 0.44935 0.0044935 0.0044935 ## [2,] -0.4170 0.09972 0.0009972 0.0009972 ## [3,] -0.7318 0.24032 0.0024032 0.0024032 ## [4,] -0.5840 0.28959 0.0028959 0.0028959 ## [5,] 0.2972 0.48248 0.0048248 0.0048248 ## [6,] 8.5156 0.75222 0.0075222 0.0075222 ## [7,] -0.4283 0.16725 0.0016725 0.0016725 ## [8,] -0.2677 0.40099 0.0040099 0.0040361 ## [9,] 0.3475 0.48749 0.0048749 0.0048749 ## [10,] 1.2454 0.81787 0.0081787 0.0081787 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 9.4953 10.07783 10.3766 10.6768666 11.27717 ## var2 -0.6136 -0.48366 -0.4158 -0.3496649 -0.22143 ## var3 -1.1985 -0.89381 -0.7327 -0.5695042 -0.25578 ## var4 -1.1527 -0.77805 -0.5827 -0.3900056 -0.01057 ## var5 -0.6587 -0.02514 0.2945 0.6224207 1.24241 ## var6 7.0210 8.01999 8.5116 9.0111246 10.00396 ## var7 -0.7558 -0.53971 -0.4285 -0.3165409 -0.10184 ## var8 -1.0600 -0.53703 -0.2676 0.0002646 0.52081 ## var9 -0.6090 0.02377 0.3500 0.6740956 1.30291 ## var10 -0.3446 0.69699 1.2418 1.7939884 2.86752 SIGMs &lt;- t(sapply(1:s, function(l) {gdata::lowerTriangle(SIGs[,,l], diag=TRUE, byrow=FALSE)})) summary(coda::mcmc(SIGMs)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.5381 0.09524 0.0009524 0.0009813 ## [2,] 0.5372 0.13144 0.0013144 0.0013144 ## [3,] 1.5225 0.26763 0.0026763 0.0026763 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.3853 0.4695 0.5267 0.5942 0.7545 ## var2 0.3181 0.4449 0.5234 0.6137 0.8359 ## var3 1.0863 1.3324 1.4924 1.6799 2.1354 hdiBs &lt;- HDInterval::hdi(t(BsCond), credMass = 0.95) # Highest posterior density credible interval hdiBs ## [,1] [,2] [,3] [,4] [,5] [,6] ## lower 9.44900 -0.6131229 -1.1769984 -1.16535714 -0.670223 7.041383 ## upper 11.22169 -0.2212954 -0.2393919 -0.02786525 1.227650 10.017329 ## [,7] [,8] [,9] [,10] ## lower -0.74678771 -1.0419989 -0.6358514 -0.3253181 ## upper -0.09525447 0.5322414 1.2748474 2.8767518 ## attr(,&quot;credMass&quot;) ## [1] 0.95 hdiSIG &lt;- HDInterval::hdi(SIGMs, credMass = 0.95) # Highest posterior density credible interval hdiSIG ## [,1] [,2] [,3] ## lower 0.3735344 0.2960049 1.023421 ## upper 0.7318534 0.8022998 2.034439 ## attr(,&quot;credMass&quot;) ## [1] 0.95 beta2 &lt;- BsCond[2,]/BsCond[7,] summary(coda::mcmc(beta2)) # Effect of property rights on GDP ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 0.9796 16.8430 0.1684 0.1684 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 0.5604 0.7984 0.9677 1.2329 2.8709 References Acemoglu, D., S. Johnson, and J. Robinson. 2001. “The Colonial Origins of Comparative Development: An Empirical Investigation.” The American Economic Review 91 (5): 1369–1401. Gujarati, Damodar N., and Dawn C. Porter. 2009. Basic Econometrics. 5th ed. New York, NY: McGraw-Hill Education. Koop, Gary M. 2003. Bayesian Econometrics. John Wiley &amp; Sons Inc. Wooldridge, Jeffrey M. 2016. Introductory Econometrics: A Modern Approach. 6th ed. Boston, MA: Cengage Learning. This model captures the potential underlying economic relationship between the variables.↩︎ Note that \\(\\mathbb{E}[u_1\\text{PAER}]\\neq 0\\), which means failing to meet a necessary condition for obtaining unbiased and consistent estimators of . See Exercise 1.↩︎ Good introductory-level textbooks on identification in linear systems include Gujarati and Porter (2009), and Jeffrey M. Wooldridge (2016).↩︎ Note that we are setting the priors in the reduced-form model. This may have unintended consequences for the posterior distributions of the structural parameters, which are ultimately the parameters of interest to researchers. For further discussion, see Koop (2003).↩︎ "],["sec72.html", "7.2 Seemingly Unrelated Regression", " 7.2 Seemingly Unrelated Regression In seemingly unrelated regression (SUR) models, there are \\(M\\) dependent variables, each with potentially different regressors, such that the stochastic errors are contemporaneously correlated. The model is given by: \\[ \\boldsymbol{y}_m = \\boldsymbol{X}_m \\boldsymbol{\\beta}_m + \\boldsymbol{\\mu}_m, \\] where \\(\\boldsymbol{y}_m\\) is an \\(N\\)-dimensional vector of observations, \\(\\boldsymbol{X}_m\\) is an \\(N \\times K_m\\) matrix of regressors, \\(\\boldsymbol{\\beta}_m\\) is a \\(K_m\\)-dimensional vector of location parameters, and \\(\\boldsymbol{\\mu}_m\\) is an \\(N\\)-dimensional vector of stochastic errors, for \\(m = 1, 2, \\dots, M\\). Let \\(\\boldsymbol{\\mu}_i = \\left[\\mu_{i1} \\ \\mu_{i2} \\ \\dots \\ \\mu_{iM}\\right]^{\\top}\\), where \\(\\boldsymbol{\\mu}_i \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\). Stacking the \\(M\\) equations, we can write the model as: \\[ \\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\mu}, \\] where \\(\\boldsymbol{y} = \\left[\\boldsymbol{y}_1^{\\top} \\ \\boldsymbol{y}_2^{\\top} \\ \\dots \\ \\boldsymbol{y}_M^{\\top}\\right]^{\\top}\\) is an \\(MN\\)-dimensional vector, \\(\\boldsymbol{\\beta} = \\left[\\boldsymbol{\\beta}_1^{\\top} \\ \\boldsymbol{\\beta}_2^{\\top} \\ \\dots \\ \\boldsymbol{\\beta}_M^{\\top}\\right]^{\\top}\\) is a \\(K\\)-dimensional vector with \\(K = \\sum_{m=1}^M K_m\\), and \\(\\boldsymbol{X}\\) is an \\(MN \\times K\\) block-diagonal matrix composed of the individual \\(\\boldsymbol{X}_m\\), i.e., \\[ \\boldsymbol{X} = \\begin{bmatrix} \\boldsymbol{X}_1 &amp; \\boldsymbol{0} &amp; \\dots &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; \\boldsymbol{X}_2 &amp; \\dots &amp; \\boldsymbol{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\boldsymbol{0} &amp; \\boldsymbol{0} &amp; \\dots &amp; \\boldsymbol{X}_M \\end{bmatrix}. \\] Similarly, the vector of errors is given by \\(\\boldsymbol{\\mu} = \\left[\\boldsymbol{\\mu}_1^{\\top} \\ \\boldsymbol{\\mu}_2^{\\top} \\ \\dots \\ \\boldsymbol{\\mu}_M^{\\top}\\right]^{\\top}\\), which is an \\(MN\\)-dimensional vector of stochastic errors, with \\(\\boldsymbol{\\mu} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma} \\otimes \\boldsymbol{I}_N)\\). The likelihood function for the parameters is then: \\[ p(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma} \\mid \\boldsymbol{y}, \\boldsymbol{X}) \\propto |\\boldsymbol{\\Sigma}|^{-N/2} \\exp\\left\\{ -\\frac{1}{2} (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})^{\\top} (\\boldsymbol{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_N) (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}) \\right\\}. \\] Using independent priors \\(\\pi(\\boldsymbol{\\beta}) \\sim \\mathcal{N}(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) and \\(\\pi(\\boldsymbol{\\Sigma}^{-1}) \\sim W(\\alpha_0, \\boldsymbol{\\Psi}_0)\\), the posterior distributions are \\[ \\boldsymbol{\\beta} \\mid \\boldsymbol{\\Sigma}, \\boldsymbol{y}, \\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{\\beta}_n, \\boldsymbol{B}_n), \\] \\[ \\boldsymbol{\\Sigma}^{-1} \\mid \\boldsymbol{\\beta}, \\boldsymbol{y}, \\boldsymbol{X} \\sim W(\\alpha_n, \\boldsymbol{\\Psi}_n), \\] where \\(\\boldsymbol{B}_n = (\\boldsymbol{X}^{\\top} (\\boldsymbol{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_N) \\boldsymbol{X} + \\boldsymbol{B}_0^{-1})^{-1}\\), \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n (\\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\boldsymbol{X}^{\\top} (\\boldsymbol{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_N) \\boldsymbol{y})\\), \\(\\alpha_n = \\alpha_0 + N\\) and \\(\\boldsymbol{\\Psi}_n = (\\boldsymbol{\\Psi}_0^{-1} + \\boldsymbol{U}^{\\top} \\boldsymbol{U})^{-1}\\), where \\(\\boldsymbol{U}\\) is an \\(N \\times M\\) matrix whose columns are \\(\\boldsymbol{y}_m - \\boldsymbol{X}_m \\boldsymbol{\\beta}_m\\). We can demonstrate, through straightforward yet tedious algebra, that by defining \\(\\boldsymbol{y}_i = [y_{i1} \\ y_{i2} \\ \\dots \\ y_{iM}]\\) and \\[ \\boldsymbol{X}_i = \\begin{bmatrix} x_{1i}^{\\top} &amp; \\boldsymbol{0} &amp; \\dots &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; x_{2i}^{\\top} &amp; \\dots &amp; \\boldsymbol{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\boldsymbol{0} &amp; \\boldsymbol{0} &amp; \\dots &amp; x_{Mi}^{\\top} \\end{bmatrix}, \\] we alternatively have \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\sum_{i=1}^N \\boldsymbol{X}_i^{\\top} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{X}_i)^{-1}\\), \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n (\\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\sum_{i=1}^N \\boldsymbol{X}_i^{\\top} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{y}_i)\\) and \\(\\boldsymbol{\\Psi}_n = (\\boldsymbol{\\Psi}_0^{-1} + \\sum_{i=1}^N (\\boldsymbol{y}_i - \\boldsymbol{X}_i^{\\top} \\boldsymbol{\\beta}) (\\boldsymbol{y}_i - \\boldsymbol{X}_i^{\\top} \\boldsymbol{\\beta})^{\\top})^{-1}\\). Observe that we have standard conditional posteriors, thus, we can employ a Gibbs sampling algorithm to get the posterior draws. Example: Utility demand Let’s use the dataset Utilities.csv to estimate a seemingly unrelated regression (SUR) model for utilities. We adopt the same setting as in Exercise 14 of Chapter 3, where we estimate a multivariate regression model while omitting households with no consumption in any utility. In this exercise, we observe that not all regressors are relevant for the demand of electricity, water, and gas. Thus, we estimate the following model: \\[\\begin{align*} \\log(\\text{electricity}_i) &amp; = \\beta_1 + \\beta_2\\log(\\text{electricity price}_i) + \\beta_3\\log(\\text{water price}_i) \\\\ &amp; + \\beta_4\\log(\\text{gas price}_i) + \\beta_5\\text{IndSocio1}_i + \\beta_6\\text{IndSocio2}_i + \\beta_7\\text{Altitude}_i \\\\ &amp; + \\beta_8\\text{Nrooms}_i + \\beta_9\\text{HouseholdMem}_i + \\beta_{10}\\log(\\text{Income}_i) + \\mu_{i1} \\\\ \\log(\\text{water}_i) &amp; = \\alpha_1 + \\alpha_2\\log(\\text{electricity price}_i) + \\alpha_3\\log(\\text{water price}_i) \\\\ &amp; + \\alpha_4\\log(\\text{gas price}_i) + \\alpha_5\\text{IndSocio1}_i + \\alpha_6\\text{IndSocio2}_i \\\\ &amp; + \\alpha_7\\text{Nrooms}_i + \\alpha_8\\text{HouseholdMem}_i + \\mu_{i2} \\\\ \\log(\\text{gas}_i) &amp; = \\gamma_1 + \\gamma_2\\log(\\text{electricity price}_i) + \\gamma_3\\log(\\text{water price}_i) \\\\ &amp; + \\gamma_4\\log(\\text{gas price}_i) + \\gamma_5\\text{IndSocio1}_i + \\gamma_6\\text{IndSocio2}_i + \\gamma_7\\text{Altitude}_i \\\\ &amp; + \\gamma_8\\text{Nrooms}_i + \\gamma_9\\text{HouseholdMem}_i + \\mu_{i3}, \\end{align*}\\] where electricity, water, and gas represent the monthly consumption of electricity (kWh), water (m\\(^3\\)), and gas (m\\(^3\\)) of Colombian households. The dataset includes information on 2,103 households, with details on the average prices of electricity (USD/kWh), water (USD/m\\(^3\\)), and gas (USD/m\\(^3\\)), as well as indicators of the socioeconomic conditions of the neighborhood where the household is located (IndSocio1 being the lowest and IndSocio3 the highest). Additionally, there is information on whether the household is located in a municipality situated at over 1,000 meters above sea level, the number of rooms in the house, the number of household members, and monthly income (USD). Since each equation has a different set of regressors, and we suspect correlation between the stochastic errors of the three equations, we should estimate a seemingly unrelated regression (SUR) model. We expect unobserved correlation across these equations because we are modeling utilities, and in some cases, a single provider handles all three services and issues one bill. The following Algorithm demonstrates how to estimate SUR models using our GUI. Our GUI utilizes the command rsurGibbs from the bayesm package in R software. See Chapter 5 for further details, including instructions on how to set up the dataset, and check the templates available in our GitHub repository (https://github.com/besmarter/BSTApp) in the DataApp and DataSim folders. Algorithm: Seemingly Unrelated Regression (SUR) Select Multivariate Models on the top panel Select Seemingly Unrelated Regression model using the left radio button Upload the dataset, selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select the number of dependent variables in the box Number of endogenous variables: m Select the number of independent variables in the box TOTAL number of Exogenous Variables: k. This is the sum of all exogenous variables over all equations, including intercepts. In the example of Utility demand, it is equal to 27 Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following code shows how to program this application using this package. We use 10,000 MCMC iterations, \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}_{27}\\), \\(\\boldsymbol{B}_0 = 100\\boldsymbol{I}_{27}\\), \\(\\alpha_0 = 5\\) and \\(\\boldsymbol{\\Psi} = 5\\boldsymbol{I}_3\\). We find that the posterior median estimates of the own-price elasticities of demand for electricity, water, and gas are -1.88, -0.36, and -0.62, respectively, and none of the 95% credible intervals encompass 0. This means that a 1% increase in the prices of electricity, water, and gas results in a 1.88%, 0.36%, and 0.62% decrease in the monthly consumption of these utilities, respectively.5 In general, there is evidence supporting the relevance of all regressors in these equations, with a few exceptions, and unobserved correlation in the demand for these services, which further supports the use of a SUR model in this application. rm(list = ls()) set.seed(010101) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:kableExtra&#39;: ## ## group_rows ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union DataUt &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/Utilities.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) DataUtEst &lt;- DataUt %&gt;% filter(Electricity != 0 &amp; Water !=0 &amp; Gas != 0) attach(DataUtEst) y1 &lt;- log(Electricity); y2 &lt;- log(Water); y3 &lt;- log(Gas) X1 &lt;- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem, Lnincome) X2 &lt;- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Nrooms, HouseholdMem) X3 &lt;- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem) regdata &lt;- NULL regdata[[1]] &lt;- list(y = y1, X = X1); regdata[[2]] &lt;- list(y = y2, X = X2); regdata[[3]] &lt;- list(y = y3, X = X3) M &lt;- length(regdata); K1 &lt;- dim(X1)[2]; K2 &lt;- dim(X2)[2]; K3 &lt;- dim(X3)[2] K &lt;- K1 + K2 + K3 # Hyperparameters b0 &lt;- rep(0, K); c0 &lt;- 100; B0 &lt;- c0*diag(K); V &lt;- 5*diag(M); a0 &lt;- M Prior &lt;- list(betabar = b0, A = solve(B0), nu = a0, V = V) #Posterior draws S &lt;- 10000; keep &lt;- 1; Mcmc &lt;- list(R = S, keep = keep, nprint = 0) PosteriorDraws &lt;- bayesm::rsurGibbs(Data = list(regdata = regdata), Mcmc = Mcmc, Prior = Prior) ## ## Starting Gibbs Sampler for SUR Regression Model ## with 3 regressions ## and 1574 observations for each regression ## ## Prior Parms: ## betabar ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [2,] 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 ## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 ## [9,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 ## [10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 ## [11,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 ## [12,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 ## [13,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 ## [14,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [15,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [16,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [17,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [18,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [19,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [20,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [21,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [22,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [23,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [24,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [25,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [26,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [27,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## [1,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [2,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [9,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [11,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [12,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [13,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [14,] 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [15,] 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [16,] 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [17,] 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [18,] 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [19,] 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 ## [20,] 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 ## [21,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 ## [22,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 ## [23,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 ## [24,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 ## [25,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 ## [26,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [27,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [,26] [,27] ## [1,] 0.00 0.00 ## [2,] 0.00 0.00 ## [3,] 0.00 0.00 ## [4,] 0.00 0.00 ## [5,] 0.00 0.00 ## [6,] 0.00 0.00 ## [7,] 0.00 0.00 ## [8,] 0.00 0.00 ## [9,] 0.00 0.00 ## [10,] 0.00 0.00 ## [11,] 0.00 0.00 ## [12,] 0.00 0.00 ## [13,] 0.00 0.00 ## [14,] 0.00 0.00 ## [15,] 0.00 0.00 ## [16,] 0.00 0.00 ## [17,] 0.00 0.00 ## [18,] 0.00 0.00 ## [19,] 0.00 0.00 ## [20,] 0.00 0.00 ## [21,] 0.00 0.00 ## [22,] 0.00 0.00 ## [23,] 0.00 0.00 ## [24,] 0.00 0.00 ## [25,] 0.00 0.00 ## [26,] 0.01 0.00 ## [27,] 0.00 0.01 ## nu = 3 ## V = ## [,1] [,2] [,3] ## [1,] 5 0 0 ## [2,] 0 5 0 ## [3,] 0 0 5 ## ## MCMC parms: ## R= 10000 keep= 1 nprint= 0 ## Bs &lt;- PosteriorDraws[[&quot;betadraw&quot;]] Names &lt;- c(&quot;Const&quot;, &quot;LnPriceElect&quot;, &quot;LnPriceWater&quot;, &quot;LnPriceGas&quot;, &quot;IndSocio1&quot;, &quot;IndSocio2&quot;, &quot;Altitude&quot;, &quot;Nrooms&quot;, &quot;HouseholdMem&quot;, &quot;Lnincome&quot;, &quot;Const&quot;, &quot;LnPriceElect&quot;, &quot;LnPriceWater&quot;, &quot;LnPriceGas&quot;, &quot;IndSocio1&quot;, &quot;IndSocio2&quot;, &quot;Nrooms&quot;, &quot;HouseholdMem&quot;,&quot;Const&quot;, &quot;LnPriceElect&quot;, &quot;LnPriceWater&quot;, &quot;LnPriceGas&quot;, &quot;IndSocio1&quot;, &quot;IndSocio2&quot;, &quot;Altitude&quot;, &quot;Nrooms&quot;, &quot;HouseholdMem&quot;) colnames(Bs) &lt;- Names summary(coda::mcmc(Bs)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Const 1.343420 0.45854 0.0045854 0.0045854 ## LnPriceElect -1.883221 0.26486 0.0026486 0.0026100 ## LnPriceWater -0.356765 0.04423 0.0004423 0.0004423 ## LnPriceGas -0.098387 0.05960 0.0005960 0.0006046 ## IndSocio1 -0.737529 0.07192 0.0007192 0.0006969 ## IndSocio2 -0.151118 0.04787 0.0004787 0.0004859 ## Altitude -0.220988 0.02531 0.0002531 0.0002531 ## Nrooms 0.070102 0.01236 0.0001236 0.0001236 ## HouseholdMem 0.086989 0.01057 0.0001057 0.0001057 ## Lnincome 0.062892 0.01244 0.0001244 0.0001223 ## Const 2.177612 0.65917 0.0065917 0.0065917 ## LnPriceElect -0.050723 0.39241 0.0039241 0.0039241 ## LnPriceWater -0.364801 0.06656 0.0006656 0.0006656 ## LnPriceGas 0.226759 0.08681 0.0008681 0.0008681 ## IndSocio1 -0.427636 0.11136 0.0011136 0.0011334 ## IndSocio2 -0.359542 0.07427 0.0007427 0.0007427 ## Nrooms 0.093014 0.01868 0.0001868 0.0001868 ## HouseholdMem 0.131650 0.01612 0.0001612 0.0001612 ## Const -1.215735 0.54177 0.0054177 0.0054177 ## LnPriceElect -1.794509 0.31960 0.0031960 0.0031960 ## LnPriceWater -0.003921 0.05257 0.0005257 0.0005257 ## LnPriceGas -0.625445 0.07270 0.0007270 0.0007270 ## IndSocio1 -0.744103 0.08706 0.0008706 0.0008706 ## IndSocio2 -0.203635 0.05893 0.0005893 0.0005893 ## Altitude 0.311631 0.03136 0.0003136 0.0003136 ## Nrooms 0.089361 0.01481 0.0001481 0.0001488 ## HouseholdMem 0.169882 0.01264 0.0001264 0.0001264 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Const 0.44452 1.03120 1.342407 1.65192 2.25376 ## LnPriceElect -2.39679 -2.06328 -1.882706 -1.70369 -1.36996 ## LnPriceWater -0.44221 -0.38678 -0.356850 -0.32669 -0.26969 ## LnPriceGas -0.21655 -0.13777 -0.098191 -0.05902 0.01872 ## IndSocio1 -0.87630 -0.78653 -0.737701 -0.68840 -0.59675 ## IndSocio2 -0.24601 -0.18286 -0.151440 -0.11896 -0.05681 ## Altitude -0.27080 -0.23838 -0.220742 -0.20385 -0.17259 ## Nrooms 0.04596 0.06178 0.070023 0.07835 0.09422 ## HouseholdMem 0.06600 0.07994 0.086857 0.09411 0.10785 ## Lnincome 0.03836 0.05421 0.062957 0.07165 0.08717 ## Const 0.88957 1.73496 2.169638 2.62170 3.47216 ## LnPriceElect -0.81956 -0.31624 -0.054075 0.21132 0.71842 ## LnPriceWater -0.49559 -0.40995 -0.364248 -0.32026 -0.23639 ## LnPriceGas 0.06075 0.16754 0.226690 0.28570 0.39476 ## IndSocio1 -0.64203 -0.50302 -0.427819 -0.35226 -0.21315 ## IndSocio2 -0.50401 -0.40949 -0.359821 -0.31063 -0.21199 ## Nrooms 0.05688 0.08023 0.093139 0.10555 0.12968 ## HouseholdMem 0.10041 0.12065 0.131506 0.14260 0.16314 ## Const -2.28569 -1.58566 -1.220078 -0.84612 -0.14787 ## LnPriceElect -2.42484 -2.01228 -1.797269 -1.57889 -1.16396 ## LnPriceWater -0.10684 -0.03923 -0.004088 0.03153 0.09905 ## LnPriceGas -0.76526 -0.67445 -0.625899 -0.57734 -0.48125 ## IndSocio1 -0.91381 -0.80243 -0.744909 -0.68577 -0.57341 ## IndSocio2 -0.31791 -0.24388 -0.203300 -0.16415 -0.09012 ## Altitude 0.24896 0.29099 0.311668 0.33256 0.37278 ## Nrooms 0.06050 0.07921 0.089386 0.09943 0.11793 ## HouseholdMem 0.14467 0.16144 0.170024 0.17843 0.19431 summary(PosteriorDraws[[&quot;Sigmadraw&quot;]]) ## Posterior Means of Std Deviations and Correlation Matrix ## Std Dev 1 2 3 ## 1 0.46 1.00 0.30 0.25 ## 2 0.72 0.30 1.00 0.23 ## 3 0.56 0.25 0.23 1.00 ## ## Upper Triangle of Var-Cov Matrix ## Summary of Posterior Marginal Distributions ## Moments ## mean std dev num se rel eff sam size ## 1,1 0.214 0.0077 7.1e-05 0.78 9000 ## 1,2 0.099 0.0087 9.2e-05 1.01 4500 ## 2,2 0.512 0.0182 1.9e-04 0.95 9000 ## 1,3 0.064 0.0068 6.6e-05 0.84 9000 ## 2,3 0.094 0.0105 1.1e-04 1.02 4500 ## 3,3 0.317 0.0114 1.2e-04 0.94 9000 ## based on 9000 valid draws (burn-in=1000) We ask in the Exercise 5 to run this application using our GUI and the information in the dataset Utilities.csv. Observe that this file should be modified to agree the structure that requires our GUI (see the dataset 5Institutions.csv in the folder DataApp of our GitHub repository -https://github.com/besmarter/BSTApp- for a template). In addition, we ask to program from scratch the Gibbs sampler algorithm in this application. References Ramı́rez–Hassan, Andrés, and Alejandro López-Vera. 2024. “Welfare Implications of a Tax on Electricity: A Semi-Parametric Specification of the Incomplete EASI Demand System.” Energy Economics 131: 1–13. This is an example where concerns about biased and inconsistent posterior mean estimates may arise, for instance, due to reverse causality between quantity and demand. These concerns are valid; however, we are using micro-level data, which implies no demand-supply simultaneity. Additionally, the utility providers operate in regulated natural monopoly markets, which mitigates endogeneity from searching provider strategies. Finally, we took prices directly from provider records, which avoids potential price measurement errors Ramı́rez–Hassan and López-Vera (2024).↩︎ "],["sec73.html", "7.3 Instrumental variable", " 7.3 Instrumental variable This inferential approach is used when there are endogeneity issues, that is, when the stochastic error is not independent of the regressors. This, in turn, generates bias in posterior mean estimates when we use an inferential approach that does not account for this issue. Endogeneity can be caused by reverse causality, omitting relevant correlated variables, or measurement error in the regressors.6 Let’s specify the dependent variable as a linear function of one endogenous regressor and some exogenous regressors. That is, \\[ y_{i} = \\boldsymbol{x}_{ei}^{\\top} \\boldsymbol{\\beta}_1 + \\beta_s x_{si} + \\mu_{i} \\] where \\[ x_{si} = \\boldsymbol{x}_{ei}^{\\top} \\boldsymbol{\\gamma}_1 + \\boldsymbol{z}_i^{\\top} \\boldsymbol{\\gamma}_2 + v_{i}, \\] \\(x_s\\) is the variable that generates the endogeneity issues (\\(\\mathbb{E}[\\mu \\mid x_{s}] \\neq 0\\)), \\(\\boldsymbol{x}_e\\) are \\(K_1\\) exogenous regressors (\\(\\mathbb{E}[\\mu \\mid \\boldsymbol{x}_{e}] = \\boldsymbol{0}\\)), and \\(\\boldsymbol{z}\\) are \\(K_2\\) instruments. The instruments are regressors that drive \\(x_s\\) (\\(\\mathbb{E}[x_{s} \\boldsymbol{z}] \\neq \\boldsymbol{0}\\)), but do not have a direct effect on \\(y\\) (\\(\\mathbb{E}[y \\boldsymbol{z} \\mid x_s] = \\boldsymbol{0}\\)). The equation for \\(y\\) is called the structural equation, and it is the equation that the researcher is ultimately interested in. Assuming \\[ (u_{i},v_i)^{\\top} \\stackrel{i.i.d.}{\\thicksim} N(0,\\boldsymbol{\\Sigma}), \\] where \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]\\), \\(l,m=1,2\\), the likelihood function is \\[ p(\\boldsymbol{\\beta},\\boldsymbol{\\gamma},\\boldsymbol{\\Sigma} \\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{Z}) = \\frac{1}{(2\\pi)^\\frac{N}{2}|\\boldsymbol{\\Sigma}|^\\frac{N}{2}} \\exp\\left\\{-\\frac{1}{2} \\sum_{i=1}^N (y_i-\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}, x_{si} -\\boldsymbol{w}_i^{\\top} \\boldsymbol{\\gamma}) \\boldsymbol{\\Sigma}^{-1} \\begin{pmatrix} y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta} \\\\ x_{si} - \\boldsymbol{w}_i^{\\top} \\boldsymbol{\\gamma} \\end{pmatrix} \\right\\}, \\] where \\[ \\boldsymbol{\\beta}=\\begin{bmatrix} \\boldsymbol{\\beta}_1^{\\top} &amp; \\beta_s \\end{bmatrix}^{\\top}, \\quad \\boldsymbol{\\gamma}=\\begin{bmatrix} \\boldsymbol{\\gamma}_1^{\\top} &amp; \\boldsymbol{\\gamma}_2^{\\top} \\end{bmatrix}^{\\top}, \\quad \\boldsymbol{x}_i=\\begin{bmatrix} \\boldsymbol{x}_{ei}^{\\top} &amp; x_{si} \\end{bmatrix}^{\\top}, \\quad \\boldsymbol{w}_i=\\begin{bmatrix} \\boldsymbol{x}_{ei}^{\\top} &amp; \\boldsymbol{z}_{i}^{\\top} \\end{bmatrix}^{\\top}. \\] We obtain the standard conditional posterior densities by specifying the following independent priors: \\[ \\boldsymbol{\\gamma} \\sim N(\\boldsymbol{\\gamma}_0, \\boldsymbol{G}_0), \\quad \\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0), \\quad \\boldsymbol{\\Sigma}^{-1} \\sim W(\\alpha_0, \\boldsymbol{\\Psi}_0). \\] In particular, the conditional distributions are: \\[ \\boldsymbol{\\beta} \\mid \\boldsymbol{\\gamma}, \\boldsymbol{\\Sigma}, \\boldsymbol{y}, \\boldsymbol{X}, \\boldsymbol{Z} \\sim N(\\boldsymbol{\\beta}_n, \\boldsymbol{B}_n), \\] \\[ \\boldsymbol{\\gamma} \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\boldsymbol{y}, \\boldsymbol{X}, \\boldsymbol{Z} \\sim N(\\boldsymbol{\\gamma}_n, \\boldsymbol{G}_n), \\] \\[ \\boldsymbol{\\Sigma}^{-1} \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}, \\boldsymbol{y}, \\boldsymbol{X}, \\boldsymbol{Z} \\sim W(\\alpha_n, \\boldsymbol{\\Psi}_n), \\] where \\[ \\boldsymbol{\\beta}_n = \\boldsymbol{B}_n \\left( \\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\omega_1^{-1} \\sum_{i=1}^{N} \\left[ \\boldsymbol{x}_i \\left( y_i - \\frac{\\sigma_{12}(x_{si} - \\boldsymbol{w}_i^{\\top} \\boldsymbol{\\gamma})}{\\sigma_{22}} \\right) \\right] \\right), \\] \\[ \\boldsymbol{B}_n = \\left( \\omega_1^{-1} \\sum_{i=1}^{N} \\boldsymbol{x}_i \\boldsymbol{x}_i^{\\top} + \\boldsymbol{B}_0^{-1} \\right)^{-1}, \\quad \\omega_1 = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}, \\] \\[ \\boldsymbol{G}_n = \\left( \\omega_2^{-1} \\sum_{i=1}^{N} \\boldsymbol{w}_i \\boldsymbol{w}_i^{\\top} + \\boldsymbol{G}_0^{-1} \\right)^{-1}, \\quad \\boldsymbol{\\gamma}_n = \\boldsymbol{G}_n \\left( \\boldsymbol{G}_0^{-1} \\boldsymbol{\\gamma}_0 + \\omega_2^{-1} \\sum_{i=1}^{N} \\left[ \\boldsymbol{w}_i \\left( x_{si} - \\frac{\\sigma_{12} (y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta})}{\\sigma_{11}} \\right) \\right] \\right), \\] \\[ \\omega_2 = \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}, \\quad \\boldsymbol{\\Psi}_n = \\left[ \\boldsymbol{\\Psi}_0^{-1} + \\sum_{i=1}^N \\begin{pmatrix} y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta} \\\\ x_{si} - \\boldsymbol{w}_i^{\\top} \\boldsymbol{\\gamma} \\end{pmatrix} (y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}, x_{si} - \\boldsymbol{w}_i^{\\top} \\boldsymbol{\\gamma}) \\right]^{-1}, \\] \\[ \\alpha_n = \\alpha_0 + N, \\quad \\sigma_{lj} \\text{ are the elements of } \\boldsymbol{\\Sigma}. \\] We also use a Gibbs sampling algorithm in this model since we have standard conditional posterior distributions. Example: Simulation exercise Let’s simulate the simple process \\(y_i=\\beta_1+\\beta_2x_{si}+\\mu_i\\) and \\(x_{si}=\\gamma_1+\\gamma_2z_i+v_i\\) where \\([\\mu_i \\ v_i]^{\\top} \\sim N(\\boldsymbol{0},\\boldsymbol{\\Sigma})\\), \\(\\boldsymbol{\\Sigma}=[\\sigma_{lj}]\\) such that \\(\\sigma_{12} \\neq 0\\), \\(i=1,2,\\dots,100\\). Observe that \\(\\mu\\mid v\\sim N\\left(\\frac{\\sigma_{12}}{\\sigma_{22}}v,\\sigma_{11}-\\frac{\\sigma_{21}^2}{\\sigma_{22}}\\right)\\), this implies that \\(\\mathbb{E}[\\mu\\mid x_s]=\\mathbb{E}[\\mu\\mid v]=\\frac{\\sigma_{12}}{\\sigma_{22}}v\\neq 0\\) given \\(\\sigma_{12}\\neq 0\\) and \\(\\mathbb{E}[\\mu\\mid z]=0\\). Let’s set all location parameters equal to 1, and \\(\\sigma_{11}=\\sigma_{22}=1\\), \\(\\sigma_{12}=0.8\\), and \\(z\\sim N(0,1)\\). We know from the large sampling properties of the posterior mean that this converges to the maximum likelihood estimator (see Section 1.1, and Lehmann and Casella (2003), Van der Vaart (2000)), which in this setting is \\[ \\hat{\\beta}_2=\\frac{\\widehat{\\text{Cov}}(x_s,y)}{\\widehat{\\text{Var}}(x_s)} \\] which converges in probability to \\[ \\beta_2+\\frac{\\sigma_{12}}{\\sigma_{22}\\text{Var}(x_s)}=\\beta_2+\\frac{\\sigma_{12}}{\\sigma_{22}(\\gamma_2^2\\text{Var}(z)+\\sigma_{22})}=1.4, \\] that is, the asymptotic bias when using the posterior mean of a linear regression without taking into account endogeneity is 0.4 in this example. We assess the sampling performance of the Bayesian estimators by simulating this setting 100 times. The following code demonstrates how to do this using a linear model that does not account for the endogeneity issue (see Section 6.1), as well as how to implement the instrumental variable model using the function rivGibbs from the package bayesm.7 In this setup, we use \\(\\boldsymbol{B}_0 = 1000 \\mathbf{I}_2\\), \\(\\boldsymbol{\\beta}_0 = \\mathbf{0}_2\\), and the parameters of the inverse gamma distribution are set to 0.0005. For the instrumental variable model, we additionally set \\(\\boldsymbol{\\gamma}_0 = \\mathbf{0}_2\\), \\(\\boldsymbol{G}_0 = 1000 \\mathbf{I}_2\\), \\(\\alpha_0 = 3\\), and \\(\\boldsymbol{\\Psi}_0 = 3 \\mathbf{I}_2\\). rm(list = ls()); set.seed(010101) N &lt;- 100; k &lt;- 2 B &lt;- rep(1, k); G &lt;- rep(1, 2); s12 &lt;- 0.8 SIGMA &lt;- matrix(c(1, s12, s12, 1), 2, 2) z &lt;- rnorm(N); Z &lt;- cbind(1, z); w &lt;- matrix(1,N,1); S &lt;- 100 U &lt;- replicate(S, MASS::mvrnorm(n = N, mu = rep(0, 2), SIGMA)) x &lt;- G[1] + G[2]*z + U[,2,]; y &lt;- B[1] + B[2]*x + U[,1,] # Hyperparameters d0 &lt;- 0.001/2; a0 &lt;- 0.001/2 b0 &lt;- rep(0, k); c0 &lt;- 1000; B0 &lt;- c0*diag(k) B0i &lt;- solve(B0); g0 &lt;- rep(0, 2) G0 &lt;- 1000*diag(2); G0i &lt;- solve(G0) nu &lt;- 3; Psi0 &lt;- nu*diag(2) # MCMC parameters mcmc &lt;- 5000; burnin &lt;- 1000 tot &lt;- mcmc + burnin; thin &lt;- 1 # Gibbs sampling Gibbs &lt;- function(x, y){ Data &lt;- list(y = y, x = x, w = w, z = Z) Mcmc &lt;- list(R = mcmc, keep = thin, nprint = 0) Prior &lt;- list(md = g0, Ad = G0i, mbg = b0, Abg = B0i, nu = nu, V = Psi0) RestIV &lt;- bayesm::rivGibbs(Data = Data, Mcmc = Mcmc, Prior = Prior) PostBIV &lt;- mean(RestIV[[&quot;betadraw&quot;]]) ResLM &lt;- MCMCpack::MCMCregress(y ~ x + w - 1, b0 = b0, B0 = B0i, c0 = a0, d0 = d0) PostB &lt;- mean(ResLM[,1]); Res &lt;- c(PostB,PostBIV) return(Res) } PosteriorMeans &lt;- sapply(1:S, function(s) {Gibbs(x = x[,s], y = y[,s])}) ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## ## ## Starting Gibbs Sampler for Linear IV Model ## ## nobs= 100 ; 2 instruments; 1 included exog vars ## Note: the numbers above include intercepts if in z or w ## ## Prior Parms: ## mean of delta ## [1] 0 0 ## Adelta ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## mean of beta/gamma ## [1] 0 0 ## Abeta/gamma ## [,1] [,2] ## [1,] 0.001 0.000 ## [2,] 0.000 0.001 ## Sigma Prior Parms ## nu= 3 V= ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 3 ## ## MCMC parms: ## R= 5000 keep= 1 nprint= 0 ## rowMeans(PosteriorMeans) ## [1] 1.408272 1.035428 Model &lt;- c(replicate(S, &quot;Ordinary&quot;), replicate(S, &quot;Instrumental&quot;)) postmeans &lt;- c(t(PosteriorMeans)) df &lt;- data.frame(postmeans, Model, stringsAsFactors = FALSE) library(ggplot2); library(latex2exp) histExo &lt;- ggplot(df, aes(x = postmeans, fill = Model)) + geom_histogram(bins = 40, position = &quot;identity&quot;, color = &quot;black&quot;, alpha = 0.5) + labs(title = &quot;Overlayed Histograms&quot;, x = &quot;Value&quot;, y = &quot;Count&quot;) + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + geom_vline(aes(xintercept = mean(postmeans[1:S])), color = &quot;black&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + geom_vline(aes(xintercept = mean(postmeans[101:200])), color = &quot;black&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + geom_vline(aes(xintercept = B[2]), color = &quot;green&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + xlab(TeX(&quot;$E[\\\\beta_2]$&quot;)) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Histogram: Posterior means simulating 100 samples&quot;) histExo The Figure displays the histograms of the posterior means of \\(\\beta_2\\) using the ordinary model, which does not account for endogeneity, and the instrumental variable model. On one hand, the mean of the posterior means for the ordinary model is 1.41 (black dashed line in the red histogram), implying a bias of 0.41, which is very close to the population bias of 0.40. On the other hand, the mean of the posterior means for the instrumental variable model is 1.04 (black dashed line in the blue histogram), which is close to the population value of \\(\\beta_2 = 1\\) (green dashed line). We also observe that the histogram of the posterior means for the ordinary model is less dispersed. That is, this estimator is more efficient, which is a well-known result in the Frequentist inferential approach when comparing ordinary least squares and two-stage least squares (see Jeffrey M. Wooldridge (2010)). Two very important aspects in the instrumental variables literature are the weakness and exogeneity of the instruments. The former refers to how strong the relationship is between the instruments and the endogenous regressors, while the latter refers to the independence of the instruments from the stochastic error in the structural equation. In Exercise 6, we ask you to use the previous code as a baseline to study these two aspects. Observe the link between the weakness and exogeneity of the instrument, and the exclusion restrictions (\\(\\mathbb{E}[x_s \\boldsymbol{z}] \\neq \\boldsymbol{0}\\) and \\(\\mathbb{E}[y \\boldsymbol{z} \\mid x_s] = \\boldsymbol{0}\\)). This is the point of departure of Conley, Hansen, and Rossi (2012), who propose assessing the plausibility of the exclusion restrictions by defining plausible exogeneity as having prior information that the effect of the instrument in the structural equation is near zero, but perhaps not exactly zero. The following Algorithm can be used to estimate the instrumental variable model using our GUI. We ask in Exercise 8 to replicate the example of the effect of institutions on per capita GDP using our GUI. Algorithm: Instrumental Variable Model Select Multivariate Models on the top panel Select Variable instrumental (two equations) model using the left radio button Upload the dataset, selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Write down the formula of the structural equation in the Main Equation box. This formula must be written using the syntax of the formula command of R software. This equation includes the intercept by default, do not include it in the equation Write down the formula of the endogenous regressor in the Instrumental Equation box. This formula must be written using the syntax of the formula command of R software. This equation includes the intercept by default, do not include it in the equation Set the hyperparameters: mean vectors, covariance matrices, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons References Conley, T., C. Hansen, and P. Rossi. 2012. “Plausibly Exogenous.” The Review of Economics and Statistics 94 (1): 260–72. Lehmann, E. L., and George Casella. 2003. Theory of Point Estimation. Second Edition. Springer. Van der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press. Wooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. MIT press. Wooldridge, Jeffrey M. 2016. Introductory Econometrics: A Modern Approach. 6th ed. Boston, MA: Cengage Learning. See Jeffrey M. Wooldridge (2016), Chap. 15 for an introductory treatment of instrumental variables in the Frequentist inferential approach.↩︎ It seems that this function does not account for the effect of the exogenous regressors in the equation for the endogenous regressors.↩︎ "],["sec74.html", "7.4 Multivariate probit model", " 7.4 Multivariate probit model In the multivariate probit model Edwards and Allenby (2003), the response variable \\(y_{il} = \\{0, 1\\}\\) indicates that individual \\(i\\) makes binary choices among \\(L\\) mutually exclusive alternatives, where \\(l = 1, 2, \\dots, L\\) and \\(i = 1, 2, \\dots, N\\). Specifically, \\[ y_{il} = \\begin{cases} 0, &amp; \\quad y_{il}^* \\leq 0 \\\\ 1, &amp; \\quad y_{il}^* &gt; 0 \\end{cases} \\] where \\(\\boldsymbol{y}_i^* = \\boldsymbol{X}_i \\boldsymbol{\\beta} + \\boldsymbol{\\mu}_i \\sim \\text{i.i.d.} \\, N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\). Here, \\(\\boldsymbol{y}_i^*\\) is an unobserved latent \\(L\\)-dimensional vector, \\(\\boldsymbol{X}_i = \\boldsymbol{x}_i^\\top \\otimes \\mathbf{I}_L\\) is an \\(L \\times K\\) design matrix of regressors, with \\(K = L \\times k\\), where \\(k\\) is the number of regressors (i.e., the length of \\(\\boldsymbol{x}_i\\)). In addition, \\(\\boldsymbol{\\beta} = \\left[\\boldsymbol{\\beta}_1^\\top \\ \\boldsymbol{\\beta}_2^\\top \\dots \\boldsymbol{\\beta}_k^\\top\\right]^\\top\\), where \\(\\boldsymbol{\\beta}_j\\) forms an \\(L\\)-dimensional vector of coefficients for \\(j = 1, 2, \\dots, k\\). The likelihood function for this model is given by \\[ p(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma} \\mid \\boldsymbol{y}, \\boldsymbol{X}) = \\prod_{i=1}^N \\prod_{l=1}^L p_{il}^{y_{il}}, \\] where \\(p_{il} = p(y_{il}^* \\geq 0)\\). Observe that \\(p({y}_{il}^*\\geq 0)=p({\\lambda}_{ll}{y}_{il}^*\\geq 0)\\), \\(\\lambda_{ll}&gt;0\\). This generates identification issues because only the correlation matrix can be identified, similar to the univariate probit model where the variance of the model is fixed to 1. We follow the post-processing strategy proposed by Edwards and Allenby (2003) to obtain identified parameters, that is, \\(\\tilde{\\boldsymbol{\\beta}}=\\text{vec}\\left\\{\\boldsymbol{\\Lambda}\\mathbf{B}\\right\\}\\) and the correlation matrix \\(\\boldsymbol{R}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Sigma}\\boldsymbol{\\Lambda}\\), where \\(\\boldsymbol{\\Lambda}=\\text{diag}\\left\\{\\sigma_{ll}\\right\\}^{-1/2}\\) and \\(\\mathbf{B}=\\left[\\boldsymbol{\\beta}_1 \\ \\boldsymbol{\\beta}_2 \\dots \\boldsymbol{\\beta}_k\\right]\\).8 We assume independent priors: \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) and \\(\\boldsymbol{\\Sigma}^{-1} \\sim W(\\alpha_0, \\boldsymbol{\\Psi}_0)\\). We can apply Gibbs sampling to this model, as it is a standard Bayesian linear regression model when data augmentation in \\(\\boldsymbol{y}^*\\) is used. The posterior conditional distributions are \\[\\begin{equation} \\boldsymbol{\\beta}\\mid \\boldsymbol{\\Sigma},\\boldsymbol{w} \\sim N(\\boldsymbol{\\beta}_n,\\boldsymbol{B}_n), \\end{equation}\\] \\[\\begin{equation} \\boldsymbol{\\Sigma}^{-1} \\mid \\boldsymbol{\\beta},\\boldsymbol{w} \\sim W(\\alpha_n,\\boldsymbol{\\Psi}_n), \\end{equation}\\] \\[\\begin{equation} y_{il}^* \\mid \\boldsymbol{y}_{i,-l}^*,\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}^{-1},\\boldsymbol{y_i} \\sim TN_{I_{il}}(m_{il},\\tau_{ll}^2) \\end{equation}\\] where \\(\\boldsymbol{B}_n=(\\boldsymbol{B}_0^{-1}+\\boldsymbol{X}^{*\\top}\\boldsymbol{X}^*)^{-1}\\), \\(\\boldsymbol{\\beta}_n=\\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\boldsymbol{X}^{*\\top}\\boldsymbol{y}^{**})\\), \\(\\boldsymbol{\\Sigma}^{-1}=\\boldsymbol{C}^{\\top}\\boldsymbol{C}\\), \\(\\boldsymbol{X}_i^{*}=\\boldsymbol{C}^{\\top}\\boldsymbol{X}_i\\), \\(\\boldsymbol{y}_i^{**}=\\boldsymbol{C}^{\\top}\\boldsymbol{y}_i^*\\), \\(\\alpha_n=\\alpha_0+N\\), \\(\\boldsymbol{\\Psi}_n=(\\boldsymbol{\\Psi}_0+\\sum_{i=1}^N (\\boldsymbol{y}_i^*-\\boldsymbol{X}_i\\boldsymbol{\\beta})(\\boldsymbol{y}_i^*-\\boldsymbol{X}_i\\boldsymbol{\\beta})^{\\top})^{-1}\\), \\[ m_{il}=\\boldsymbol{x}_{il}^{\\top}\\boldsymbol{\\beta}+\\boldsymbol{f}_l^{\\top}(\\boldsymbol{y}_{i,-l}^*-\\boldsymbol{X}_{i,-l}\\boldsymbol{\\beta}), \\] where \\(\\boldsymbol{y}_{i,-l}^*\\) is an \\(L-1\\) dimensional vector of all components of \\(\\boldsymbol{y}_i^*\\) excluding \\(y_{il}^*\\), \\(\\boldsymbol{x}_{il}^{\\top}\\) is the \\(l\\)-th row of \\(\\boldsymbol{X}_i\\), \\(\\boldsymbol{X}_{i,-l}\\) is \\(\\boldsymbol{X}_{i}\\) after deleting the \\(l\\)-th row, \\[ \\boldsymbol{f}_l^{\\top}=\\boldsymbol{\\omega}_{l,-l}^{\\top}\\boldsymbol{\\Sigma}_{-l,-l}^{-1}, \\] where \\(\\boldsymbol{\\omega}_{l,-l}^{\\top}\\) and \\(\\boldsymbol{\\Sigma}_{-l,-l}\\) are the \\(l\\)-th row of \\(\\boldsymbol{\\Sigma}\\) extracting the \\(l\\)-th element, and the sub-matrix of \\(\\boldsymbol{\\Sigma}\\) extracting the \\(l,l\\) element, and \\[ \\tau_{ll}^2=\\sigma_{l,l}-\\boldsymbol{\\omega}_{l,-l}^{\\top}\\boldsymbol{\\Sigma}_{-l,-l}^{-1}\\boldsymbol{\\omega}_{-l,l}, \\] and \\[ \\boldsymbol{X}^*= \\begin{bmatrix} \\boldsymbol{X}_1^*\\\\ \\boldsymbol{X}_2^*\\\\ \\vdots\\\\ \\boldsymbol{X}_N^* \\end{bmatrix}, \\quad I_{il}= \\begin{Bmatrix} y_{il}^*&gt; 0, &amp; y_{il}=1\\\\ y_{il}^*\\leq 0 , &amp; y_{il}=0 \\end{Bmatrix}, \\quad \\boldsymbol{\\Sigma}= \\begin{bmatrix} \\boldsymbol{\\omega}_1^{\\top} \\\\ \\boldsymbol{\\omega}_2^{\\top} \\\\ \\vdots \\\\ \\boldsymbol{\\omega}_{L}^{\\top} \\end{bmatrix}. \\] The setting in our GUI has the same regressors in each binary decision. However, we can see that the multivariate probit model is similar to a SUR model in latent variables. We ask in Exercise 9 to implement a Gibbs sampling algorithm for a multivariate probit model with different regressors in each equation. Example: Self selection in hospitalization due to a subsidized health care program We use the dataset 7HealthMed.csv, where the dependent variable is \\(y = \\left[\\text{Hosp} \\ \\text{SHI}\\right]^{\\top}\\), with \\(\\text{Hosp} = 1\\) if an individual was hospitalized in the year prior to the survey (0 otherwise), and \\(\\text{SHI} = 1\\) if the individual had subsidized health insurance (0 otherwise). Recall that our application of binary response models aimed to uncover the determinants of hospitalization in Medellín (Colombia), where one of the regressors was a binary indicator of participation in a subsidized health care program (Section 6.3). We can use a bivariate probit model if we suspect there is dependence between the decisions regarding these two variables. A priori, we would expect that being in a subsidized health care program increases the probability of hospitalization ceteris paribus, due to reduced costs for the patient. However, if an individual expects to be hospitalized in the future, and the factors influencing this decision are unobserved by the modeler, a feedback effect may exist from hospitalization to enrollment in the subsidized health care program. We considered seven regressors: a constant, gender (female), age, self-perception of health status (with categories fair, good, and excellent, using bad as the reference category), and the proportion of the individual’s age spent living in their neighborhood. The last variable attempts to account for social capital, which can affect enrollment in the subsidized health insurance program, as the target population is identified by the local government (Ramírez-Hassan and Guerra-Urzola 2021). The dataset includes 12,975 individuals who can “choose” two options: hospitalization and enrollment in the subsidized health insurance regime. The following Algorithm shows how to run a multivariate probit model using our GUI. Algorithm: Multivariate Probit Model Select Multivariate Models on the top panel Select Multivariate Probit model using the left radio button Upload the dataset, selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Write down the number of cross-sectional units in the Number of individuals: n box Write down the number of exogenous variables in the Number of exogenous variables: k box Write down the number of choices in the Number of choices: l box Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons We set 20,000 MCMC iterations with a thinning parameter equal to 5. The hyperparameters are \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}_{14}\\), \\(\\boldsymbol{B}_0 = 100\\boldsymbol{I}_{14}\\), \\(\\alpha_0 = 4\\), and \\(\\boldsymbol{\\Psi}_0 = 4\\boldsymbol{I}_2\\).9 rm(list = ls()); set.seed(010101) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/7HealthMed.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(Data); str(Data) ## The following object is masked from DataUtEst: ## ## id ## &#39;data.frame&#39;: 25950 obs. of 9 variables: ## $ id : int 1 1 2 2 3 3 4 4 5 5 ... ## $ y : int 0 1 0 1 0 1 0 1 0 0 ... ## $ Constant : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Female : int 0 0 1 1 1 1 1 1 0 0 ... ## $ Age : int 7 7 39 39 23 23 15 15 8 8 ... ## $ Fair : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Good : int 1 1 1 1 1 1 1 1 0 0 ... ## $ Excellent: int 0 0 0 0 0 0 0 0 1 1 ... ## $ PTL : num 0.43 0.43 0 0 0 0 0 0 0 0 ... p &lt;- 2; nd &lt;- 7; N &lt;- length(y)/p; y &lt;- y Xd &lt;- as.matrix(Data[seq(1, p*N, 2),3:9]) XcreateMP&lt;-function(p,nxs,nind,Data){ pandterm = function(message) { stop(message, call. = FALSE) } if (missing(nxs)) pandterm(&quot;requires number of regressors: include intercept if required&quot;) if (missing(nind)) pandterm(&quot;requires number of units (individuals)&quot;) if (missing(Data)) pandterm(&quot;requires dataset&quot;) if (nrow(Data)!=nind*2) pandterm(&quot;check dataset! number of units times number alternatives should be equal to dataset rows&quot;) XXDat&lt;-array(0,c(p,1+nxs,nind)) XX&lt;-array(0,c(p,nxs*p,nind)) YY&lt;-array(0,c(p,1,nind)) is&lt;- seq(p,nind*p,p) cis&lt;- seq(nxs,nxs*p+1,nxs) for(i in is){ j&lt;-which(i==is) XXDat[,,j]&lt;-as.matrix(Data[c((i-(p-1)):i),-1]) YY[,,j]&lt;-XXDat[,1,j] for(l in 1:p){ XX[l,((cis[l]-(nxs-1)):cis[l]),j]&lt;-XXDat[l,-1,j] } } return(list(y=YY,X=XX)) } Dat &lt;- XcreateMP(p = p, nxs = nd, nind = N, Data = Data) y&lt;-NULL; X&lt;-NULL for(i in 1:dim(Dat$y)[3]){ y&lt;-c(y,Dat$y[,,i]) X&lt;-rbind(X,Dat$X[,,i]) } DataMP = list(p=p, y=y, X=X) # Hyperparameters k &lt;- dim(X)[2]; b0 &lt;- rep(0, k); c0 &lt;- 1000 B0 &lt;- c0*diag(k); B0i &lt;- solve(B0) a0 &lt;- p - 1 + 3; Psi0 &lt;- a0*diag(p) Prior &lt;- list(betabar = b0, A = B0i, nu = a0, V = Psi0) # MCMC parameters mcmc &lt;- 20000; thin &lt;- 5; Mcmc &lt;- list(R = mcmc, keep = thin, nprint = 0) Results &lt;- bayesm::rmvpGibbs(Data = DataMP, Mcmc = Mcmc, Prior = Prior) ## Table of y values ## y ## 0 1 ## 15653 10297 ## ## Starting Gibbs Sampler for MVP ## 12975 obs of 2 binary indicators; 14 indep vars (including intercepts) ## ## Prior Parms: ## betabar ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [2,] 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [3,] 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [4,] 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [5,] 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [6,] 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## [7,] 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 ## [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 ## [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 ## [10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 ## [11,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 ## [12,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 ## [13,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [14,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [,13] [,14] ## [1,] 0.000 0.000 ## [2,] 0.000 0.000 ## [3,] 0.000 0.000 ## [4,] 0.000 0.000 ## [5,] 0.000 0.000 ## [6,] 0.000 0.000 ## [7,] 0.000 0.000 ## [8,] 0.000 0.000 ## [9,] 0.000 0.000 ## [10,] 0.000 0.000 ## [11,] 0.000 0.000 ## [12,] 0.000 0.000 ## [13,] 0.001 0.000 ## [14,] 0.000 0.001 ## nu ## [1] 4 ## V ## [,1] [,2] ## [1,] 4 0 ## [2,] 0 4 ## ## MCMC Parms: ## 20000 reps; keeping every 5 th draw nprint= 0 ## initial beta= 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## initial sigma= ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 ## betatilde1 &lt;- Results$betadraw[,1:7] / sqrt(Results$sigmadraw[,1]) summary(coda::mcmc(betatilde1)) ## ## Iterations = 1:4000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 4000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] -0.973678 0.12756 0.0020170 2.414e-03 ## [2,] 0.122745 0.04872 0.0007704 1.326e-03 ## [3,] 0.002941 0.00110 0.0000174 2.577e-05 ## [4,] -0.522298 0.11435 0.0018080 1.781e-03 ## [5,] -1.234641 0.11131 0.0017599 1.859e-03 ## [6,] -1.093871 0.13165 0.0020816 2.591e-03 ## [7,] -0.064123 0.05942 0.0009395 1.619e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 -1.2247602 -1.059257 -0.970870 -0.889741 -0.733411 ## var2 0.0269885 0.090098 0.121863 0.155585 0.220662 ## var3 0.0007652 0.002207 0.002925 0.003685 0.005049 ## var4 -0.7477149 -0.598898 -0.522549 -0.445173 -0.296718 ## var5 -1.4520842 -1.309922 -1.234633 -1.160468 -1.018396 ## var6 -1.3503717 -1.182381 -1.092511 -1.005472 -0.837939 ## var7 -0.1791758 -0.103506 -0.064418 -0.024499 0.051849 betatilde2 &lt;- Results$betadraw[,8:14] / sqrt(Results$sigmadraw[,4]) summary(coda::mcmc(betatilde2)) ## ## Iterations = 1:4000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 4000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.567199 0.1346170 2.128e-03 2.409e-03 ## [2,] 0.306014 0.0242202 3.830e-04 3.672e-04 ## [3,] 0.009125 0.0006758 1.068e-05 1.109e-05 ## [4,] -0.221882 0.1347631 2.131e-03 2.447e-03 ## [5,] -0.421087 0.1307593 2.067e-03 2.364e-03 ## [6,] -0.435578 0.1370951 2.168e-03 2.449e-03 ## [7,] 0.224500 0.0304865 4.820e-04 4.829e-04 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.306343 0.477265 0.564698 0.656616 0.82932 ## var2 0.258347 0.289819 0.305902 0.322116 0.35284 ## var3 0.007848 0.008656 0.009124 0.009591 0.01045 ## var4 -0.488810 -0.313532 -0.218459 -0.130373 0.04144 ## var5 -0.677686 -0.511529 -0.418139 -0.332415 -0.17322 ## var6 -0.703355 -0.527642 -0.433378 -0.341355 -0.16989 ## var7 0.164388 0.203623 0.224533 0.245306 0.28513 sigmadraw12 &lt;- Results$sigmadraw[,3] / (Results$sigmadraw[,1]*Results$sigmadraw[,4])^0.5 summary(coda::mcmc(sigmadraw12)) ## ## Iterations = 1:4000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 4000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## -0.003338 0.033076 0.000523 0.001288 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## -0.070515 -0.025009 -0.002895 0.018432 0.060986 The previous R code demonstrates how to obtain the posterior draws using the rmvpGibbs command from the bayesm package. The results suggest that females, older individuals, and those who self-assess their health as poor are more likely to be hospitalized. Furthermore, females, older individuals, and those with a poor or fair self-perception of health, who have lived a larger proportion of their life in their current neighborhood, are more likely to be enrolled in the subsidized health care system. However, the results indicate that there is no unobserved correlation between the two equations, as the 95% credible interval for the correlation is (-0.07, 0.06). References Edwards, Y. D., and G. M. Allenby. 2003. “Multivariate Analysis of Multiple Response Data.” Journal of Marketing Research 40: 321–34. Ramírez-Hassan, A., and R. Guerra-Urzola. 2021. “Bayesian Treatment Effects Due to a Subsidized Health Program: The Case of Preventive Health Care Utilization in Medellín (Colombia).” Empirical Economics 60: 1477–1506. In a Bayesian setting, a model can be non-identified; however, the posterior distribution of the model parameters exists as long as a proper prior distribution is specified Edwards and Allenby (2003).↩︎ Note that the order of the location coefficients in our GUI follows the equations, not the order of the regressors as in the theoretical setting presented in this section. This distinction is important for correctly setting the hyperparameters and interpreting the results of the location parameters.↩︎ "],["sec75.html", "7.5 Summary", " 7.5 Summary In this chapter, we present the setting and posterior distributions of the most common multivariate models. The multivariate framework allows us to address endogeneity issues by using the conditional distribution of a multivariate normal vector. Moreover, we always obtain posterior conditional distributions that belong to standard families (multivariate normal, Wishart, and truncated normal) in these models. This property enables the implementation of the Gibbs sampling algorithm for all these models. "],["sec76.html", "7.6 Exercises", " 7.6 Exercises Show that \\(\\mathbb{E}[u_1\\text{PAER}] = \\frac{\\alpha_1}{1 - \\beta_1\\alpha_1} \\sigma_1^2\\), assuming that \\(\\mathbb{E}[u_1 u_2] = 0\\), where \\(\\text{Var}(u_1) = \\sigma_1^2\\), in the example of the effect of institutions on per capita GDP. Show that \\(\\beta_1=\\pi_1/\\gamma_1\\), in the example of the effect of institutions on per capita GDP. The effect of institutions on per capita gross domestic product continues I Use the rmultireg command from the bayesm package to perform inference in the example of the effect of institutions on per capita GDP. Demand and supply simulation Given the structural demand-supply model: \\[ \\begin{aligned} q_i^d &amp;= \\beta_1 + \\beta_2 p_i + \\beta_3 y_i + \\beta_4 pc_i + \\beta_5 ps_i + u_{i1}, \\\\ q_i^s &amp;= \\alpha_1 + \\alpha_2 p_i + \\alpha_3 er_i + u_{i2}, \\end{aligned} \\] where \\(q^d\\) is demand, \\(q^s\\) is supply, \\(p\\), \\(y\\), \\(pc\\), \\(ps\\), and \\(er\\) are price, income, complementary price, substitute price, and exchange rate, respectively. Complementary and substitute prices refer to the prices of complementary and substitute goods for \\(q\\). Assume that \\[ \\boldsymbol{\\beta} = \\begin{bmatrix} 5 \\\\ -0.5 \\\\ 0.8 \\\\ -0.4 \\\\ 0.7 \\end{bmatrix}, \\quad \\boldsymbol{\\alpha} = \\begin{bmatrix} -2 \\\\ 0.5 \\\\ -0.4 \\end{bmatrix}, \\] \\(u_1 \\sim N(0, 0.5^2)\\), and \\(u_2 \\sim N(0, 0.5^2)\\). Additionally, assume that \\(y \\sim N(10, 1)\\), \\(pc \\sim N(5, 1)\\), \\(ps \\sim N(5, 1)\\), and \\(er \\sim N(15, 1)\\). Find the reduced-form model by using the condition that in equilibrium, demand and supply are equal, i.e., \\(q^d = q^s\\). This condition defines the observable quantity, \\(q\\). Simulate \\(p\\) and \\(q\\) from the reduced-form equations. Perform inference for the reduced-form model using the rmultireg command from the bayesm package. Use the posterior draws of the reduced-form parameters to perform inference for the structural parameters. Any issues? Hint: Are all structural parameters exactly identified? Utility demand continues Run the Utility demand application using our GUI and the information in the dataset Utilities.csv. Hint: This file should be modified to agree with the structure that our GUI requires (see the dataset 5Institutions.csv in the folder DataApp of our GitHub repository - https://github.com/besmarter/BSTApp - for a template). Program from scratch the Gibbs sampler algorithm in this application. Simulation exercise of instrumental variables continues I Use the setting of the simulation exercise with instrumental variables to analyze the impact of a weak instrument. For instance, set \\(\\gamma_2 = 0.2\\) and compare the performance of the posterior means of the ordinary and instrumental variable models. Perform a simulation to analyze how the degree of exogeneity of the instrument affects the performance of the posterior mean in the instrumental variable model. Simulation exercise of instrumental variables continues II Program from scratch the Gibbs sampling algorithm of the instrumental model for the simulation exercise of the instrumental variables. The effect of institutions on per capita gross domestic product continues II Estimate the structural Equation (7.1) using the instrumental variable model where the instrument of PAER is \\(\\log(\\textit{Mort})\\). Compare the effect of property rights on per capita GDP of this model with the effect estimated in the example of the effect of institutions on per capita gross domestic product. Use the file 6Institutions.csv to do this exercise in our GUI, and set \\[ \\boldsymbol{B}_0=100\\boldsymbol{I}_5, \\quad \\boldsymbol{\\beta}_0=\\boldsymbol{0}_5, \\quad \\boldsymbol{\\gamma}_0=\\boldsymbol{0}_2, \\quad \\boldsymbol{G}_0=100\\boldsymbol{I}_2, \\quad \\alpha_0=3, \\quad \\boldsymbol{\\Psi}_0=3\\boldsymbol{I}_2. \\] The MCMC iterations, burn-in, and thinning parameters are 50000, 1000, and 5, respectively. Multivariate probit with different regressors Let’s do a simulation exercise where \\[ \\begin{aligned} y_{i1}^* &amp;= 0.5 - 1.2x_{i11} + 0.7x_{i12} + 0.8x_{i3} + \\mu_{i1}, \\\\ y_{i2}^* &amp;= 1.5 - 0.8x_{i21} + 0.5x_{i22} + \\mu_{i2}, \\end{aligned} \\] with \\[ \\boldsymbol{\\Sigma}= \\begin{bmatrix} 1 &amp; 0.5 \\\\ 0.5 &amp; 1 \\end{bmatrix}, \\] where all regressors follow a standard normal distribution, and \\(N=5000\\). Use \\[ \\boldsymbol{\\beta}_0=\\boldsymbol{0}, \\quad \\boldsymbol{B}_0=1000\\boldsymbol{B}, \\quad \\alpha_0=4, \\quad \\boldsymbol{\\Psi}_0=4\\boldsymbol{I}_2. \\] Set the number of iterations to 2000 and a thinning parameter equal to 5. Perform inference using the setting of Section 7.4, that is, assuming that \\(x_{i3}\\) could have an effect on \\(y_{i2}\\). Program a Gibbs sampling algorithm taking into account that there are different regressors in each binary decision, that is, \\(x_{i3}\\) does not have an effect on \\(y_{i2}\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
