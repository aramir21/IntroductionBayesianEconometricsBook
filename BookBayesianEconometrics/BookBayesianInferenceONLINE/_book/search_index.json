[["Chap14.html", "Chapter 14 Approximate Bayesian methods", " Chapter 14 Approximate Bayesian methods Approximate Bayesian methods are a family of techniques designed to handle situations where the likelihood function lacks an analytical expression, is highly complex, or the problem is high-dimensional, whether due to a large parameter space or a massive dataset (Martin, Frazier, and Robert 2024). In the former case, traditional Markov Chain Monte Carlo (MCMC) and importance sampling algorithms fail to provide a solution. In the latter, these algorithms struggle to produce accurate estimates within a reasonable time frame, unless users modify them (see Chapter 12). However, there is no free lunch. Approximate Bayesian methods address these challenges at the cost of providing an approximation to the posterior distribution rather than the exact posterior. Nonetheless, asymptotic results show that the approximation improves as the sample size increases. In this chapter, I first present simulation-based approaches, which are designed to address situations where the likelihood is highly complex and may lack an analytical solution. In the second part, I introduce optimization approaches, which are intended to handle high-dimensional problems. Specifically, I discuss approximate Bayesian computation (ABC) and Bayesian synthetic likelihood (BSL), the two most common simulation-based approaches. Then, I present integrated nested Laplace approximations (INLA) and variational Bayes (VB), the two most common optimization approaches for high-dimensional problems. References Martin, Gael M, David T Frazier, and Christian P Robert. 2024. “Approximating Bayes in the 21st Century.” Statistical Science 39 (1): 20–45. "],["sec14_1.html", "14.1 Simulation-based approaches", " 14.1 Simulation-based approaches Taking into account the fundamental equation for performing parameter inference in the Bayesian framework, \\[\\begin{align*} \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) &amp; \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\times \\pi(\\boldsymbol{\\theta}), \\end{align*}\\] we see in Section 4.1 that MCMC algorithms, such as the Gibbs sampler (Section 4.1.1 and Metropolis-Hastings (Section 4.1.2), require evaluation of the likelihood function \\(p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\) in the posterior conditional distribution or the acceptance probability, respectively. This is also the case for importance sampling when calculating the importance weights (Section 4.2). Thus, what happens when the likelihood function does not have an analytical expression? This situation arises in many models involving unobserved heterogeneity (i.e., unobserved taste preferences), models defined by quantile functions (e.g., the g-and-k distribution), or dynamic equilibrium models (e.g., repeated game models). Simulation-based algorithms provide a Bayesian solution when we face this situation, namely, when the likelihood function lacks an analytical expression or is highly complex. The only requirement is that we must be able to simulate synthetic data from the model conditional on the parameters. Therefore, these algorithms obtain an approximation to the posterior draws by simulating from the prior distribution \\(\\pi(\\boldsymbol{\\theta})\\) and then using these draws to simulate from the likelihood \\(p(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\). 14.1.1 Approximate Bayesian computation Approximate Bayesian Computation (ABC) is designed to handle inferential situations where the likelihood function \\(p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\) is intractable or highly complex, with \\(\\boldsymbol{y} \\in \\mathbb{R}^N\\). It was introduced in population genetics by Tavaré et al. (1997) and Pritchard et al. (1999), and later generalized by Beaumont, Zhang, and Balding (2002). The basic intuitive origin of ABC appears to have been introduced by Rubin (1984). A growing body of literature explores its applications in biology, cosmology, finance, economics, and other fields. The requirement in ABC is the ability to simulate from the parametric model. The process begins by drawing samples from the prior distribution \\(\\pi(\\boldsymbol{\\theta})\\) multiple times, where \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^K\\), and then simulating data from the model given each draw \\(\\boldsymbol{\\theta}^{(s)}\\), for \\(s = 1, 2, \\dots, S\\). The resulting synthetic data, \\(\\boldsymbol{z}^{(s)} \\in \\mathbb{R}^N\\), is used to compute summary statistics \\(\\boldsymbol{\\eta}(\\boldsymbol{z}^{(s)}) \\in \\mathbb{R}^L\\), with \\(L \\geq K\\). These summary statistics are crucial for the performance of ABC and should be selected based on a thorough understanding of the model. Next, we compare the synthetic summary statistics with the observed summary statistics \\(\\boldsymbol{\\eta}(\\boldsymbol{y})\\) using a distance metric \\(d\\left\\{ \\boldsymbol\\eta(\\boldsymbol{y}), \\boldsymbol\\eta(\\boldsymbol{z}^{(s)}) \\right\\}\\), typically the Euclidean distance. We retain the prior draws that generate synthetic summary statistics closest to the observed ones, that is, those for which \\[ d\\left\\{ \\boldsymbol\\eta(\\boldsymbol{y}), \\boldsymbol\\eta(\\boldsymbol{z}^{(s)}) \\right\\} \\leq \\epsilon, \\] forming an approximation of the posterior distribution \\[ \\pi_{\\epsilon}(\\boldsymbol{\\theta}, \\boldsymbol{z} \\mid \\boldsymbol{\\eta}(\\boldsymbol{y})). \\] The simplest algorithm is the accept/reject approximate Bayesian computation (ABC-AR): Algorithm: Accept/reject ABC For \\(s = 1, \\dots, S\\): Draw \\(\\boldsymbol{\\theta}^{(s)}\\) from the prior \\(\\pi(\\boldsymbol{\\theta})\\) Simulate \\(\\boldsymbol{z}^{(s)} = (z_1^{(s)}, z_2^{(s)}, \\dots, z_n^{(s)})^\\top\\) from the model \\(p(\\cdot \\mid \\boldsymbol{\\theta}^{(s)})\\) Calculate the distance \\[ d_{(s)} = d\\left\\{ \\boldsymbol{\\eta}(\\boldsymbol{y}), \\boldsymbol{\\eta}(\\boldsymbol{z}^{(s)}) \\right\\} \\] End for Order the distances: \\[ d_{(1)} \\leq d_{(2)} \\leq \\cdots \\leq d_{(S)} \\] Select all \\(\\boldsymbol{\\theta}^{(s)}\\) such that \\(d_{(s)} \\leq \\epsilon\\), where \\(\\epsilon &gt; 0\\) is the tolerance level. Note that the posterior distribution is conditional on the summary statistics \\(\\boldsymbol{\\eta}(\\boldsymbol{y})\\) and the tolerance parameter \\(\\epsilon\\). This implies that we obtain an approximation to the target distribution \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\), that is, \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}(\\boldsymbol{y}))\\), because \\(\\boldsymbol{\\eta}(\\boldsymbol{y})\\) is not a sufficient statistic in most cases, and \\(\\epsilon &gt; 0\\); these conditions introduce bias (Blum 2010). However, ABC performs well compared to full-likelihood approaches in low-dimensional parameter spaces (Beaumont, Zhang, and Balding 2002). Furthermore, D. T. Frazier et al. (2018) show in Theorems 1 and 2 that Bayesian consistency and asymptotic normality hold, provided that \\(\\epsilon \\to 0\\) fast enough as \\(N \\to +\\infty\\). In particular, the requirement is that the proportion of accepted draws converges to 0 at a rate faster than \\(N^{-K / 2}\\). Additionally, Theorem 2 in D. T. Frazier et al. (2018) shows that \\(100(1 - \\alpha)\\%\\) Bayesian credible regions using ABC have frequentist coverage of \\(100(1 - \\alpha)\\%\\). We should note from these asymptotic results that ABC suffers from the curse of dimensionality. Specifically, given a sample size of 1,000 and two parameters, the proportion of accepted draws should be 0.1%, meaning we would require one million prior draws to obtain 1,000 posterior draws. On the other hand, if the number of parameters is three, we would require 31.62 million prior draws. This limitation of ABC has attracted attention; see Chapter 8 of Sisson et al. (2018) for some potential solutions. It is common practice in ABC to perform a regression adjustment after retaining the draws (Beaumont, Zhang, and Balding 2002; Leuenberger and Wegmann 2010; Sisson et al. 2018). This adjustment reduces bias in posterior draws by performing a simple linear regression between the selected draws and the discrepancy between the observed and simulated summary statistics: \\[ {\\theta}^{(s)}_k = \\alpha_k + \\left(\\boldsymbol{\\eta}(\\boldsymbol{y}) - \\boldsymbol{\\eta}(\\boldsymbol{z}^{(s)})\\right)^{\\top} \\boldsymbol{\\beta}_k + \\mu^{(s)}_k, \\quad k = 1, 2, \\dots, K \\] Then, the posterior draws are adjusted using the slope estimate: \\[ {\\theta}^{\\text{adj},(s)}_k = {\\theta}^{(s)}_k - \\left(\\boldsymbol{\\eta}(\\boldsymbol{y}) - \\boldsymbol{\\eta}(\\boldsymbol{z}^{(s)})\\right)^{\\top} \\hat{\\boldsymbol{\\beta}}_k \\] Other regression adjustment strategies are also used, such as local linear regression, ridge regression, and neural networks. See the abc package in R. The favorable asymptotic sampling properties of ABC rely on correct model specification. D. T. Frazier, Robert, and Rousseau (2020) demonstrate that when the assumed model is misspecified, the asymptotic behavior of ABC can deteriorate. In particular, the posterior shape becomes asymptotically non-Gaussian, and the behavior of the posterior mean remains generally unknown. Additionally, regression adjustment approaches can produce posteriors that differ significantly from their simpler accept/reject counterparts. Given these concerns, testing model specification in ABC is essential. This can be done using simulated goodness-of-fit statistics (Bertorelle, Benazzo, and Mona 2010; Lintusaari et al. 2017), predictive p-values (Bertorelle, Benazzo, and Mona 2010), discrepancy diagnostics (D. T. Frazier, Robert, and Rousseau 2020), and asymptotic tests (Ramı́rez-Hassan and Frazier 2024) to evaluate model adequacy. The accept/reject ABC algorithm is inefficient, as all draws are independent; thus, there is no learning from previous draws. This intensifies the computational burden. Therefore, Marjoram et al. (2003) and Wegmann, Leuenberger, and Excoffier (2009) introduced Markov Chain Monte Carlo ABC (ABC-MCMC) algorithms, and S. A. Sisson, Fan, and Tanaka (2007), C. C. Drovandi and Pettitt (2011a), Del Moral, Doucet, and Jasra (2012), and Lenormand, Jabot, and Deffuant (2013) proposed sequential Monte Carlo approaches (ABC-SMC). However, results comparing ABC-MCMC and ABC-SMC with ABC-AR are controversial regarding computational efficiency (Bertorelle, Benazzo, and Mona 2010). In addition, ABC-AR is very simple and easily allows parallel computing (D. T. Frazier et al. 2019). Nevertheless, ABC-SMC is now the recommended approach, as it does not require tuning the algorithm’s tolerance (Martin, Frazier, and Robert 2024), and there are open-source implementations that facilitate its use. New developments in ABC have focused on using empirical measures calculated from the observed (\\(\\hat{\\mu}_n\\)) and synthetic (\\(\\hat{\\mu}_{\\boldsymbol{\\theta}}^{(s)}\\)) data to replace summary statistics. Thus, \\(d\\left\\{ \\boldsymbol\\eta (\\boldsymbol y), \\boldsymbol \\eta (\\boldsymbol z^{(s)}) \\right\\}\\) is replaced by \\({D}\\left\\{ \\hat{\\mu}_n, \\hat{\\mu}_{\\boldsymbol{\\theta}}^{(s)} \\right\\}\\), where the latter is a discrepancy measure, such as the Kullback-Leibler divergence (Jiang 2018). However, C. Drovandi and Frazier (2022) found in their simulation exercises that the best-performing summary statistics approach performs at least as well as the best discrepancy-measure approaches. The key point is to select informative summary statistics. Example: g-and-k distribution for financial returns The g-and-k distribution is a highly flexible distribution capable of capturing skewness and heavy tails through its parameters. This makes it particularly useful for modeling real-world data that deviate from normality, especially in fields like finance, where outliers are common. However, this distribution lacks a closed-form expression for its density function. The g-and-k distribution is defined by its quantile function (C. C. Drovandi and Pettitt 2011b). Specifically, it is specified through its inverse cumulative distribution function, \\[ Q(p \\mid \\theta) = F^{-1}(p \\mid \\theta), \\] where \\(F = P(U \\leq u)\\), and \\(Q\\) represents the \\(p\\)-quantile (Rayner and MacGillivray 2002). The quantile function of the g-and-k distribution is given by \\[ Q^{gk}\\left\\{z(p) \\mid a, b, c, g, k\\right\\} = a + b\\left[1 + c \\frac{1 - \\exp\\left\\{-gz(p)\\right\\}}{1 + \\exp\\left\\{-gz(p)\\right\\}}\\right] \\left\\{1 + z(p)^2\\right\\}^k z(p), \\] where \\(z(p)\\) is the standard normal quantile function, and \\(c = 0.8\\) is a commonly suggested value. In the g-and-k distribution, \\(a\\) is the location parameter, and \\(b\\) is the scale parameter, controlling the dispersion. The parameters \\(g\\) and \\(k\\) determine the levels of skewness and kurtosis, respectively, while \\(c\\) modifies the impact of skewness, and is typically set to 0.8. C. C. Drovandi and Pettitt (2011b) propose a moving average of order one using a g-and-k distribution to model exchange rate log returns. In particular, \\[ z_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1}, \\quad t = 1, \\dots, 524, \\] where \\(\\epsilon_t \\sim N(0,1)\\). The values of \\(z_t\\) are then divided by \\((1 + \\theta_1^2)^{1/2}\\) to ensure that they marginally follow a standard normal distribution. Thus, simulating g-and-k data requires only substituting \\(z_t\\) into the quantile function. We model exchange rate log daily returns from USD/EUR one year before and after the WHO declared the COVID-19 pandemic on 11 March 2020. We use the dataset ExchangeRate.csv from our GitHub repository. Our ABC implementation uses twelve summary statistics: the seven octiles, the interquartile range, robust measures of skewness and kurtosis, and the autocorrelations of order one and two (see C. C. Drovandi and Pettitt (2011b) and code below). We adopt the prior distributions proposed by Ramı́rez-Hassan and Frazier (2024): \\[ \\theta_1 \\sim U(-1,1), \\quad a \\sim U(0,5), \\quad b \\sim U(0,5), \\quad g \\sim U(-5,5), \\quad k \\sim U(-0.5, 5) \\] We use the EasyABC package in R to implement the ABC accept/reject (ABC-AR) algorithm using 150,000 prior draws with an acceptance rate of 0.67%. We also apply the ABC Markov chain Monte Carlo (ABC-MCMC) method (Marjoram et al. 2003) and the sequential Monte Carlo ABC (ABC-SMC) method (Lenormand, Jabot, and Deffuant 2013) to compare the results across different ABC algorithms.1 We generate 100,000 samples and retain 1% in ABC-MCMC, and 30,000 samples, keeping 3.4%, with a stopping criterion of 5% in ABC-SMC. These settings imply that the three algorithms require approximately the same computational time. As ABC is a simulation-based method, the computational burden is mostly driven by the speed of simulating the process; thus, the EasyABC package has parallel computing algorithms to speed up the processes. Users can refer to the cited references for algorithmic details, and the EasyABC package for parallel computing implementation. In Exercise 1, we ask to program the ABC accept/reject algorithm from scratch and compare the results with those obtained using the ABC-AR implementation in the EasyABC package. The following code presents the results, and the figure compares the posterior distributions of \\(\\theta_1\\), \\(g\\), and \\(k\\) using the three methods. In this figure, we observe that ABC-MCMC (red) and ABC-SMC (green) exhibit similar performance, and both approaches provide more information than ABC-AR (blue). There is marginal positive evidence that the moving average coefficient is positive, and the three algorithms yield similar means, although ABC-AR exhibits lower precision. Since the posterior distribution of \\(g\\) is centered around zero, the distribution appears to be symmetric around its median. Meanwhile, a positive \\(k\\) indicates that the distribution has heavier tails than a normal distribution, implying a higher likelihood of extreme values (outliers) in the exchange rate USD/EURO. ######### ABC Exchange rate og returns: USD/EURO rm(list = ls()); set.seed(010101) library(EasyABC) dfExcRate &lt;- read.csv(file = &quot;https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/ExchangeRate.csv&quot;, sep = &quot;,&quot;, header = T) attach(dfExcRate); n &lt;- length(USDEUR) # Summary statistics SumSt &lt;- function(y) { Oct &lt;- quantile(y, c(0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875)) eta1 &lt;- Oct[6] - Oct[2] eta2 &lt;- (Oct[6] + Oct[2] - 2 * Oct[4]) / eta1 eta3 &lt;- (Oct[7] - Oct[5] + Oct[3] - Oct[1]) / eta1 autocor &lt;- acf(y, lag = 2, plot = FALSE) autocor[[&quot;acf&quot;]][2:3] Etay &lt;- c(Oct, eta1, eta2, eta3, autocor[[&quot;acf&quot;]][2:3]) return(Etay) } # g-and-k distribution RGKnewSum &lt;- function(par) { z &lt;- NULL theta &lt;- par[1]; a &lt;- par[2]; b &lt;- par[3]; g &lt;- par[4]; k &lt;- par[5] e &lt;- rnorm(n + 1) for(t in 2:(n + 1)){ zt &lt;- e[t] + theta * e[t-1] z &lt;- c(z, zt) } zs &lt;- z / (1 + theta^2)^0.5 x &lt;- a + b * (1 + 0.8 * (1 - exp(-g * zs)) / (1 + exp(-g * zs))) * (1 + zs^2)^k * zs Etaz &lt;- SumSt(x) return(Etaz) } toy_prior &lt;- list(c(&quot;unif&quot;,-1,1), c(&quot;unif&quot;,0,5), c(&quot;unif&quot;, 0,5), c(&quot;unif&quot;, -5,5), c(&quot;unif&quot;, -0.5,5)) sum_stat_obs &lt;- SumSt(USDEUR) tick &lt;- Sys.time() ABC_AR &lt;- ABC_rejection(model=RGKnewSum, prior=toy_prior, summary_stat_target = sum_stat_obs, nb_simul=150000, tol = 0.0067, progress_bar = TRUE) tock &lt;- Sys.time() tock - tick PostABCAR &lt;- coda::mcmc(ABC_AR$param) summary(PostABCAR) tick &lt;- Sys.time() ABC_MCMC &lt;- ABC_mcmc(method=&quot;Marjoram&quot;, model=RGKnewSum, prior=toy_prior, summary_stat_target=sum_stat_obs, n_rec = 100000, progress_bar = TRUE) tock &lt;- Sys.time() tock - tick PostABCMCMC &lt;- coda::mcmc(ABC_MCMC[[&quot;param&quot;]][order(ABC_MCMC[[&quot;dist&quot;]])[1:1000],]) summary(PostABCMCMC) tick &lt;- Sys.time() ABC_SMC&lt;-ABC_sequential(method=&quot;Lenormand&quot;, model=RGKnewSum, prior=toy_prior, summary_stat_target=sum_stat_obs, nb_simul = 30000, alpha = 0.034, p_acc_min = 0.05, progress_bar = TRUE) tock &lt;- Sys.time() tock - tick PostABCSMC &lt;- coda::mcmc(ABC_SMC[[&quot;param&quot;]]) summary(PostABCSMC) # Figures library(ggplot2); library(latex2exp) Sp &lt;- 1000 df1 &lt;- data.frame( Value = c(PostABCAR[1:Sp,1], PostABCMCMC[1:Sp,1], PostABCSMC[1:Sp,1]), Distribution = factor(c(rep(&quot;AR&quot;, Sp), rep(&quot;MCMC&quot;, Sp), rep(&quot;SMC&quot;, Sp))) ) dentheta &lt;- ggplot(df1, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = TeX(&quot;Posterior density plot: $theta$&quot;), x = TeX(&quot;$theta$&quot;), y = &quot;Posterior density&quot;) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;)) + theme_minimal() + theme(legend.title = element_blank()) df2 &lt;- data.frame( Value = c(PostABCAR[1:Sp,4], PostABCMCMC[1:Sp,4], PostABCSMC[1:Sp,4]), Distribution = factor(c(rep(&quot;AR&quot;, Sp), rep(&quot;MCMC&quot;, Sp), rep(&quot;SMC&quot;, Sp))) ) deng &lt;- ggplot(df2, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = &quot;Posterior density plot: g&quot;, x = &quot;g&quot;, y = &quot;Posterior density&quot;) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;)) + theme_minimal() + theme(legend.title = element_blank()) df3 &lt;- data.frame( Value = c(PostABCAR[1:Sp,5], PostABCMCMC[1:Sp,5], PostABCSMC[1:Sp,5]), Distribution = factor(c(rep(&quot;AR&quot;, Sp), rep(&quot;MCMC&quot;, Sp), rep(&quot;SMC&quot;, Sp))) ) denk &lt;- ggplot(df3, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = &quot;Posterior density plot: k&quot;, x = &quot;k&quot;, y = &quot;Posterior density&quot;) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;)) + theme_minimal() + theme(legend.title = element_blank()) library(ggpubr) ggarrange(dentheta, deng, denk, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), ncol = 3, nrow = 1, legend = &quot;bottom&quot;, common.legend = TRUE) 14.1.2 Bayesian synthetic likelihood Note that in ABC, in most cases, we target the posterior distribution \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}(\\boldsymbol{y}))\\) rather than \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\), as \\(\\boldsymbol{\\eta}(\\boldsymbol{y})\\) is not a sufficient statistic, \\(\\boldsymbol{y} \\in \\mathbb{R}^N\\), \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^K\\), \\(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\in \\mathbb{R}^L\\), with \\(L \\geq K\\). This can be beneficial since discarding information may improve the behavior of the likelihood or make the inference more robust to model misspecification (Price et al. 2018). Given the intractability of \\(p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\), it is highly likely that \\(p(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta})\\) is also intractable. Wood (2010) addressed this issue by introducing an auxiliary model for the summary statistics, assuming \\[ p_a(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}) = {N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}). \\] Bayesian synthetic likelihood (BSL) arises when this auxiliary likelihood is combined with a prior distribution on the parameter (C. C. Drovandi, Pettitt, and Lee 2015; Price et al. 2018): \\[ \\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}(\\boldsymbol{y})) \\propto p_a(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}), \\] where the subscript \\(a\\) indicates that this is an approximation due to the Gaussian assumption. This is referred to as the idealized BSL posterior. However, note that \\(p_a(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta})\\) is rarely available, as \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}\\) and \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}\\) are generally unknown. Therefore, we estimate these quantities using simulations from the model given a realization of \\(\\boldsymbol{\\theta}\\).2 We estimate the mean and covariance as follows: \\[ \\widehat{\\boldsymbol{\\mu}}_{\\boldsymbol{\\theta}} = \\frac{1}{M} \\sum_{m=1}^M \\boldsymbol{\\eta}(\\boldsymbol{z}^{(m)}), \\] \\[ \\widehat{\\boldsymbol{\\Sigma}}_{\\boldsymbol{\\theta}} = \\frac{1}{M - 1} \\sum_{m=1}^M (\\boldsymbol{\\eta}(\\boldsymbol{z}^{(m)}) - \\widehat{\\boldsymbol{\\mu}}_{\\boldsymbol{\\theta}})(\\boldsymbol{\\eta}(\\boldsymbol{z}^{(m)}) - \\widehat{\\boldsymbol{\\mu}}_{\\boldsymbol{\\theta}})^\\top. \\] Then, we have: \\[ \\pi_{a,M}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\eta}(\\boldsymbol{y})) \\propto p_{a,M}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}), \\] where \\(p_{a,M}(\\boldsymbol{\\eta}(\\boldsymbol{y}))\\) uses the estimates, which depend on the number of draws \\(M\\). Note that even though we can have unbiased estimators for the mean and covariance, in general, \\[ {N}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\widehat{\\boldsymbol{\\mu}}_{\\boldsymbol{\\theta}}, \\widehat{\\boldsymbol{\\Sigma}}_{\\boldsymbol{\\theta}}) \\] is not an unbiased estimator of \\[ {N}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}). \\] An, South, and Drovandi (2022) show an unbiased estimator for BSL. D. Frazier et al. (2023) show that \\(\\pi_{a,M}(\\boldsymbol{\\theta})\\) converges asymptotically to a Gaussian distribution and that the \\(100(1 - \\alpha)\\%\\) Bayesian credible regions using BSL have frequentist coverage of \\(100(1 - \\alpha)\\%\\). The posterior mean is also asymptotically Gaussian. These results require convenient estimation of the covariance matrix and that \\(M \\to \\infty\\) as \\(N \\to \\infty\\): \\[ M = C \\lfloor N^\\gamma \\rfloor, \\quad C &gt; 0, \\quad \\gamma &gt; 0, \\] where \\(\\lfloor x \\rfloor\\) is the floor function. Thus, the choice of \\(M\\) does not drastically affect the asymptotic properties of BSL (D. Frazier et al. 2023). Price et al. (2018) also find in their examples that posterior inference depends only weakly on \\(M\\). Therefore, \\(M\\) can be chosen to balance computational efficiency, such that the standard deviation of the log synthetic likelihood is between 1 and 3 (An et al. 2019). A critical aspect of BSL is the estimation of the covariance matrix, which can be computationally demanding in high-dimensional settings. However, D. Frazier et al. (2023) propose an adjusted BSL approach that allows the use of a simple, though potentially misspecified, covariance estimator (see Equation 5 and the related discussion in their paper). If the normality assumption for summary statistics is too restrictive, An, Nott, and Drovandi (2020) propose a robust BSL method based on a semi-parametric approach. Moreover, D. T. Frazier and Drovandi (2021) show that when the model is misspecified (i.e., the assumed model is incompatible with the true data-generating process), BSL may yield unreliable parameter inference. They propose a new BSL method that detects model misspecification and produces more reliable inference. We can perform BSL using the following Algorithm. We can use a random walk Metropolis-Hastings to set the proposal distribution. The covariance matrix of the random walk proposal can be tuned using an initial pilot run. Algorithm: Bayesian Synthetic Likelihood For \\(s = 1, \\dots, S\\): Draw \\(\\boldsymbol{\\theta}^c \\sim q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{s-1})\\) For \\(m = 1, \\dots, M\\): Simulate \\(\\boldsymbol{z}^{(m)} = (z_1^{(m)}, z_2^{(m)}, \\dots, z_n^{(m)})^{\\top}\\) from the model \\(p(\\cdot \\mid \\boldsymbol{\\theta}^c)\\) Calculate \\(\\boldsymbol{\\eta}(\\boldsymbol{z}^{(m)})\\) End for Calculate \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^c}\\) and \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^c}\\) Compute \\[ p_a^c(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}^c) = {N}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^c}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^c}) \\] and \\[ p_a^{s-1}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}^{s-1}) = {N}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^{s-1}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^{s-1}}) \\] Compute the acceptance probability: \\[ \\alpha(\\boldsymbol{\\theta}^{s-1}, \\boldsymbol{\\theta}^c) = \\min\\left\\{1,\\, \\frac{ p_a^c(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}^c)\\,\\pi(\\boldsymbol{\\theta}^c)\\,q(\\boldsymbol{\\theta}^{s-1} \\mid \\boldsymbol{\\theta}^c) }{ p_a^{s-1}(\\boldsymbol{\\eta}(\\boldsymbol{y}) \\mid \\boldsymbol{\\theta}^{s-1})\\,\\pi(\\boldsymbol{\\theta}^{s-1})\\,q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{s-1}) } \\right\\} \\] Draw \\(u \\sim {U}(0,1)\\) If \\(u &lt; \\alpha\\), then: Set \\(\\boldsymbol{\\theta}^s = \\boldsymbol{\\theta}^c\\) \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^s} = \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^c}\\) \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^s} = \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^c}\\) Else: Set \\(\\boldsymbol{\\theta}^s = \\boldsymbol{\\theta}^{s-1}\\) \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^s} = \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}^{s-1}}\\) \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^s} = \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}^{s-1}}\\) End for An advantage of BSL over ABC is that it does not require selecting a tolerance parameter \\(\\epsilon\\). Furthermore, BSL is more computationally efficient than ABC when dealing with a high-dimensional vector of summary statistics, as the acceptance rate of the former is asymptotically non-vanishing (D. Frazier et al. 2023). On the other hand, ABC is asymptotically more efficient than BSL, and it imposes very weak requirements on the choice of summary statistics, whereas BSL requires summary statistics that satisfy central limit theorems (CLTs) and consistent estimators of the covariance matrix. However, asymptotically, both approaches provide reliable inference, provided their respective requirements are met (Martin, Frazier, and Robert 2024). In practice, BSL may be more convenient than ABC when the summary statistics are high-dimensional and satisfy CLT conditions. Otherwise, ABC may be the better alternative. Example: Simulation exercise We simulate a dataset following the specification of the g-and-k distribution for the financial returns example, setting \\(\\theta_1 = 0.8\\), \\(a = 1\\), \\(b = 0.5\\), \\(g = -1\\), and \\(k = 1\\), with a sample size of 500. We use the same priors and summary statistics as in that example. We use the BSL package in R to perform posterior inference using BSL. Additionally, we implement our own BSL sampler to compare the results with the vanilla algorithm from the package. We run the BSL algorithms with \\(M = 200\\), \\(S = 11{,}000\\), a burn-in of 1,000, and a thinning parameter of 10, using a random walk proposal distribution. This setting results in similar computational times between the two implementations (scratch and package) and is also comparable to the ABC implementation in Exercise 1. Take into account that BSL is a simulation-based method; therefore, the computational burden is mostly driven by the speed of simulating the process. The BSL package has parallel computing algorithms to speed up the processes, and users should refer to the BSL package for details. The following code demonstrates this procedure. The summary statistics appear to be approximately normally distributed, as shown in the first Figure, where the red line represents the normal density and the black line represents the estimated density of the summary statistic. The second Figure displays the posterior distributions from both algorithms for \\(\\theta_1\\), \\(g\\), and \\(k\\). In general, the 95% credible intervals encompass the true parameter values. However, the posterior draws from the BSL package are more informative compared to our implementation of Algorithm. Additionally, BSL outperforms the results of ABC in Exercise 1, yielding more precise posterior distributions. This pattern has been observed in other settings (C. Drovandi and Frazier 2022; Martin, Frazier, and Robert 2024). Both ABC and BSL produce posterior distributions centered at the true parameter values, although the posterior distribution of \\(\\theta_1\\) deviates from this pattern. ######## BSL: g-and-k simulation ############ rm(list = ls()); set.seed(010101); library(BSL) # Simulate g-and-k data RGKnew &lt;- function(par) { z &lt;- NULL theta &lt;- par[1]; a &lt;- par[2]; b &lt;- par[3]; g &lt;- par[4]; k &lt;- par[5] e &lt;- rnorm(n + 1) for(t in 2:(n + 1)){ zt &lt;- e[t] + theta * e[t-1] z &lt;- c(z, zt) } zs &lt;- z / (1 + theta^2)^0.5 x &lt;- a + b * (1 + 0.8 * (1 - exp(-g * zs)) / (1 + exp(-g * zs))) * (1 + zs^2)^k * zs return(x) } # Summary statistics SumSt &lt;- function(y) { Oct &lt;- quantile(y, c(0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875)) eta1 &lt;- Oct[6] - Oct[2] eta2 &lt;- (Oct[6] + Oct[2] - 2 * Oct[4]) / eta1 eta3 &lt;- (Oct[7] - Oct[5] + Oct[3] - Oct[1]) / eta1 autocor &lt;- acf(y, lag = 2, plot = FALSE) autocor[[&quot;acf&quot;]][2:3] Etay &lt;- c(Oct, eta1, eta2, eta3, autocor[[&quot;acf&quot;]][2:3]) return(Etay) } # Prior function LogPrior &lt;- function(par){ LogPi &lt;- log(par[1] &gt; -1 &amp; par[1] &lt; 1 &amp; par[2] &gt; 0 &amp; par[2] &lt; 5 &amp; par[3] &gt; 0 &amp; par[3] &lt; 5 &amp; par[4] &gt; -5 &amp; par[4] &lt; 5 &amp; par[5] &gt; -0.5 &amp; par[5] &lt; 5) return(LogPi) } # Population parameters theta1 &lt;- 0.8; a &lt;- 1; b &lt;- 0.5; g &lt;- -1; k &lt;- 1 parpop &lt;- c(theta1, a, b, g, k) K &lt;- 5 n &lt;- 500 y &lt;- RGKnew(par = parpop) # Algorithm parameters M &lt;- 200 # Number of iterations to calculate mu and sigma S &lt;- 11000 # Number of MCMC iterations burnin &lt;- 1000 # Burn in iterations thin &lt;- 10 # Thining parameter keep &lt;- seq(burnin + 1, S, thin) par0 &lt;- c(0.5, 2, 1, 0, 1) Modelgk &lt;- newModel(fnSim = RGKnew, fnSum = SumSt, theta0 = par0, fnLogPrior = LogPrior, verbose = FALSE) validObject(Modelgk) # Check if the summary statistics are roughly normal simgk &lt;- simulation(Modelgk, n = M, theta = par0, seed = 10) par(mfrow = c(4, 3)) for (i in 1:12){ eval &lt;- seq(min(simgk$ssx[, i]), max(simgk$ssx[, i]), 0.001) densnorm &lt;- dnorm(eval, mean = mean(simgk$ssx[, i]), sd(simgk$ssx[, i])) plot(density(simgk$ssx[, i]), main = &quot;&quot;, xlab = &quot;&quot;) lines(eval, densnorm, col = &quot;red&quot;) } ##### Program from scratch ####### Lims &lt;- matrix(c(-1, 0, 0, -5, -0.5, 1, rep(5, 4)), 5, 2) parPost &lt;- matrix(NA, S, K); parPost[1,] &lt;- par0 ; EtaY &lt;- SumSt(y = y) Zsl &lt;- replicate(M, RGKnew(par = parPost[1,])) EtasZsl &lt;- t(apply(Zsl, 2, SumSt)) Usl &lt;- colMeans(EtasZsl); SIGMAsl &lt;- var(EtasZsl) dnormsl &lt;- mvtnorm::dmvnorm(EtaY, Usl, SIGMAsl, log = TRUE) tune &lt;- 0.005; CoVarRW &lt;- tune*diag(K) accept &lt;- rep(0, S); tick1 &lt;- Sys.time() pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = S, width = 300) for (s in 2:S){ parc &lt;- MASS::mvrnorm(1, mu = parPost[s-1,], Sigma = CoVarRW) RestCheck &lt;- NULL for(j in 1:K){ if(parc[j] &lt; Lims[j,1] | parc[j] &gt; Lims[j,2]){ Rej &lt;- 1 }else{Rej &lt;- 0} RestCheck &lt;- c(RestCheck, Rej) } if(sum(RestCheck) != 0){ parPost[s,] &lt;- parPost[s-1,]; accept[s] &lt;- 0 }else{ Z &lt;- replicate(M, RGKnew(par = parc)) EtasZ &lt;- t(apply(Z, 2, SumSt)) Um &lt;- colMeans(EtasZ); SIGMAm &lt;- var(EtasZ) dnormc &lt;- mvtnorm::dmvnorm(EtaY, Um, SIGMAm, log = TRUE) if(s &gt; round(0.1*S, 0) &amp; s &lt; round(0.2*S, 0)){ CoVarRW &lt;- var(parPost, na.rm = TRUE) } if(dnormc == -Inf){ parPost[s,] &lt;- parPost[s-1,]; accept[s] &lt;- 0 }else{ alpha &lt;- min(1, exp(dnormc - dnormsl)); U &lt;- runif(1) if(U &lt;= alpha){ parPost[s,] &lt;- parc; Usl &lt;- Um; SIGMAsl &lt;- SIGMAm dnormsl &lt;- dnormc; accept[s] &lt;- 1 }else{ parPost[s,] &lt;- parPost[s-1,]; accept[s] &lt;- 0 } } } setWinProgressBar(pb, s, title=paste( round(s/S*100, 0),&quot;% done&quot;)) } close(pb); tock1 &lt;- Sys.time(); tock1 - tick1 mean(accept) PostChainOwn &lt;- coda::mcmc(parPost[keep,]) summary(PostChainOwn) #### BSL package tick &lt;- Sys.time() Resultsgk &lt;- bsl(y = y, n = M, M = S, model = Modelgk, covRandWalk = CoVarRW, method = &quot;BSL&quot;, thetaNames = expression(theta, a, b, g, k), plotOnTheFly = TRUE) tock &lt;- Sys.time(); tock - tick PostChain &lt;- coda::mcmc(Resultsgk@theta[keep,]) summary(PostChain) Resultsgk@acceptanceRate plot(Resultsgk@loglike[keep], type = &quot;l&quot;) sd(Resultsgk@loglike[keep]) # Figures library(ggplot2); library(latex2exp) Sp &lt;- length(keep) df1 &lt;- data.frame(Value = c(PostChain[,1], PostChainOwn[,1]), Distribution = factor(c(rep(&quot;BSL&quot;, Sp), rep(&quot;BSLscratch&quot;, Sp)))) dentheta &lt;- ggplot(df1, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = TeX(&quot;Posterior density plot: $theta$&quot;), x = TeX(&quot;$theta$&quot;), y = &quot;Posterior density&quot;) + geom_vline(xintercept = theta1, linetype = &quot;dashed&quot;, color = &quot;red&quot;, linewidth = 1) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + theme_minimal() + theme(legend.title = element_blank()) df2 &lt;- data.frame(Value = c(PostChain[,4], PostChainOwn[,4]), Distribution = factor(c(rep(&quot;BSL&quot;, Sp), rep(&quot;BSLscratch&quot;, Sp)))) deng &lt;- ggplot(df2, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = &quot;Posterior density plot: g&quot;, x = &quot;g&quot;, y = &quot;Posterior density&quot;) + geom_vline(xintercept = g, linetype = &quot;dashed&quot;, color = &quot;red&quot;, linewidth = 1) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + theme_minimal() + theme(legend.title = element_blank()) df3 &lt;- data.frame(Value = c(PostChain[,5], PostChainOwn[,5]), Distribution = factor(c(rep(&quot;BSL&quot;, Sp), rep(&quot;BSLscratch&quot;, Sp)))) denk &lt;- ggplot(df3, aes(x = Value, color = Distribution)) + geom_density(linewidth = 1) + labs(title = &quot;Posterior density plot: k&quot;, x = &quot;k&quot;, y = &quot;Posterior density&quot;) + geom_vline(xintercept = k, linetype = &quot;dashed&quot;, color = &quot;red&quot;, linewidth = 1) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + theme_minimal() + theme(legend.title = element_blank()) library(ggpubr) ggarrange(dentheta, deng, denk, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), ncol = 3, nrow = 1, legend = &quot;bottom&quot;, common.legend = TRUE) References An, Ziwen, David J Nott, and Christopher Drovandi. 2020. “Robust Bayesian Synthetic Likelihood via a Semi-Parametric Approach.” Statistics and Computing 30 (3): 543–57. An, Ziwen, Leah F South, and Christopher Drovandi. 2022. “BSL: An r Package for Efficient Parameter Estimation for Simulation-Based Models via Bayesian Synthetic Likelihood.” Journal of Statistical Software 101: 1–33. An, Ziwen, Leah F South, David J Nott, and Christopher C Drovandi. 2019. “Accelerating Bayesian Synthetic Likelihood with the Graphical Lasso.” Journal of Computational and Graphical Statistics 28 (2): 471–75. Beaumont, Mark A, Wenyang Zhang, and David J Balding. 2002. “Approximate Bayesian Computation in Population Genetics.” Genetics 162 (4): 2025–35. Bertorelle, Giorgio, Andrea Benazzo, and S Mona. 2010. “ABC as a Flexible Framework to Estimate Demography over Space and Time: Some Cons, Many Pros.” Molecular Ecology 19 (13): 2609–25. Blum, Michael GB. 2010. “Approximate Bayesian Computation: A Nonparametric Perspective.” Journal of the American Statistical Association 105 (491): 1178–87. Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. 2012. “An Adaptive Sequential Monte Carlo Method for Approximate Bayesian Computation.” Statistics and Computing 22: 1009–20. Drovandi, Christopher C, and Anthony N Pettitt. 2011a. “Estimation of Parameters for Macroparasite Population Evolution Using Approximate Bayesian Computation.” Biometrics 67 (1): 225–33. ———. 2011b. “Likelihood-Free Bayesian Estimation of Multivariate Quantile Distributions.” Computational Statistics &amp; Data Analysis 55 (9): 2541–56. Drovandi, Christopher C, Anthony N Pettitt, and Anthony Lee. 2015. “Bayesian Indirect Inference Using a Parametric Auxiliary Model.” Statistical Science 30 (1): 72--95. Drovandi, Christopher, and David T Frazier. 2022. “A Comparison of Likelihood-Free Methods with and Without Summary Statistics.” Statistics and Computing 32 (3): 42. Frazier, David T, and Christopher Drovandi. 2021. “Robust Approximate Bayesian Inference with Synthetic Likelihood.” Journal of Computational and Graphical Statistics 30 (4): 958–76. Frazier, David T, Worapree Maneesoonthorn, Gael M Martin, and Brendan PM McCabe. 2019. “Approximate Bayesian Forecasting.” International Journal of Forecasting 35 (2): 521–39. Frazier, David T, Gael M Martin, Christian P Robert, and Judith Rousseau. 2018. “Asymptotic Properties of Approximate Bayesian Computation.” Biometrika 105 (3): 593–607. Frazier, David T, Christian P Robert, and Judith Rousseau. 2020. “Model Misspecification in Approximate Bayesian Computation: Consequences and Diagnostics.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (2): 421–44. Frazier, David, David J Nott, Christopher Drovandi, and Robert Kohn. 2023. “Bayesian Inference Using Synthetic Likelihood: Asymptotics and Adjustments.” Journal of the American Statistical Association 118 (544): 2821--2832. Jiang, Bai. 2018. “Approximate Bayesian Computation with Kullback-Leibler Divergence as Data Discrepancy.” In International Conference on Artificial Intelligence and Statistics, 1711–21. PMLR. Lenormand, Maxime, Franck Jabot, and Guillaume Deffuant. 2013. “Adaptive Approximate Bayesian Computation for Complex Models.” Computational Statistics 28 (6): 2777–96. Leuenberger, Christoph, and Daniel Wegmann. 2010. “Bayesian Computation and Model Selection Without Likelihoods.” Genetics 184 (1): 243–52. Lintusaari, Jarno, Michael U Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. 2017. “Fundamentals and Recent Developments in Approximate Bayesian Computation.” Systematic Biology 66 (1): e66–82. Marjoram, Paul, John Molitor, Vincent Plagnol, and Simon Tavaré. 2003. “Markov Chain Monte Carlo Without Likelihoods.” Proceedings of the National Academy of Sciences of the USA 100 (26): 15324–28. Martin, Gael M, David T Frazier, and Christian P Robert. 2024. “Approximating Bayes in the 21st Century.” Statistical Science 39 (1): 20–45. Price, Leah F, Christopher C Drovandi, Anthony Lee, and David J Nott. 2018. “Bayesian Synthetic Likelihood.” Journal of Computational and Graphical Statistics 27 (1): 1–11. Pritchard, Jonathan K, Mark T Seielstad, Anna Perez-Lezaun, and Marcus W Feldman. 1999. “Population Growth of Human Y Chromosomes: A Study of Y Chromosome Microsatellites.” Molecular Biology and Evolution 16 (12): 1791–98. Ramı́rez-Hassan, Andrés, and David T. Frazier. 2024. “Testing Model Specification in Approximate Bayesian Computation Using Asymptotic Properties.” Journal of Computational and Graphical Statistics 33 (3): 1–14. Rayner, Glen D, and Helen L MacGillivray. 2002. “Numerical Maximum Likelihood Estimation for the g-and-k and Generalized g-and-h Distributions.” Statistics and Computing 12 (1): 57–75. Rubin, Donald B. 1984. “Bayesianly Justifiable and Relevant Frequency Calculations for the Applies Statistician.” The Annals of Statistics 12 (4): 1151–72. Sisson, Scott A, Yanan Fan, and Mark M Tanaka. 2007. “Sequential Monte Carlo Without Likelihoods.” Proceedings of the National Academy of Sciences of the USA 104 (6): 1760–65. Sisson, Scott A, Fan, Yanan, Beaumont, and Mark. 2018. Handbook of Approximate Bayesian Computation. CRC Press. Tavaré, Simon, David J Balding, Robert C Griffiths, and Peter Donnelly. 1997. “Inferring Coalescence Times from DNA Sequence Data.” Genetics 145 (2): 505–18. Wegmann, Daniel, Christoph Leuenberger, and Laurent Excoffier. 2009. “Efficient Approximate Bayesian Computation Coupled with Markov Chain Monte Carlo Without Likelihood.” Genetics 182 (4): 1207–18. Wood, Simon N. 2010. “Statistical Inference for Noisy Nonlinear Ecological Dynamic Systems.” Nature 466 (7310): 1102–4. Note that this setting does not satisfy the asymptotic requirements for Bayesian consistency. However, it serves as a pedagogical exercise.↩︎ There are better ways to calculate the covariance matrix, see D. Frazier et al. (2023).↩︎ "],["sec14_2.html", "14.2 Optimization approaches", " 14.2 Optimization approaches Traditional MCMC and importance sampling (IS) algorithms require pointwise evaluation of the likelihood function, which entails a massive number of operations when applied to very large datasets. Unfortunately, these algorithms are not designed to be scalable, at least in their standard form (see Chapter 12 for alternatives). Moreover, when the parameter space is large, they also lack scalability with respect to the number of parameters. Therefore, approximation methods should be considered even when the likelihood function has an analytical expression. Optimization approaches are designed to scale efficiently with high-dimensional parameter spaces and large datasets. The key idea is to replace simulation with optimization. In this section, we introduce the most common optimization approaches within the Bayesian inferential framework. 14.2.1 Integrated nested Laplace approximations Integrated Nested Laplace Approximations (INLA) is a deterministic approach for Bayesian inference in latent Gaussian models (LGMs) (Rue, Martino, and Chopin 2009). In particular, INLA approximates the marginal posterior distributions using a combination of Laplace approximations, low-dimensional deterministic integration, and optimization steps in sparse covariance settings (Rue et al. 2017). The advantages of INLA compared to MCMC are that it is fast and does not suffer from poor mixing. The point of departure is a structured additive regression model, where the response variable \\(y_i\\) belongs to the exponential family such that the mean \\(\\mu_i\\) is linked to a linear predictor \\(\\eta_i\\) through the link function \\(g(\\cdot)\\), that is, \\(\\eta_i = g(\\mu_i)\\), where \\[\\begin{equation*} \\mu_i = \\alpha + \\boldsymbol{\\beta}^{\\top}\\boldsymbol{x}_{i} + \\sum_{j=1}^{J}f^{(j)}(u_{ji}) + \\epsilon_{i}, \\end{equation*}\\] where \\(\\alpha\\) is the general intercept, \\(f^{(j)}\\) are unknown functions of the covariates \\(\\boldsymbol{u}_i\\), \\(\\boldsymbol{\\beta}\\) is a \\(K\\)-dimensional vector of linear effects associated with regressors \\(\\boldsymbol{x}_i\\), and \\(\\epsilon_{i}\\) is the unstructured error. Note that latent Gaussian models encompass a wide range of relevant empirical models depending on the specific elements and structure involved in \\(f^{(j)}\\), such as generalized linear models with unobserved heterogeneity, spatial and/or temporal dependence, and semi-parametric models. Latent Gaussian models assign a Gaussian prior to \\(\\alpha\\), \\(\\boldsymbol{\\beta}\\), \\(f^{(j)}\\), and \\(\\epsilon_i\\). Let \\(\\boldsymbol{z}\\) denote the vector of all latent Gaussian variables \\(\\{\\alpha, \\boldsymbol{\\beta}, f^{(j)}, \\eta_i\\}\\), where the dimension is potentially \\(P = 1 + K + J + N\\) (although this is not always the case), and let \\(\\boldsymbol{\\theta}\\) be the \\(m\\)-dimensional vector of hyperparameters. Then, the density \\(\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}_1)\\) is Gaussian with mean zero and precision matrix (i.e., the inverse of the covariance matrix) \\(\\boldsymbol{Q}(\\boldsymbol{\\theta}_1)\\). The distribution of \\(\\boldsymbol{y}\\) is \\(p(\\boldsymbol{y} \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}_2)\\) such that \\(y_i\\) are conditionally independent given \\(z_i, \\boldsymbol{\\theta}_2\\), and \\(\\boldsymbol{\\theta} = [\\boldsymbol{\\theta}_1^{\\top} \\ \\boldsymbol{\\theta}_2^{\\top}]^{\\top}\\) for \\(i = 1,2,\\dots,N\\). Thus, the posterior distribution is \\[\\begin{align} \\pi(\\boldsymbol{z}, \\boldsymbol{\\theta} \\mid \\boldsymbol{y}) &amp;\\propto \\pi(\\boldsymbol{\\theta}) \\times \\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}_1) \\times \\prod_{i=1}^{N} p(y_i \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}_2) \\tag{14.1} \\\\ &amp;\\propto \\pi(\\boldsymbol{\\theta}) \\, |\\boldsymbol{Q}(\\boldsymbol{\\theta}_1)|^{1/2} \\exp\\left\\{-\\frac{1}{2} \\boldsymbol{z}^{\\top} \\boldsymbol{Q}(\\boldsymbol{\\theta}_1) \\boldsymbol{z} + \\sum_{i=1}^N \\log p(y_i \\mid z_i, \\boldsymbol{\\theta}_2) \\right\\}.\\notag \\end{align}\\] Most models in INLA assume a conditional independence structure within the high-dimensional latent Gaussian field; that is, \\(\\boldsymbol{z}\\) is a Gaussian Markov random field (GMRF) with a sparse precision matrix \\(\\boldsymbol{Q}(\\boldsymbol{\\theta}_1)\\). A second key assumption is that the dimension of \\(\\boldsymbol{\\theta}\\) is small, for instance, \\(m &lt; 15\\). These assumptions are essential for enabling fast approximate inference. The main aim in INLA is to approximate the marginal posterior distributions \\(\\pi(z_i \\mid \\boldsymbol{y})\\) and \\(\\pi(\\theta_l \\mid \\boldsymbol{y})\\), for \\(l = 1, 2, \\dots, m\\). The posterior distribution of \\(\\boldsymbol{\\theta}\\) is: \\[\\begin{align*} \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) &amp;= \\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{p(\\boldsymbol{y})} \\\\ &amp;= \\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{p(\\boldsymbol{y})} \\times \\frac{\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})}{\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})} \\\\ &amp;= \\frac{p(\\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{y})}{p(\\boldsymbol{y}) \\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})} \\\\ &amp;\\propto \\frac{p(\\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{y})}{\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})} \\\\ &amp;\\propto \\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}) \\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})}. \\end{align*}\\] The numerator in the previous expression is easy to calculate (see Equation (14.1)), but the denominator is generally not available in closed form and is difficult to compute. Thus, INLA approximates it at specific values \\(\\boldsymbol{\\theta}_g\\) as follows: \\[\\begin{align*} \\pi_a(\\boldsymbol{\\theta}_g \\mid \\boldsymbol{y}) &amp;\\propto \\frac{p(\\boldsymbol{y} \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}_g) \\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}_g) \\pi(\\boldsymbol{\\theta}_g)}{\\pi_{a,G}(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}_g, \\boldsymbol{y})}, \\end{align*}\\] where \\(\\pi_{a,G}(z_i \\mid \\boldsymbol{\\theta}_g, \\boldsymbol{y})\\) is a Gaussian approximation that matches the mode and covariance matrix of the full posterior \\(\\pi(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})\\). The approximation error of \\(\\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) is \\(O(N^{-1})\\) under standard conditions, meaning that the error, when multiplied by \\(N\\), remains bounded (Tierney and Kadane 1986; Rue, Martino, and Chopin 2009). However, in many applications using INLA, the dimension of the latent Gaussian variables, \\(P\\), increases with the sample size. In such cases, the error rate becomes \\(O(P/N)\\). When \\(P/N \\rightarrow 1\\), which occurs in many models, the approximation error becomes \\(O(1)\\): bounded, but potentially large. Therefore, it is important to check the effective number of parameters, as the asymptotic error of \\(\\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) depends on the dimension of \\(\\boldsymbol{z}\\). According to Rue, Martino, and Chopin (2009), in most of their applications the effective number of parameters is small relative to the sample size in regions near the mode of \\(\\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\). The marginal posterior \\(\\pi(z_i \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})\\) is more challenging to compute due to the potentially high dimension of \\(\\boldsymbol{z}\\). It may seem intuitive to use the Gaussian approximation \\(\\pi_{a,G}(\\boldsymbol{z} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})\\); however, this is often not sufficiently accurate due to its lack of skewness. An alternative is to use the following expression \\[\\begin{align*} \\pi(z_i\\mid \\boldsymbol{\\theta},\\boldsymbol{y})&amp;=\\frac{p(z_i,\\boldsymbol{\\theta},\\boldsymbol{y})}{p(\\boldsymbol{\\theta},\\boldsymbol{y})}\\\\ &amp;=\\frac{p(z_i,\\boldsymbol{\\theta},\\boldsymbol{y})}{p(\\boldsymbol{\\theta},\\boldsymbol{y})}\\times \\frac{\\pi(\\boldsymbol{z}\\mid\\boldsymbol{\\theta},\\boldsymbol{y})}{\\pi(\\boldsymbol{z}\\mid\\boldsymbol{\\theta},\\boldsymbol{y})}\\\\ &amp;=\\frac{\\pi(\\boldsymbol{z}\\mid\\boldsymbol{\\theta},\\boldsymbol{y})}{p(\\boldsymbol{z},\\boldsymbol{\\theta},\\boldsymbol{y})}\\times p(z_i,\\boldsymbol{\\theta},\\boldsymbol{y})\\\\ &amp;=\\frac{\\pi(\\boldsymbol{z}\\mid\\boldsymbol{\\theta},\\boldsymbol{y})}{\\pi(\\boldsymbol{z}_{-i}\\mid z_i,\\boldsymbol{\\theta},\\boldsymbol{y})}\\\\ &amp;\\propto \\frac{p(\\boldsymbol{y}\\mid \\boldsymbol{z},\\boldsymbol{\\theta})\\pi(\\boldsymbol{z}\\mid\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{z}_{-i}\\mid z_i,\\boldsymbol{\\theta},\\boldsymbol{y})}, \\end{align*}\\] where the second-to-last equality follows from the identity \\(\\pi(\\boldsymbol{z}_{-i} \\mid z_i, \\boldsymbol{\\theta}, \\boldsymbol{y}) = \\frac{p(\\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{y})}{p(z_i, \\boldsymbol{\\theta}, \\boldsymbol{y})}\\), \\(-i\\) denotes all elements of \\(\\boldsymbol{z}\\) except the \\(i\\)-th, and approximating the denominator in the last expression using a simplified Laplace approximation, which corrects the Gaussian approximation for location and skewness via a Taylor series expansion about the mode of the Laplace approximation . This follows the spirit of the approximations developed by for posterior moments and marginal densities, and is similar to those used in Chapter 10 when performing Bayesian model averaging (BMA) using the BIC information criterion. The discussion about the asymptotic error of the approximation \\(\\pi_{a}(z_i \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})\\) follows the same arguments as those for \\(\\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\). We can obtain the marginal posterior distributions integrating with respect to \\(\\boldsymbol{\\theta}\\), \\[\\begin{align*} \\pi(z_i\\mid \\boldsymbol{y})&amp;=\\int_{{\\Theta}} \\pi(z_i\\mid \\boldsymbol{\\theta},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}\\mid\\boldsymbol{y})d\\boldsymbol{\\theta}\\\\ \\pi(\\theta_l\\mid\\boldsymbol{y})&amp;=\\int_{{\\Theta}} \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}_{-l}. \\end{align*}\\] This integrals are solve numerically using a smart grid around the mode of \\(\\pi_a(\\boldsymbol{\\theta}\\mid\\boldsymbol{y})\\). In particular, \\[\\begin{align*} \\pi_a(z_i\\mid \\boldsymbol{y})&amp;=\\sum_{g=1}^G \\pi_{a}(z_i\\mid \\boldsymbol{\\theta}_g,\\boldsymbol{y})\\pi_a(\\boldsymbol{\\theta}_g\\mid \\boldsymbol{y})\\Delta_g, \\end{align*}\\] where \\(\\pi_{a}(z_i\\mid \\boldsymbol{\\theta}_g,\\boldsymbol{y})\\) is the approximation of \\(\\pi(z_i\\mid \\boldsymbol{\\theta},\\boldsymbol{y})\\) evaluated at \\(\\boldsymbol{\\theta}_g\\). Then, we have the sum over the values of \\(\\boldsymbol{\\theta}\\) with area weights \\(\\Delta_g\\). If all support points are equidistant, then \\(\\Delta_g = 1\\). The following Algorithm presents the INLA algorithm. Note that stages 2 and 3 correspond to the nested Laplace approximations, whereas stage 4 involves the integration step. Algorithm: Integrated Nested Laplace Approximations Obtain the mode of \\(\\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\), that is, \\(\\boldsymbol{\\theta}^*\\), by maximizing \\(\\log \\pi_a(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\). Compute \\(\\pi_a(\\boldsymbol{\\theta}_g \\mid \\boldsymbol{y})\\) for a set of high-density points \\(\\boldsymbol{\\theta}_g\\), \\(g = 1, 2, \\dots, G\\). Compute the approximation \\(\\pi_a(z_i \\mid \\boldsymbol{\\theta}_g, \\boldsymbol{y})\\) for \\(g = 1, 2, \\dots, G\\). Compute \\(\\pi_a(z_i \\mid \\boldsymbol{y})\\) and \\(\\pi_a(\\theta_l \\mid \\boldsymbol{y})\\) using numerical integration. Thus, INLA can be used in latent Gaussian models (LGMs) that satisfy the following conditions (Rue et al. 2017; Martino and Riebler 2019): There is conditional independence, that is, \\(y_i \\perp y_s \\mid \\eta_i, \\boldsymbol{\\theta}\\), such that \\[ p(\\boldsymbol{y} \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}) = \\prod_{i=1}^{N} p(y_i \\mid \\eta_i, \\boldsymbol{\\theta}). \\] The dimension of \\(\\boldsymbol{\\theta}\\) is small, typically less than 15. \\(\\boldsymbol{z}\\) is a Gaussian Markov random field (GMRF) with a sparse precision matrix. The linear predictor depends linearly on the smooth unknown functions of covariates. Inference is focused on the marginal posterior distributions \\(\\pi(z_i \\mid \\boldsymbol{y})\\) and \\(\\pi(\\theta_l \\mid \\boldsymbol{y})\\). Rue, Martino, and Chopin (2009) also discusses how to approximate the marginal likelihood and compute the deviance information criterion (DIC) (Spiegelhalter et al. 2002) for model selection, as well as how to perform predictive analysis. The starting point for INLA is the class of latent Gaussian models (LGMs); therefore, discrete latent classes are not supported. Additionally, since INLA relies on local approximations around the mode, it may struggle with multimodal posteriors, as there is no global exploration of the parameter space. Implementing INLA from scratch is complex (Martino and Riebler 2019); applications are therefore generally limited to the models available in the INLA package in R.3 However, new packages have been developed for specialized models, and recent approaches combine INLA with MCMC (see Table 2 in Martino and Riebler (2019)). Example: Poisson model with unobserved heterogeneity Let’s simulate the model \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\(\\lambda_i = \\exp\\left\\{1 + x_i + \\epsilon_i\\right\\}\\), with \\(\\epsilon_i \\sim {N}(0, 0.5^2)\\) and \\(x_i \\sim {N}(0, 1^2)\\) for \\(i = 1, 2, \\dots, 10,\\!000\\). Note that \\(\\epsilon_i\\) represents unobserved heterogeneity. The following code demonstrates how to perform inference using the INLA package. Keep in mind that INLA specifies Gaussian priors in terms of precision (the inverse of the variance). We present results using the three available approximation strategies in INLA: Simplified Laplace (default), Gaussian, and full Laplace. In this example, the results are practically identical across methods (see the Figures), and all 95% credible intervals contain the true population parameters. Despite the large sample size, INLA performs inference in a matter of seconds. # Install Rtools according to your R version # Check Rtolls is properly installed # Install INLA # install.packages(&quot;INLA&quot;,repos=c(getOption(&quot;repos&quot;),INLA=&quot;https://inla.r-inla-download.org/R/stable&quot;), dep=TRUE) rm(list = ls()); set.seed(010101); library(INLA) n &lt;- 10000; x &lt;- rnorm(n, sd = 1); u &lt;- rnorm(n, sd = 0.5) intercept &lt;- 1; beta &lt;- 1; id &lt;- 1:n y &lt;- rpois(n, lambda = exp(intercept + beta * x + u)) my.data &lt;- data.frame(y, x, id) formula &lt;- y ~ 1 + x + f(id, model=&quot;iid&quot;) inla.sla &lt;- inla(formula, data = my.data, family = &quot;poisson&quot;, control.compute=list(return.marginals.predictor=TRUE)) inla.ga &lt;- inla(formula, data = my.data, family = &quot;poisson&quot;, control.inla = list(strategy = &quot;gaussian&quot;, int.strategy = &quot;eb&quot;), control.compute=list(return.marginals.predictor=TRUE)) inla.la &lt;- inla(formula, data = my.data, family = &quot;poisson&quot;, control.inla = list(strategy = &quot;laplace&quot;, int.strategy = &quot;grid&quot;, dz=0.1, diff.logdens=20), control.compute=list(return.marginals.predictor=TRUE)) summary(inla.sla) marg_sla &lt;- inla.sla$marginals.fixed$x marg_ga &lt;- inla.ga$marginals.fixed$x marg_la &lt;- inla.la$marginals.fixed$x plot(marg_sla, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, xlab = expression(beta[x]), ylab = &quot;Density&quot;, main = &quot;Posterior of slope under different INLA strategies&quot;) lines(marg_ga, col = &quot;green&quot;, lwd = 2, lty = 2) lines(marg_la, col = &quot;red&quot;, lwd = 2, lty = 3) abline(v = 1, col = &quot;black&quot;, lty = 4, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Simplified Laplace&quot;, &quot;Gaussian&quot;, &quot;Full Laplace&quot;, &quot;True = 1&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;black&quot;), lwd = 2, lty = c(1, 2, 3, 4), bty = &quot;n&quot;) # Summary beta sla inla.qmarginal(c(0.025, 0.5, 0.975), marg_sla) # 95% credible interval # Variance marg.prec.sla &lt;- inla.sla$marginals.hyperpar[[&quot;Precision for id&quot;]] marg.prec.ga &lt;- inla.ga$marginals.hyperpar[[&quot;Precision for id&quot;]] marg.prec.la &lt;- inla.la$marginals.hyperpar[[&quot;Precision for id&quot;]] marg.var.sla &lt;- inla.tmarginal(function(x) 1/x, marg.prec.sla) marg.var.ga &lt;- inla.tmarginal(function(x) 1/x, marg.prec.ga) marg.var.la &lt;- inla.tmarginal(function(x) 1/x, marg.prec.la) # Base plot plot(marg.var.sla, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, xlab = expression(sigma^2), ylab = &quot;Density&quot;, main = &quot;Posterior of Random Effect Variance&quot;) lines(marg.var.ga, col = &quot;green&quot;, lwd = 2, lty = 2) lines(marg.var.la, col = &quot;red&quot;, lwd = 2, lty = 3) abline(v = 0.25, col = &quot;black&quot;, lty = 4, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Simplified Laplace&quot;, &quot;Gaussian&quot;, &quot;Full Laplace&quot;, &quot;True Variance (0.25)&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;black&quot;), lwd = 2, lty = c(1, 2, 3, 4), bty = &quot;n&quot;) # Summary variance sla inla.qmarginal(c(0.025, 0.5, 0.975), marg.var.sla) # 95% credible interval Example: Spatial econometrics model The starting point of spatial econometrics is the contiguity or adjacency matrix \\(W\\), which defines which spatial polygons (regions) are considered neighbors. This is an \\(N \\times N\\) matrix, where each row and column corresponds to a spatial polygon, and a non-zero element indicates that two polygons are neighbors. By construction, the main diagonal is zero, meaning no polygon is a neighbor to itself. Given its structure, the contiguity matrix is sparse. There are various ways to define contiguity between spatial units. Two common criteria are the queen and rook criteria, inspired by chess. Under the queen criterion, two units are neighbors if they share any part of a boundary or a point. In contrast, under the rook criterion, two units are neighbors only if they share a common edge, touching at a corner is not sufficient. However, users may define contiguity in other ways depending on context. For example, contiguity could be based on travel time between centroids or proximity between main towns. Typically, the matrix \\(W\\) is binary: a 1 indicates two regions are neighbors, and a 0 otherwise. Often, the matrix is row-standardized to aid in analyzing spatial stationarity. LeSage and Pace (2009) and Bivand, Gómez-Rubio, and Rue (2015) describe the most widely used models in spatial econometrics: the spatial error model (SEM), spatial autoregressive model (SAR), and spatial Durbin model (SDM), which can be combined into the general nesting spatial (GNS) model (Elhorst et al. 2014; Ramírez Hassan 2017). In particular, the SEM is defined as: \\[\\begin{align*} \\boldsymbol{y} &amp;= \\boldsymbol{X\\beta} + \\boldsymbol{\\mu}, \\\\ \\boldsymbol{\\mu} &amp;= \\lambda \\boldsymbol{W\\mu} + \\boldsymbol{\\epsilon}, \\end{align*}\\] where \\(\\boldsymbol{\\epsilon} \\sim {N}(\\boldsymbol{0}_N, \\sigma^2 \\boldsymbol{I}_N)\\). Then, \\[ \\boldsymbol{\\mu} = (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} \\boldsymbol{\\epsilon}, \\] which implies \\[ \\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{e}, \\] where \\[ \\boldsymbol{e} \\sim {N}(\\boldsymbol{0}, \\sigma^2 (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} (\\boldsymbol{I}_N - \\rho \\boldsymbol{W}^{\\top})^{-1}). \\] In this model, \\(\\lambda\\) controls the degree of spatial dependence. The SAR model is given by: \\[\\begin{align*} \\boldsymbol{y} &amp;= \\rho \\boldsymbol{W y} + \\boldsymbol{X\\beta} + \\boldsymbol{\\mu}, \\\\ &amp;= (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} \\boldsymbol{X\\beta} + \\boldsymbol{\\epsilon}, \\end{align*}\\] where \\(\\boldsymbol{\\epsilon} = (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} \\boldsymbol{\\mu}\\). The SDM model is: \\[\\begin{align*} \\boldsymbol{y} &amp;= \\rho \\boldsymbol{W y} + \\boldsymbol{X\\beta} + \\boldsymbol{W X \\delta} + \\boldsymbol{\\mu}, \\\\ &amp;= (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} (\\boldsymbol{X\\beta} + \\boldsymbol{W X \\delta}) + \\boldsymbol{\\epsilon}, \\end{align*}\\] where \\(\\boldsymbol{\\epsilon} = (\\boldsymbol{I}_N - \\rho \\boldsymbol{W})^{-1} \\boldsymbol{\\mu}\\). The degree of spatial dependence in the SAR and SDM models is determined by the parameter \\(\\rho\\). Note that the SDM model is similar to the SAR model, except that it also includes spatial lags of the regressors as additional covariates. Ord (1975) and Anselin (1982) show that a necessary condition for weak stationarity in spatial autoregressive processes with a row-standardized contiguity matrix is that the spatial autocorrelation coefficient must lie between \\(1/\\omega_{\\min}\\) and 1, where \\(\\omega_{\\min}\\) is the smallest (most negative) eigenvalue of the contiguity matrix.4 Note that these three models, conditional on the spatial parameter, are standard linear regressions, which can be easily estimated using INLA. Bivand, Gómez-Rubio, and Rue (2015) use the INLABMA package to estimate these models conditional on different values of the spatial parameters, and then perform Bayesian Model Averaging (BMA) using the resulting estimates. In particular, it is necessary to define a grid over the spatial parameters, perform Bayesian inference using INLA for each value in the grid, and then aggregate the posterior results using BMA. We now perform Bayesian inference on a spatial econometric model using the INLA and INLABMA packages, based on the dataset provided by Ramı́rez Hassan and Montoya Blandón (2019), who conducted a Bayesian analysis of electricity demand in the department of Antioquia (Colombia), accounting for spatial dependence between municipalities using a Conditional Autoregressive (CAR) spatial model. In particular, we conduct inference using the following specification: \\[\\begin{align*} \\log(\\text{Electricity}_i) &amp;= \\beta_1 + \\beta_2 \\log(\\text{Elect. price}_i) + \\beta_3 \\log(\\text{Income}_i) \\\\ &amp;\\quad + \\beta_4 \\log(\\text{Subs. price}_i) + \\mu_i, \\end{align*}\\] where \\(\\boldsymbol{\\mu} = \\lambda \\boldsymbol{W} \\boldsymbol{\\mu} + \\epsilon\\). Electricity is the average per capita annual consumption of electricity by individuals living in households of stratum one in each municipality of Antioquia. Elect. price is the average price of electricity (per kWh), Income is average per capita annual income, and Subs. price is the average price of an electricity substitute (see Ramı́rez Hassan and Montoya Blandón (2019) for details). The following code illustrates how to carry out a spatial econometric analysis. First, we download the files needed to plot the maps and construct the contiguity matrix. These files are available in the folder DataApp/Antioquia of our GitHub repository: https://github.com/besmarter/BSTApp. The initial part of the code demonstrates how to download the GitHub repository and extract the necessary files for mapping. The first Figure displays the average electricity consumption per municipality. We observe the presence of spatial clusters, particularly in the northwestern region, which corresponds to a low-altitude area along the Caribbean coast. The second part of the code constructs the contiguity matrix using the queen criterion. The second Figure illustrates the spatial links between municipalities. Following this, we estimate a standard Ordinary Least Squares (OLS) regression and conduct spatial autocorrelation tests on the residuals. The null hypothesis of both the global and local Moran’s I tests is the absence of spatial autocorrelation in the OLS residuals. We reject the null hypothesis. Finally, we perform Bayesian inference over a predefined grid of values for the spatial coefficient and apply Bayesian Model Averaging (BMA) using the INLABMA package. The third Figure shows the BMA posterior distribution of the spatial coefficient in the SEM, which indicates the presence of spatial dependence. The fourth Figure displays the posterior density of the own-price elasticity of electricity demand. rm(list = ls()); set.seed(010101) library(sf); library(tmap); library(classInt) library(RColorBrewer); library(spdep); library(parallel) library(INLABMA); library(sf); library(INLA) zip_url &lt;- &quot;https://github.com/BEsmarter-consultancy/BSTApp/archive/refs/heads/master.zip&quot; temp_zip &lt;- tempfile(fileext = &quot;.zip&quot;) download.file(zip_url, temp_zip, mode = &quot;wb&quot;) temp_dir &lt;- tempdir() unzip(temp_zip, exdir = temp_dir) antioquia_path &lt;- file.path(temp_dir, &quot;BSTApp-master&quot;, &quot;DataApp&quot;, &quot;Antioquia&quot;) list.files(antioquia_path) antioquia_path &lt;- file.path(temp_dir, &quot;BSTApp-master&quot;, &quot;DataApp&quot;, &quot;Antioquia&quot;) shp_file &lt;- file.path(antioquia_path, &quot;Antioquia.shp&quot;) antioquia &lt;- st_read(shp_file) print(antioquia) plot(st_geometry(antioquia), main = &quot;Map of Antioquia&quot;) interval &lt;- classIntervals(antioquia$CONS_OLD, 5, style = &quot;quantile&quot;) antioquia$cons_class &lt;- cut(antioquia$CONS_OLD,breaks = interval$brks,include.lowest = TRUE) plotcolors &lt;- brewer.pal(5, &quot;Reds&quot;) tmap_mode(&quot;plot&quot;) tm_shape(antioquia) + tm_fill(fill = &quot;cons_class&quot;, fill.scale = tm_scale(values = plotcolors), fill.legend = tm_legend(title = &quot;Electricity consumption&quot;)) + tm_borders(col = &quot;grey90&quot;) + tm_compass(type = &quot;8star&quot;, position = c(&quot;right&quot;, &quot;top&quot;)) + tm_layout(legend.outside = TRUE) nb_object &lt;- poly2nb(antioquia, queen = TRUE) centroids &lt;- st_centroid(st_geometry(antioquia)) coords &lt;- st_coordinates(centroids) plot(st_geometry(antioquia), border = &quot;grey&quot;) plot(nb_object, coords, add = TRUE, col = &quot;red&quot;) attach(antioquia) fform &lt;- L_CONS_OLD ~ L_P_OLD + L_ING_US + L_P_SUST RegOLS &lt;- lm(fform) summary(RegOLS) res &lt;- RegOLS$residuals NBList &lt;- nb2listw(nb_object, style = &quot;B&quot;) moran_mc&lt;- moran.mc(res, listw = NBList, 10000) LM&lt;-localmoran(as.vector(res), NBList) sum(LM[,5]&lt;0.05) # Bayesian estimation zero.variance &lt;- list(prec = list(initial = 25, fixed = TRUE)) ant.mat &lt;- nb2mat(nb_object) bmsp &lt;- as(ant.mat, &quot;CsparseMatrix&quot;) antioquia$idx &lt;- 1:nrow(antioquia) rrho1 &lt;- seq(0.5, 0.95, len = 10) semmodels &lt;- mclapply(rrho1, function(rho) { sem.inla(fform, d = as.data.frame(antioquia), W = bmsp, rho = rho, family = &quot;gaussian&quot;, impacts = FALSE, control.family = list(hyper = zero.variance), control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, cpo = TRUE), control.inla = list(print.joint.hyper = TRUE)) }) bmasem &lt;- INLABMA(semmodels, rrho1, 0, impacts = FALSE) #Display results plot(bmasem$rho$marginal, type=&quot;l&quot;, col = &quot;blue&quot;, lwd = 2, xlab = expression(lambda), ylab = &quot;Density&quot;, main = &quot;Spatial error model: Posterior spatial coefficient&quot;) bmasem[[&quot;rho&quot;]][[&quot;quantiles&quot;]] marg_sla &lt;- bmasem[[&quot;marginals.fixed&quot;]][[&quot;L_P_OLD&quot;]] plot(marg_sla, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, xlab = expression(beta[x]), ylab = &quot;Density&quot;, main = &quot;Spatial error model: Posterior price elasticity&quot;) bmasem[[&quot;summary.fixed&quot;]] 14.2.2 Variational Bayes Variational Bayes (VB) is a method from machine learning (Jordan et al. 1999; Wainwright, Jordan, et al. 2008) that replaces \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) with an approximation obtained through optimization using the calculus of variations, hence the name variational Bayes. This approach is useful when the posterior distribution is complex (e.g., multimodal) or when the parameter space is high-dimensional, making MCMC or IS algorithms computationally expensive. The goal in VB is to approximate the posterior distribution using a distribution \\(q(\\boldsymbol{\\theta})\\) from a variational family \\(\\mathcal{Q}\\), a class of distributions that is computationally convenient yet flexible enough to closely approximate the true posterior (Blei, Kucukelbir, and McAuliffe 2017). The distribution \\(q\\) is called the variational approximation to the posterior, and a particular \\(q\\) in \\(\\mathcal{Q}\\) is defined by a specific set of variational parameters. Typically, this approximation is obtained by minimizing the Kullback–Leibler (KL) divergence between \\(q(\\boldsymbol{\\theta})\\) and \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\). \\[\\begin{align} q^*(\\boldsymbol{\\theta}):=\\underset{q \\in \\mathcal{Q}}{argmin} \\ \\text{KL}(q(\\boldsymbol{\\theta})||\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})), \\tag{14.2} \\end{align}\\] where \\(\\text{KL}(q(\\boldsymbol{\\theta}) \\| \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})) = \\mathbb{E}_q\\left[\\log\\left(\\frac{q(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})}\\right)\\right]\\). Note that in relatively complex models, the optimization in Equation ((14.2) is not computable because it depends on the marginal likelihood \\(p(\\boldsymbol{y})\\), which is typically unknown due to the intractability of the integral involved. However, there is a solution to this problem. Let’s see: \\[\\begin{align*} \\log(p(\\boldsymbol{y}))&amp;=\\log\\left(\\int_{\\boldsymbol{\\Theta}}p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\right)\\\\ &amp;=\\log\\left(\\int_{\\boldsymbol{\\Theta}}p(\\boldsymbol{y}, \\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\right)\\\\ &amp;=\\log\\left(\\int_{\\boldsymbol{\\Theta}}\\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{q(\\boldsymbol{\\theta})}q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\right)\\\\ &amp;=\\log \\mathbb{E}_q\\left(\\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{q(\\boldsymbol{\\theta})}\\right)\\\\ &amp;\\geq \\mathbb{E}_q\\log\\left(\\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{q(\\boldsymbol{\\theta})}\\right)\\\\ &amp;=\\mathbb{E}_q\\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))-\\mathbb{E}_q\\log(q(\\boldsymbol{\\theta}))\\\\ &amp;=\\text{ELBO}(q(\\boldsymbol{\\theta})), \\end{align*}\\] where the inequality follows from Jensen’s inequality, since \\(\\log(\\cdot)\\) is concave. The last term is the (ELBO), which serves as a lower bound for the marginal likelihood. Note that the gap between the marginal likelihood and the ELBO is given by \\[\\begin{align*} \\log(p(\\boldsymbol{y})) - \\text{ELBO}(q(\\boldsymbol{\\theta})) &amp; = \\log(p(\\boldsymbol{y})) - \\mathbb{E}_q\\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))+\\mathbb{E}_q\\log(q(\\boldsymbol{\\theta}))\\\\ &amp;=\\mathbb{E}_q\\left(\\log(q(\\boldsymbol{\\theta}))-\\log\\left(\\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{p(\\boldsymbol{y})}\\right)\\right)\\\\ &amp;=\\mathbb{E}_q\\left(\\log(q(\\boldsymbol{\\theta}))-\\log\\left(\\frac{p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta})\\pi( \\boldsymbol{\\theta})}{p(\\boldsymbol{y})}\\right)\\right)\\\\ &amp;=\\mathbb{E}_q\\left(\\log(q(\\boldsymbol{\\theta}))-\\log(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y}))\\right)\\\\ &amp;=\\text{KL}(q(\\boldsymbol{\\theta})||\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})). \\end{align*}\\] Then, \\[\\begin{align*} \\log(p(\\boldsymbol{y})) = \\text{KL}(q(\\boldsymbol{\\theta})||\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})) + \\text{ELBO}(q(\\boldsymbol{\\theta})), \\end{align*}\\] which implies that maximizing the ELBO with respect to \\(q(\\boldsymbol{\\theta})\\) is equivalent to minimizing the KL divergence, since \\(\\log(p(\\boldsymbol{y}))\\) does not depend on \\(q(\\boldsymbol{\\theta})\\). This avoids the need to compute the marginal likelihood and, consequently, makes the variational problem easier to solve. In addition, it provides a lower bound for the marginal likelihood, which can potentially be used for model selection. Thus, solving problem (14.2) is equivalent to solving \\[\\begin{align} q^*(\\boldsymbol{\\theta}):=\\underset{q \\in \\mathcal{Q}}{argmax} \\ \\text{ELBO}(q(\\boldsymbol{\\theta})). \\tag{14.2} \\end{align}\\] Note that the ELBO can be also expressed as \\[\\begin{align*} \\text{ELBO}(q(\\boldsymbol{\\theta}))&amp;=\\mathbb{E}_q\\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))-\\mathbb{E}_q\\log(q(\\boldsymbol{\\theta}))\\\\ &amp;=\\mathbb{E}_q\\log(p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta}))+\\mathbb{E}_q\\log(\\pi(\\boldsymbol{\\theta}))-\\mathbb{E}_q\\log(q(\\boldsymbol{\\theta}))\\\\ &amp;=\\mathbb{E}_q\\log(p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta}))-\\text{KL}(q(\\boldsymbol{\\theta})||\\pi(\\boldsymbol{\\theta})). \\end{align*}\\] This means that when maximizing the ELBO, we seek the distribution \\(q(\\boldsymbol{\\theta})\\) that both maximizes the likelihood and minimizes the KL divergence between the variational distribution and the prior. In other words, we aim to strike a balance between the prior and the likelihood, which aligns with the core principle of Bayesian inference deriving the posterior distribution. The most common approach for specifying \\(q(\\boldsymbol{\\theta})\\) is to assume independence across blocks of \\(\\boldsymbol{\\theta}\\), i.e., \\(q(\\boldsymbol{\\theta}) = \\prod_{l=1}^K q_l(\\boldsymbol{\\theta}_l)\\). This is known as the mean-field variational family, a term that originates from statistical physics. Each \\(q_l(\\boldsymbol{\\theta}_l)\\) is parameterized by a set of variational parameters, and optimization is performed with respect to these parameters. Note that the mean-field approximation does not capture dependencies between parameters, although it can approximate the marginal distributions. Let us now decompose the ELBO under the mean-field variational family (Nguyen 2023), \\[\\begin{align*} \\text{ELBO}(q(\\boldsymbol{\\theta}))&amp;=\\mathbb{E}_q\\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))-\\mathbb{E}_q\\log(q(\\boldsymbol{\\theta}))\\\\ &amp;=\\int_{\\mathbb{R}^K} \\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}-\\int_{\\mathbb{R}^K}\\log(q(\\boldsymbol{\\theta}))q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\\\ &amp;=\\int_{\\mathbb{R}^K} \\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right)\\prod_{l=1}^K q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l-\\int_{\\mathbb{R}^K}\\log\\left(\\prod_{l=1}^K q_l(\\boldsymbol{\\theta}_l)\\right)\\prod_{l=1}^K q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\\\ &amp;=\\int_{\\mathbb{R}} \\underbrace{\\left(\\int_{\\mathbb{R}^{K-1}}\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right)\\prod_{l\\neq k} q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\right)}_{\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))}q_k(\\boldsymbol{\\theta}_k)d\\boldsymbol{\\theta}_k\\\\ &amp;-\\int_{\\mathbb{R}}\\log(q_k(\\boldsymbol{\\theta}_k))q_k(\\boldsymbol{\\theta}_k)d\\boldsymbol{\\theta}_k-\\sum_{l\\neq k}\\int_{\\mathbb{R}}\\log(q_l(\\boldsymbol{\\theta}_l))q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\\\ &amp;=\\int_{\\mathbb{R}}\\log\\left\\{\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))\\right)\\right\\}q_k(\\boldsymbol{\\theta}_k)d\\boldsymbol{\\theta}_k\\\\ &amp;-\\int_{\\mathbb{R}}\\log(q_k(\\boldsymbol{\\theta}_k))q_k(\\boldsymbol{\\theta}_k)d\\boldsymbol{\\theta}_k-\\sum_{l\\neq k}\\int_{\\mathbb{R}}\\log(q_l(\\boldsymbol{\\theta}_l))q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\\\ &amp;=\\int_{\\mathbb{R}}\\log\\left\\{\\frac{\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))\\right)}{q_k(\\boldsymbol{\\theta}_k)}\\right\\}q_k(\\boldsymbol{\\theta}_k)d\\boldsymbol{\\theta}_k\\\\ &amp;-\\sum_{l\\neq k}\\int_{\\mathbb{R}}\\log(q_l(\\boldsymbol{\\theta}_l))q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\\\ &amp;=-\\text{KL}(q_k(\\boldsymbol{\\theta}_k)||\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))\\right))-\\sum_{l\\neq k}\\int_{\\mathbb{R}}\\log(q_l(\\boldsymbol{\\theta}_l))q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l\\\\ &amp;\\leq -\\sum_{l\\neq k}\\int_{\\mathbb{R}}\\log(q_l(\\boldsymbol{\\theta}_l))q_l(\\boldsymbol{\\theta}_l)d\\boldsymbol{\\theta}_l, \\end{align*}\\] where \\(\\mathbb{E}_{-k}\\) denotes expectation with respect to the distribution \\(\\prod_{l\\neq k}q(\\boldsymbol{\\theta}_l)\\). Note that we maximize the ELBO when \\(\\text{KL}(q_k(\\boldsymbol{\\theta}_k)||\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))\\right))\\) equals 0, that is, when \\[\\begin{align} q_k^*(\\boldsymbol{\\theta}_k)&amp;\\propto\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right))\\right)\\nonumber\\\\ &amp;=\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{y}, \\boldsymbol{\\theta}_{-k},\\boldsymbol{\\theta}_k)\\right))\\right)\\nonumber\\\\ &amp;=\\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p({\\boldsymbol{\\theta}}_{k}\\mid\\boldsymbol{y},\\boldsymbol\\theta_{-k})p(\\boldsymbol{y},\\boldsymbol\\theta_{-k})\\right))\\right)\\nonumber\\\\ &amp;\\propto \\exp\\left(\\mathbb{E}_{-k}(\\log\\left(p(\\boldsymbol{\\theta}_{k}\\mid\\boldsymbol{y},\\boldsymbol\\theta_{-k})\\right))\\right), k=1,2,\\dots,K. \\tag{14.3} \\end{align}\\] Note the circular dependency inherent in \\(q_k^*(\\boldsymbol{\\theta}_k)\\): it depends on \\(q_{-k}^*(\\boldsymbol{\\theta}_{-k})\\). Therefore, this situation must be addressed algorithmically. One of the most common algorithms used to solve the optimization problem in (14.2) using (14.3) is the coordinate ascent variational inference (CAVI) algorithm (Bishop and Nasrabadi 2006). The algorithm starts from an initial solution and iteratively cycles through each \\(q_k^*(\\boldsymbol{\\theta}_k)\\), for \\(k = 1, 2, \\dots, K\\), updating each component in turn. The following Algorithm outlines the basic structure of the CAVI algorithm. Algorithm: Variational Bayes – Coordinate Ascent Variational Inference Initialize the variational factors \\(q_l^*(\\boldsymbol{\\theta}_l),\\ l = 1, 2, \\dots, K\\) While ELBO \\(&gt; \\epsilon\\), where \\(\\epsilon\\) is small: For \\(l = 1, 2, \\dots, L\\): Update \\[ q_l^*(\\boldsymbol{\\theta}_l) \\propto \\exp\\left(\\mathbb{E}_{-l}\\left[\\log\\left(p(\\theta_{l} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta}_{-l})\\right)\\right]\\right) \\] Compute \\[ \\text{ELBO}(q) = \\mathbb{E}_q[\\log(p(\\boldsymbol{y}, \\boldsymbol{\\theta}))] - \\mathbb{E}_q[\\log(q(\\boldsymbol{\\theta}))] \\] Return \\(q(\\boldsymbol{\\theta})\\) We should note that the CAVI algorithm is sensitive to the initial variational factors because it guarantees convergence to a local maximum, which may depend heavily on the initialization point. This issue can be mitigated by using multiple starting points and selecting the solution with the highest ELBO (Blei and Jordan 2006). In addition, it is well known that VB tends to overestimate the precision of the posterior distribution, although it does not necessarily suffer in terms of accuracy (Blei, Kucukelbir, and McAuliffe 2017). An important feature of the method is that it is deterministic rather than stochastic; that is, it does not require approximating the posterior distribution via sampling from simpler distributions. This often makes VB faster than MCMC methods, which are inherently stochastic. Instead, VB solves a deterministic optimization problem to find the best variational distribution within a chosen family, typically from the exponential family, by optimizing the variational parameters to minimize the KL divergence from the posterior distribution. Once the variational parameters are obtained, we can sample from the variational distribution to conduct estimation, hypothesis testing, prediction, and other tasks. A limitation of the CAVI algorithm is that it requires evaluation at each data point, making it non-scalable in large data settings. In such situations, we can use stochastic variational inference (SVI) (Hoffman et al. 2013), an algorithm that optimizes the ELBO using natural gradients combined with stochastic optimization. SVI is particularly effective when each complete conditional belongs to the exponential family (see Section 3.1), which includes most of the models discussed in this book. It is especially useful for conditionally conjugate models that include local latent variables (\\(\\boldsymbol{z}_i\\)) associated with specific data points, and global parameters (\\(\\boldsymbol{\\phi}\\)) shared across the entire dataset. That is, we define \\(\\boldsymbol\\theta = [\\boldsymbol{z}^{\\top} \\ \\boldsymbol{\\phi}^{\\top}]^{\\top}\\). An example is the probit model using data augmentation (Tanner and Wong 1987). Given the global-local exchangeable structure, the joint distribution can be expressed as \\(p(\\boldsymbol{\\phi}, \\boldsymbol{z}, \\boldsymbol{y}) = \\pi(\\boldsymbol{\\phi} \\mid \\boldsymbol{\\alpha}) \\prod_{i=1}^{N} p(\\boldsymbol{z}_i, \\boldsymbol{y}_i \\mid \\boldsymbol{\\phi})\\), where \\(\\boldsymbol{\\alpha} = [\\boldsymbol{\\alpha}_1^{\\top} \\ \\alpha_2]^{\\top}\\) represents the hyperparameters of the prior distribution of the global parameters \\(\\boldsymbol{\\phi}\\). Assuming that the joint distribution \\(p(\\boldsymbol{z}_i, \\boldsymbol{y}_i \\mid \\boldsymbol{\\phi})\\) is in the canonical form of the exponential family, and that each prior distribution is conjugate, the complete conditional distribution of the global parameters is also in the exponential family. The posterior parameters are given by \\(\\boldsymbol{\\alpha}_n = [\\boldsymbol{\\alpha}_1^{\\top} + \\sum_{i=1}^N t(\\boldsymbol{z}_i, \\boldsymbol{y}_i)^{\\top} \\ \\alpha_2 + N]^{\\top}\\), where \\(t(\\boldsymbol{z}_i, \\boldsymbol{y}_i)\\) is a sufficient statistic of \\(\\boldsymbol{z}_i\\) and \\(\\boldsymbol{y}_i\\) (see Section 3.2). Additionally, the structure of the model implies that \\(\\boldsymbol{z}_i\\) is independent of \\(\\boldsymbol{z}_{-i}\\) and \\(\\boldsymbol{y}_{-i}\\) given \\(\\boldsymbol{y}_i\\) and \\(\\boldsymbol{\\phi}\\). Thus, if \\(p(\\boldsymbol{z}_i \\mid \\boldsymbol{y}_i, \\boldsymbol{\\phi})\\) is in the canonical form of the exponential family, then the local variational update is given by \\(\\boldsymbol{\\psi}_i = \\mathbb{E}_{\\boldsymbol{\\xi}}[\\boldsymbol{\\eta}(\\boldsymbol{\\phi}, \\boldsymbol{y}_i)]\\), where the parameter set \\(\\boldsymbol{\\eta}(\\boldsymbol{\\phi}, \\boldsymbol{y}_i)\\) is a function of the conditional set, \\(\\boldsymbol{\\xi}\\), and \\(\\boldsymbol{\\psi}_i\\) are the variational parameters of the variational approximations \\(q(\\boldsymbol{\\phi} \\mid \\boldsymbol{\\xi})\\) and \\(q(\\boldsymbol{z}_i \\mid \\boldsymbol{\\psi}_i)\\) for the posterior distributions of \\(\\boldsymbol{\\phi}\\) and \\(\\boldsymbol{z}_i\\), respectively. The global variational update is given by \\(\\boldsymbol{\\xi} = [\\boldsymbol{\\alpha}_1^{\\top} + \\sum_{i=1}^N \\mathbb{E}_{\\boldsymbol{\\psi}_i} t(\\boldsymbol{z}_i, \\boldsymbol{y}_i)^{\\top} \\ \\alpha_2 + N]^{\\top}\\). SVI focuses on the global parameters, updating them using the ELBO natural gradients with respect to \\(\\boldsymbol{\\xi}\\), where natural gradients are the usual gradients premultiplied by the inverse covariance matrix of the sufficient statistic. These gradients are easily calculated in exponential families. In particular, the updates are given by \\[\\begin{align*} \\boldsymbol{\\xi}_t=\\boldsymbol{\\xi}_{t-1}+\\epsilon_t g(\\boldsymbol{\\xi}_{t-1}), \\end{align*}\\] where \\(\\epsilon_t\\) is the step size, and \\(g(\\boldsymbol{\\xi}_t)=\\mathbb{E}_{\\boldsymbol{\\psi}_i}[\\boldsymbol{\\alpha}_n-\\boldsymbol{\\xi}_t]\\) is the natural gradient of the global variational parameters (Blei, Kucukelbir, and McAuliffe 2017). Consequently, \\[\\begin{align*} \\boldsymbol{\\xi}_t=(1-\\epsilon_t)\\boldsymbol{\\xi}_{t-1}+\\epsilon_t \\mathbb{E}_{\\boldsymbol{\\psi}}[\\boldsymbol{\\alpha}_n]. \\end{align*}\\] However, calculating this update requires using the entire dataset, which is computationally burdensome. Therefore, we should use stochastic optimization, which follows noisy but cheap-to-compute unbiased gradients to optimize the function. The key idea is to construct the natural gradient using only one random draw from the dataset, and then scale it (by multiplying it with the sample size) to approximate the sample information: \\[\\begin{align*} \\hat{\\boldsymbol{\\xi}}&amp;=\\boldsymbol{\\alpha}+N(\\mathbb{E}_{\\boldsymbol{\\psi}_i^*}[t(\\boldsymbol{z}_i,\\boldsymbol{y}_i)]^{\\top},1)^{\\top}\\\\ \\boldsymbol{\\xi}_t&amp;=(1-\\epsilon_t)\\boldsymbol{\\xi}_{t-1}+\\epsilon_t\\hat{\\boldsymbol{\\xi}}, \\end{align*}\\] where \\((\\boldsymbol{z}_i, \\boldsymbol{y}_i)\\) is a random draw from the sample and its corresponding latent variable. Finally, the step size schedule to update the global parameters are given by \\[\\begin{align*} \\epsilon_t&amp;=t^{-\\kappa}, \\ 0.5 &lt; \\kappa \\leq 1. \\end{align*}\\] This step size schedule satisfies the Robbins and Monro conditions necessary for stochastic optimization (Robbins and Monro 1951). The following Algorithm shows the stochastic variational inference algorithm. Algorithm: Variational Bayes – Stochastic Variational Inference Initialize the variational global parameter \\(\\boldsymbol{\\phi}_0\\) Set the step size schedule \\(\\epsilon_t = t^{-\\kappa},\\ 0.5 &lt; \\kappa \\leq 1\\) While TRUE: Randomly select a data point \\(\\boldsymbol{y}_i \\sim U(1, 2, \\dots, N)\\) Optimize its local variational parameters: \\[ \\boldsymbol{\\psi}_i^* = \\mathbb{E}_{\\boldsymbol{\\xi}_{t-1}}[\\boldsymbol{\\eta}(\\boldsymbol{\\phi}, y_i)] \\] Compute the coordinate updates assuming \\(\\boldsymbol{y}_i\\) was repeated \\(N\\) times: \\[ \\hat{\\boldsymbol{\\xi}} = \\boldsymbol{\\alpha} + N \\cdot \\left( \\mathbb{E}_{\\boldsymbol{\\psi}_i^*}[t(\\boldsymbol{z}_i, \\boldsymbol{y}_i)]^\\top, 1 \\right)^\\top \\] Update the global variational parameters: \\[ \\boldsymbol{\\xi}_t = (1 - \\epsilon_t) \\boldsymbol{\\xi}_{t-1} + \\epsilon_t \\hat{\\boldsymbol{\\xi}} \\] Return \\(q_{\\boldsymbol{\\xi}}(\\boldsymbol{\\phi})\\) Zhang and Gao (2020) analyzes the asymptotic properties of the VB posterior by decomposing the convergence rates of VB into the convergence rate of the true posterior and the approximation error induced by the variational family. These authors show that the VB posterior concentrates entirely in a neighborhood of the true posterior distribution. In addition, if the loss function is convex, there exists a point estimator that converges at the same rate. Zhang and Gao (2020) also shows that, for specific cases such as sparse linear models, the concentration rate of the VB posterior is faster than the concentration rate of the exact posterior. Variational Bayes (VB) shares some similarities with Gibbs sampling. In Gibbs sampling, we iteratively sample from the conditional posterior distributions, whereas in VB we iteratively update the variational parameters of the variational family. The former is stochastic, while the latter is deterministic, and consequently faster in complex models or large datasets. VB also bears resemblance to Expectation Propagation (EP): both are deterministic algorithms that approximate the posterior distribution by minimizing the Kullback-Leibler (KL) divergence. However, VB minimizes \\(\\text{KL}(q(\\boldsymbol{\\theta}) \\| \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}))\\), whereas EP minimizes \\(\\text{KL}(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\| q(\\boldsymbol{\\theta}))\\). As a result, VB tends to approximate the mode of the posterior by maximizing the ELBO, while EP focuses on matching the mean and variance through moment matching (Bishop and Nasrabadi 2006; Gelman et al. 2021). Although EP often provides better uncertainty quantification, it tends to be less stable and does not scale well to large datasets. VB also shares some conceptual features with the Expectation-Maximization (EM) algorithm. In both methods, there is an initial step involving the computation of expectations — used in VB to derive the variational distributions — and an iterative step that maximizes a target function (the ELBO in VB, the expected complete-data log-likelihood in EM). However, EM yields point estimates, whereas VB yields full posterior approximations. Example: Linear regression Let’s perform variational Bayes inference in the linear regression model with conjugate family. In particular, \\[ \\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\mu}, \\] where \\(\\boldsymbol{\\mu} \\sim N(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I})\\). This implies that \\[ \\boldsymbol{y} \\sim N(\\boldsymbol{X} \\boldsymbol{\\beta}, \\sigma^2 \\boldsymbol{I}). \\] The conjugate priors for the parameters are: \\[\\begin{align*} \\boldsymbol{\\beta}\\mid \\sigma^2 &amp; \\sim N(\\boldsymbol{\\beta}_0, \\sigma^2 \\boldsymbol{B}_0),\\\\ \\sigma^2 &amp; \\sim IG(\\alpha_0/2, \\delta_0/2). \\end{align*}\\] Then, the posterior distributions are \\[ \\boldsymbol{\\beta} \\mid \\sigma^2, \\boldsymbol{y}, \\boldsymbol{X} \\sim N(\\boldsymbol{\\beta}_n, \\sigma^2 \\boldsymbol{B}_n), \\quad \\sigma^2 \\mid \\boldsymbol{y}, \\boldsymbol{X} \\sim IG(\\alpha_n/2, \\delta_n/2), \\] where - \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\boldsymbol{X}^{\\top} \\boldsymbol{X})^{-1}\\) - \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})\\), - \\(\\hat{\\boldsymbol{\\beta}}\\) is the MLE, - \\(\\alpha_n = \\alpha_0 + N\\), - \\(\\delta_n = \\delta_0 + \\boldsymbol{y}^{\\top} \\boldsymbol{y} + \\boldsymbol{\\beta}_0^{\\top} \\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_n^{\\top} \\boldsymbol{B}_n^{-1} \\boldsymbol{\\beta}_n\\) (see Section 3.3). Let’s use the mean-field variational family \\(q(\\boldsymbol{\\beta},\\sigma^2)=q(\\boldsymbol{\\beta})q(\\sigma^2)\\approx \\pi(\\boldsymbol{\\beta},\\sigma^2\\mid\\boldsymbol{y},\\boldsymbol{X})\\). Then, \\[\\begin{align*} \\log q^*(\\boldsymbol{\\beta})&amp;\\propto\\mathbb{E}_{\\sigma^2}[\\log p(\\boldsymbol{y},\\boldsymbol{\\beta},\\sigma^2\\mid\\boldsymbol{X})]\\\\ &amp;=\\mathbb{E}_{\\sigma^2}[\\log p(\\boldsymbol{y}\\mid\\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{X}) +\\log \\pi(\\boldsymbol{\\beta}\\mid\\sigma^2)+\\log \\pi(\\sigma^2)]\\\\ &amp;=\\mathbb{E}_{\\sigma^2}\\left(-\\frac{N}{2}\\log\\sigma^2-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})-\\frac{K}{2}\\log\\sigma^2\\right.\\\\ &amp;\\left.-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)^{\\top}\\boldsymbol{B}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\right)+c_1\\\\ &amp;=-0.5\\mathbb{E}_{\\sigma^2}\\left(\\frac{1}{\\sigma^2}\\right)[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)]+c_2.\\\\ \\end{align*}\\] The last equality follows the same steps to obtain the posterior distribution of \\(\\boldsymbol{\\beta}\\), \\(c_1\\) and \\(c_2\\) are arbitrary constants. This expression implies that \\(q^*(\\boldsymbol{\\beta})\\) is \\(N\\left(\\boldsymbol{\\beta}_n,\\left(\\mathbb{E}_{\\sigma^2}\\left(\\frac{1}{\\sigma^2}\\right)\\right)^{-1}\\boldsymbol{B}_n\\right)\\). In addition, \\[\\begin{align*} \\log q^*(\\sigma^2)&amp;\\propto\\mathbb{E}_{\\boldsymbol{\\beta}}[\\log p(\\boldsymbol{y},\\boldsymbol{\\beta},\\sigma^2\\mid\\boldsymbol{X})]\\\\ &amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}[\\log p(\\boldsymbol{y}\\mid\\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{X}) +\\log \\pi(\\boldsymbol{\\beta}\\mid\\sigma^2)+\\log \\pi(\\sigma^2)]\\\\ &amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}\\left(-\\frac{N}{2}\\log\\sigma^2-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})-\\frac{K}{2}\\log\\sigma^2\\right.\\\\ &amp;\\left.-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)^{\\top}\\boldsymbol{B}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)-(\\alpha_0/2+1)\\log \\sigma^2 -\\frac{\\delta_0}{2\\sigma^2}\\right)+c_1\\\\ &amp;=-\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[\\frac{1}{2\\sigma^2}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)^{\\top}\\boldsymbol{B}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)+\\delta_0\\right]\\\\ &amp;-\\left(\\frac{N+K+\\alpha_0}{2}+1\\right)\\log\\sigma^2+c_1. \\end{align*}\\] This means that \\(q^*(\\sigma^2)\\) is \\(IG(\\alpha_n/2,\\delta_n/2)\\), where \\(\\alpha_n=N+K+\\alpha_0\\), and \\[\\begin{align*} \\delta_n&amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)^{\\top}\\boldsymbol{B}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)+\\delta_0\\right]\\\\ &amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)\\right]-\\boldsymbol{\\beta}_n^{\\top}\\boldsymbol{B}_n^{-1}\\boldsymbol{\\beta}_n+\\boldsymbol{y}^{\\top}\\boldsymbol{y}+\\boldsymbol{\\beta}_0^{\\top}\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\delta_0. \\end{align*}\\] This result implies that \\(\\mathbb{E}_{\\sigma^2}\\left(\\frac{1}{\\sigma^2}\\right) = \\alpha_n / \\delta_n\\), since the inverse of a gamma-distributed random variable (in the rate parametrization) follows an inverse-gamma distribution. Therefore, \\(\\operatorname{Var}(\\boldsymbol{\\beta}) = \\frac{\\delta_n}{\\alpha_n} \\boldsymbol{B}_n\\). Note that \\[\\begin{align*} \\mathbb{E}_{\\boldsymbol{\\beta}}\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)\\right]&amp;=tr\\left\\{\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)\\right]\\right\\}\\\\ &amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}\\left\\{tr\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)\\right]\\right\\}\\\\ &amp;=\\mathbb{E}_{\\boldsymbol{\\beta}}\\left\\{tr\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\boldsymbol{B}_n^{-1}\\right]\\right\\}\\\\ &amp;=tr\\left\\{\\mathbb{E}_{\\boldsymbol{\\beta}}\\left[(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_n)^{\\top}\\right]\\boldsymbol{B}_n^{-1}\\right\\}\\\\ &amp;=tr\\left\\{Var(\\boldsymbol{\\beta})\\boldsymbol{B}_n^{-1}\\right\\},\\\\ \\end{align*}\\] where we use that the trace of a scalar is the scalar itself, that expectation and trace can be interchanged since both are linear operators, and that the trace operator is invariant under cyclic permutations. Then, \\[\\begin{align*} \\delta_n&amp;=tr\\left\\{Var(\\boldsymbol{\\beta})\\boldsymbol{B}_n^{-1}\\right\\}-\\boldsymbol{\\beta}_n^{\\top}\\boldsymbol{B}_n^{-1}\\boldsymbol{\\beta}_n+\\boldsymbol{y}^{\\top}\\boldsymbol{y}+\\boldsymbol{\\beta}_0^{\\top}\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\delta_0\\\\ &amp;=\\left(\\frac{\\alpha_0+N+K}{\\alpha_0+K}\\right)\\left(-\\boldsymbol{\\beta}_n^{\\top}\\boldsymbol{B}_n^{-1}\\boldsymbol{\\beta}_n+\\boldsymbol{y}^{\\top}\\boldsymbol{y}+\\boldsymbol{\\beta}_0^{\\top}\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\delta_0\\right), \\end{align*}\\] where the last equality follows from \\(tr\\left\\{Var(\\boldsymbol{\\beta})\\boldsymbol{B}_n^{-1}\\right\\}=\\delta_n/\\alpha_n tr\\left\\{\\boldsymbol{I}_K\\right\\}\\). In addition (Exercise 3), \\[\\begin{align*} \\text{ELBO}(q(\\boldsymbol{\\beta},\\sigma^2))&amp;=\\mathbb{E}_{\\boldsymbol{\\beta},\\sigma^2}[\\log p(\\boldsymbol{y},\\boldsymbol{\\beta},\\sigma^2\\mid\\boldsymbol{X})]-\\mathbb{E}_{\\boldsymbol{\\beta},\\sigma^2}[\\log q(\\boldsymbol{\\beta},\\sigma^2)]\\\\ &amp;=-\\frac{N}{2}\\log(2\\pi)+\\frac{\\alpha_0}{2}\\log(\\delta_0/2)-\\frac{\\alpha_n}{2}\\log(\\delta_n/2)-0.5\\log|\\boldsymbol{B}_0|\\\\ &amp;+0.5\\log|\\boldsymbol{B}_n|-\\log\\Gamma(\\alpha_0/2)+\\log\\Gamma(\\alpha_n/2)\\\\ &amp;-K/2\\log(\\alpha_n/\\delta_n)+0.5tr\\left\\{Var(\\boldsymbol{\\beta})\\boldsymbol{B}_n^{-1}\\right\\}. \\end{align*}\\] Note that the first two lines of the ELBO share the same structure as the log marginal likelihood \\(p(\\boldsymbol{y})\\) in Section 3.3. The following code presents a simulation setting with a sample size of 500 and two regressors drawn from standard normal distributions. The population parameters are set to 1, and we use non-informative priors with 10,000 posterior draws. First, we perform VB inference using the LaplacesDemon package, and then implement it from scratch. Although we have analytical solutions in this setting, we apply our own CAVI algorithm to assess its performance. We also compare the results with those from the Gibbs sampler, the marginal likelihood, and the ELBO. In general, all calculations seem to perform well: the 95% credible intervals contain the population parameters, and the posterior means are very close to them. set.seed(010101) library(LaplacesDemon) N &lt;- 500; K &lt;- 2 sig2 &lt;- 1; B &lt;- rep(1, K + 1) X &lt;- cbind(1, matrix(rnorm(N*K), N, K)) e &lt;- rnorm(N, 0, sig2^0.5) y &lt;- X%*%B + e ######################### Data List Preparation ######################### mon.names &lt;- &quot;mu[1]&quot; parm.names &lt;- as.parm.names(list(beta=rep(0,K + 1), sigma2=0)) pos.beta &lt;- grep(&quot;beta&quot;, parm.names) pos.sigma2 &lt;- grep(&quot;sigma2&quot;, parm.names) PGF &lt;- function(Data) { beta &lt;- rnorm(Data$K) sigma2 &lt;- runif(1) return(c(beta, sigma2)) } MyData &lt;- list(K=K, PGF=PGF, X=X, mon.names=mon.names, parm.names=parm.names, pos.beta=pos.beta, pos.sigma2=pos.sigma2, y=y) ########################## Model Specification ########################## b0 &lt;- 0; B0 &lt;- 1000; a0 &lt;- 0.01; d0 &lt;- 0.01 Model &lt;- function(parm, Data) { ### Parameters beta &lt;- parm[Data$pos.beta] sigma2 &lt;- interval(parm[Data$pos.sigma2], 1e-100, Inf) parm[Data$pos.sigma2] &lt;- sigma2 ### Log-Prior beta.prior &lt;- sum(dnormv(beta, b0, B0, log=TRUE)) sigma2.prior &lt;- dinvgamma(sigma2, a0/2, d0/2, log=TRUE) ### Log-Likelihood mu &lt;- tcrossprod(Data$X, t(beta)) LL &lt;- sum(dnorm(Data$y, mu, sigma2^0.5, log=TRUE)) ### Log-Posterior LP &lt;- LL + beta.prior + sigma2.prior Modelout &lt;- list(LP=LP, Dev=-2*LL, Monitor=mu[1], yhat=rnorm(length(mu), mu, sigma2^0.5), parm=parm) return(Modelout) } Initial.Values &lt;- rep(0,K+2); S &lt;- 10000 Fit &lt;- VariationalBayes(Model, Initial.Values, Data=MyData, Covar=NULL, Iterations=S, Method=&quot;Salimans2&quot;, Stop.Tolerance=1e-2, CPUs=1) print(Fit) PosteriorChecks(Fit) caterpillar.plot(Fit, Parms=&quot;beta&quot;) plot(Fit, MyData, PDF=FALSE) Pred &lt;- predict(Fit, Model, MyData, CPUs=1) summary(Pred, Discrep=&quot;Chi-Square&quot;) ####### MCMC ####### # Posterior parameters b0 &lt;- rep(b0, K+1); B0 &lt;- B0*diag(K+1) bhat &lt;- solve(t(X)%*%X)%*%t(X)%*%y Bn &lt;- as.matrix(Matrix::forceSymmetric(solve(solve(B0) + t(X)%*%X))) bn &lt;- Bn%*%(solve(B0)%*%b0 + t(X)%*%X%*%bhat) dn &lt;- as.numeric(d0 + t(y)%*%y+t(b0)%*%solve(B0)%*%b0-t(bn)%*%solve(Bn)%*%bn) an &lt;- a0 + N # Posterior draws sig2 &lt;- MCMCpack::rinvgamma(S,an/2,dn/2) summary(coda::mcmc(sig2)) Betas &lt;- t(sapply(1:S, function(s){MASS::mvrnorm(1, bn, sig2[s]*Bn)})) summary(coda::mcmc(Betas)) ####### VB from scratch ####### dnVB &lt;- ((a0+N+K)/(a0+N))*dn; anVB &lt;- a0 + N + K BnVB &lt;- Bn; bnVB &lt;- bn sig2VB &lt;- MCMCpack::rinvgamma(S,anVB/2,dnVB/2) BetasVB &lt;- MASS::mvrnorm(S, mu = bnVB, Sigma = (dn/an)*BnVB) summary(coda::mcmc(sig2VB)); summary(coda::mcmc(BetasVB)) ELBO &lt;- -N/2*log(2*pi) + a0/2*log(d0/2) - anVB/2*log(dnVB/2) - 0.5*log(det(B0)) + 0.5*log(det(BnVB)) - lgamma(a0/2) + lgamma(anVB/2) - K/2*log(anVB/dnVB) + K/2 LogMarLik &lt;- -N/2*log(2*pi) + a0/2*log(d0/2) - an/2*log(dn/2) - 0.5*log(det(B0)) + 0.5*log(det(Bn)) - lgamma(a0/2) + lgamma(an/2) ELBO; LogMarLik; ELBO &lt; LogMarLik # CAVI ELBOfunc &lt;- function(d,B){ ELBOi &lt;- -N/2*log(2*pi) + a0/2*log(d0/2) - anVB/2*log(d/2) - 0.5*log(det(B0)) + 0.5*log(det(B)) - lgamma(a0/2) + lgamma(anVB/2) - K/2*log(anVB/d) + 0.5*(anVB/d)*sum(diag(B%*%solve(Bn))) return(ELBOi) } d &lt;- 100; B &lt;- diag(K) Esig2inv &lt;- anVB/d; ELBOs &lt;- rep(-Inf, S); epsilon &lt;- 1e-5 for(s in 2:S){ B &lt;- Esig2inv^(-1)*Bn EbQb &lt;- sum(diag(B%*%solve(Bn))) - t(bn)%*%solve(Bn)%*%bn + t(y)%*%y + t(b0)%*%solve(B0)%*%b0 d &lt;- EbQb + d0 Esig2inv &lt;- as.numeric(anVB/d) ELBOs[s] &lt;- ELBOfunc(d = d, B = B) if (ELBOs[s] &lt; ELBOs[s - 1]) { message(&quot;Lower bound decreases!\\n&quot;)} # Check for convergence if (ELBOs[s] - ELBOs[s - 1] &lt; epsilon) { break } # Check if VB converged in the given maximum iterations if (s == S) {warning(&quot;VB did not converge!\\n&quot;)} } sig2VBscratch &lt;- MCMCpack::rinvgamma(S,anVB/2,d/2) BetasVBscratch &lt;- MASS::mvrnorm(S, mu = bnVB, Sigma = B) summary(coda::mcmc(sig2VBscratch)); summary(coda::mcmc(BetasVBscratch)) References Anselin, Luc. 1982. “A Note of Small Sample Properties of Estimators in a First Order Spatial Autoregressive Model.” Environment and Planning 14: 1023–30. Bishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Springer. Bivand, Roger, Virgilio Gómez-Rubio, and Håvard Rue. 2015. “Spatial Data Analysis with r-INLA with Some Extensions.” Journal of Statistical Software 63: 1–31. Blei, David M, and Michael I Jordan. 2006. “Variational Inference for Dirichlet Process Mixtures.” Bayesian Analysis 1 (1): 121–43. Blei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77. Elhorst, J Paul et al. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. Vol. 479. Springer. Gelman, Andrew, John B Carlin, Hal S Stern, David Dunson, Aki Vehtari, and Donald B Rubin. 2021. Bayesian Data Analysis. Chapman; Hall/CRC. Haining, Robert. 1990. Spatial Data Analysis in the Social and Environmental Sciences. First. United of Kingdom: Cambridge University Press. Hoffman, Matthew D, David M Blei, Chong Wang, and John Paisley. 2013. “Stochastic Variational Inference.” The Journal of Machine Learning Research 14 (1): 1303–47. Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. 1999. “An Introduction to Variational Methods for Graphical Models.” Machine Learning 37: 183–233. LeSage, James, and Robert Kelley Pace. 2009. Introduction to Spatial Econometrics. Chapman; Hall/CRC. Martino, Sara, and Andrea Riebler. 2019. “Integrated Nested Laplace Approximations (INLA).” arXiv Preprint arXiv:1907.01248. Nguyen, Duy. 2023. “An in Depth Introduction to Variational Bayes Note.” Available at SSRN 4541076. Ord, K. 1975. “Estimation Methods for Models of Spatial Interaction.” Journal of the American Statistical Association 70 (349): 120–26. Ramírez Hassan, A. 2017. “The Interplay Between the Bayesian and Frequentist Approaches: A General Nesting Spatial Panel Data Model.” Spatial Economic Analysis 12 (1): 92–112. Ramı́rez Hassan, Andrés, and Santiago Montoya Blandón. 2019. “Welfare Gains of the Poor: An Endogenous Bayesian Approach with Spatial Random Effects.” Econometric Reviews 38 (3): 301–18. Robbins, Herbert, and Sutton Monro. 1951. “A Stochastic Approximation Method.” The Annals of Mathematical Statistics, 400–407. Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.” Journal of the Royal Statistical Society Series B: Statistical Methodology 71 (2): 319–92. Rue, Håvard, Andrea Riebler, Sigrunn H Sørbye, Janine B Illian, Daniel P Simpson, and Finn K Lindgren. 2017. “Bayesian Computing with INLA: A Review.” Annual Review of Statistics and Its Application 4 (1): 395–421. Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2002. “Bayesian Measures of Model Complexity and Fit.” Journal of the Royal Statistical Society: Series b (Statistical Methodology) 64 (4): 583–639. Tanner, M. A., and W. H. Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” Journal of the American Statistical Association 82 (398): 528–40. Tierney, Luke, and Joseph B Kadane. 1986. “Accurate Approximations for Posterior Moments and Marginal Densities.” Journal of the American Statistical Association 81 (393): 82–86. Wainwright, Martin J, Michael I Jordan, et al. 2008. “Graphical Models, Exponential Families, and Variational Inference.” Foundations and Trends in Machine Learning 1 (1–2): 1–305. Zhang, Fengshuo, and Chao Gao. 2020. “Convergence Rates of Variational Posterior Distributions.” The Annals of Statistics 48 (4): 2180–2207. Visit https://www.r-inla.org/ for documentation.↩︎ This is a necessary condition to ensure weak stationarity but is not sufficient due to edge and corner effects Haining (1990).↩︎ "],["summary-1.html", "14.3 Summary", " 14.3 Summary In this chapter, we present approximate methods designed for situations where the likelihood function does not have a closed-form expression (e.g., ABC and BSL), or where the sample size and parameter space are large (e.g., INLA and VI), making traditional MCMC and importance sampling (IS) methods ineffective. We provide the theoretical foundations and include applications to illustrate the potential of these methods. Simulation-based algorithms are affected by the curse of dimensionality in the parameter space, while optimization-based approaches typically require the evaluation of likelihood functions. As a result, recent developments, known as hybrid methods, combine these two strategies to address scenarios where both challenges arise simultaneously. See (Martin, Frazier, and Robert 2024) for further references on this topic. References Martin, Gael M, David T Frazier, and Christian P Robert. 2024. “Approximating Bayes in the 21st Century.” Statistical Science 39 (1): 20–45. "],["sec14_4.html", "14.4 Exercises", " 14.4 Exercises g-and-k distribution for financial returns continues Simulate a dataset following the specification given in the g-and-k distribution for financial returns example. Set \\(\\theta_1 = 0.8\\), \\(a = 1\\), \\(b = 0.5\\), \\(g = -1\\), and \\(k = 1\\), with a sample size of 500, and use the same priors as in that example. Implement the ABC accept/reject algorithm from scratch using one million prior draws, selecting the 1,000 draws with the smallest distance.5 Perform a linear regression adjustment using the posterior draws of our ABC-AR algorithm (ABC-AR-Adj). Compare the results with those obtained using the ABC-AR implementation in the EasyABC package, ensuring that the computational time is relatively similar between both implementations. Compare the posterior results of ABC-AR, ABC-AR-Adj, and EasyABC with the population values. Simulation: g-and-k distribution continues Perform the simulation example of the Bayesian synthetic likelihood presented in the book, using the same population parameters and setting \\(M = 500\\), \\(S = 6{,}000\\), with burn-in and thinning parameters set to 1,000 and 5, respectively. Use the BSL package in R to perform inference using the vanilla, unbiased, semi-parametric, and misspecified (mean and variance) versions of BSL. Compare the posterior distributions of the methods with the true population parameters. Simulate a multinomial logit model (see 6.5) with 3 alternatives, 2 alternative-specific regressors, and 1 individual-specific regressor. The population parameters for the alternative-specific regressors are \\(-0.3\\) and \\(1.2\\), while the population values for the individual-specific regressor are \\(0.3\\), \\(0\\), and \\(0.5\\). All regressors are assumed to follow a standard normal distribution, and the sample size is 1,000. Perform inference using the INLA package, and note that the Poisson trick should be used for multinomial models in this exercise (see Serafini (2019) for details). Get the expression for the ELBO in the linear regression model with conjugate family. Linear regression continues Perform inference in the linear regression example using stochastic variational inference via the automatic differentiation variational inference (ADVI) approach (Kucukelbir et al. 2017), implemented in the rstan package. In particular, consider the model \\[ y_i = \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta} + \\mu_i, \\] assuming non-informative independent priors: \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) and \\(\\sigma^2 \\sim {IG}(\\alpha_0/2, \\delta_0/2)\\), where \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}_3\\), \\(\\boldsymbol{B}_0 = 1000\\boldsymbol{I}_3\\), and \\(\\alpha_0 = \\delta_0 = 0.01\\). The sample size is one million. Let’s retake the mixture regression model of Chapter 11, that is, the simple regression mixture with two components such that \\(z_{i1} \\sim \\text{Ber}(0.5)\\), consequently, \\(z_{i2} = 1 - z_{i1}\\), and assume one regressor, \\(x_i \\sim N(0, 1)\\), \\(i = 1, 2, \\dots, 1,000\\). Then, \\[ p(y_i \\mid \\boldsymbol{x}_i) = 0.5 \\phi(y_i \\mid 2 + 1.5 x_i, 1^2) + 0.5 \\phi(y_i \\mid -1 + 0.5 x_i, 0.8^2). \\] Let’s set \\(\\alpha_{h0} = \\delta_{h0} = 0.01\\), \\(\\boldsymbol{\\beta}_{h0} = \\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_{h0} = \\boldsymbol{I}_2\\), and \\(\\boldsymbol{a}_0 = [1/2 \\ 1/2]^{\\top}\\). Perform VB inference in this model using the CAVI algorithm. References Kucukelbir, Alp, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. 2017. “Automatic Differentiation Variational Inference.” Journal of Machine Learning Research 18 (14): 1–45. Serafini, Francesco. 2019. “Multinomial Logit Models with INLA.” R-INLA Tutorial. Https://Inla.r-Inla-Download.org/r-Inla.org/Doc/Vignettes/Multinomial.pdf. Note that this setting does not satisfy the asymptotic requirements for Bayesian consistency. However, it serves as a pedagogical exercise.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
