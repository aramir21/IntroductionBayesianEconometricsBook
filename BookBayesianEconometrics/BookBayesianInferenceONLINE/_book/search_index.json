[["Chap4.html", "Chapter 4 Simulation methods", " Chapter 4 Simulation methods In the previous chapters, we focused on conjugate families, where the posterior and predictive distributions have standard analytical forms (e.g., normal, Student’s t, gamma, binomial, Poisson, etc.) and where the marginal likelihood has a closed-form analytical solution. However, realistic models are often more complex and lack such closed-form solutions. To address this complexity, we rely on simulation (stochastic) methods to draw samples from posterior and predictive distributions. This chapter introduces posterior simulation, a cornerstone of Bayesian inference. We discuss Markov Chain Monte Carlo (MCMC) methods, including Gibbs sampling, Metropolis-Hastings, and Hamiltonian Monte Carlo, as well as other techniques like importance sampling and particle filtering (sequential Monte Carlo). The simulation methods discussed in this chapter are specifically applied throughout this book. However, we do not delve into deterministic methods, such as numerical integration (quadrature), or other simulation methods, including discrete approximation, the probability integral transform, the method of composition, accept-reject sampling, and slice sampling algorithms. While these methods are also widely used, they are not as common as the approaches explicitly employed in this book. For readers interested in these alternative methods, we recommend exploring Christian P. Robert and Casella (2010), Christian P. Robert and Casella (2011), Greenberg (2012), and Gelman et al. (2021). References Gelman, Andrew, John B Carlin, Hal S Stern, David Dunson, Aki Vehtari, and Donald B Rubin. 2021. Bayesian Data Analysis. Chapman; Hall/CRC. Greenberg, Edward. 2012. Introduction to Bayesian Econometrics. Cambridge University Press. Robert, Christian P., and George Casella. 2011. Monte Carlo Statistical Methods. 2nd ed. New York: Springer. Robert, Christian P, and George Casella. 2010. Introducing Monte Carlo Methods with r. Vol. 18. Springer. "],["sec51.html", "4.1 Markov Chain Monte Carlo methods", " 4.1 Markov Chain Monte Carlo methods Markov Chain Monte Carlo (MCMC) methods are algorithms used to approximate complex probability distributions by constructing a Markov chain. This chain is a sequence of random samples where each sample depends only on the previous one. The goal of MCMC methods is to obtain draws from the posterior distribution as the equilibrium distribution. The key point in MCMC methods is the transition kernel or density, \\(q(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{\\theta}^{(s-1)})\\), which generates a draw \\(\\boldsymbol{\\theta}^{(s)}\\) at stage \\(s\\) that depends solely on \\(\\boldsymbol{\\theta}^{(s-1)}\\). This transition distribution must be designed such that the Markov chain converges to a unique stationary distribution, which, in our case, is the posterior distribution, that is, \\[ \\pi(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{y})=\\int_{\\boldsymbol{\\Theta}}q(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{\\theta}^{(s-1)})\\pi(\\boldsymbol{\\theta}^{(s-1)}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)}. \\] Given that we start at an arbitrary point, \\(\\boldsymbol{\\theta}^{(0)}\\), the algorithm requires that the Markov chain be irreducible, meaning that the process can reach any other state with positive probability. Additionally, the process must be aperiodic, meaning that for each state, the greatest common divisor of the number of steps it takes to return to the state is 1, ensuring that there are no cycles forcing the system to return to a state only after a fixed number of steps. Furthermore, the process must be recurrent, meaning that it will return to any state an infinite number of times with probability one. However, to ensure convergence to the stationary distribution, a stronger condition is required: the process must be positive recurrent, meaning that the expected return time to a state is finite. Given an irreducible, aperiodic, and positive recurrent transition density, the Markov chain algorithm will asymptotically converge to the stationary posterior distribution we are seeking. For more details, see Christian P. Robert and Casella (2011). 4.1.1 Gibbs sampler The Gibbs sampler algorithm is one of the most widely used MCMC methods for sampling from non-standard distributions in Bayesian analysis. While it is a special case of the Metropolis-Hastings (MH) algorithm, it originated from a different theoretical background (Geman and Geman 1984; A. E. Gelfand and Smith 1990). The key requirement for implementing the Gibbs sampling algorithm is the availability of conditional posterior distributions. The algorithm works by cycling through the conditional posterior distributions corresponding to different blocks of the parameter space under inference. To simplify concepts, let’s focus on a parameter space composed of two blocks, \\(\\boldsymbol{\\theta} = [\\boldsymbol{\\theta}_1 \\ \\boldsymbol{\\theta}_2]^{\\top}\\). The Gibbs sampling algorithm uses the transition kernel \\[ q(\\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s-1)},\\boldsymbol{\\theta}_2^{(s-1)})=\\pi(\\boldsymbol{\\theta}_1^{(s)}\\mid \\boldsymbol{\\theta}_2^{(s-1)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y}). \\] Thus, \\[ \\begin{aligned} \\int_{\\boldsymbol{\\Theta}}q(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{\\theta}^{(s-1)})\\pi(\\boldsymbol{\\theta}^{(s-1)}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)} &amp;=\\int_{\\boldsymbol{\\Theta}_2}\\int_{\\boldsymbol{\\Theta}_1}\\pi(\\boldsymbol{\\theta}_1^{(s)}\\mid \\boldsymbol{\\theta}_2^{(s-1)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}^{(s-1)}_1,\\boldsymbol{\\theta}^{(s-1)}_2\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)}_1d\\boldsymbol{\\theta}^{(s-1)}_2\\\\ &amp;=\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y})\\int_{\\boldsymbol{\\Theta}_2}\\int_{\\boldsymbol{\\Theta}_1}\\pi(\\boldsymbol{\\theta}_1^{(s)}\\mid \\boldsymbol{\\theta}_2^{(s-1)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}^{(s-1)}_1,\\boldsymbol{\\theta}^{(s-1)}_2\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)}_1d\\boldsymbol{\\theta}^{(s-1)}_2\\\\ &amp;=\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y})\\int_{\\boldsymbol{\\Theta}_2}\\pi(\\boldsymbol{\\theta}_1^{(s)}\\mid \\boldsymbol{\\theta}_2^{(s-1)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}^{(s-1)}_2\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)}_2\\\\ &amp;=\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y})\\int_{\\boldsymbol{\\Theta}_2}\\pi(\\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{\\theta}_2^{(s-1)}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}^{(s-1)}_2\\\\ &amp;=\\pi(\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{y})\\pi(\\boldsymbol{\\theta}_1^{(s)}\\mid \\boldsymbol{y})\\\\ &amp;=\\pi(\\boldsymbol{\\theta}_1^{(s)},\\boldsymbol{\\theta}_2^{(s)}\\mid \\boldsymbol{y}). \\end{aligned} \\] Then, \\(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\) is the stationary distribution for the Gibbs transition kernel. A word of caution! Even if we have well-defined conditional posterior distributions \\(\\pi(\\boldsymbol{\\theta}_1^{(s)} \\mid \\boldsymbol{\\theta}_2^{(s-1)}, \\boldsymbol{y})\\) and \\(\\pi(\\boldsymbol{\\theta}_2^{(s)} \\mid \\boldsymbol{\\theta}_1^{(s)}, \\boldsymbol{y})\\), and we can simulate from them, the joint posterior distribution \\(\\pi(\\boldsymbol{\\theta}_1^{(s)}, \\boldsymbol{\\theta}_2^{(s)} \\mid \\boldsymbol{y})\\) may not correspond to any proper distribution. We should be mindful of this situation, especially when dealing with improper prior distributions (see Christian P. Robert and Casella (2011) for details). Algorithm 1 demonstrates the implementation of a Gibbs sampler with \\(d\\) blocks. The number of iterations (\\(S\\)) is chosen to ensure convergence to the stationary distribution. In Section 4.4, we review several convergence diagnostics to assess whether the posterior draws have reached convergence. Algorithm: Gibbs sampling Set θ2(0), θ3(0), ..., θd(0), For s=1,2,...,S do Draw θ1(s) from π(θ1(s)|θ2(s-1),...,θd(s-1),y) Draw θ2(s) from π(θ2(s)|θ1(s),...,θd(s-1),y) ... Draw θd(s) from π(θd(s)|θ1(s),...,θd-1(s),y) End for Example: Mining disaster change point Let’s use the dataset Mining.csv provided by Carlin, Gelfand, and Smith (1992). This dataset records the number of mining disasters per year from 1851 to 1962 in British coal mines. We assume there is an unknown structural change point in the number of mining disasters, where the parameters of the Poisson distributions change. In particular: \\[ \\begin{align*} p(y_t) = \\begin{cases} \\frac{\\exp(-\\lambda_1) \\lambda_1^{y_t}}{y_t!}, &amp; t = 1, 2, \\dots, H \\\\ \\frac{\\exp(-\\lambda_2) \\lambda_2^{y_t}}{y_t!}, &amp; t = H+1, \\dots, T \\end{cases} \\end{align*} \\] where \\(H\\) is the changing point. We use conjugate families for \\(\\lambda_l\\), \\(l = 1, 2\\), where \\(\\lambda_l \\sim G(\\alpha_{l0}, \\beta_{l0})\\), and set \\(\\pi(H) = 1 / T\\), which corresponds to a discrete uniform distribution for the change point. This implies that, a priori, we assume equal probability for any time to be the change point. The posterior distribution is: \\[ \\begin{align*} \\pi(\\lambda_1, \\lambda_2, H \\mid \\mathbf{y}) &amp;\\propto \\prod_{t=1}^{H} \\frac{\\exp(-\\lambda_1) \\lambda_1^{y_t}}{y_t!} \\prod_{t=H+1}^{T} \\frac{\\exp(-\\lambda_2) \\lambda_2^{y_t}}{y_t!} \\\\ &amp;\\times \\exp(-\\beta_{10} \\lambda_1) \\lambda_1^{\\alpha_{10}-1} \\exp(-\\beta_{20} \\lambda_2) \\lambda_2^{\\alpha_{20}-1} \\frac{1}{T} \\\\ &amp;\\propto \\exp(-H \\lambda_1) \\lambda_1^{\\sum_{t=1}^{H} y_t} \\exp(-(T-H) \\lambda_2) \\lambda_2^{\\sum_{t=H+1}^{T} y_t} \\\\ &amp;\\times \\exp(-\\beta_{10} \\lambda_1) \\lambda_1^{\\alpha_{10}-1} \\exp(-\\beta_{20} \\lambda_2) \\lambda_2^{\\alpha_{20}-1} \\end{align*} \\] Then, the conditional posterior distribution of \\(\\lambda_1 \\mid \\lambda_2, H, \\mathbf{y}\\) is: \\[ \\begin{align*} \\pi(\\lambda_1 \\mid \\lambda_2, H, \\mathbf{y}) &amp;\\propto \\exp(-(H + \\beta_{10}) \\lambda_1) \\lambda_1^{\\sum_{t=1}^{H} y_t + \\alpha_{10} - 1} \\end{align*} \\] That is, \\(\\lambda_1 \\mid \\lambda_2, H, \\mathbf{y} \\sim G(\\alpha_{1n}, \\beta_{1n})\\), \\(\\beta_{1n} = H + \\beta_{10}\\) and \\(\\alpha_{1n} = \\sum_{t=1}^{H} y_t + \\alpha_{10}\\). The conditional posterior distribution of \\(\\lambda_2\\mid \\lambda_1,H,y\\) is \\[\\begin{align*} \\pi(\\lambda_2\\mid \\lambda_1,H,y)&amp;\\propto\\exp(-((T-H)+\\beta_{20})\\lambda_2)\\lambda_2^{\\sum_{t=H+1}^T y_t+\\alpha_{20}-1}, \\end{align*}\\] that is, \\(\\lambda_2\\mid \\lambda_1,H,y\\sim G(\\alpha_{2n},\\beta_{2n})\\), \\(\\beta_{2n}=(T-H)+\\beta_{20}\\) and \\(\\alpha_{2n}=\\sum_{t=H+1}^T y_t+\\alpha_{20}\\). The conditional posterior distribution of the change point is \\[\\begin{align*} \\pi(H\\mid \\lambda_1,\\lambda_2,y)&amp;\\propto\\exp(-H\\lambda_1)\\lambda_1^{\\sum_{t=1}^H y_t}\\exp(-(T-H)\\lambda_2)\\lambda_2^{\\sum_{t=H+1}^T y_t}\\\\ &amp;\\propto \\exp(-H(\\lambda_1-\\lambda_2))\\lambda_1^{\\sum_{t=1}^H y_t}\\lambda_2^{\\sum_{t=H+1}^T y_t} \\exp(-T\\lambda_2) \\frac{\\lambda_2^{\\sum_{t=1}^H y_t}}{\\lambda_2^{\\sum_{t=1}^H y_t}}\\\\ &amp;\\propto \\exp(-H(\\lambda_1-\\lambda_2))\\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^{\\sum_{t=1}^H y_t}. \\end{align*}\\] Thus, the conditional posterior distribution of \\(H\\) is \\[\\begin{align*} \\pi(H\\mid \\lambda_1,\\lambda_2,y)=&amp; \\frac{\\exp(-H(\\lambda_1-\\lambda_2))\\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^{\\sum_{t=1}^H y_t}}{\\sum_{H=1}^T \\exp(-H(\\lambda_1-\\lambda_2))\\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^{\\sum_{t=1}^H y_t}}, &amp; H=1,2,\\dots,T. \\end{align*}\\] The following code shows how to do a Gibbs sampling algorithm to perform inference of this model using the hyperparameters suggested by Greenberg (2012), \\(\\alpha_{l0}=0.5\\) and \\(\\beta_{l0}=1\\), \\(l=1,2\\). # Clean workspace and set seed rm(list = ls()) set.seed(10101) # Load data dataset &lt;- read.csv( &quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/MiningDataCarlin.csv&quot;, header = TRUE ) str(dataset) ## &#39;data.frame&#39;: 112 obs. of 2 variables: ## $ year : int 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 ... ## $ Count: int 4 5 4 1 0 4 3 4 0 6 ... # Hyperparameters a10 &lt;- 0.5; b10 &lt;- 1 a20 &lt;- 0.5; b20 &lt;- 1 # Extract data y &lt;- dataset$Count sum_y &lt;- sum(y) N &lt;- length(y) # Storage for posterior samples S &lt;- 10000 theta1 &lt;- numeric(S) theta2 &lt;- numeric(S) kk &lt;- numeric(S) # Initial value of change point k &lt;- 60 # Gibbs sampler for (i in 1:S) { a1 &lt;- a10 + sum(y[1:k]) b1 &lt;- b10 + k theta1[i] &lt;- rgamma(1, shape = a1, rate = b1) a2 &lt;- a20 + sum(y[(k + 1):N]) b2 &lt;- b20 + (N - k) theta2[i] &lt;- rgamma(1, shape = a2, rate = b2) pp &lt;- numeric(N) for (l in 1:N) { pp[l] &lt;- exp(l * (theta2[i] - theta1[i])) * (theta1[i] / theta2[i])^sum(y[1:l]) } prob &lt;- pp / sum(pp) k &lt;- sample(1:N, 1, prob = prob) kk[i] &lt;- k } # Summaries library(coda) summary(mcmc(theta1)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 3.051805 0.283456 0.002835 0.003054 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 2.513 2.856 3.046 3.237 3.632 summary(mcmc(theta2)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 0.915383 0.117658 0.001177 0.001268 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 0.6996 0.8341 0.9117 0.9932 1.1570 summary(mcmc(kk)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 40.15020 2.48875 0.02489 0.02903 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 36 39 40 41 46 # Plot histogram hist( kk, main = &quot;Histogram: Posterior mean change point&quot;, xlab = &quot;Posterior mean&quot;, col = &quot;blue&quot;, breaks = 25 ) The posterior results indicate that the rate of disasters decrease from 3.1 to 0.92 per year in 1890. The figure shows the histogram of the posterior draws of the change point in mining disasters. 4.1.2 Metropolis-Hastings The Metropolis-Hastings (M-H) algorithm (Metropolis et al. 1953; Hastings 1970) is a general MCMC method that does not require standard closed-form solutions for the conditional posterior distributions. The key idea is to use a transition kernel whose unique invariant distribution is \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\). This kernel must satisfy the balancing condition, meaning that, given a realization \\(\\boldsymbol{\\theta}^{(s-1)}\\) at stage \\(s-1\\) from the stationary distribution \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\), we generate a candidate draw \\(\\boldsymbol{\\theta}^{c}\\) from the proposal distribution \\(q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{(s-1)})\\) at stage \\(s\\) such that: \\[ q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{(s-1)}) \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\mathbf{y}) = q(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{\\theta}^{c}) \\pi(\\boldsymbol{\\theta}^{c} \\mid \\mathbf{y}), \\] which implies that the probability of moving from \\(\\boldsymbol{\\theta}^{(s-1)}\\) to \\(\\boldsymbol{\\theta}^{c}\\) is equal to the probability of moving from \\(\\boldsymbol{\\theta}^{c}\\) to \\(\\boldsymbol{\\theta}^{(s-1)}\\). In general, the balancing condition is not automatically satisfied, and we must introduce an acceptance probability \\(\\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^{c})\\) to ensure that the condition holds: \\[ q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{(s-1)}) \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\mathbf{y}) \\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^{c}) = q(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{\\theta}^{c}) \\pi(\\boldsymbol{\\theta}^{c} \\mid \\mathbf{y}). \\] Thus, the acceptance probability is given by: \\[ \\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^{c}) = \\min\\left\\{\\frac{q(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{\\theta}^{c}) \\pi(\\boldsymbol{\\theta}^{c} \\mid \\mathbf{y})}{q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{(s-1)}) \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\mathbf{y})}, 1\\right\\}, \\] where \\(q(\\boldsymbol{\\theta}^{c} \\mid \\boldsymbol{\\theta}^{(s-1)})\\) and \\(\\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\mathbf{y})\\) must be nonzero, as transitioning from \\(\\boldsymbol{\\theta}^{(s-1)}\\) to \\(\\boldsymbol{\\theta}^{c}\\) is only possible under these conditions. Algorithm 2 shows how to implement a Metropolis-Hastings algorithm. The number of iterations (\\(S\\)) is chosen to ensure convergence to the stationary distribution. Algorithm: Metropolis-Hastings Set θ(0) in the support of π(θ|y) For s=1,2,...,S do Draw θc from q(θc|θ(s-1)) Calculate α(θ(s-1),θc)=min((q(θ(s-1)|θc)π(θc|y))/(q(θc|θ(s-1))π(θ(s-1)|y)),1) Draw U from U(0,1) θ(s)= θc if U (s-1),θc) θ(s)= θ(s-1) otherwise End for Some remarks: First, we do not need to know the marginal likelihood to implement the M-H algorithm, as it cancels out when calculating the acceptance probability. Specifically, given that \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\propto \\pi(\\boldsymbol{\\theta}) \\times p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\), we can use the right-hand side expression to compute the acceptance probability. Second, the Gibbs sampling algorithm is a particular case of the M-H algorithm where the acceptance probability is equal to 1 (Gelman and Rubin (1992) and Christian P. Robert and Casella (2011), see Exercise 2). Third, we can combine the M-H and Gibbs sampling algorithms when dealing with relatively complex posterior distributions. Specifically, the Gibbs sampling algorithm can be used for blocks with conditional posterior distributions in standard closed forms, while the M-H algorithm is applied to sample from conditional posterior distributions that do not have standard forms. This approach is known as the M-H within Gibbs sampling algorithm. Fourth, we can note that the transition kernel in the M-H algorithm is a mixture of a continuous density (\\(q(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{\\theta}^{(s-1)})\\)) and a probability mass function (\\(\\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^c)\\)) (Chib and Greenberg 1995). Fifth, a crucial point associated with the proposal densities is the acceptance probability. Low or high acceptance probabilities are not ideal. A low rate implies poor mixing, meaning the chain does not move effectively through the support of the posterior distribution. Conversely, a high acceptance rate implies that the chain will converge too slowly. A sensible value depends on the dimension of the parameter space. A rule of thumb is that if the dimension is less than or equal to 2, the acceptance rate should be around 0.50. If the dimension is greater than 2, the acceptance rate should be approximately 0.25 (Roberts, Gelman, and Gilks 1997). For technical details of the Metropolis-Hastings algorithm, see Christian P. Robert and Casella (2011), Chap. 7. Regarding the proposal density, it must be positive everywhere the posterior distribution is positive. This ensures that the Markov chain can explore the entire support of the posterior distribution. Additionally, the proposal density must allow the Markov chain to reach any region of the posterior distribution’s support. There are three standard approaches for choosing the proposal density: the independent proposal, the random walk proposal, and the tailored proposal. In the independent proposal, \\(q(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{\\theta}^{(s-1)}) = q(\\boldsymbol{\\theta}^c)\\), which implies that \\[ \\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^c) = \\min\\left\\{\\frac{q(\\boldsymbol{\\theta}^{(s-1)}) \\pi(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta}^c) \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{y})}, 1\\right\\}. \\] In this case, a move from \\(\\boldsymbol{\\theta}^{(s-1)}\\) to \\(\\boldsymbol{\\theta}^c\\) is always accepted if \\(q(\\boldsymbol{\\theta}^{(s-1)}) \\pi(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{y}) \\geq q(\\boldsymbol{\\theta}^c) \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{y})\\). In the random walk proposal, \\(\\boldsymbol{\\theta}^c = \\boldsymbol{\\theta}^{(s-1)} + \\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon}\\) is a random perturbation. If \\(p(\\boldsymbol{\\epsilon}) = p(-\\boldsymbol{\\epsilon})\\), meaning the distribution \\(p(\\boldsymbol{\\epsilon})\\) is symmetric around zero, then \\(q(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{\\theta}^{(s-1)}) = q(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{\\theta}^c)\\). This was the original Metropolis algorithm (Metropolis et al. 1953). Thus, the acceptance rate is \\[ \\alpha(\\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{\\theta}^c) = \\min\\left\\{\\frac{\\pi(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{y})}{\\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{y})}, 1\\right\\}. \\] In this case, a move from \\(\\boldsymbol{\\theta}^{(s-1)}\\) to \\(\\boldsymbol{\\theta}^c\\) is always accepted if \\(\\pi(\\boldsymbol{\\theta}^c \\mid \\boldsymbol{y}) \\geq \\pi(\\boldsymbol{\\theta}^{(s-1)} \\mid \\boldsymbol{y})\\). In the tailored proposal, the density is designed to have fat tails, is centered at the mode of the posterior distribution, and its scale matrix is given by the negative inverse Hessian matrix evaluated at the mode. Specifically, for two blocks, the log posterior distribution is maximized with respect to \\(\\boldsymbol{\\theta}_1\\) given \\(\\boldsymbol{\\theta}_2\\). This process is repeated at each iteration of the algorithm because \\(\\boldsymbol{\\theta}_2\\) changes at different stages. As a result, the algorithm can be slow since the optimization process is computationally demanding (see Greenberg (2012), Chaps. 7 and 9 for examples). A sensible recommendation when performing the M-H algorithm is to use a random walk proposal such that \\(\\boldsymbol{\\epsilon} \\sim N(\\boldsymbol{0}, c^2 \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\Sigma}\\) is the negative inverse Hessian matrix evaluated at the mode, that is, maximize with respect to all parameters, and set \\(c \\approx 2.4 / \\sqrt{\\text{dim}(\\boldsymbol{\\theta})}\\), which is the most efficient scale compared to independent sampling (Gelman et al. 2021). After some iterations of the algorithm, adjust the scale matrix \\(\\boldsymbol{\\Sigma}\\) as before, and increase or decrease \\(c\\) if the acceptance rate of the simulations is too high or low, respectively. The objective is to bring the acceptance rate to the stated rule of thumb: if the dimension is less than or equal to 2, the acceptance rate should be around 0.50, and if the dimension is greater than 2, the acceptance rate should be around 0.25. Once this is achieved, we should run the algorithm without modifications and use this part of the algorithm to perform inference. Example: Ph.D. students sleeping hours continues In the Ph.D. students sleeping hours exercise of Chapter 3 we get a posterior distribution that is Beta with parameters 16.55 and 39.57. We can sample from this posterior distribution using the function rbeta from R. However, we want to compare the performance of a M-H algorithm using as proposal density a \\(U(0,1)\\) distribution. The following code shows how to do a M-H algorithm to sample from the beta distribution using the uniform distribution. # Clear workspace and set seed for reproducibility rm(list = ls()) set.seed(10101) # Beta distribution parameters a_n &lt;- 16.55 b_n &lt;- 39.57 # Number of samples S &lt;- 100000 # Initialize vectors samples &lt;- numeric(S) accept_flags &lt;- logical(S) # Initial value samples[1] &lt;- runif(1) # Metropolis-Hastings sampling for (s in 2:S) { candidate &lt;- runif(1) alpha &lt;- dbeta(candidate, a_n, b_n) / dbeta(samples[s - 1], a_n, b_n) u &lt;- runif(1) if (u &lt;= alpha) { samples[s] &lt;- candidate accept_flags[s] &lt;- TRUE } else { samples[s] &lt;- samples[s - 1] accept_flags[s] &lt;- FALSE } } # Posterior summaries mean_accept &lt;- mean(accept_flags) mean_sample &lt;- mean(samples) sd_sample &lt;- sd(samples) # True mean and standard deviation of Beta(a_n, b_n) true_mean &lt;- a_n / (a_n + b_n) true_sd &lt;- sqrt((a_n * b_n) / ((a_n + b_n)^2 * (a_n + b_n + 1))) # Print summaries cat(&quot;Acceptance rate:&quot;, mean_accept, &quot;\\n&quot;) ## Acceptance rate: 0.1949 cat(&quot;Sample mean:&quot;, mean_sample, &quot; | True mean:&quot;, true_mean, &quot;\\n&quot;) ## Sample mean: 0.2954284 | True mean: 0.2949038 cat(&quot;Sample SD: &quot;, sd_sample, &quot; | True SD: &quot;, true_sd, &quot;\\n&quot;) ## Sample SD: 0.060951 | True SD: 0.06033513 # Histogram and overlaid density hist_info &lt;- hist( samples, breaks = 50, col = &quot;blue&quot;, xlab = &quot;Proportion of Ph.D. students sleeping ≥ 6 hours&quot;, main = &quot;Beta draws from a Metropolis-Hastings algorithm&quot; ) x_vals &lt;- seq(min(samples), max(samples), length.out = 50) y_vals &lt;- dbeta(x_vals, a_n, b_n) y_vals &lt;- y_vals * diff(hist_info$mids[1:2]) * length(samples) lines(x_vals, y_vals, col = &quot;red&quot;, lwd = 2) The results indicate that the mean and standard deviation obtained from the posterior draws are similar to the population values. Furthermore, this figure presents the histogram of the posterior draws alongside the density of the beta distribution, demonstrating a good match between them. 4.1.3 Hamiltonian Monte Carlo Hamiltonian Monte Carlo (HMC) was proposed by Duane et al. (1987) and later introduced to the statistical community by Neal (1996). HMC extends the Metropolis algorithm to efficiently explore the parameter space by introducing momentum variables, which help overcome the random walk behavior of Gibbs sampling and the Metropolis-Hastings algorithm. Known also as hybrid Monte Carlo, HMC is particularly advantageous for high-dimensional posterior distributions, as it reduces the risk of getting stuck in local modes and significantly improves mixing (Neal 2011). However, HMC is designed to work with strictly positive target densities. Therefore, transformations are required to handle bounded parameters, such as variances and proportions. For example, logarithmic and logit transformations can be applied. These transformations necessitate the use of the change-of-variable theorem to compute the log posterior density and its gradient, which are essential for implementing the HMC algorithm. HMC leverages concepts from physics, specifically Hamiltonian mechanics, to propose transitions in the Markov chain. In Hamiltonian mechanics, two key variables define the total energy of the system: the position (\\(\\boldsymbol{\\theta}\\)) and the momentum (\\(\\boldsymbol{\\delta}\\)). The Hamiltonian represents the total energy of the system, consisting of potential energy (energy due to position) and kinetic energy (energy associated with motion). The objective is to identify trajectories that preserve the system’s total energy, meaning the Hamiltonian remains invariant, while avoiding trajectories that do not. This approach enhances the acceptance rate of proposed transitions. To implement HMC, we solve the differential equations derived from the Hamiltonian, which involve derivatives with respect to position and momentum. However, these equations rarely have analytical solutions, requiring numerical methods for approximation. This necessitates discretizing Hamilton’s equations, which introduces errors. To mitigate these errors, HMC uses the leapfrog integrator, a numerical method with smaller errors compared to simpler approaches like the Euler method. HMC uses a momentum variable (\\(\\delta_k\\)) for each \\(\\theta_k\\), so that the transition kernel of \\(\\boldsymbol{\\theta}\\) is determined by \\(\\boldsymbol{\\delta}\\). Both vectors are updated using a Metropolis algorithm at each stage such that the distribution of \\(\\boldsymbol{\\theta}\\) remains invariant (Neal 2011). The joint density in HMC is given by \\(p(\\boldsymbol{\\theta}, \\boldsymbol{\\delta} \\mid \\boldsymbol{y}) = \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\times p(\\boldsymbol{\\delta})\\), where \\(\\boldsymbol{\\delta} \\sim N(\\boldsymbol{0}, \\boldsymbol{M})\\), and \\(\\boldsymbol{M}\\) is a diagonal matrix such that \\(\\delta_k \\sim N(0, M_{kk})\\). Algorithm 3 outlines the HMC implementation. The gradient vector \\(\\frac{d \\log(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}))}{d \\boldsymbol{\\theta}}\\) must be computed analytically, as using finite differences can be computationally expensive. However, it is advisable to verify the analytical calculations by evaluating the gradient at the maximum posterior estimate, where the function should return values close to 0, or by comparing results with finite differences at a few points. Algorithm: Hamiltonian Monte Carlo Set θ(0) in the support of π(θ|y), and set step size ε, number of leapfrog steps L, and total iterations S Draw δ(0) from N(0, M) For s=1,2,...,S do For l=1,2,...,L do if l=1 then δc ← δ(s-1) + 0.5 ε dlog(π(θ|y))/dθ θc ← θ(s-1) + ε M-1 δc else if l=2,...,L-1 then δc ← δc + ε dlog(π(θ|y))/dθ θc ← θc + ε M-1 δc else δc ← δc + 0.5 ε dlog(π(θ|y))/dθ θc ← θc + ε M-1 δc End if End if End for Calculate α([θ, δ](s-1),[θ, δ]c)=min((p(δc)π(θc|y))/(p(δ(s-1))π(θ(s-1)|y)),1) Draw U from U(0,1) θ(s)= θc if U (s-1),[θ, δ]c) θ(s)= θ(s-1) otherwise End for Note that HMC does not require the marginal likelihood, as neither the gradient vector \\(\\frac{d \\log(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}))}{d \\boldsymbol{\\theta}}\\) nor the acceptance rate depend on it. That is, we can use only \\(\\pi(\\boldsymbol{\\theta}) \\times p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\) to implement HMC. In addition, we do not retain \\(\\boldsymbol{\\delta}\\) after it is updated at the beginning of each iteration, as it is not required subsequently. To begin, the step size (\\(\\epsilon\\)) can be drawn randomly from a uniform distribution between 0 and \\(2\\epsilon_0\\), and the number of leapfrog steps (\\(L\\)) is set as the largest integer near \\(1/\\epsilon\\), ensuring \\(\\epsilon \\times L \\approx 1\\). We need to set \\(\\boldsymbol{M}\\) to be the inverse of the posterior covariance matrix evaluated at the maximum a posteriori estimate under this setting. The acceptance rate should be checked, with the optimal rate around 65% (Gelman et al. 2021). If the acceptance rate is much higher than 65%, increase \\(\\epsilon_0\\); if it is much lower, decrease it. This strategy may not always work, and alternative strategies can be tested, such as setting \\(\\boldsymbol{M} = \\boldsymbol{I}\\) and fine-tuning \\(\\epsilon\\) and \\(L\\) to achieve an acceptance rate near 65%. Finally, the number of iterations (\\(S\\)) is chosen to ensure convergence to the stationary distribution. Example: Sampling from a bi-variate Gaussian distribution As a toy example, let’s compare the Gibbs sampling, M-H, and HMC algorithms when the posterior distribution is a bi-variate Gaussian distribution with mean \\(\\boldsymbol{0}\\) and covariance matrix \\(\\boldsymbol{\\Sigma} = \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}\\). Let’s set \\(\\rho = 0.98\\). The Gibbs sampler requires the conditional posterior distributions, which in this case are \\(\\theta_1 \\mid \\theta_2 \\sim N(\\rho \\theta_2, 1 - \\rho^2)\\) and \\(\\theta_2 \\mid \\theta_1 \\sim N(\\rho \\theta_1, 1 - \\rho^2)\\). We use the random walk proposal distribution for the M-H algorithm, where \\(\\boldsymbol{\\theta}^c \\sim N(\\boldsymbol{\\theta}^{(s-1)}, \\text{diag}\\left\\{0.18^2\\right\\})\\). We set \\(\\epsilon = 0.05\\), \\(L = 20\\), and \\(\\boldsymbol{M} = \\boldsymbol{I}_2\\) for the HMC algorithm, and given that \\(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\propto \\exp\\left\\{-\\frac{1}{2} \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\theta}\\right\\}\\), then \\(\\frac{d \\log(\\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}))}{d \\boldsymbol{\\theta}} = -\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\theta}\\). The following code shows how to implement the Gibbs sampler, the random walk M-H algorithm, and the HMC in this example such that the effective number of posterior draws is 400. # Load packages library(MASS) library(mvtnorm) library(coda) library(ggplot2) library(ggpubr) library(latex2exp) set.seed(10101) ###---------------------------------------------- ### Functions ###---------------------------------------------- # Gibbs sampler gibbs_sampler &lt;- function(theta, rho) { rnorm(1, mean = rho * theta, sd = sqrt(1 - rho^2)) } # Metropolis-Hastings sampler mh_sampler &lt;- function(theta, rho, sig2) { sigma_target &lt;- matrix(c(1, rho, rho, 1), 2, 2) sigma_proposal &lt;- matrix(c(1, sig2, sig2, 1), 2, 2) theta_candidate &lt;- mvrnorm(1, mu = theta, Sigma = sigma_proposal) a &lt;- dmvnorm(theta_candidate, mean = c(0, 0), sigma = sigma_target) / dmvnorm(theta, mean = c(0, 0), sigma = sigma_target) if (runif(1) &lt;= a) { list(theta = theta_candidate, accept = 1) } else { list(theta = theta, accept = 0) } } # Hamiltonian Monte Carlo hmc_sampler &lt;- function(theta, rho, epsilon, M) { sigma_target &lt;- matrix(c(1, rho, rho, 1), 2, 2) L &lt;- ceiling(1 / epsilon) M_inv &lt;- solve(M) K &lt;- length(theta) # Initial momentum momentum &lt;- t(rmvnorm(1, mean = rep(0, K), sigma = M)) theta_old &lt;- theta # Initial log posterior + kinetic energy log_start &lt;- dmvnorm(theta_old, mean = rep(0, K), sigma = sigma_target, log = TRUE) + dmvnorm(as.vector(momentum), mean = rep(0, K), sigma = M, log = TRUE) # Leapfrog steps for (l in 1:L) { grad &lt;- -solve(sigma_target) %*% theta if (l == 1 || l == L) { momentum &lt;- momentum + 0.5 * epsilon * grad } else { momentum &lt;- momentum + epsilon * grad } theta &lt;- theta + epsilon * M_inv %*% momentum } # Final log posterior + kinetic energy log_end &lt;- dmvnorm(as.vector(theta), mean = rep(0, K), sigma = sigma_target, log = TRUE) + dmvnorm(as.vector(momentum), mean = rep(0, K), sigma = M, log = TRUE) alpha &lt;- min(1, exp(log_end - log_start)) if (runif(1) &lt;= alpha) { theta_new &lt;- as.vector(theta) } else { theta_new &lt;- theta_old } list(theta = theta_new, prob = alpha) } ###---------------------------------------------- ### Parameters and storage ###---------------------------------------------- rho &lt;- 0.98 sig2 &lt;- 0.18^2 S_gibbs &lt;- 8000 thin &lt;- 20 K &lt;- 2 keep &lt;- seq(0, S_gibbs, by = thin)[-1] theta_post_gibbs &lt;- matrix(NA, S_gibbs, K) theta_post_mh &lt;- matrix(NA, S_gibbs, K) accept_mh &lt;- logical(S_gibbs) theta_gibbs &lt;- c(-2, 3) theta_mh &lt;- c(-2, 3) ###---------------------------------------------- ### Gibbs and Metropolis-Hastings ###---------------------------------------------- for (s in 1:S_gibbs) { theta1 &lt;- gibbs_sampler(theta_gibbs[2], rho) theta2 &lt;- gibbs_sampler(theta1, rho) theta_gibbs &lt;- c(theta1, theta2) res_mh &lt;- mh_sampler(theta_mh, rho, sig2) theta_mh &lt;- res_mh$theta theta_post_gibbs[s, ] &lt;- theta_gibbs theta_post_mh[s, ] &lt;- theta_mh accept_mh[s] &lt;- res_mh$accept } cat(&quot;MH acceptance rate:&quot;, mean(accept_mh[keep]), &quot;\\n&quot;) ## MH acceptance rate: 0.165 mcmc_gibbs &lt;- mcmc(theta_post_gibbs[keep, ]) mcmc_mh &lt;- mcmc(theta_post_mh[keep, ]) summary(mcmc_gibbs) ## ## Iterations = 1:400 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 400 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.09561 0.9230 0.04615 0.06976 ## [2,] 0.09338 0.9258 0.04629 0.07029 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 -1.748 -0.4606 0.07596 0.6520 1.937 ## var2 -1.652 -0.5319 0.10553 0.6702 1.881 autocorr.plot(mcmc_gibbs) plot(mcmc_mh) autocorr.plot(mcmc_mh) ###---------------------------------------------- ### Hamiltonian Monte Carlo ###---------------------------------------------- S_hmc &lt;- 400 epsilon &lt;- 0.05 M &lt;- diag(2) theta_post_hmc &lt;- matrix(NA, S_hmc, K) accept_prob_hmc &lt;- numeric(S_hmc) theta_hmc &lt;- c(-2, 3) for (s in 1:S_hmc) { res_hmc &lt;- hmc_sampler(theta_hmc, rho, epsilon, M) theta_hmc &lt;- res_hmc$theta theta_post_hmc[s, ] &lt;- theta_hmc accept_prob_hmc[s] &lt;- res_hmc$prob } mcmc_hmc &lt;- mcmc(theta_post_hmc) summary(mcmc_hmc) ## ## Iterations = 1:400 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 400 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.1628 0.8562 0.04281 0.1029 ## [2,] 0.1768 0.8916 0.04458 0.1052 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 -1.442 -0.4048 0.1834 0.7126 1.753 ## var2 -1.500 -0.4235 0.1279 0.7540 1.952 summary(accept_prob_hmc) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2422 0.8005 0.9705 0.8747 1.0000 1.0000 plot(mcmc_hmc) autocorr.plot(mcmc_hmc) ###---------------------------------------------- ### Comparison Plot ###---------------------------------------------- df_plot &lt;- data.frame( Iter = 1:S_hmc, HMC = theta_post_hmc[, 1], MH = theta_post_mh[keep, 1], Gibbs = theta_post_gibbs[keep, 1] ) g1 &lt;- ggplot(df_plot, aes(x = Iter)) + geom_point(aes(y = HMC), color = &quot;black&quot;) + labs(x = &quot;Iteration&quot;, y = TeX(&quot;$\\\\theta_{1}$&quot;), title = &quot;HMC algorithm&quot;) g2 &lt;- ggplot(df_plot, aes(x = Iter)) + geom_point(aes(y = MH), color = &quot;black&quot;) + labs(x = &quot;Iteration&quot;, y = TeX(&quot;$\\\\theta_{1}$&quot;), title = &quot;M-H algorithm&quot;) g3 &lt;- ggplot(df_plot, aes(x = Iter)) + geom_point(aes(y = Gibbs), color = &quot;black&quot;) + labs(x = &quot;Iteration&quot;, y = TeX(&quot;$\\\\theta_{1}$&quot;), title = &quot;Gibbs sampling&quot;) ggarrange(g3, g2, g1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), ncol = 3) The figure shows the posterior draws of \\(\\theta_1\\) using the Gibbs sampler (Panel A, left), the Metropolis-Hastings algorithm (Panel B, middle), and the Hamiltonian Monte Carlo (Panel C, right). The convergence diagnostic plots (no shown) suggests that the three algorithms perform a good job. Although, the acceptance rate in HMC is higher than the M-H due to the HMC producing larger changes in \\(\\boldsymbol{\\theta}\\) than a corresponding number of random-walk M-H iterations (Neal 2011). References Carlin, Bradley P, Alan E Gelfand, and Adrian FM Smith. 1992. “Hierarchical Bayesian Analysis of Changepoint Problems.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 41 (2): 389–405. Chib, Siddhartha, and Edward Greenberg. 1995. “Understanding the Metropolis-Hastings Algorithm.” The American Statistician 49 (4): 327–35. Duane, Simon, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. 1987. “Hybrid Monte Carlo.” Physics Letters B 195 (2): 216–22. Gelfand, A. E., and A. F. M. Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” Journal of the American Statistical Association 85: 398–409. Gelman, Andrew, John B Carlin, Hal S Stern, David Dunson, Aki Vehtari, and Donald B Rubin. 2021. Bayesian Data Analysis. Chapman; Hall/CRC. Gelman, Andrew, and Donald B. Rubin. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science 7 (4): 457–72. https://doi.org/10.1214/ss/1177011136. Geman, S, and D. Geman. 1984. “Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence 6: 721–41. Greenberg, Edward. 2012. Introduction to Bayesian Econometrics. Cambridge University Press. Hastings, W. 1970. “Monte Carlo Sampling Methods Using Markov Chains and Their Application.” Biometrika 57: 97–109. Metropolis, N., A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. 1953. “Equations of State Calculations by Fast Computing Machines.” J. Chem. Phys 21: 1087–92. Neal, Radford M. 1996. Bayesian Learning for Neural Networks. Vol. 118. Lecture Notes in Statistics. Springer. https://doi.org/10.1007/978-1-4612-0745-0. ———. 2011. “MCMC Using Hamiltonian Dynamics.” In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng, 113–62. Chapman; Hall/CRC. Robert, Christian P., and George Casella. 2011. Monte Carlo Statistical Methods. 2nd ed. New York: Springer. Roberts, G. O., A. Gelman, and W. R. Gilks. 1997. “Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms.” The Annals of Applied Probability 7 (1): 110–20. "],["sec52.html", "4.2 Importance sampling", " 4.2 Importance sampling Up to this section, we have introduced MCMC methods for sampling from the posterior distribution when it does not have a standard closed form. However, MCMC methods have some limitations. First, the samples are generated sequentially, which complicates parallel computing. Although multiple MCMC chains can be run simultaneously, this approach—often referred to as brute-force parallelization—does not fully address the sequential nature of individual chains. Second, consecutive samples are correlated, which reduces the effective sample size and complicates convergence diagnostics. Thus, in this section, we introduce importance sampling (IS), a simulation method for drawing samples from the posterior distribution that avoids these limitations. Unlike MCMC, IS does not require satisfying the balancing condition, making it conceptually and mathematically simpler to implement in certain situations. Moreover, importance weights can be reused to analyze posterior quantities, compute marginal likelihoods, compare models, approximate new target distributions, and allow for straightforward parallelization in large-scale problems. However, the critical challenge in IS lies in selecting an appropriate proposal distribution. This involves satisfying both support and stability conditions, which can be difficult to achieve, particularly in high-dimensional problems. In such cases, MCMC methods may be more suitable. The starting point is evaluating the integral: \\[\\begin{align} \\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})] &amp;= \\int_{\\Theta} h(\\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta} \\mid y) d\\boldsymbol{\\theta}, \\tag{4.1} \\end{align}\\] where \\(\\mathbb{E}_{\\pi}\\) denotes expected value under the posterior distribution. Thus, we can approximate Equation (4.1) by \\[\\begin{align} \\bar{h}(\\boldsymbol{\\theta})_S &amp;= \\frac{1}{S} \\sum_{s=1}^S h(\\boldsymbol{\\theta}^{(s)}), \\tag{4.2} \\end{align}\\] where \\(\\boldsymbol{\\theta}^{(s)}\\) are draws from \\(\\pi(\\boldsymbol{\\theta} \\mid y)\\). The strong law of large numbers shows that \\(\\bar{h}(\\boldsymbol{\\theta})_S\\) converges (almost surely) to \\(\\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})]\\) as \\(S \\to \\infty\\). The challenge arises when we do not know how to obtain samples from \\(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\). The ingenious idea is to express Equation (4.1) in a different way using the importance sampling fundamental identity (Christian P. Robert and Casella 2011): \\[\\begin{align} \\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})] &amp;= \\int_{\\boldsymbol{\\Theta}} h(\\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\frac{q(\\boldsymbol{\\theta})}{q(\\boldsymbol{\\theta})}d\\boldsymbol{\\theta} \\nonumber \\\\ &amp;= \\mathbb{E}_{q}\\left[\\frac{h(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right], \\tag{4.3} \\end{align}\\] where \\(q(\\boldsymbol{\\theta})\\) is the proposal distribution. Thus, we have \\[\\begin{align} \\frac{1}{S}\\sum_{s=1}^S \\left[\\frac{h(\\boldsymbol{\\theta}^{(s)})\\pi(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta}^{(s)})}\\right] &amp;= \\frac{1}{S}\\sum_{s=1}^S h(\\boldsymbol{\\theta}^{(s)})w(\\boldsymbol{\\theta}^{(s)}), \\end{align}\\] where \\(w(\\boldsymbol{\\theta}^{(s)})= \\left[\\frac{\\pi(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta}^{(s)})}\\right]\\) are called the importance weights, and \\(\\boldsymbol{\\theta}^{(s)}\\) are samples from the proposal distribution. This expression converges to \\(\\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})]\\) given that the support of \\(q(\\boldsymbol{\\theta})\\) includes the support of \\(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\). There are many proposal distributions that satisfy the support condition. However, the stability of the method depends heavily on the variability of the importance weights. In particular, the variance of \\[\\begin{align} \\frac{1}{S}\\sum_{s=1}^S h(\\boldsymbol{\\theta}^{(s)})w(\\boldsymbol{\\theta}^{(s)}) \\end{align}\\] can be large if the proposal distribution has lighter tails than the posterior distribution. In this case, the weights \\(w(\\boldsymbol{\\theta}^{(s)})\\) will vary widely, assigning too much importance to a few values of \\(\\boldsymbol{\\theta}^{(s)}\\). Thus, it is important to use proposals that have thicker tails than the posterior distribution. In any case, we should check the adequacy of the proposal distribution by analyzing the behavior of the importance weights. If they are distributed more or less uniformly over the support, it is a good sign. Consider, for instance, the extreme case where \\(q(\\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\), then \\(w(\\boldsymbol{\\theta}^{(s)}) = 1\\) everywhere. A natural choice in Bayesian inference is to use the prior distribution as the proposal, given that it is a proper density function. The prior distribution typically has heavier tails than the posterior by construction, and it is usually a distribution that allows for easy sampling. The most relevant point for us is that importance sampling provides a way to simulate from the posterior distribution when there is no closed-form solution. The method generates samples \\(\\boldsymbol{\\theta}^{(s)}\\) from \\(q(\\boldsymbol{\\theta})\\) and computes the importance weights \\(w(\\boldsymbol{\\theta}^{(s)})\\). Thus, if we resample with replacement from \\(\\boldsymbol{\\theta}^{(1)},\\boldsymbol{\\theta}^{(2)},\\dots,\\boldsymbol{\\theta}^{(S)}\\), selecting \\(\\boldsymbol{\\theta}^{(s)}\\) with probability proportional to \\(w(\\boldsymbol{\\theta}^{(s)})\\), we would get a sample \\(\\boldsymbol{\\theta}^{*(1)},\\boldsymbol{\\theta}^{*(2)},\\dots,\\boldsymbol{\\theta}^{*(L)}\\) of size \\(L\\) from \\(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\) (Smith and Gelfand 1992; Rubin 1988). This is named sampling/importance resampling (SIR) algorithm. Observe that the number of times \\(L^{(s)}\\) each particular point \\(\\boldsymbol{\\theta}^{(s)}\\) is selected follows a binomial distribution with size \\(L\\), and probabilities proportional to \\(w^{(s)}\\). Consequently, the vector \\(L_{\\boldsymbol{\\theta}} = \\left\\{L_{\\boldsymbol{\\theta}^1}, L_{\\boldsymbol{\\theta}^2}, \\dots, L_{\\boldsymbol{\\theta}^S}\\right\\}\\) follows a multinomial distribution with \\(L\\) trials and probabilities proportional to \\(w(\\boldsymbol{\\theta}^{(s)})\\), \\(s = 1, 2, \\dots, S\\) (Olivier Cappé, Godsill, and Moulines 2007). Therefore, the resampling step ensures that points in the first-stage sample with small importance weights are more likely to be discarded, while points with high weights are replicated in proportion to their importance weights. In most applications, it is typical to have \\(S \\gg L\\). The intuition is that importance weights are scaling factors that correct for the bias introduced by drawing from \\(q(\\boldsymbol{\\theta}^{(s)})\\) instead of \\(\\pi(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{y})\\); thus, when combined, the samples and weights effectively recreate the posterior distribution, ensuring the resampled data set reflects the posterior. Let’s prove this: \\[\\begin{align*} P(\\boldsymbol{\\theta}^*\\in A) &amp;=\\frac{1}{S}\\sum_{s=1}^S{w}^{(s)}\\mathbb{1}_{A}(\\boldsymbol{\\theta}^{(s)})\\\\ &amp;\\rightarrow \\mathbb{E}_q\\left[\\mathbb{1}_{\\in A}(\\boldsymbol{\\theta})\\frac{\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right]\\\\ &amp;=\\int_{A}\\left[\\frac{\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right]q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\\\ &amp;=\\int_{A}\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}. \\end{align*}\\] Thus, \\(\\boldsymbol{\\theta}^*\\) is approximately distributed as an observation from \\(\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})\\). However, the weights \\(\\pi(\\boldsymbol{\\theta}^{(s)}\\mid \\boldsymbol{y})/(S q(\\boldsymbol{\\theta}^{(s)}))\\) do not sum up to 1, and we need to standardize them: \\[ w^*(\\boldsymbol{\\theta}^{(s)})=\\frac{\\frac{1}{S} w(\\boldsymbol{\\theta}^{(s)})}{\\frac{1}{S}\\sum_{s=1}^S w(\\boldsymbol{\\theta}^{(s)})}. \\] Note that we could alternatively arrive at these weights as follows: \\[\\begin{align*} \\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})]&amp;=\\int_{\\boldsymbol{\\Theta}} \\left[\\frac{h(\\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right]q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\\\ &amp;=\\frac{\\int_{\\boldsymbol{\\Theta}}\\left[\\frac{h(\\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right] q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}{\\int_{\\boldsymbol{\\Theta}}\\left[\\frac{ \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{q(\\boldsymbol{\\theta})}\\right] q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}}. \\end{align*}\\] Then, \\[ \\frac{\\frac{1}{S}\\sum_{s=1}^S h(\\boldsymbol{\\theta}^{(s)})w(\\boldsymbol{\\theta}^{(s)})}{\\frac{1}{S}\\sum_{s=1}^S w(\\boldsymbol{\\theta}^{(s)})}= \\sum_{s=1}^S h(\\boldsymbol{\\theta}^{(s)})w^*(\\boldsymbol{\\theta}^{(s)}). \\] This alternative expression also converges (almost surely) to \\(\\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})]\\). In addition, this expression is very useful because if we do not have the marginal likelihood in the posterior distribution, this constant cancels out in \\(w^*(\\boldsymbol{\\theta}^{(s)})\\). Although this estimator is biased, the bias is small and provides good gains in variance reduction compared with the non-standardized option (Christian P. Robert and Casella 2011). A nice by-product of implementing IS is that it easily allows the calculation of the marginal likelihood. In particular, we know from Bayes’ rule that \\[ p(\\boldsymbol{y})^{-1}=\\frac{\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})}{p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta})\\times \\pi(\\boldsymbol{\\theta})}, \\] then, \\[\\begin{align*} \\int_{\\boldsymbol{\\Theta}}p(\\boldsymbol{y})^{-1}q(\\boldsymbol{\\theta})d\\boldsymbol{\\theta} &amp;=\\int_{{\\Theta}}\\frac{q(\\boldsymbol{\\theta})}{p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta})\\times \\pi(\\boldsymbol{\\theta})}\\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y})d\\boldsymbol{\\theta}\\\\ &amp;=\\mathbb{E}_{\\pi}\\left[\\frac{q(\\boldsymbol{\\theta})}{p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta})\\times \\pi(\\boldsymbol{\\theta})}\\right]. \\end{align*}\\] Thus, an estimate of the marginal likelihood is \\[ \\left[\\frac{1}{S}\\sum_{s=1}^S\\frac{q(\\boldsymbol{\\theta}^{*(s)})}{p(\\boldsymbol{y}\\mid \\boldsymbol{\\theta}^{*(s)})\\times\\pi(\\boldsymbol{\\theta}^{*(s)})}\\right]^{-1}. \\] This is the Gelfand-Dey method to calculate the marginal likelihood (Alan E. Gelfand and Dey 1994). Example: Cauchy distribution Let’s assume that the posterior distribution is Cauchy with parameters 0 and 1. We perform an importance sampling algorithm using as proposals a standard normal distribution and a Student’s t distribution with 3 degrees of freedom. The following code shows how to do this. # Load required libraries library(ggplot2) library(ggpubr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union set.seed(10101) # Parameters S &lt;- 20000 # Proposal sample size L &lt;- 10000 # Posterior sample size ###---------------------------------- ### Importance Sampling: Normal proposal ###---------------------------------- theta_norm &lt;- rnorm(S) weights_norm &lt;- dcauchy(theta_norm) / dnorm(theta_norm) weights_norm &lt;- weights_norm / sum(weights_norm) theta_cauchy_norm &lt;- sample(theta_norm, size = L, replace = TRUE, prob = weights_norm) df_norm &lt;- data.frame(x = theta_cauchy_norm) g1 &lt;- ggplot(df_norm, aes(x = x)) + geom_histogram(aes(y = after_stat(density)), bins = 50, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + stat_function(fun = dcauchy, color = &quot;red&quot;, linewidth = 1.2) + labs(title = &quot;Cauchy draws via Importance Sampling&quot;, subtitle = &quot;Normal(0,1) proposal&quot;, x = &quot;x&quot;, y = &quot;Density&quot;) + theme_minimal() ###---------------------------------- ### Importance Sampling: Student&#39;s t proposal ###---------------------------------- df_t &lt;- 3 theta_t &lt;- rt(S, df = df_t) weights_t &lt;- dcauchy(theta_t) / dt(theta_t, df = df_t) weights_t &lt;- weights_t / sum(weights_t) theta_cauchy_t &lt;- sample(theta_t, size = L, replace = TRUE, prob = weights_t) df_tpost &lt;- data.frame(x = theta_cauchy_t) g2 &lt;- ggplot(df_tpost, aes(x = x)) + geom_histogram(aes(y = after_stat(density)), bins = 50, fill = &quot;lightgreen&quot;, color = &quot;black&quot;) + stat_function(fun = dcauchy, color = &quot;red&quot;, linewidth = 1.2) + labs(title = &quot;Cauchy draws via Importance Sampling&quot;, subtitle = &quot;Student&#39;s t (df = 3) proposal&quot;, x = &quot;x&quot;, y = &quot;Density&quot;) + theme_minimal() ###---------------------------------- ### Plot Importance Weights ###---------------------------------- df_weights &lt;- data.frame( index = 1:S, Normal = weights_norm, Student_t = weights_t ) g3 &lt;- ggplot(df_weights) + geom_point(aes(x = index, y = Normal), color = &quot;black&quot;, alpha = 0.6, size = 0.7) + geom_point(aes(x = index, y = Student_t), color = &quot;blue&quot;, alpha = 0.6, size = 0.7) + labs(title = &quot;Importance Sampling Weights&quot;, x = &quot;Index&quot;, y = &quot;Weight&quot;) + scale_y_continuous(trans = &quot;log1p&quot;) + theme_minimal() ###---------------------------------- ### Arrange and display plots ###---------------------------------- ggarrange(g1, g2, g3, ncol = 1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) The first and second figures show the histograms of the posterior draws using the normal and Student’s t-distributions, respectively, along with the density of the Cauchy distribution. The spike in the posterior draws from the standard normal proposal arises due to the lighter tails of the standard normal compared to the Cauchy distribution, consequently assigning too much weight to a specific draw from the normal distribution. The third figure shows the weights using the standard normal distribution (black dots) and the Student’s t-distribution with 3 degrees of freedom (blue dots) as proposals. We observe that a few draws carry too much weight when using the normal proposal; this occurs because the normal distribution has much lighter tails compared to the Cauchy distribution. In contrast, using the Student’s t-distribution with 3 degrees of freedom improves this situation. References Cappé, Olivier, Simon J Godsill, and Eric Moulines. 2007. “An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.” Proceedings of the IEEE 95 (5): 899–924. Gelfand, Alan E, and Dipak K Dey. 1994. “Bayesian Model Choice: Asymptotics and Exact Calculations.” Journal of the Royal Statistical Society: Series B (Methodological) 56 (3): 501–14. Robert, Christian P., and George Casella. 2011. Monte Carlo Statistical Methods. 2nd ed. New York: Springer. Rubin, Donald B. 1988. “Using the SIR Algorithm to Simulate Posterior Distributions.” In Bayesian Statistics 3, edited by J. M. Bernardo, M. H. DeGroot, D. V. Lindley, and A. F. M. Smith, 395–402. Oxford University Press. Smith, Adrian FM, and Alan E Gelfand. 1992. “Bayesian Statistics Without Tears: A Sampling–Resampling Perspective.” The American Statistician 46 (2): 84–88. "],["sec53.html", "4.3 Particle filtering", " 4.3 Particle filtering Now, we consider the scenario where we need to sample from a posterior distribution whose dimension increases over time, \\(\\pi(\\boldsymbol{\\theta}_{0:t}\\mid \\boldsymbol{y}_{0:t})\\), for \\(t = 0, 1, \\dots\\). The challenge arises from the fact that, even if this posterior distribution is known, the computational complexity of implementing a sampling scheme in this context increases linearly with \\(t\\). This makes MCMC methods, which operate in batch mode and require a complete re-run whenever new information becomes available, less optimal. Consequently, we present sequential algorithms, which operate incrementally as new data becomes available, and are often a better alternative. These algorithms are typically faster and are well-suited for scenarios requiring real-time updates, commonly referred to as online mode. Specifically, we consider the dynamic system in the state-space representation. This is a system where there is an unobservable state vector \\(\\boldsymbol{\\theta}_t\\in\\mathbb{R}^K\\), and an observed variable \\(\\boldsymbol{Y}_t\\), \\(t=0,1,\\dots\\) such that: \\(\\boldsymbol{\\theta}_t\\) is a Markov process, that is, \\[ \\pi(\\boldsymbol{\\theta}_{t}\\mid \\boldsymbol{\\theta}_{1:t-1})=\\pi(\\boldsymbol{\\theta}_{t}\\mid \\boldsymbol{\\theta}_{t-1}), \\] for \\(t=1,2,\\dots\\). All the relevant information to define \\(\\boldsymbol{\\theta}_{t}\\) is in \\(\\boldsymbol{\\theta}_{t-1}\\).1 \\(\\boldsymbol{Y}_t\\perp \\boldsymbol{Y}_s\\mid \\boldsymbol{\\theta}_{t}\\), for \\(s&lt;t\\). That is, there is independence between observable variables regarding their history conditional on the actual state vector. We can see in the next figure a graphical representation of the dynamic system. Formally, \\[ \\begin{aligned} \\boldsymbol{\\theta}_t &amp;= h(\\boldsymbol{\\theta}_{t-1}, \\boldsymbol{w}_t) &amp; \\text{(State equations)}\\\\ Y_t &amp; = f(\\boldsymbol{\\theta}_t, \\mu_t)&amp; \\text{(Observation equation)}, \\end{aligned} \\] where \\(\\boldsymbol{w}_t\\) and \\(\\mu_t\\) are stochastic errors such that their probability distributions define the transition density \\(\\pi(\\boldsymbol{\\theta}_t\\mid \\boldsymbol{\\theta}_{t-1})\\) and observation density \\(p(Y_t\\mid \\boldsymbol{\\theta}_t)\\). We present particle filtering, a specific case of sequential Monte Carlo (SMC), which is one of the most commonly used algorithms for scenarios requiring sequential updates of the posterior distribution as described by the state-space model. The starting point is sequential importance sampling (SIS), originally proposed by Handschin and Mayne (1969), which is a modification of IS to compute an estimate \\(\\pi(\\boldsymbol{\\theta}_{0:t}\\mid \\boldsymbol{y}_{0:t})\\) without altering the past trajectories \\(\\left\\{\\boldsymbol{\\theta}^{(s)}_{1:t-1}, s=1,2,\\dots,S\\right\\}\\). The key idea is to use a proposal density that takes the form \\[ \\begin{aligned} q(\\boldsymbol{\\theta}_{0:t}\\mid \\boldsymbol{y}_{0:t}) &amp;= q(\\boldsymbol{\\theta}_{0:t-1}\\mid \\boldsymbol{y}_{1:t-1})q(\\boldsymbol{\\theta}_t\\mid \\boldsymbol{\\theta}_{t-1},\\boldsymbol{y}_{t}) \\\\ &amp;= q(\\boldsymbol{\\theta}_0)\\prod_{h=1}^{t}q(\\boldsymbol{\\theta}_h\\mid \\boldsymbol{\\theta}_{h-1},\\boldsymbol{y}_{h}). \\end{aligned} \\] This proposal density allows calculating the weights sequentially, \\[ \\begin{aligned} w_{t}(\\boldsymbol{\\theta}^{(s)}_{0:t})&amp;=\\frac{\\pi(\\boldsymbol{\\theta}_{0:t}^{(s)}\\mid \\boldsymbol{y}_{0:t})}{q(\\boldsymbol{\\theta}_{0:t}^{(s)}\\mid \\boldsymbol{y}_{0:t})}\\\\ &amp;=\\frac{p(\\boldsymbol{y}_{0:t}\\mid \\boldsymbol{\\theta}_{0:t}^{(s)})\\pi(\\boldsymbol{\\theta}_{0:t}^{(s)})}{p(\\boldsymbol{y}_{0:t})q(\\boldsymbol{\\theta}_{0:t}^{(s)}\\mid \\boldsymbol{y}_{0:t})}\\\\ &amp;=\\frac{p(\\boldsymbol{y}_{t}\\mid \\boldsymbol{\\theta}_{t}^{(s)})p(\\boldsymbol{y}_{1:t-1}\\mid \\boldsymbol{\\theta}_{0:t-1}^{(s)})\\pi(\\boldsymbol{\\theta}_{t}^{(s)}\\mid \\boldsymbol{\\theta}_{t-1}^{(s)})\\pi(\\boldsymbol{\\theta}_{0:t-1}^{(s)})}{p(\\boldsymbol{y}_{0:t})q(\\boldsymbol{\\theta}_{t}^{(s)}\\mid \\boldsymbol{\\theta}_{t-1},\\boldsymbol{y}_{t}^{(s)})q(\\boldsymbol{\\theta}_{0:t-1}^{(s)}\\mid \\boldsymbol{y}_{1:t-1})}\\\\ &amp;\\propto w_{t-1}^*(\\boldsymbol{\\theta}^{(s)}_{0:t-1})\\frac{p(\\boldsymbol{y}_{t}\\mid \\boldsymbol{\\theta}_{t}^{(s)})\\pi(\\boldsymbol{\\theta}_{t}\\mid \\boldsymbol{\\theta}_{t-1}^{(s)})}{q(\\boldsymbol{\\theta}_t^{(s)}\\mid \\boldsymbol{\\theta}_{t-1}^{(s)},\\boldsymbol{y}_{t})}. \\end{aligned} \\] Take into account that \\(p(\\boldsymbol{y}_{0:t})\\) does not depend on \\(\\boldsymbol{\\theta}^{(s)}_{0:t}\\). The term \\(\\alpha_t(\\boldsymbol{\\theta}_{0:t}^{(s)})=\\frac{p(\\boldsymbol{y}_{t}\\mid \\boldsymbol{\\theta}_{t}^{(s)})\\pi(\\boldsymbol{\\theta}_{t}\\mid \\boldsymbol{\\theta}_{t-1}^{(s)})}{q(\\boldsymbol{\\theta}_t^{(s)}\\mid \\boldsymbol{\\theta}_{t-1}^{(s)},\\boldsymbol{y}_{t})}\\) is called the incremental importance weight, and implies that \\[ w_t(\\boldsymbol{\\theta}^{s}_{0:t})=w_0(\\boldsymbol{\\theta}^{s}_{0})\\prod_{h=1}^{t}\\alpha_h(\\boldsymbol{\\theta}_{1:h}^{(s)}). \\] This algorithm possesses the desirable property of maintaining fixed computational complexity. Consequently, we sequentially obtain draws \\(\\boldsymbol{\\theta}_t^{(s)}\\), referred to as particles: \\(\\boldsymbol{\\theta}_0^{(s)}\\) is drawn from \\(q(\\boldsymbol{\\theta}_0)\\) at \\(t=0\\), and subsequently, \\(\\boldsymbol{\\theta}_h^{(s)}\\) is drawn from \\(q(\\boldsymbol{\\theta}_h\\mid \\boldsymbol{\\theta}_{h-1},\\boldsymbol{y}_{h})\\) at \\(t=h\\) (Doucet, De Freitas, and Gordon 2001; Olivier Cappé, Godsill, and Moulines 2007). A relevant case is when the proposal distribution takes the form of the prior distribution, that is, \\[ q(\\boldsymbol{\\theta}_{0:t}\\mid \\boldsymbol{y}_{0:t}) = \\pi(\\boldsymbol{\\theta}_{0:t}) = \\pi(\\boldsymbol{\\theta}_0)\\prod_{h=1}^{t}\\pi(\\boldsymbol{\\theta}_h\\mid \\boldsymbol{\\theta}_{h-1}). \\] This implies that \\[ w_{t}(\\boldsymbol{\\theta}^{(s)}_{0:t})\\propto w_{t-1}^*(\\boldsymbol{\\theta}^{(s)}_{0:t-1})p(\\boldsymbol{y}_{t}\\mid \\boldsymbol{\\theta}_{t}^{(s)}), \\] which means that the incremental importance weight is given by \\(p(\\boldsymbol{y}_{t}\\mid \\boldsymbol{\\theta}_{t}^{(s)})\\). Algorithm 4 shows how to perform SIS (Olivier Cappé, Godsill, and Moulines 2007). We set \\(w_t^{(s)}:=w_t(\\boldsymbol{\\theta}_{0:t}^{(s)})\\) to simplify notation. Algorithm: Sequential importance sampling For s=1,2,...,S do Sample θ0(s) from q(θ0|y0) Calculate the importance weights w0(s) ∝(p(y0|θ0(s))π(θ0(s)))/q(θ0(s)|y0) End for for t=1,2,...,T do for s=1,2,...,S do Draw particles θt(s) from qt(θt|θt-1,y0) Compute the weights wt(s) ∝ wt-1*(s) (p(yt|θt(s))π(θt(s)|θt-1(s)))/q(θt(s)|θt-1(s),yt) End for Standardize the weights wt*(s) = wt(s)/(∑hwt(h)), s=1,2,...,S End for Example: Dynamic linear model Let’s assume that the state-space representation is \\[ \\theta_t = \\theta_{t-1} + w_t \\quad \\text{(State equation)} \\\\ Y_t = \\phi \\theta_t + \\mu_t \\quad \\text{(Observation equation)}, \\] where \\(w_t \\sim N(0, \\sigma_w^2)\\) and \\(\\mu_t \\sim N(0, \\sigma_{\\mu}^2)\\), \\(t = 1, 2, \\dots, 50\\). In addition, we use the proposal distribution \\(q(\\theta_t \\mid y_t) = \\pi(\\theta_t)\\), which is normal with mean \\(\\theta_{t-1}\\) and variance \\(\\sigma_w^2\\). Then, the weights are given by the recursion \\[ w_t^{(s)} \\propto w_{t-1}^{*(s)} p(y_t \\mid \\theta_t, \\sigma_{\\mu}^2), \\] where \\(p(y_t \\mid \\theta_t, \\sigma_{\\mu}^2)\\) is \\(N(\\phi \\theta_t, \\sigma_{\\mu}^2)\\). We can compute the mean and standard deviation of the state at each \\(t\\) using \\[ \\hat{\\theta}_t = \\sum_{s=1}^S w_t^{*(s)} \\theta_t^{(s)} \\] and \\[ \\hat{\\sigma}_{\\theta} = \\left(\\sum_{s=1}^S w_t^{*(s)} \\theta_t^{2(s)} - \\hat{\\theta}_t^2\\right)^{1/2}. \\] The following code demonstrates the implementation of this algorithm, setting \\(\\sigma_w^2 = \\sigma_{\\mu}^2 = 1\\) and \\(\\phi = 0.5\\). First, we simulate the process, and then we implement the SIS algorithm. # Load packages library(dplyr) library(ggplot2) library(latex2exp) set.seed(10101) # Parameters n_particles &lt;- 50000 sigma_w &lt;- 1 # State noise sigma_mu &lt;- 1 # Observation noise phi &lt;- 0.5 # Observation coefficient T &lt;- 50 # Time points ###------------------------------------- ### Simulate true states and observations ###------------------------------------- theta_true &lt;- numeric(T) theta_true[1] &lt;- rnorm(1, mean = 0, sd = sigma_w) for (t in 2:T) { theta_true[t] &lt;- rnorm(1, mean = theta_true[t - 1], sd = sigma_w) } y_obs &lt;- rnorm(T, mean = phi * theta_true, sd = sigma_mu) ###------------------------------------- ### Sequential Importance Sampling (SIS) ###------------------------------------- particles &lt;- matrix(0, nrow = n_particles, ncol = T) weights &lt;- matrix(0, nrow = n_particles, ncol = T) weights_normalized &lt;- matrix(0, nrow = n_particles, ncol = T) # Initialization particles[, 1] &lt;- rnorm(n_particles, mean = 0, sd = sigma_w) weights[, 1] &lt;- dnorm(y_obs[1], mean = phi * particles[, 1], sd = sigma_mu) weights_normalized[, 1] &lt;- weights[, 1] / sum(weights[, 1]) # Recursive updates for (t in 2:T) { particles[, t] &lt;- rnorm(n_particles, mean = particles[, t - 1], sd = sigma_w) weights[, t] &lt;- weights_normalized[, t - 1] * dnorm(y_obs[t], mean = phi * particles[, t], sd = sigma_mu) weights_normalized[, t] &lt;- weights[, t] / sum(weights[, t]) } ###------------------------------------- ### Posterior filtering mean and std dev ###------------------------------------- posterior_mean &lt;- colSums(particles * weights_normalized) posterior_sd &lt;- sqrt(colSums((particles^2) * weights_normalized) - posterior_mean^2) ###------------------------------------- ### Plot filtering estimates ###------------------------------------- df_sis &lt;- tibble( t = 1:T, estimate = posterior_mean, lower = posterior_mean - 2 * posterior_sd, upper = posterior_mean + 2 * posterior_sd, theta_true = theta_true ) plot_filtering_estimates &lt;- function(df) { ggplot(df, aes(x = t)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = &quot;95% Credible Interval&quot;), alpha = 0.5) + geom_line(aes(y = theta_true, color = &quot;True State&quot;), linewidth = 0.5) + geom_line(aes(y = estimate, color = &quot;Posterior Mean&quot;), linewidth = 0.5) + scale_color_manual(values = c(&quot;True State&quot; = &quot;black&quot;, &quot;Posterior Mean&quot; = &quot;blue&quot;)) + scale_fill_manual(values = c(&quot;95% Credible Interval&quot; = &quot;lightblue&quot;)) + ylab(TeX(&quot;$\\\\theta_t$&quot;)) + xlab(&quot;Time&quot;) + labs(color = &quot;Line&quot;, fill = &quot;Interval&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) } # Display plot plot_filtering_estimates(df_sis) The figure shows the trajectory of the true state vector (black line), the posterior mean (blue line), and the area defined by \\(\\pm2\\hat{\\sigma}_{\\theta}\\) (light blue shaded area). Sequential importance sampling is effective for sampling from the posterior distribution in the short term. However, it is important to note that SIS is a particular case of IS and, consequently, inherits the drawbacks of importance sampling. In particular, the variance of the weights increases exponentially with \\(t\\) (Kong, Liu, and Wong 1994). This implies that, as \\(t\\) increases, the importance weights tend to degenerate in the long run; that is, all probability mass concentrates on a few weights, a phenomenon known as sample impoverishment or weight degeneracy. This is because it is impossible to accurately represent a distribution on a space of arbitrarily high dimension with a sample of fixed, finite size. This phenomenon can be observed, for instance, in the dynamic linear model example, where the highest standardized weight at \\(t = 50\\) is 53%, and 7 out of 50,000 particles account for 87% of the total probability. Given that, in practice, we are often interested in lower-dimensional marginal distributions, ideas from sampling/importance resampling can be employed. This strategy avoids the accumulation of errors due to resetting the system, although resampling introduces some additional Monte Carlo variation. Gordon, Salmond, and Smith (1993) proposed the Bootstrap filter, where, at each time step, resampling is performed by drawing \\(S\\) particles from the current set using the standardized weights as probabilities of selection. This ensures that particles with small weights have a low probability of being selected. After resampling, the standardized weights are set equal to \\(1/S\\). Note that the Bootstrap filter involves multiple iterations of the SIR algorithm, which implies that the resampled trajectories are no longer independent. This multinomial resampling provides an unbiased approximation to the posterior distribution obtained by SIS (Doucet, Johansen, et al. 2009). Algorithm 5 shows how to perform the particle filter. We set \\(w_t^{(s)} := w_t(\\boldsymbol{\\theta}_{0:t}^{(s)})\\) to simplify notation (Doucet, Johansen, et al. 2009). Algorithm: Particle filter For s=1,2,...,S do Sample θ0(s) from q(θ0|y0) Calculate the importance weights w0(s) ∝(p(y0|θ0(s))π(θ0(s)))/q(θ0(s)|y0) End for Standardize the weights w0*(s) = w0(s)/(∑hw0(h)), s=1,2,...,S Select S particle from {θ0(s),w0*(s)} to obtain {θ0r(s),1/S} for t=1,2,...,T do for s=1,2,...,S do Draw particles θt(s) from qt(θt|θt-1,y0) Set θ1:t(s) ← (θ1:t-1r(s), θt(s)) Compute the weights αt(s) = (p(yt|θt(s))π(θt(s)|θt-1(s)))/q(θt(s)|θt-1(s),yt) End for Standardize the weights wt*(s) = wt(s)/(∑hwt(h)), s=1,2,...,S Select S particle from {θ1:t(s),wt*(s)} to obtain {θ1:tr(s),1/S} End for Example: Dynamic linear model continues If we apply the SIS algorithm to the dynamic linear model with a sample size of 200, the algorithm’s performance deteriorates as \\(t\\) increases. This is due to particle degeneration; at \\(t=200\\), a single particle holds a weight close to 100%. Let’s perform particle filtering in this example. The following code illustrate the procedure. The figure shows the performance of particle filtering in this example. There is the true state vector (black line), the means based on \\(\\left\\{\\boldsymbol{\\theta}_{1:t}^{(s)},w_t^{*(s)}\\right\\}\\) (blue line) and \\(\\left\\{\\boldsymbol{\\theta}_{1:t}^{r(s)},1/S\\right\\}\\) (purple line), and the area defined by \\(\\pm2\\hat{\\sigma}_{\\theta}\\) based on the former (light blue shaded area). Note that the particle filtering algorithm has better performance than the SIS algorithm. rm(list = ls()); set.seed(10101) S &lt;- 50000 # Number of particles sigma_w &lt;- 1; sigma_mu &lt;- 1 # # State and observation noises phi &lt;- 0.5 # Coefficient in observation equation T &lt;- 200 # Sample size # Simulate true states and observations theta_true &lt;- numeric(T); y_obs &lt;- numeric(T) theta_true[1] &lt;- rnorm(1, mean = 0, sd = sigma_w) for (t in 2:T) { theta_true[t] &lt;- rnorm(1, mean = theta_true[t-1], sd = sigma_w) } y_obs &lt;- rnorm(T, mean = phi*theta_true, sd = sigma_mu) # Particle filtering particles &lt;- matrix(0, nrow = S, ncol = T) # Store particles particlesT &lt;- matrix(0, nrow = S, ncol = T) # Store resampling particles weights &lt;- matrix(0, nrow = S, ncol = T) # Store weights weightsSt &lt;- matrix(0, nrow = S, ncol = T) # Store standardized weights weightsSTT &lt;- matrix(1/S, nrow = S, ncol = T) # Store standardized weights logalphas &lt;- matrix(0, nrow = S, ncol = T) # Store log incremental weights particles[, 1] &lt;- rnorm(S, mean = 0, sd = sigma_w) weights[, 1] &lt;- dnorm(y_obs[1], mean = phi*particles[, 1], sd = sigma_mu) # Importance weights weightsSt[, 1] &lt;- weights[, 1] / sum(weights[, 1]) # Normalize weights ind &lt;- sample(1:S, size = S, replace = TRUE, prob = weightsSt[, 1]) # Resample particles[, 1] &lt;- particles[ind, 1] # Resampled particles particlesT[, 1] &lt;- particles[, 1] # Resampled particles # Sequential updating pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = T, width = 300) for (t in 2:T) { particles[, t] &lt;- rnorm(S, mean = particles[, t-1], sd = sigma_w) logalphas[, t] &lt;- dnorm(y_obs[t], mean = phi*particles[, t], sd = sigma_mu, log = TRUE) weights[, t] &lt;- exp(logalphas[, t]) weightsSt[, t] &lt;- weights[, t] / sum(weights[, t]) if(t &lt; T){ ind &lt;- sample(1:S, size = S, replace = TRUE, prob = weightsSt[, t]) particles[, 1:t] &lt;- particles[ind, 1:t] }else{ ind &lt;- sample(1:S, size = S, replace = TRUE, prob = weightsSt[, t]) particlesT[, 1:t] &lt;- particles[ind, 1:t] } setWinProgressBar(pb, t, title=paste( round(t/T*100, 0), &quot;% done&quot;)) } close(pb) FilterDist &lt;- colSums(particles * weightsSt) SDFilterDist &lt;- (colSums(particles^2 * weightsSt) - FilterDist^2)^0.5 FilterDistT &lt;- colSums(particlesT * weightsSTT) SDFilterDistT &lt;- (colSums(particlesT^2 * weightsSTT) - FilterDistT^2)^0.5 MargLik &lt;- colMeans(weights) plot(MargLik, type = &quot;l&quot;) library(dplyr) library(ggplot2) require(latex2exp) ggplot2::theme_set(theme_bw()) df &lt;- tibble(t = 1:T, mean = FilterDist, lower = FilterDist - 2*SDFilterDist, upper = FilterDist+ 2*SDFilterDist, meanT = FilterDistT, lowerT = FilterDistT - 2*SDFilterDistT, upperT = FilterDistT + 2*SDFilterDistT, x_true = theta_true) plot_filtering_estimates &lt;- function(df) { p &lt;- ggplot(data = df, aes(x = t)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1, fill = &quot;lightblue&quot;) + geom_line(aes(y = x_true), colour = &quot;black&quot;, alpha = 1, linewidth = 0.5) + geom_line(aes(y = mean), colour = &quot;blue&quot;, linewidth = 0.5) + geom_line(aes(y = meanT), colour = &quot;purple&quot;, linewidth = 0.5) + ylab(TeX(&quot;$\\\\theta_{t}$&quot;)) + xlab(&quot;Time&quot;) print(p) } plot_filtering_estimates(df) Algorithm 5 performs resampling at every time step. However, it is common to perform resampling only when the effective sample size of the particles (\\(ESS = (\\sum_{s=1}^S (w_t^{*(s)})^{2})^{-1}\\)) falls below a specific threshold, such as 50% of the initial number of particles. Note that when \\(w_t^{*(s)} = 1/S\\), the effective sample size is \\(S\\), the total number of particles. Additionally, we should use \\(\\left\\{\\boldsymbol{\\theta}_{1:t}^{(s)}, w_t^{*(s)}\\right\\}\\) to estimate the posterior distribution, as it results in lower Monte Carlo error compared to calculations based on \\(\\left\\{\\boldsymbol{\\theta}_{1:t}^{r(s)}, 1/S\\right\\}\\) (Olivier Cappé, Godsill, and Moulines 2007). Finally, an estimate of the marginal likelihood can be obtained using \\[ \\hat{p}(y_t) = \\frac{1}{S}\\sum_{s=1}^S w_t^{(s)}. \\] Particle filtering offers several advantages, such as being quick and easy to implement, its modularity—allowing one to simply adjust the expressions for the importance distribution and weights when changing the problem—and its suitability for parallel algorithms. Moreover, it enables straightforward sequential inference for very complex models. However, there are also disadvantages. The resampling step introduces extra Monte Carlo variability. Using the state transition (prior) density as the importance distribution often leads to poor performance, manifested in a lack of robustness with respect to the observed sequence. For instance, performance deteriorates when outliers occur in the data or when the variance of the observation noise is small. Furthermore, the procedure is not well suited for sampling from \\(\\pi(\\boldsymbol{\\theta}_{0:t} \\mid y_{1:t})\\) because most particles originate from the same ancestor. Alternative resampling approaches, such as residual resampling (Liu and Chen 1995) and systematic resampling (Carpenter, Clifford, and Fearnhead 1999), preserve unbiasedness while reducing variance. Additionally, auxiliary particle filtering (O. Cappé, Godsill, and Moulines 2007) can help decrease Monte Carlo variability. Lastly, estimating fixed parameters such as \\(\\sigma_w^2\\), \\(\\sigma_{\\mu}^2\\), and \\(\\phi\\) in the dynamic linear model poses a challenge. Various methods exist to address this issue; see N. Kantas et al. (2009), Nikolas Kantas et al. (2015) for a comprehensive review and Andrieu, Doucet, and Holenstein (2010) for a seminal work in particle MCMC methods. References Andrieu, C., A. Doucet, and R. Holenstein. 2010. “Particle Markov Chain Monte Carlo Methods.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 72 (3): 269–342. Cappé, O., S. J. Godsill, and E. Moulines. 2007. “An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.” Proceedings of the IEEE 95 (5): 899–924. Cappé, Olivier, Simon J Godsill, and Eric Moulines. 2007. “An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.” Proceedings of the IEEE 95 (5): 899–924. Carpenter, J., P. Clifford, and P. Fearnhead. 1999. “Improved Particle Filter for Nonlinear Problems.” IEE Proceedings-Radar, Sonar and Navigation 146 (1): 2–7. Doucet, Arnaud, Nando De Freitas, and Neil Gordon. 2001. “An Introduction to Sequential Monte Carlo Methods.” Sequential Monte Carlo Methods in Practice, 3–14. Doucet, Arnaud, Adam M Johansen, et al. 2009. “A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later.” Handbook of Nonlinear Filtering 12 (656-704): 3. Gordon, N. J., D. J. Salmond, and A. F. Smith. 1993. “Novel Approach to Nonlinear/Non-Gaussian Bayesian State Estimation.” IEE Proceedings F (Radar and Signal Processing) 140 (2): 107–13. Handschin, Johannes Edmund, and David Q Mayne. 1969. “Monte Carlo Techniques to Estimate the Conditional Expectation in Multi-Stage Non-Linear Filtering.” International Journal of Control 9 (5): 547–59. Kantas, N., A. Doucet, S. S. Singh, and J. M. Maciejowski. 2009. “An Overview of Sequential Monte Carlo Methods for Parameter Estimation in General State–Space Models.” IFAC Proceedings Volumes 42 (10): 774–85. Kantas, Nikolas, Arnaud Doucet, Sumeetpal S Singh, Jan Maciejowski, Nicolas Chopin, et al. 2015. “On Particle Methods for Parameter Estimation in State-Space Models.” Statistical Science 30 (3): 328–51. Kong, Augustine, Jun S Liu, and Wing Hung Wong. 1994. “Sequential Imputations and Bayesian Missing Data Problems.” Journal of the American Statistical Association 89 (425): 278–88. Liu, J. S., and R. Chen. 1995. “Blind Deconvolution via Sequential Imputations.” Journal of the American Statistical Association 90 (430): 567–76. \\(\\boldsymbol{\\theta}_{0}\\) comes from the given distribution \\(\\pi(\\boldsymbol{\\theta}_{0})\\).↩︎ "],["sec54.html", "4.4 Convergence diagnostics", " 4.4 Convergence diagnostics MCMC methods rely on irreducibility, positive recurrence, and aperiodicity, ensuring that, after a sufficient burn-in (warm-up) period, the posterior draws are sampled from the invariant stationary posterior distribution. This can be achieved by running multiple chains initiated at different points and then mixing them, or by running a single longer chain. In most of the second part of this book, we follow the latter approach, as suggested by Geyer (1992). In this section, we present diagnostics to assess whether the sample draws come from the stationary posterior distribution. First, we calculate the numerical standard error associated with the MCMC algorithm. Next, we review the effective number of simulation draws and various convergence tests. Finally, we examine potential errors in the posterior simulator. 4.4.1 Numerical standard error Many times, the goal in Bayesian inference is to obtain a set of independent draws \\(\\boldsymbol{\\theta}^{(s)}\\), \\(s = 1, 2, \\dots, S\\), from the posterior distribution, such that a measure of interest can be estimated with reasonable precision. In particular, we approximate Equation (4.1) using Equation (4.2). By the central limit theorem, we know that \\[ \\begin{equation} \\frac{\\bar{h}(\\boldsymbol{\\theta})_S - \\mathbb{E}_{\\pi}[h(\\boldsymbol{\\theta})]}{\\sigma_h(\\boldsymbol{\\theta})/\\sqrt{S}} \\stackrel{d}{\\rightarrow} N(0, 1), \\tag{4.4} \\end{equation} \\] where \\(\\sigma^2_h(\\boldsymbol{\\theta})\\) is the variance of \\(h(\\boldsymbol{\\theta})\\). If we have independent draws, we can estimate \\(\\sigma^2_h(\\boldsymbol{\\theta})\\) using the posterior draws as follows: \\[ \\hat{\\sigma}^2_{Sh}(\\boldsymbol{\\theta}) = \\frac{1}{S} \\sum_{s=1}^S \\left[h(\\boldsymbol{\\theta}^{(s)})\\right]^2 - \\left[\\bar{h}(\\boldsymbol{\\theta})_S\\right]^2. \\] However, if there are dependent draws, we have \\[ \\hat{\\sigma}^{2*}_{Sh}(\\boldsymbol{\\theta}) = \\frac{1}{S} \\left\\{\\sum_{s=1}^S \\left[h(\\boldsymbol{\\theta}^{(s)})-\\bar{h}(\\boldsymbol{\\theta})_S\\right]^2 + 2\\sum_{l=k+1}^K \\big(h(\\boldsymbol{\\theta}^{(l)}) - \\bar{h}(\\boldsymbol{\\theta})\\big)\\big(h(\\boldsymbol{\\theta}^{(l-k)}) - \\bar{h}(\\boldsymbol{\\theta})\\big)\\right\\}. \\] The numerical standard error is given by \\(\\sigma_h(\\boldsymbol{\\theta})/\\sqrt{S}\\) and serves as a measure of the approximation error in the Monte Carlo integration. Note that this error can be decreased by increasing \\(S\\). For instance, \\(S = 1000\\) implies an error proportional to 3.2%, while \\(S = 10000\\) reduces the error to approximately 1%. 4.4.2 Effective number of simulation draws MCMC posterior draws are not independent; therefore, the effective sample size of the posterior chains is not equal to \\(S\\). To assess the effective sample size of the posterior draws, we use the following measure: \\[ S_{\\text{ef}} = \\frac{S}{1 + 2\\sum_{k=1}^{\\infty} \\rho_k(h)}, \\] where \\(\\rho_k(h)\\) is the autocorrelation of the sequence \\(h(\\boldsymbol{\\theta})\\) at lag \\(k\\). The sample counterpart of this expression is: \\[ \\hat{S}_{\\text{ef}} = \\frac{S}{1 + 2\\sum_{k=1}^{K} \\hat{\\rho}_k(h)}, \\] where \\[ \\hat{\\rho}_k(h) = \\frac{\\sum_{l=k+1}^K \\big(h(\\boldsymbol{\\theta}^{(l)}) - \\bar{h}(\\boldsymbol{\\theta})\\big)\\big(h(\\boldsymbol{\\theta}^{(l-k)}) - \\bar{h}(\\boldsymbol{\\theta})\\big)}{\\sum_{s=1}^K \\big(h(\\boldsymbol{\\theta}^{(s)}) - \\bar{h}(\\boldsymbol{\\theta})\\big)^2}. \\] If \\(\\hat{\\rho}_k(h)\\) declines to zero slowly as \\(k\\) increases, it indicates significant memory in the draws. Consequently, the effective sample size of the posterior draws is small, and it becomes necessary to either decrease the autocorrelation or increase the number of posterior draws. Note that \\[ \\hat{\\sigma}^{2*}_{Sh}(\\boldsymbol{\\theta}) = \\hat{\\sigma}^2_{Sh}\\left(\\boldsymbol{\\theta}\\right) (1+2\\sum_{k=1}^K \\hat{\\rho}_k(h)), \\] where \\(\\hat{\\sigma}^{2*}_{Sh}(\\boldsymbol{\\theta})\\) and \\(\\hat{\\sigma}^2_{Sh}\\) are the simulation variances using dependent and independent draws, and \\(\\hat{\\kappa}(h) = (1+2\\sum_{k=1}^K \\hat{\\rho}_k(h))\\) is called the inefficiency factor, which represents the inflation of the simulation variance due to autocorrelation in the draws. Values near one indicate draws with little correlation. 4.4.3 Tests of convergence Regarding convergence issues, there are several diagnostics to assess the adequacy of the posterior chains. In particular, graphical approaches such as trace plots and autocorrelation plots are widely used. Trace plots display the sampled values of a parameter (or multiple parameters) as a function of the iteration number, while autocorrelation plots graphically represent \\(\\hat{\\rho}_k\\). The latter shows how correlated the values of \\(\\boldsymbol{\\theta}\\), or functions of \\(\\boldsymbol{\\theta}\\), are at different lags. Trace plots should fluctuate around a stable mean, exploring the entire parameter space without becoming stuck in any particular region. Autocorrelation plots, on the other hand, should exhibit values close to zero or diminish quickly as the lag increases. Additionally, Geweke’s test (J. Geweke 1992) provides a simple two-sample test of means. If the mean of the first window (10% of the chain) is not significantly different from the mean of the second window (50% of the chain), we do not reject the null hypothesis that the two segments of the chain are drawn from the same stationary distribution. The Raftery and Lewis test (Raftery and Lewis 1992) is designed to calculate the approximate number of iterations (\\(S\\)), burn-in (\\(b\\)), and thinning parameter (\\(d\\)) required to estimate \\(p\\left[H(\\boldsymbol{\\theta}) \\leq h\\right]\\), where \\(H(\\boldsymbol{\\theta}): \\mathcal{R}^K \\rightarrow \\mathcal{R}\\). This calculation is based on a specific quantile of interest (\\(q\\)), precision (\\(r\\)), and probability (\\(p\\)). The diagnostic is based on the dependence factor, \\(I = \\frac{S + b}{S_{\\text{Min}}}\\), where \\(S_{\\text{Min}} = \\Phi^{-1}\\left(\\frac{1}{2}(p+1)\\right)^2 q(1-q) / r^2\\), and \\(\\Phi(\\cdot)\\) is the standard normal cumulative distribution function. Values of \\(I\\) much greater than 5 indicate a high level of dependence. Heidelberger and Welch’s test (Heidelberger and Welch 1983) uses a Cramér-von Mises statistic to test the null hypothesis that the sampled values, \\(\\boldsymbol{\\theta}^{(s)}\\), are drawn from a stationary distribution. The statistic is given by: \\[ \\text{CVM}(B_S) = \\int_0^1 B_S(t)^2 \\, dt, \\] where \\(B_S(t) = \\frac{S_{\\left[St\\right]} - \\left[St\\right] \\bar{\\boldsymbol{\\theta}}^S}{\\sqrt{S p(0)}}\\), \\(S_S = \\sum_{s=1}^S \\boldsymbol{\\theta}^{(s)}\\), \\(\\bar{\\boldsymbol{\\theta}}^S = S_S / S\\), and \\(p(0)\\) is the spectral density at 0, with \\(0 \\leq t \\leq 1\\). Under the null hypothesis, \\(B_S(t)\\) converges in distribution to a Brownian bridge. This test is recursively applied until either the null hypothesis is not rejected, or \\(s = 50\\%\\) of the chain has been discarded. Subsequently, the half-width test calculates a 95% confidence interval for the mean using the portion of the chain that passed the stationarity test. If the ratio of the half-width of this interval to the mean is less than 0.1, the test is considered passed. This indicates no evidence to reject the null hypothesis that the estimated mean is accurate and stable. There are other diagnostics in Bayesian inference that we do not mention here, such as the Gelman and Rubin test (Gelman and Rubin 1992). This is because we focus on the available diagnostics in our Graphical User Interface (GUI). 4.4.4 Checking for errors in the posterior simulator In this book, we provide basic code templates to get posterior draws for performing inference under the Bayesian framework when there is no closed-form solution. We are prone to making mistakes and greatly appreciate your feedback to help improve our code and identify any other potential issues. One way to check if our code works correctly is to perform simulations where the population parameters are known. If the code is functioning properly, the posterior estimates should converge to these values as the sample size increases due to the Bayesian consistency. This is an informal approach to identifying potential mistakes. John Geweke (2004) offers a more formal method for code validation. The starting point is the joint density \\(p(\\boldsymbol{y}, \\boldsymbol{\\theta}) = p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta})\\) and a test function \\(h(\\boldsymbol{y}, \\boldsymbol{\\theta})\\) such that \\(\\sigma_h^2 = \\text{Var}[h(\\boldsymbol{y}, \\boldsymbol{\\theta})] &lt; \\infty\\). Assume that there is a marginal-conditional simulator for the joint distribution of \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{\\theta}\\): \\[\\begin{align} \\boldsymbol{\\theta}^{(s)} &amp;\\sim \\pi(\\boldsymbol{\\theta}) \\\\ \\boldsymbol{y}^{(s)} &amp;\\sim p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}^{(s)}) \\\\ h^{(s)} &amp;= h(\\boldsymbol{y}^{(s)}, \\boldsymbol{\\theta}^{(s)}). \\end{align}\\] The sequence \\(\\left\\{\\boldsymbol{y}^{(s)}, \\boldsymbol{\\theta}^{(s)}\\right\\}\\) is i.i.d., \\(\\bar{h}_S\\) converges almost surely to \\(\\mathbb{E}[h(\\boldsymbol{y}, \\boldsymbol{\\theta})]\\), and there is convergence in distribution when \\(\\bar{h}_S\\) is well standardized (see Equation (4.4)) and \\(\\hat{\\sigma}^2_{Sh}(\\boldsymbol{\\theta})\\) converges to \\({\\sigma}^2_h(\\boldsymbol{\\theta})\\) almost surely. A posterior simulator produces draws \\(\\boldsymbol{\\theta}^{(s)}\\) given a particular realization \\(\\boldsymbol{y}_{\\text{Obs}}\\), using the transition density \\(q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(s-1)}, \\boldsymbol{y}_{\\text{Obs}})\\). Thus, a successive-conditional simulator consists of an initial draw \\(\\boldsymbol{\\theta}^{(0)}\\) from \\(\\pi(\\boldsymbol{\\theta})\\) followed by: \\[\\begin{align} \\boldsymbol{y}^{(l)} &amp;\\sim p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}^{(l-1)}) \\\\ \\boldsymbol{\\theta}^{(l)} &amp;\\sim q(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}^{(l)}, \\boldsymbol{\\theta}^{(l-1)}) \\\\ h^{(l)} &amp;= h(\\boldsymbol{y}^{(l)}, \\boldsymbol{\\theta}^{(l)}), \\end{align}\\] where \\(\\bar{h}_L = L^{-1} \\sum_{l=1}^L h(\\boldsymbol{y}^{(l)}, \\boldsymbol{\\theta}^{(l)})\\) converges almost surely to \\(\\mathbb{E}[h(\\boldsymbol{y}, \\boldsymbol{\\theta})]\\), and there is convergence in distribution when \\(\\bar{h}_L\\) is well standardized, and \\(\\hat{\\sigma}^{*2}_{Lh}(\\boldsymbol{\\theta})\\) converges to \\({\\sigma}^2_h(\\boldsymbol{\\theta})\\) almost surely, for \\(l = 1, 2, \\dots, L\\). Thus, \\[\\begin{align} \\frac{\\bar{h}_S - \\bar{h}_L}{\\left( S^{-1} \\hat{\\sigma}^2_{Sh}(\\boldsymbol{\\theta}) + L^{-1} \\hat{\\sigma}^{*2}_{Lh}(\\boldsymbol{\\theta}) \\right)^{1/2}} &amp;\\stackrel{d}{\\rightarrow} N(0, 1). \\end{align}\\] Thus, we can test \\(H_0. \\ \\bar{h}_S - \\bar{h}_L = 0\\) versus \\(H_1. \\ \\bar{h}_S - \\bar{h}_L \\neq 0\\). Rejection of the null indicates potential errors in implementing the posterior simulator. Example: Mining disaster change point continues Let’s revisit the mining disaster change point example from subsection 4.1.1 and examine some convergence diagnostics for the posterior draws of the rate of disasters after the change point (\\(\\lambda_2\\)). The following code demonstrates how to perform these diagnostics using the R package coda. For clarity and replicability of the results, we present the Gibbs sampler again. The following two figures show the trace and autocorrelation plots. We observe that the posterior draws of \\(\\lambda_2\\) appear stationary around their mean, and the autocorrelation decreases rapidly to zero. The mean and standard deviation of the rate after the change point are 0.92 and 0.12, respectively. The naive and time series standard errors are 0.0008245 and 0.0008945, respectively. The naive standard error assumes iid posterior draws, whereas the time series standard error accounts for autocorrelation. Both standard errors are very similar, indicating a low level of autocorrelation, which is consistent with the results shown in the second figure. The effective sample size of the posterior draws is 16,991, while the total number of posterior draws is 20,000 after a burn-in period of 1,000. The Geweke test statistic is 1.43, which implies no statistical evidence to reject the null hypothesis of equal means in the two segments of the posterior draws. The Raftery and Lewis test yields a dependence factor near 1, indicating a low level of dependence. The Heidelberger and Welch test does not reject the null hypothesis of stationarity for the posterior draws and also confirms that the mean is accurate and stable. In summary, all posterior diagnostics indicate that the posterior draws originate from an invariant stationary distribution. The second part of the code implements the proposal by John Geweke (2004) to assess the reliability of the posterior simulator. The parameter vector is defined as \\(\\boldsymbol{\\theta} = [\\lambda_1 \\ \\lambda_2 \\ H]\\), and the first moments of these parameters are used as test functions. We do not reject the null hypothesis of equal means across the three test functions, indicating that the posterior simulator is functioning correctly. To evaluate the effectiveness of the test, we run the marginal-conditional simulator with prior parameters \\(\\alpha_{l0} = 0.5\\) and \\(\\beta_{l0} = 1\\), \\(l = 1, 2\\). In contrast, for the successive-conditional simulator, we use prior parameters \\(\\alpha_{l0} = 1\\) and \\(\\beta_{l0} = 0.5\\), \\(l = 1, 2\\). In this case, we reject the null hypothesis in two out of three test functions, suggesting that the test performs well in this example. rm(list = ls()) set.seed(10101) dataset&lt;-read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/MiningDataCarlin.csv&quot;,header=T) attach(dataset) str(dataset) a10 &lt;- 0.5; a20 &lt;- 0.5 b10 &lt;- 1; b20 &lt;- 1 y &lt;- Count sumy &lt;- sum(Count); T &lt;- length(Count) theta1 &lt;- NULL; theta2 &lt;- NULL kk &lt;- NULL; H &lt;- 60 MCMC &lt;- 20000; burnin &lt;- 1000; S &lt;- MCMC + burnin; keep &lt;- (burnin+1):S pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = S, width = 300) for(s in 1:S){ a1 &lt;- a10 + sum(y[1:H]) b1 &lt;- b10+H theta11 &lt;- rgamma(1,a1,b1) theta1 &lt;- c(theta1,theta11) a2 &lt;- a20 + sum(y[(1+H):T]) b2 &lt;- b20 + T-H theta22 &lt;- rgamma(1,a2,b2) theta2 &lt;- c(theta2,theta22) pp&lt;-NULL for(l in 1:T){ p &lt;- exp(l*(theta22-theta11))*(theta11/theta22)^(sum(y[1:l])) pp &lt;- c(pp,p) } prob &lt;- pp/sum(pp) H &lt;- sample(1:T,1,prob=prob) kk &lt;- c(kk,H) setWinProgressBar(pb, s, title=paste( round(s/S*100, 0),&quot;% done&quot;)) } close(pb) library(coda); library(latex2exp) theta1Post &lt;- mcmc(theta1[keep]); summary(theta1Post) HPost &lt;- mcmc(kk); summary(HPost) theta2Post &lt;- mcmc(theta2[keep]); summary(theta2Post) plot(theta2Post, density = FALSE, main = &quot;Trace plot&quot;, ylab = TeX(&quot;$\\\\theta_{2}$&quot;)) autocorr.plot(theta2Post, main = &quot;Autocorrelation plot&quot;) raftery.diag(theta2Post, q = 0.025, r = 0.005, s = 0.95) geweke.diag(theta2Post, frac1 = 0.1, frac2 = 0.5) heidel.diag(theta2Post, eps = 0.1, pvalue = 0.05) effectiveSize(theta2Post) # Marginal-conditional simulator Theta1Prior &lt;- rgamma(MCMC,a10,b10); Theta2Prior &lt;- rgamma(MCMC,a20,b20) kPrior &lt;- sample(1:T, MCMC, replace = TRUE, prob = rep(1/T,T)) ytPrior &lt;- function(par){ y1t &lt;- rpois(par[3], par[1]) if(par[3] == T){y2t &lt;- NULL }else{y2t &lt;- rpois(T-par[3], par[2]) } yt &lt;- c(y1t, y2t) return(yt) } pars1 &lt;- cbind(Theta1Prior, Theta2Prior, kPrior); Yt &lt;- apply(pars1, 1, ytPrior) parsmcmc1 &lt;- coda::mcmc(pars1); Summ1 &lt;- summary(parsmcmc1) # Successive-conditional simulator SucConSim &lt;- function(a10, b10, a20, b20, par){ y &lt;- ytPrior(par) theta1 &lt;- par[1]; theta2 &lt;- par[2]; H &lt;- par[3] a1 &lt;- a10 + sum(y[1:H]); b1 &lt;- b10+H theta11 &lt;- rgamma(1,a1,b1) if(H == T){ a2 &lt;- a20 }else{ a2 &lt;- a20 + sum(y[(1+H):T]) } b2 &lt;- b20 + T-H; theta22 &lt;- rgamma(1,a2,b2); pp&lt;-NULL for(l in 1:T){ p &lt;- l*(theta22-theta11) + (sum(y[1:l]))*log(theta11/theta22) pp &lt;- c(pp,p) } pps &lt;- exp(pp - max(pp)); prob &lt;- pps/sum(pps) H &lt;- sample(1:T, 1, prob=prob) parNew &lt;- list(y = y, pars = c(theta11, theta22, H)) return(parNew) } a10 &lt;- 0.5; b10 &lt;- 1; a20 &lt;- 0.5; b20 &lt;- 1 # a10 &lt;- 1; b10 &lt;- 0.5; a20 &lt;- 1; b20 &lt;- 0.5 par1 &lt;- rgamma(1,a10,b10); par2 &lt;- rgamma(1,a20,b20) par3 &lt;- sample(1:T, 1, replace = TRUE, prob = rep(1/T,T)) pars2 &lt;- matrix(NA, MCMC, 3); pars2[1,] &lt;- c(par1, par2, par3) for(s in 2:MCMC){ Res &lt;- SucConSim(a10 = a10, b10 = b10, a20 = a20, b20 = b20, par = pars2[s-1,]) pars2[s, ] &lt;- Res$pars } parsmcmc2 &lt;- coda::mcmc(pars2); Summ2 &lt;- summary(parsmcmc2) TestGeweke &lt;- function(j){ Test &lt;- (Summ1[[&quot;statistics&quot;]][j,1] - Summ2[[&quot;statistics&quot;]][j,1])/(Summ1[[&quot;statistics&quot;]][j,4]+Summ2[[&quot;statistics&quot;]][j,4])^0.5 Reject &lt;- abs(Test) &gt; qnorm(0.975) return(list(Test = Test, Reject = Reject)) } TestGeweke(1); TestGeweke(2); TestGeweke(3) References Gelman, Andrew, and Donald B. Rubin. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” Statistical Science 7 (4): 457–72. https://doi.org/10.1214/ss/1177011136. Geweke, J. 1992. “Bayesian Statistics.” In. Clarendon Press, Oxford, UK. Geweke, John. 2004. “Getting It Right: Joint Distribution Tests of Posterior Simulators.” Journal of the American Statistical Association 99 (467): 799–804. Geyer, Charles J. 1992. “Practical Markov Chain Monte Carlo.” Statistical Science, 473–83. Heidelberger, P., and P. D. Welch. 1983. “Simulation Run Length Control in the Presence of an Initial Transient.” Operations Research 31 (6): 1109–44. Raftery, A. E., and S. M. Lewis. 1992. “One Long Run with Diagnostics: Implementation Strategies for Markov Chain Monte Carlo.” Statistical Science 7: 493–97. "],["sec55.html", "4.5 Summary", " 4.5 Summary In this chapter, we present the most popular methods for obtaining posterior draws when the posterior distribution does not have a standard closed-form solution. In particular, Markov chain Monte Carlo (MCMC) methods, such as Gibbs sampling and the Metropolis-Hastings algorithm, are the most commonly used approaches in this book. However, Hamiltonian Monte Carlo is gaining particular relevance in high-dimensional problems, while particle filtering (Sequential Monte Carlo) is widely applied in time series models. Each problem requires careful consideration to determine the most appropriate method, and in many cases, a combination of methods is necessary. For instance, estimating fixed parameters in state-space models typically requires MCMC methods, while recursion of the state vector requires particle filtering. Additionally, convergence diagnostics are crucial because MCMC methods rely on technical assumptions that must be verified. "],["sec56.html", "4.6 Exercises", " 4.6 Exercises Example: The normal model with independent priors Let’s recap the math test exercise in Chapter 3, this time assuming independent priors. Specifically, let \\(Y_i \\sim N(\\mu, \\sigma^2)\\), where \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0 / 2, \\delta_0 / 2)\\). The sample size is 50, and the mean and standard deviation of the math scores are 102 and 10, respectively. We set \\(\\mu_0 = 100\\), \\(\\sigma_0^2 = 100\\), and \\(\\alpha_0 = \\delta_0 = 0.001\\). Find the posterior distribution of \\(\\mu\\) and \\(\\sigma^2\\). Program a Gibbs sampler algorithm and plot the histogram of the posterior draws of \\(\\mu\\). Show that the Gibbs sampler is a particular case of the Metropolis-Hastings where the acceptance probability is equal to 1. Implement a Metropolis-Hastings to sample from the Cauchy distribution, \\(C(0,1)\\), using as proposals a standard normal distribution and a Student’s t distribution with 5 degrees of freedom. This exercise was proposed by Professor Hedibert Freitas Lopes, who cites Thomas and Tu (2021) as a useful reference for an introduction to Hamiltonian Monte Carlo in R and the hmclearn package. The task is to obtain posterior draws using the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms for the posterior distribution given by \\[ \\pi(\\theta_1,\\theta_2\\mid \\mathbf{y}) \\propto \\exp\\left\\{-\\frac{1}{2}(\\theta_1^2\\theta_2^2 + \\theta_1^2 + \\theta_2^2 - 8\\theta_1 - 8\\theta_2)\\right\\}. \\] Ph.D. students sleeping hours continues Use importance sampling based on a \\(U(0,1)\\) proposal to obtain draws of \\(\\boldsymbol{\\theta}\\mid \\mathbf{y} \\sim B(16.55,39.57)\\) in the Ph.D. students’ sleeping hours example in Chapter 3. Note that, based on Exercise 15 in Chapter 3, \\(\\alpha_0 = 1.44\\) and \\(\\beta_0 = 2.57\\). Compute the marginal likelihood in this context (Bernoulli-Beta model) and compare it to the result obtained using the Gelfand-Dey method. Example 4.1 in Gordon, Salmond, and Smith (1993) is \\[\\begin{align*} \\theta_t &amp;= 0.5\\theta_{t-1} + 25\\frac{\\theta_{t-1}}{1+\\theta_{t-1}^2} + 8 \\cos(1.2t) + w_t \\\\ y_t &amp;= \\frac{\\theta_{t}^2}{20} + \\mu_t, \\end{align*}\\] where \\(\\theta_0 \\sim N(0, \\sqrt{10})\\), \\(w_t \\sim \\mathcal{N}(0, \\sqrt{10})\\) and \\(\\mu_t \\sim N(0, \\sqrt{1})\\). Perform sequential importance sampling in this example. Perform particle (Bootstrap) filtering in this example. Estimate the marginal likelihood in this example. Ph.D. students sleeping hours continues Perform the diagnostics of Section 4.4 in this example. Check if there are errors in the posterior simulator of the Metropolis-Hastings algorithm in this example using the Geweke approach using as test functions the first moments of \\(p\\) and \\(p^2\\). Remember from Exercise 15 in Chapter 3 that the sample size is 52, and \\(\\alpha_0 = 1.22\\) and \\(\\beta_0 = 2.57\\). Run the Geweke test using \\(\\alpha_0 = 2.57\\) and \\(\\beta_0 = 1.22\\), and check the results. References Gordon, N. J., D. J. Salmond, and A. F. Smith. 1993. “Novel Approach to Nonlinear/Non-Gaussian Bayesian State Estimation.” IEE Proceedings F (Radar and Signal Processing) 140 (2): 107–13. Thomas, Samuel, and Wanzhu Tu. 2021. “Learning Hamiltonian Monte Carlo in r.” The American Statistician 75 (4): 403–13. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
