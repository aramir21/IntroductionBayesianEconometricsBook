[["Chap6.html", "Chapter 6 Univariate regression", " Chapter 6 Univariate regression We describe how to perform Bayesian inference in some of the most common univariate models: normal-inverse gamma, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap in linear models. The point of departure is assuming a random sample of cross-sectional units. We then show the posterior distributions of the parameters and some applications. In addition, we show how to perform inference in various models using three levels of programming skills: our graphical user interface (GUI), packages from R, and programming the posterior distributions. The first requires no programming skills, the second requires an intermediate level, and the third demands more advanced skills. We also include mathematical and computational exercises. We can run our GUI typingshiny::runGitHub(\"besmarter/BSTApp\", launch.browser=T) in the R console or any R code editor and execute it. However, users should see Chapter 5 for details. "],["sec61.html", "6.1 The Gaussian linear model", " 6.1 The Gaussian linear model The Gaussian linear model specifies \\[ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mu \\] such that \\(\\mu \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I}_N)\\) is a stochastic error, \\(\\mathbf{X}\\) is an \\(N \\times K\\) matrix of regressors, \\(\\boldsymbol{\\beta}\\) is a \\(K\\)-dimensional vector of location coefficients, \\(\\sigma^2\\) is the variance of the model (scale parameter), \\(\\mathbf{y}\\) is an \\(N\\)-dimensional vector of a dependent variable, and \\(N\\) is the sample size. We describe this model using the conjugate family in 3.3, that is, \\[ \\pi(\\boldsymbol{\\beta},\\sigma^2) = \\pi(\\boldsymbol{\\beta} \\mid \\sigma^2) \\times \\pi(\\sigma^2), \\] which allows obtaining the posterior marginal distribution for \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\). We assume independent priors in this section, that is, \\[ \\pi(\\boldsymbol{\\beta},\\sigma^2) = \\pi(\\boldsymbol{\\beta}) \\times \\pi(\\sigma^2), \\] where \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\mathbf{B}_0)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\), with \\(\\alpha_0/2\\) and \\(\\delta_0/2\\) as the shape and rate parameters. This setting allows deriving the posterior conditional distributions, \\[ \\pi(\\boldsymbol{\\beta} \\mid \\sigma^2, \\mathbf{y}, \\mathbf{X}) \\] and \\[ \\pi(\\sigma^2 \\mid \\boldsymbol{\\beta}, \\mathbf{y}, \\mathbf{X}), \\] which in turn enables the use of the Gibbs sampler algorithm to perform posterior inference on \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\). The likelihood function in this model is \\[\\begin{align} p(\\mathbf{y} \\mid \\boldsymbol{\\beta}, \\sigma^2, \\mathbf{X}) = (2\\pi\\sigma^2)^{-\\frac{N}{2}} \\exp \\left\\{-\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\right\\}. \\end{align}\\] Then, the conditional posterior distributions are \\[\\begin{align} \\boldsymbol{\\beta} \\mid \\sigma^2, \\mathbf{y}, \\mathbf{X} \\sim N(\\boldsymbol{\\beta}_n, \\mathbf{B}_n), \\end{align}\\] and \\[\\begin{align} \\sigma^2 \\mid \\boldsymbol{\\beta}, \\mathbf{y}, \\mathbf{X} \\sim IG(\\alpha_n/2, \\delta_n/2), \\end{align}\\] where \\[ \\mathbf{B}_n = (\\mathbf{B}_0^{-1} + \\sigma^{-2} \\mathbf{X}^{\\top} \\mathbf{X})^{-1}, \\] \\[ \\boldsymbol{\\beta}_n= \\mathbf{B}_n (\\mathbf{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\sigma^{-2} \\mathbf{X}^{\\top} \\mathbf{y}), \\] \\[ \\alpha_n = \\alpha_0 + N, \\] \\[ \\delta_n = \\delta_0 + (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^{\\top} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}). \\] This model can be extended to consider heteroskedasticity such that \\(y_i \\sim N(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2/\\tau_i)\\), where \\(\\tau_i \\sim G(v/2,v/2)\\). See Exercise 2 for details. Example: The market value of soccer players in Europe Let’s analyze the determinants of the market value of soccer players in Europe. In particular, we use the dataset 1ValueFootballPlayers.csv, which is in the folder DataApp in our GitHub repository: https://github.com/besmarter/BSTApp. This dataset was used by Serna Rodríguez, Ramírez Hassan, and Coad (2019) to find the determinants of high-performance soccer players in the five most important national leagues in Europe. The specification of the model is \\[\\begin{align} \\log(\\text{Value}_i) &amp;= \\beta_1 + \\beta_2 \\text{Perf}_i + \\beta_3 \\text{Age}_i + \\beta_4 \\text{Age}^2_i + \\beta_5 \\text{NatTeam}_i \\\\ &amp;\\quad + \\beta_6 \\text{Goals}_i + \\beta_7 \\text{Exp}_i + \\beta_8 \\text{Exp}^2_i + \\mu_i, \\end{align}\\] where Value is the market value in Euros (2017), Perf is a measure of performance, Age is the player’s age in years, NatTeam is an indicator variable that takes the value of 1 if the player has been on the national team, Goals is the number of goals scored by the player during his career, and Exp is his experience in years. We assume that the dependent variable follows a normal distribution, so we use a normal-inverse gamma model with vague conjugate priors where \\[ \\mathbf{B}_0 = 1000 \\mathbf{I}_{8}, \\quad \\boldsymbol{\\beta}_0 = \\mathbf{0}_{8}, \\quad \\alpha_0 = 0.001, \\quad \\delta_0 = 0.001. \\] We perform a Gibbs sampler with 5,000 MCMC iterations, plus a burn-in of 5,000, and a thinning parameter equal to 1. Once our GUI is displayed (see the beginning of this chapter), we should follow the next Algorithm to run linear Gaussian models in our GUI (see Chapter 5 for details). Algorithm: Gaussian linear model Select Univariate Models on the top panel Choose the Normal model using the left radio button Upload the dataset by selecting if there is a header and specifying the separator (comma, semicolon, or tab) Use the Browse button to select the file and preview the dataset Adjust MCMC iterations, burn-in, and thinning using the Range sliders Specify dependent and independent variables using the Formula builder Click the Build formula button to generate the model formula in R Modify the formula in the Main equation box if necessary Set hyperparameters (mean vector, covariance matrix, shape, and scale parameters) if needed Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons We can see in the following R code examples how to perform the linear Gaussian model using the MCMCregress command from the MCMCpack package, as well as how to program the Gibbs sampler ourselves. We should obtain similar results using all three approaches: GUI, package, and our function. In fact, our GUI relies on the MCMCregress command. For instance, the value of a top soccer player in Europe increases by 134% (\\(\\exp(0.85)-1\\)) on average when he has played for the national team, with a 95% credible interval of (86%, 197%). rm(list = ls()) set.seed(010101) ########################## Linear regression: Value of soccer players ########################## Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(Data) y &lt;- log(Value) # Value: Market value in Euros (2017) of soccer players # Regressors quantity including intercept X &lt;- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2) # Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. # NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared # Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists k &lt;- dim(X)[2] N &lt;- dim(X)[1] # Hyperparameters d0 &lt;- 0.001/2 a0 &lt;- 0.001/2 b0 &lt;- rep(0, k) c0 &lt;- 1000 B0 &lt;- c0*diag(k) B0i &lt;- solve(B0) # MCMC parameters mcmc &lt;- 5000 burnin &lt;- 5000 tot &lt;- mcmc + burnin thin &lt;- 1 # Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix posterior &lt;- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin) summary(coda::mcmc(posterior)) ## ## Iterations = 1:5000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 5000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 3.695499 2.228060 3.151e-02 3.151e-02 ## XPerf 0.035445 0.004299 6.079e-05 6.079e-05 ## XAge 0.778410 0.181362 2.565e-03 2.565e-03 ## XAge2 -0.016617 0.003380 4.781e-05 4.781e-05 ## XNatTeam 0.850362 0.116861 1.653e-03 1.689e-03 ## XGoals 0.009097 0.001603 2.266e-05 2.266e-05 ## XExp 0.206208 0.062713 8.869e-04 8.428e-04 ## XExp2 -0.006992 0.002718 3.844e-05 3.719e-05 ## sigma2 0.969590 0.076091 1.076e-03 1.076e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X -0.545746 2.174460 3.653373 5.171463 8.177948 ## XPerf 0.026933 0.032570 0.035421 0.038368 0.043817 ## XAge 0.419057 0.656975 0.779534 0.902753 1.125442 ## XAge2 -0.022967 -0.018928 -0.016651 -0.014366 -0.009919 ## XNatTeam 0.627228 0.771339 0.852420 0.928765 1.075360 ## XGoals 0.005914 0.007984 0.009108 0.010180 0.012272 ## XExp 0.082206 0.164290 0.206742 0.248716 0.329809 ## XExp2 -0.012290 -0.008829 -0.007002 -0.005188 -0.001762 ## sigma2 0.832320 0.915580 0.965122 1.018776 1.127566 # Posterior distributions programming the Gibbs sampling # Auxiliary parameters XtX &lt;- t(X)%*%X bhat &lt;- solve(XtX)%*%t(X)%*%y an &lt;- a0 + N # Gibbs sampling functions PostSig2 &lt;- function(Beta){ dn &lt;- d0 + t(y - X%*%Beta)%*%(y - X%*%Beta) sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2){ Bn &lt;- solve(B0i + sig2^(-1)*XtX) bn &lt;- Bn%*%(B0i%*%b0 + sig2^(-1)*XtX%*%bhat) Beta &lt;- MASS::mvrnorm(1, bn, Bn) return(Beta) } PostBetas &lt;- matrix(0, mcmc+burnin, k) PostSigma2 &lt;- rep(0, mcmc+burnin) Beta &lt;- rep(0, k) for(s in 1:tot){ sig2 &lt;- PostSig2(Beta = Beta) PostSigma2[s] &lt;- sig2 Beta &lt;- PostBeta(sig2 = sig2) PostBetas[s,] &lt;- Beta } keep &lt;- seq((burnin+1), tot, thin) PosteriorBetas &lt;- PostBetas[keep,] colnames(PosteriorBetas) &lt;- c(&quot;Intercept&quot;, &quot;Perf&quot;, &quot;Age&quot;, &quot;Age2&quot;, &quot;NatTeam&quot;, &quot;Goals&quot;, &quot;Exp&quot;, &quot;Exp2&quot;) summary(coda::mcmc(PosteriorBetas)) ## ## Iterations = 1:5000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 5000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Intercept 3.664526 2.194335 3.103e-02 3.167e-02 ## Perf 0.035360 0.004317 6.105e-05 6.105e-05 ## Age 0.780262 0.178609 2.526e-03 2.526e-03 ## Age2 -0.016640 0.003335 4.716e-05 4.716e-05 ## NatTeam 0.850323 0.119434 1.689e-03 1.689e-03 ## Goals 0.009157 0.001600 2.262e-05 2.262e-05 ## Exp 0.205964 0.062906 8.896e-04 8.597e-04 ## Exp2 -0.006995 0.002720 3.847e-05 3.717e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Intercept -0.57909 2.170408 3.649759 5.111483 8.023950 ## Perf 0.02694 0.032478 0.035315 0.038163 0.044043 ## Age 0.42908 0.662650 0.781947 0.900849 1.126256 ## Age2 -0.02308 -0.018866 -0.016662 -0.014410 -0.010000 ## NatTeam 0.62161 0.768733 0.847537 0.930666 1.092413 ## Goals 0.00605 0.008072 0.009152 0.010245 0.012236 ## Exp 0.08150 0.163626 0.206184 0.248795 0.327549 ## Exp2 -0.01231 -0.008870 -0.006976 -0.005186 -0.001681 PosteriorSigma2 &lt;- PostSigma2[keep] summary(coda::mcmc(PosteriorSigma2)) ## ## Iterations = 1:5000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 5000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 0.973309 0.077316 0.001093 0.001116 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 0.8361 0.9189 0.9685 1.0228 1.1421 References Serna Rodríguez, M., A. Ramírez Hassan, and A. Coad. 2019. “Uncovering Value Drivers of High Performance Soccer Players.” Journal of Sport Economics 20 (6): 819–49. "],["sec62.html", "6.2 The logit model", " 6.2 The logit model In the logit model, the dependent variable is binary, \\(y_i=\\left\\{1,0\\right\\}\\), which follows a Bernoulli distribution, \\(y_i \\stackrel{ind}{\\sim} B(\\pi_i)\\), such that \\(p(y_i=1)=\\pi_i\\), where \\(\\pi_i = \\frac{\\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}}{1 + \\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}}\\), and \\(\\mathbf{x}_i\\) is a \\(K\\)-dimensional vector of regressors. The likelihood function of the logit model is: \\[ p({\\mathbf{y}} \\mid \\boldsymbol{\\beta}, {\\mathbf{X}}) = \\prod_{i=1}^N \\pi_i^{y_i}(1 - \\pi_i)^{1 - y_i} \\] \\[ = \\prod_{i=1}^N \\left( \\frac{\\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}}{1 + \\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}} \\right)^{y_i} \\left( \\frac{1}{1 + \\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}} \\right)^{1 - y_i}. \\] We can specify a Normal distribution as a prior, \\(\\boldsymbol{\\beta} \\sim N({\\boldsymbol{\\beta}}_0, {\\mathbf{B}}_0)\\). Then, the posterior distribution is: \\[ \\pi(\\boldsymbol{\\beta} \\mid {\\mathbf{y}}, {\\mathbf{X}}) \\propto \\prod_{i=1}^N \\left( \\frac{\\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}}{1 + \\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}} \\right)^{y_i} \\left( \\frac{1}{1 + \\exp\\left\\{{\\mathbf{x}}_i^{\\top}\\boldsymbol{\\beta}\\right\\}} \\right)^{1 - y_i} \\] \\[ \\times \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0)^{\\top} {\\mathbf{B}}_0^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0)\\right\\}. \\] The logit model does not have a standard posterior distribution. Therefore, a random walk Metropolis–Hastings algorithm can be used to obtain draws from the posterior distribution. A potential proposal distribution is a multivariate normal, centered at the current value, with covariance matrix \\(\\tau^2({\\mathbf{B}}_0^{-1} + \\widehat{{\\mathbf{\\Sigma}}}^{-1})^{-1}\\), where \\(\\tau &gt; 0\\) is a tuning parameter and \\(\\widehat{\\mathbf{\\Sigma}}\\) is the sample covariance matrix obtained from the maximum likelihood estimation (Martin, Quinn, and Park 2011). Tuning parameters should be set in a way that ensures reasonable diagnostic criteria and acceptance rates. Observe that: \\[ \\log(p({\\mathbf{y}} \\mid \\boldsymbol{\\beta}, {\\mathbf{X}})) = \\sum_{i=1}^N y_i {\\mathbf{x}}_i^{\\top} \\boldsymbol{\\beta} - \\log(1 + \\exp({\\mathbf{x}}_i^{\\top} \\boldsymbol{\\beta})). \\] This expression can be used when calculating the acceptance parameter in the computational implementation of the Metropolis-Hastings algorithm. In particular, the acceptance parameter is: \\[ \\alpha = \\min\\left\\{1, \\exp\\left(\\log(p({\\mathbf{y}} \\mid \\boldsymbol{\\beta}^{c}, {\\mathbf{X}})) + \\log(\\pi(\\boldsymbol{\\beta}^c)) - \\left(\\log(p({\\mathbf{y}} \\mid \\boldsymbol{\\beta}^{(s-1)}, {\\mathbf{X}})) + \\log(\\pi(\\boldsymbol{\\beta}^{(s-1)}))\\right)\\right)\\right\\}, \\] where \\(\\boldsymbol{\\beta}^c\\) and \\(\\boldsymbol{\\beta}^{(s-1)}\\) are the draws from the proposal distribution and the previous iteration of the Markov chain, respectively. Formulating the acceptance rate using \\(\\log\\) helps mitigate computational problems. Example: Simulation exercise Let’s do a simulation exercise to check the performance of the algorithm. Set \\(\\boldsymbol{\\beta} = \\begin{bmatrix}0.5 &amp; 0.8 &amp; -1.2\\end{bmatrix}^{\\top}\\), \\(x_{ik} \\sim N(0,1)\\), \\(k=2,3\\) and \\(i=1,2,\\dots,10000\\). We set as hyperparameters \\(\\boldsymbol{\\beta}_0 = [0 \\ 0 \\ 0]^{\\top}\\) and \\({\\mathbf{B}}_0 = 1000 {\\mathbf{I}}_3\\). The tuning parameter for the Metropolis-Hastings algorithm is equal to 1. Once our GUI is displayed (see beginning of this chapter), we should follow the next Algorithm to run logit models in our GUI (see Chapter 5 for details): Algorithm: Logit model Select Univariate Models on the top panel Select Logit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors Select the tuning parameter for the Metropolis-Hastings algorithm. This step is not necessary as by default our GUI sets the tuning parameter at 1.1 Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons We can see in the following R code how to perform the logit model using the MCMClogit command from the MCMCpack package, as well as by programming the Metropolis-Hastings algorithm ourselves. We should obtain similar results using the three approaches: GUI, package, and our function. Our GUI relies on the MCMClogit command. In particular, we achieve an acceptance rate of 0.46, and the diagnostics suggest that the posterior chains behave well. In general, the 95% credible intervals encompass the population values, and both the mean and median are very close to these values. ########################## Logit: Simulation ########################## # Simulate data rm(list = ls()) set.seed(010101) N &lt;- 10000 # Sample size B &lt;- c(0.5, 0.8, -1.2) # Population location parameters x2 &lt;- rnorm(N) # Regressor x3 &lt;- rnorm(N) # Regressor X &lt;- cbind(1, x2, x3) # Regressors XB &lt;- X%*%B PY &lt;- exp(XB)/(1 + exp(XB)) # Probability of Y = 1 Y &lt;- rbinom(N, 1, PY) # Draw Y&#39;s table(Y) # Frequency ## Y ## 0 1 ## 4115 5885 # write.csv(cbind(Y, x2, x3), file = &quot;DataSimulations/LogitSim.csv&quot;) # Export data # MCMC parameters iter &lt;- 5000; burnin &lt;- 1000; thin &lt;- 5; tune &lt;- 1 # Hyperparameters K &lt;- dim(X)[2] b0 &lt;- rep(0, K) c0 &lt;- 1000 B0 &lt;- c0*diag(K) B0i &lt;- solve(B0) # Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix RegLog &lt;- MCMCpack::MCMClogit(Y~X-1, mcmc = iter, burnin = burnin, thin = thin, b0 = b0, B0 = B0i, tune = tune) summary(RegLog) ## ## Iterations = 1001:5996 ## Thinning interval = 5 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 0.4896 0.02550 0.0008064 0.001246 ## Xx2 0.8330 0.02730 0.0008632 0.001406 ## Xx3 -1.2104 0.03049 0.0009643 0.001536 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X 0.4424 0.4728 0.4894 0.5072 0.5405 ## Xx2 0.7787 0.8159 0.8327 0.8505 0.8852 ## Xx3 -1.2758 -1.2296 -1.2088 -1.1902 -1.1513 # Posterior distributions programming the Metropolis-Hastings algorithm MHfunc &lt;- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 6000, burnin = 1000, thin = 5){ Xm &lt;- cbind(1, X) # Regressors K &lt;- dim(Xm)[2] # Number of location parameters BETAS &lt;- matrix(0, iter + burnin, K) # Space for posterior chains Reg &lt;- glm(y ~ Xm - 1, family = binomial(link = &quot;logit&quot;)) # Maximum likelihood estimation BETA &lt;- Reg$coefficients # Maximum likelihood parameter estimates tot &lt;- iter + burnin # Total iterations M-H algorithm COV &lt;- vcov(Reg) # Maximum likelihood covariance matrix COVt &lt;- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution Accep &lt;- rep(0, tot) # Space for calculating the acceptance rate # Create progress bar in case that you want to see iterations progress pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(it in 1:tot){ BETAc &lt;- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter likecand &lt;- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood for the candidate likepast &lt;- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log likelihood for the actual draw priorcand &lt;- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior for candidate priorpast &lt;- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior for actual draw alpha &lt;- min(1, exp((likecand + priorcand) - (likepast + priorpast))) #Probability of selecting candidate u &lt;- runif(1) # Decision rule for selecting candidate if(u &lt; alpha){ BETA &lt;- BETAc # Changing reference for candidate if selected Accep[it] &lt;- 1 # Indicator if the candidate is accepted } BETAS[it, ] &lt;- BETA # Saving draws setWinProgressBar(pb, it, title=paste( round(it/tot*100, 0), &quot;% done&quot;)) } close(pb) keep &lt;- seq(burnin, tot, thin) return(list(Bs = BETAS[keep[-1], ], AceptRate = mean(Accep[keep[-1]]))) } Posterior &lt;- MHfunc(y = Y, X = cbind(x2, x3), iter = iter, burnin = burnin, thin = thin) # Running our M-H function changing some default parameters. paste(&quot;Acceptance rate equal to&quot;, round(Posterior$AceptRate, 2), sep = &quot; &quot;) ## [1] &quot;Acceptance rate equal to 0.46&quot; &quot;Acceptance rate equal to 0.46&quot; ## [1] &quot;Acceptance rate equal to 0.46&quot; PostPar &lt;- coda::mcmc(Posterior$Bs) # Names colnames(PostPar) &lt;- c(&quot;Cte&quot;, &quot;x1&quot;, &quot;x2&quot;) # Summary posterior draws summary(PostPar) ## ## Iterations = 1:1000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 1000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Cte 0.4893 0.02427 0.0007674 0.001223 ## x1 0.8309 0.02699 0.0008536 0.001440 ## x2 -1.2107 0.02943 0.0009308 0.001423 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Cte 0.4431 0.4721 0.4899 0.5059 0.5344 ## x1 0.7817 0.8123 0.8305 0.8505 0.8833 ## x2 -1.2665 -1.2309 -1.2107 -1.1911 -1.1538 # Trace and density plots plot(PostPar) # Autocorrelation plots coda::autocorr.plot(PostPar) # Convergence diagnostics coda::geweke.diag(PostPar) ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## Cte x1 x2 ## -0.975 -3.112 1.326 coda::raftery.diag(PostPar,q=0.5,r=0.05,s = 0.95) ## ## Quantile (q) = 0.5 ## Accuracy (r) = +/- 0.05 ## Probability (s) = 0.95 ## ## Burn-in Total Lower bound Dependence ## (M) (N) (Nmin) factor (I) ## Cte 6 731 385 1.90 ## x1 6 703 385 1.83 ## x2 6 725 385 1.88 coda::heidel.diag(PostPar) ## ## Stationarity start p-value ## test iteration ## Cte passed 1 0.4436 ## x1 passed 101 0.3470 ## x2 passed 1 0.0872 ## ## Halfwidth Mean Halfwidth ## test ## Cte passed 0.489 0.00240 ## x1 passed 0.832 0.00268 ## x2 passed -1.211 0.00279 References Martin, Andrew D., Kevin M. Quinn, and Jong Hee Park. 2011. “MCMCpack: Markov Chain Monte Carlo in R.” Journal of Statistical Software 42 (9): 1–21. "],["sec63.html", "6.3 The probit model", " 6.3 The probit model The probit model also has a binary dependent variable. In this case, there is a latent variable (\\(y_i^*\\), which is unobserved) that defines the structure of the estimation problem. In particular, \\[ y_i = \\begin{cases} 0, &amp; \\text{if } y_i^* \\leq 0, \\\\ 1, &amp; \\text{if } y_i^* &gt; 0. \\end{cases} \\] such that \\(y_i^* = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} + \\mu_i\\), where \\(\\mu_i \\stackrel{i.i.d.}{\\sim} N(0,1)\\).1 This implies \\(P(y_i = 1) = \\pi_i = \\Phi(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta})\\), where \\(\\mathbf{x}_i\\) is a \\(K\\)-dimensional vector of regressors. Albert and Chib (1993) implemented data augmentation (Tanner and Wong 1987) to apply a Gibbs sampling algorithm to this model. Augmenting this model with \\(y_i^*\\), we can express the likelihood contribution from observation \\(i\\) as: \\[ p(y_i \\mid y_i^*) = \\mathbb{1}({y_i = 0}) \\mathbb{1}({y_i^* \\leq 0}) + \\mathbb{1}({y_i = 1}) \\mathbb{1}({y_i^* &gt; 0}), \\] where \\(\\mathbb{1}(A)\\) is an indicator function that takes the value of 1 when the condition \\(A\\) is satisfied. The posterior distribution is: \\[ \\pi(\\boldsymbol{\\beta}, \\mathbf{y^*} \\mid \\mathbf{y}, \\mathbf{X}) \\propto \\prod_{i=1}^N \\left[\\mathbb{1}({y_i = 0}) \\mathbb{1}({y_i^* \\leq 0}) + \\mathbb{1}({y_i = 1}) \\mathbb{1}({y_i^* &gt; 0}) \\right] \\] \\[ \\times N_N(\\mathbf{y^*} \\mid \\mathbf{X\\boldsymbol{\\beta}}, \\mathbf{I}_n) \\times N_K(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\beta}_0, \\mathbf{B}_0), \\] where we assume a Gaussian prior for \\(\\boldsymbol{\\beta}\\): \\(\\boldsymbol{\\beta} \\sim N_K(\\boldsymbol{\\beta}_0, \\mathbf{B}_0)\\). This implies \\[ y_i^* \\mid \\boldsymbol{\\beta}, \\mathbf{y}, \\mathbf{X} \\sim \\begin{cases} TN_{(-\\infty,0]}(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, 1) &amp; \\text{if } y_i = 0, \\\\ TN_{(0,\\infty)}(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, 1) &amp; \\text{if } y_i = 1, \\end{cases}, \\] where \\(TN\\) denotes a truncated normal density. \\[ \\boldsymbol{\\beta} \\mid \\mathbf{y}^*, \\mathbf{X} \\sim N(\\boldsymbol{\\beta}_n, \\mathbf{B}_n), \\] where \\(\\mathbf{B}_n = (\\mathbf{B}_0^{-1} + \\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\), and \\(\\boldsymbol{\\beta}_n = \\mathbf{B}_n (\\mathbf{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\mathbf{X}^{\\top} \\mathbf{y}^*)\\). Example: Determinants of hospitalization We use the dataset named 2HealthMed.csv, which is located in the DataApp folder of our GitHub repository (https://github.com/besmarter/BSTApp), and was used by Ramírez Hassan, Cardona Jiménez, and Cadavid Montoya (2013). The dependent variable is a binary indicator, taking the value 1 if an individual was hospitalized in 2007, and 0 otherwise. The specification of the model is \\[ \\text{Hosp}_i = \\boldsymbol{\\beta}_1 + \\boldsymbol{\\beta}_2 \\text{SHI}_i + \\boldsymbol{\\beta}_3 \\text{Female}_i + \\boldsymbol{\\beta}_4 \\text{Age}_i + \\boldsymbol{\\beta}_5 \\text{Age}_i^2 + \\boldsymbol{\\beta}_6 \\text{Est2}_i + \\boldsymbol{\\beta}_7 \\text{Est3}_i + \\boldsymbol{\\beta}_8 \\text{Fair}_i + \\boldsymbol{\\beta}_9 \\text{Good}_i + \\boldsymbol{\\beta}_{10} \\text{Excellent}_i, \\] where SHI is a binary variable equal to 1 if the individual is enrolled in a subsidized health care program and 0 otherwise, Female is an indicator of gender, Age is in years, Est2 and Est3 are indicators of socioeconomic status, with Est1 being the reference category (the lowest status), and HealthStatus is a self-perception of health status, where bad is the reference category. We set \\(\\boldsymbol{\\beta}_0 = {\\boldsymbol{0}}_{10}\\), \\({\\boldsymbol{B}}_0 = {\\boldsymbol{I}}_{10}\\), with iterations, burn-in, and thinning parameters equal to 10000, 1000, and 1, respectively. We can use the next Algorithm to run the probit model in our GUI. Our GUI relies on the rbprobitGibbs command from the bayesm package to perform inference in the probit model. The following R code shows how to run this example using the rbprobitGibbs command. We also asked you to implement a Gibbs sampler algorithm to perform inference in the probit model in the exercises. Algorithm: Probit model Select Univariate Models on the top panel Select Probit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors Select the tuning parameter for the Metropolis-Hastings algorithm. This step is not necessary as by default our GUI sets the tuning parameter at 1.1 Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons Our analysis finds evidence that gender and self-perceived health status significantly affect the probability of hospitalization. Women have a higher probability of being hospitalized than men, and individuals with a better perception of their health status have a lower probability of hospitalization. mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(mydata) ## The following objects are masked from Data: ## ## Age, Age2 str(mydata) ## &#39;data.frame&#39;: 12975 obs. of 22 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ MedVisPrev : int 0 0 0 0 0 0 0 0 0 0 ... ## $ MedVisPrevOr: int 1 1 1 1 1 1 1 1 1 1 ... ## $ Hosp : int 0 0 0 0 0 0 0 0 0 0 ... ## $ SHI : int 1 1 1 1 0 0 1 1 0 0 ... ## $ Female : int 0 1 1 1 0 1 0 1 0 1 ... ## $ Age : int 7 39 23 15 8 54 64 40 6 7 ... ## $ Age2 : int 49 1521 529 225 64 2916 4096 1600 36 49 ... ## $ FemaleAge : int 0 39 23 15 0 54 0 40 0 7 ... ## $ Est1 : int 1 0 0 0 0 0 0 0 0 0 ... ## $ Est2 : int 0 1 1 1 0 1 1 1 0 0 ... ## $ Est3 : int 0 0 0 0 1 0 0 0 1 1 ... ## $ Bad : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Fair : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Good : int 1 1 1 1 0 0 0 1 1 1 ... ## $ Excellent : int 0 0 0 0 1 1 0 0 0 0 ... ## $ NoEd : int 1 0 0 0 1 0 0 0 1 1 ... ## $ PriEd : int 0 0 0 0 0 1 1 1 0 0 ... ## $ HighEd : int 0 1 1 1 0 0 0 0 0 0 ... ## $ VocEd : int 0 0 0 0 0 0 0 0 0 0 ... ## $ UnivEd : int 0 0 0 0 0 0 0 0 0 0 ... ## $ PTL : num 0.43 0 0 0 0 0.06 0 0.38 0 1 ... K &lt;- 10 # Number of regressors b0 &lt;- rep(0, K) # Prio mean B0i &lt;- diag(K) # Prior precision (inverse of covariance) Prior &lt;- list(betabar = b0, A = B0i) # Prior list y &lt;- Hosp # Dependent variables X &lt;- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors Data &lt;- list(y = y, X = X) # Data list Mcmc &lt;- list(R = 10000, keep = 1, nprint = 0) # MCMC parameters RegProb &lt;- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package ## ## Starting Gibbs Sampler for Binary Probit Model ## with 12975 observations ## Table of y Values ## y ## 0 1 ## 12571 404 ## ## Prior Parms: ## betabar ## [1] 0 0 0 0 0 0 0 0 0 0 ## A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 0 0 0 0 0 0 0 0 0 ## [2,] 0 1 0 0 0 0 0 0 0 0 ## [3,] 0 0 1 0 0 0 0 0 0 0 ## [4,] 0 0 0 1 0 0 0 0 0 0 ## [5,] 0 0 0 0 1 0 0 0 0 0 ## [6,] 0 0 0 0 0 1 0 0 0 0 ## [7,] 0 0 0 0 0 0 1 0 0 0 ## [8,] 0 0 0 0 0 0 0 1 0 0 ## [9,] 0 0 0 0 0 0 0 0 1 0 ## [10,] 0 0 0 0 0 0 0 0 0 1 ## ## MCMC parms: ## R= 10000 keep= 1 nprint= 0 ## PostPar &lt;- coda::mcmc(RegProb$betadraw) # Posterior draws colnames(PostPar) &lt;- c(&quot;Cte&quot;, &quot;SHI&quot;, &quot;Female&quot;, &quot;Age&quot;, &quot;Age2&quot;, &quot;Est2&quot;, &quot;Est3&quot;, &quot;Fair&quot;, &quot;Good&quot;, &quot;Excellent&quot;) # Names summary(PostPar) # Posterior summary ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Cte -9.443e-01 1.347e-01 1.347e-03 3.413e-03 ## SHI -7.227e-03 5.934e-02 5.934e-04 2.229e-03 ## Female 1.260e-01 4.807e-02 4.807e-04 1.780e-03 ## Age 1.112e-04 3.551e-03 3.551e-05 1.164e-04 ## Age2 4.078e-05 4.222e-05 4.222e-07 1.249e-06 ## Est2 -8.658e-02 5.370e-02 5.370e-04 1.898e-03 ## Est3 -4.254e-02 8.112e-02 8.112e-04 2.774e-03 ## Fair -4.961e-01 1.119e-01 1.119e-03 2.013e-03 ## Good -1.204e+00 1.114e-01 1.114e-03 2.205e-03 ## Excellent -1.061e+00 1.316e-01 1.316e-03 3.136e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Cte -1.210e+00 -1.034e+00 -9.450e-01 -8.523e-01 -0.6839311 ## SHI -1.207e-01 -4.742e-02 -8.527e-03 3.225e-02 0.1125873 ## Female 3.273e-02 9.359e-02 1.261e-01 1.585e-01 0.2219169 ## Age -6.849e-03 -2.343e-03 6.224e-05 2.535e-03 0.0071123 ## Age2 -4.198e-05 1.215e-05 4.177e-05 6.976e-05 0.0001213 ## Est2 -1.911e-01 -1.235e-01 -8.634e-02 -4.971e-02 0.0168394 ## Est3 -2.018e-01 -9.709e-02 -4.153e-02 1.256e-02 0.1129273 ## Fair -7.141e-01 -5.704e-01 -4.971e-01 -4.218e-01 -0.2752697 ## Good -1.419e+00 -1.279e+00 -1.205e+00 -1.131e+00 -0.9832597 ## Excellent -1.323e+00 -1.147e+00 -1.062e+00 -9.730e-01 -0.8028882 References Albert, J. H., and S. Chib. 1993. “Bayesian Analysis of Binary and Polychotomous Response Data.” Journal of the American Statistical Association 88 (422): 669–79. Ramírez Hassan, A., J. Cardona Jiménez, and R. Cadavid Montoya. 2013. “The Impact of Subsidized Health Insurance on the Poor in Colombia: Evaluating the Case of Medellín.” Economia Aplicada 17 (4): 543–56. Tanner, M. A., and W. H. Wong. 1987. “The Calculation of Posterior Distributions by Data Augmentation.” Journal of the American Statistical Association 82 (398): 528–40. The variance in this model is set to 1 due to identification restrictions. Observe that \\(P(y_i = 1 \\mid \\mathbf{x}_i) = P(y_i^* &gt; 0 \\mid \\mathbf{x}_i) = P(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} + \\mu_i &gt; 0 \\mid \\mathbf{x}_i) = P(\\mu_i &gt; -\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} \\mid \\mathbf{x}_i) = P(c \\times \\mu_i &gt; -c \\times \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} \\mid \\mathbf{x}_i)\\) for all \\(c &gt; 0\\). Multiplying by a positive constant does not affect the probability of \\(y_i = 1\\).↩︎ "],["sec64.html", "6.4 The multinomial probit model", " 6.4 The multinomial probit model The multinomial probit model is used to model the choice of the \\(l\\)-th alternative over a set of \\(L\\) mutually exclusive options. We observe the following: \\[ y_{il} = \\begin{cases} 1, &amp; \\text{if } y_{il}^* \\geq \\max\\left\\{\\boldsymbol{y}_i^*\\right\\}, \\\\ 0, &amp; \\text{otherwise,} \\end{cases} \\] where \\(\\boldsymbol{y}_i^* = \\boldsymbol{X}_{i} \\boldsymbol{\\delta} + \\boldsymbol{\\mu}_i\\), with \\(\\boldsymbol{\\mu}_i \\stackrel{i.i.d.}{\\sim} N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\). The vector \\(\\boldsymbol{y}_i^*\\) is an unobserved latent vector of dimension \\(L\\). The matrix \\(\\boldsymbol{X}_i = \\left[(1 \\ \\boldsymbol{c}_i^{\\top}) \\otimes \\boldsymbol{I}_L \\ \\boldsymbol{A}_i\\right]\\) is an \\(L \\times j\\) matrix of regressors for each alternative, where \\(l = 1, 2, \\dots, L\\), and \\(j = L \\times (1 + \\text{dim}(\\boldsymbol{c}_i)) + a\\). Here, \\(\\boldsymbol{c}_i\\) is a vector of individual-specific characteristics, \\(\\boldsymbol{A}_i\\) is an \\(L \\times a\\) matrix of alternative-varying regressors, \\(a\\) is the number of alternative-varying regressors, and \\(\\boldsymbol{\\delta}\\) is a \\(j\\)-dimensional vector of parameters. We take into account simultaneously the alternative-varying regressors (alternative attributes) and alternative-invariant regressors (individual characteristics).2 The vector \\(\\boldsymbol{y}_i^*\\) can be stacked into a multiple regression model with correlated stochastic errors, i.e., \\(\\boldsymbol{y}^* = \\boldsymbol{X} \\boldsymbol{\\delta} + \\boldsymbol{\\mu}\\), where \\(\\boldsymbol{y}^* = \\left[\\boldsymbol{y}_1^{*\\top} \\ \\boldsymbol{y}_2^{*\\top} \\ \\dots \\ \\boldsymbol{y}_N^{*\\top}\\right]\\), \\(\\boldsymbol{X} = \\left[\\boldsymbol{X}_1^{\\top} \\ \\boldsymbol{X}_2^{\\top} \\ \\dots \\ \\boldsymbol{X}_N^{\\top}\\right]^{\\top}\\), and \\(\\boldsymbol{\\mu} = \\left[\\boldsymbol{\\mu}_1^{\\top} \\ \\boldsymbol{\\mu}_2^{\\top} \\ \\dots \\ \\boldsymbol{\\mu}_N^{\\top}\\right]^{\\top}\\). Following the practice of expressing \\(y_{il}^*\\) relative to \\(y_{iL}^*\\) by letting \\(\\boldsymbol{w}_i = \\left[w_{i1} \\ w_{i2} \\ \\dots \\ w_{iL-1}\\right]^{\\top}\\), where \\(w_{il} = y_{il}^* - y_{iL}^*\\), we can write \\(\\boldsymbol{w}_i = \\boldsymbol{R}_i \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}_i\\), with \\(\\boldsymbol{\\epsilon}_i \\sim N(\\boldsymbol{0}, \\boldsymbol{\\Omega})\\), where \\(\\boldsymbol{R}_i = \\left[(1 \\ \\boldsymbol{c}_i^{\\top}) \\otimes \\boldsymbol{I}_{L-1} \\ \\boldsymbol{\\Delta A}_i\\right]\\) is an \\((L-1) \\times K\\) matrix, with \\(\\Delta \\boldsymbol{A}_i = \\boldsymbol{A}_{li} - \\boldsymbol{A}_{Li}\\), for \\(l = 1, 2, \\dots, L-1\\). That is, the last row of \\(\\boldsymbol{A}_i\\) is subtracted from each row of \\(\\boldsymbol{A}_i\\), and \\(\\boldsymbol{\\beta}\\) is a \\(K\\)-dimensional vector, where \\(K = (L-1) \\times (1 + \\text{dim}(\\boldsymbol{c}_i)) + a\\). Observe that \\(\\boldsymbol{\\beta}\\) contains the same last \\(a\\) elements as \\(\\boldsymbol{\\delta}\\), that is, the alternative-specific attribute coefficients. However, the first \\((L-1) \\times (1 + \\text{dim}(\\boldsymbol{c}_i))\\) elements of \\(\\boldsymbol{\\beta}\\) are the differences \\(\\delta_{jl} - \\delta_{jL}\\), for \\(j = 1, \\dots, \\text{dim}(\\boldsymbol{c}_i)\\) and \\(l = 1, 2, \\dots, L-1\\). That is, these elements represent the difference between the coefficients of each qualitative response and the \\(L\\)-th alternative for the individuals’ characteristics. This makes it difficult to interpret the multinomial probit coefficients. Note that in multinomial models, for each alternative-specific attribute, it is only necessary to estimate one coefficient for all alternatives. However, for individuals’ characteristics (non-alternative-specific regressors), it is required to estimate \\(L-1\\) coefficients, since the coefficient for the base alternative is set equal to 0. The likelihood function in this model is given by \\[ p(\\boldsymbol{\\beta}, \\boldsymbol{\\Omega} \\mid \\boldsymbol{y}, \\boldsymbol{R}) = \\prod_{i=1}^N \\prod_{l=1}^L p_{il}^{y_{il}}, \\] where \\(p_{il} = p(y_{il}^* \\geq \\max(\\boldsymbol{y}_i^*))\\). We assume independent priors for the parameters: \\[ \\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0) \\quad \\text{and} \\quad \\boldsymbol{\\Omega}^{-1} \\sim W(\\alpha_0, \\boldsymbol{\\Sigma}_0), \\] where \\(W\\) denotes the Wishart density. We can employ Gibbs sampling in this model, as it is a standard Bayesian linear regression model when data augmentation is used for \\(\\boldsymbol{w}\\). The posterior conditional distributions are given by \\[\\begin{equation*} \\boldsymbol{\\beta}\\mid \\boldsymbol{\\Omega},\\boldsymbol{w}\\sim{N}(\\boldsymbol{\\beta}_n,\\boldsymbol{B}_n), \\end{equation*}\\] \\[\\begin{equation*} \\boldsymbol{\\Omega}^{-1}\\mid \\boldsymbol{\\beta},\\boldsymbol{w}\\sim{W}(\\alpha_n,\\boldsymbol{\\Sigma}_n), \\end{equation*}\\] where \\(\\boldsymbol{B}_n=(\\boldsymbol{B}_0^{-1}+\\boldsymbol{X}^{*\\top}\\boldsymbol{X}^*)^{-1}\\), \\(\\boldsymbol{\\beta}_n=\\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\boldsymbol{X}^{*\\top}\\boldsymbol{w}^*)\\), \\(\\boldsymbol{\\Omega}^{-1}=\\boldsymbol{C}^{\\top}\\boldsymbol{C}\\), \\(\\boldsymbol{X}_i^{*\\top}=\\boldsymbol{C}^{\\top}\\boldsymbol{R}_i\\), \\(\\boldsymbol{w}_i^*=\\boldsymbol{C}^{\\top}\\boldsymbol{w}_i\\), \\(\\boldsymbol{X}^*=\\begin{bmatrix}\\boldsymbol{X}_1^*\\\\ \\boldsymbol{X}_2^*\\\\ \\vdots\\\\ \\boldsymbol{X}_N^* \\end{bmatrix}\\), \\(\\alpha_n=\\alpha_0+N\\), \\(\\boldsymbol{\\Sigma}_n=(\\boldsymbol{\\Sigma}_0+\\sum_{i=1}^N (\\boldsymbol{w}_i-\\boldsymbol{R}_i\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{w}_i-\\boldsymbol{R}_i\\boldsymbol{\\beta}))^{-1}\\). We can collapse the multinomial vector \\(\\boldsymbol{y}_i\\) into the indicator variable \\(d_i=\\sum_{l=1}^{L-1}l\\times \\mathbb{1}({\\max(\\boldsymbol{w}_{l})=w_{il}})\\).3 Then the distribution of \\(\\boldsymbol{w}_i\\mid \\boldsymbol{\\beta},\\boldsymbol{\\Omega}^{-1},d_i\\) is an \\(L-1\\) dimensional Gaussian distribution truncated over the appropriate cone in \\(\\mathcal{R}^{L-1}\\). R. McCulloch and Rossi (1994) propose drawing from the univariate conditional distributions \\(w_{il}\\mid \\boldsymbol{w}_{i,-l},\\boldsymbol{\\beta},\\boldsymbol{\\Omega}^{-1},d_i\\sim TN_{I_{il}}(m_{il},\\tau_{ll}^2)\\), where \\[\\begin{equation*} I_{il}=\\begin{Bmatrix} w_{il}&gt;\\max(\\boldsymbol{w}_{i,-l},0), &amp; d_i=l\\\\ w_{il}&lt;\\max(\\boldsymbol{w}_{i,-l},0), &amp; d_i\\neq l\\\\ \\end{Bmatrix}, \\end{equation*}\\] and permuting the columns and rows of \\(\\boldsymbol{\\Omega}^{-1}\\) so that the \\(l\\)-th column and row is the last, \\[\\begin{equation*} \\boldsymbol{\\Omega}^{-1}=\\begin{bmatrix} \\boldsymbol{\\Omega}_{-l,-l} &amp; \\boldsymbol\\omega_{-l,l}\\\\ \\boldsymbol\\omega_{l,-1} &amp; \\omega_{l,l}\\\\ \\end{bmatrix}^{-1} =\\begin{bmatrix} \\boldsymbol{\\Omega}_{-l,-l}^{-1}+{\\tau}^{-2}_{ll}\\boldsymbol{f}_l\\boldsymbol{f}_l^{\\top} &amp; -\\boldsymbol{f}_l\\tau^{-2}_{ll}\\\\ -{\\tau}^{-2}_{ll}\\boldsymbol{f}_l^{\\top} &amp; {\\tau}^{-2}_{ll}\\\\ \\end{bmatrix} \\end{equation*}\\] where \\(\\boldsymbol{f}_l=\\boldsymbol{\\Omega}_{-l,-l}^{-1}\\boldsymbol{\\omega}_{-l,l}\\), \\(\\tau_{ll}^2= \\omega_{ll}-\\boldsymbol{\\omega}_{l,-l}\\boldsymbol{\\Omega}^{-1}_{-l,-1}\\boldsymbol{\\omega}_{-l,l}\\), \\(m_{il}=\\boldsymbol{r}_{il}^{\\top}\\boldsymbol{\\beta}+\\boldsymbol{f}_l^{\\top}(\\boldsymbol{w}_{i,-l}-\\boldsymbol{R}_{i,-l}\\boldsymbol{\\beta})\\), \\(\\boldsymbol{w}_{i,-l}\\) is an \\(L-2\\) dimensional vector of all components of \\(\\boldsymbol{w}_i\\) excluding \\(w_{il}\\), \\(\\boldsymbol{r}_{il}\\) is the \\(l\\)-th row of \\(\\boldsymbol{R}_i\\), \\(l=1,2,\\dots,L-1\\). The identified parameters are obtained by normalizing with respect to one of the diagonal elements \\(\\frac{1}{\\omega_{1,1}^{0.5}}\\boldsymbol{\\beta}\\) and \\(\\frac{1}{\\omega_{1,1}}\\boldsymbol{\\Omega}\\).4 Warning: This model is an example where decisions must be made about setting the model in an identified parameter space versus an unidentified parameter space. The mixing properties of the posterior draws can be better in the latter case (R. E. McCulloch, Polson, and Rossi 2000), which typically results in less computational burden. However, it is important to recover the identified space in a final stage. Additionally, defining priors in the unidentified space may have unintended consequences on the posterior distributions in the identified space (Nobile 2000). The multinomial probit model presented in this section is set in the unidentified space (R. McCulloch and Rossi 1994), while a version of the multinomial probit in the identified space is presented by R. E. McCulloch, Polson, and Rossi (2000). Example: Choice of fishing mode We used in this application the dataset 3Fishing.csv from Cameron and Trivedi (2005). The dependent variable is mutually exclusive alternatives regarding fishing modes (mode), where beach is equal to 1, pier is equal to 2, private boat is equal to 3, and chartered boat (baseline alternative) is equal to 4. In this model, we have \\[ \\mathbf{X}_i = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\text{Income}_i &amp; 0 &amp; 0 &amp; 0 &amp; \\text{Price}_{i,1} &amp; \\text{Catch rate}_{i,1}\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\text{Income}_i &amp; 0 &amp; 0 &amp; \\text{Price}_{i,2} &amp; \\text{Catch rate}_{i,2}\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\text{Income}_i &amp; 0 &amp; \\text{Price}_{i,3} &amp; \\text{Catch rate}_{i,3}\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\text{Income}_i &amp; \\text{Price}_{i,4} &amp; \\text{Catch rate}_{i,4}\\\\ \\end{bmatrix} \\] In this example, chartered boat is the base category, the number of choice categories is four, there are two alternative-specific regressors (price and catch rate), and one non-alternative-specific regressor (income). This setting involves the estimation of eight location parameters (\\(\\boldsymbol{\\beta}\\)): three intercepts, three for income, one for price, and one for catch rate. This is the order of the posterior chains in our GUI. Note that the location coefficients are set equal to 0 for the baseline category. For multinomial models, we strongly recommend using the last category as the baseline. We also get posterior estimates for a \\(3\\times 3\\) covariance matrix (four alternatives minus one), where the element (1,1) is equal to 1 due to identification restrictions, and elements 2 and 4 are the same, as well as 3 and 7, and 6 and 8, due to symmetry. Observe that this identification restriction implies NaN values in Geweke (1992) and Heidelberger and Welch (1983) tests for element (1,1) of the covariance matrix, and just eight dependence factors associated with the remaining elements of the covariance matrix. Once our GUI is displayed (see beginning of this chapter), we should follow the next Algorithm to run multinomial probit models in our GUI (see Chapter 5 for details), which in turn uses the command rmnpGibbs from the bayesm package. Algorithm: Multinomial Probit model Select Univariate Models on the top panel Select Multinomial Probit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Select the number of the Base Alternative Select the Number of choice categorical alternatives Select the Number of alternative specific variables Select the Number of Non-alternative specific variables Click the Build formula button to generate the formula in R syntax Set the hyperparameters: mean vector, covariance matrix, scale matrix, and degrees of freedom. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons We ran 100,000 MCMC iterations plus 10,000 as burn-in with a thinning parameter equal to 5, where all priors use default values for the hyperparameters in our GUI. We found that the 95% credible intervals of the coefficient associated with income for beach and private boat alternatives are equal to (8.58e-06, 8.88e-05) and (3.36e-05, 1.45e-04). This suggests that the probability of choosing these alternatives increases compared to a chartered boat when income increases. In addition, an increase in the price or a decrease in the catch rate for specific fishing alternatives imply lower probabilities of choosing them as the 95% credible intervals are (-9.91e-03, -3.83e-03) and (1.40e-01, 4.62e-01), respectively. However, the posterior chain diagnostics suggest there are convergence issues with the posterior draws (see Exercise 5). References Cameron, Colin, and Pravin Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge. Geweke, J. 1992. “Bayesian Statistics.” In. Clarendon Press, Oxford, UK. Heidelberger, P., and P. D. Welch. 1983. “Simulation Run Length Control in the Presence of an Initial Transient.” Operations Research 31 (6): 1109–44. McCulloch, Robert E, Nicholas G Polson, and Peter E Rossi. 2000. “A Bayesian Analysis of the Multinomial Probit Model with Fully Identified Parameters.” Journal of Econometrics 99 (1): 173–93. McCulloch, R., and P. Rossi. 1994. “An Exact Likelihood Analysis of the Multinomial Probit Model.” Journal of Econometrics 64: 207–40. Nobile, Agostino. 2000. “Comment: Bayesian Multinomial Probit Models with a Normalization Constraint.” Journal of Econometrics 99 (2): 335–45. Note that this model is not identified if \\(\\boldsymbol{\\Sigma}\\) is unrestricted. The likelihood function remains the same if a scalar random variable is added to each of the \\(L\\) latent regressions.↩︎ Observe that the identification issue in this model is due to scaling \\(w_{il}\\) by a positive constant does not change the value of \\(d_i\\).↩︎ Our GUI is based on the bayesm package that takes into account this identification restriction to display the outcomes of the posterior chains.↩︎ "],["sec65.html", "6.5 The multinomial logit model", " 6.5 The multinomial logit model The multinomial logit model is used to model mutually exclusive discrete outcomes or qualitative response variables. However, this model assumes the independence of irrelevant alternatives (IIA), meaning that the choice between two alternatives does not depend on a third alternative. We consider the multinomial mixed logit model (not to be confused with the random parameters logit model), which accounts for both alternative-varying regressors (conditional) and alternative-invariant regressors (multinomial) simultaneously.5 In this setting, there are \\(L\\) mutually exclusive alternatives, and the dependent variable \\(y_{il}\\) is equal to 1 if the \\(l\\)th alternative is chosen by individual \\(i\\), and 0 otherwise, where \\(l=\\left\\{1,2,\\dots,L\\right\\}\\). The likelihood function is \\[ p(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\boldsymbol{X}) = \\prod_{i=1}^{N} \\prod_{l=1}^{L} p_{il}^{y_{il}}, \\] where the probability that individual \\(i\\) chooses the alternative \\(l\\) is given by \\[ p_{il} := p(y_i = l \\mid \\boldsymbol{\\beta}, \\boldsymbol{X}) = \\frac{\\exp\\left\\{\\boldsymbol{x}_{il}^{\\top} \\boldsymbol{\\beta}_l\\right\\}}{\\sum_{j=1}^{L} \\exp\\left\\{\\boldsymbol{x}_{ij}^{\\top} \\boldsymbol{\\beta}_j\\right\\}}, \\] \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{X}\\) are the vector and matrix of the dependent variable and regressors, respectively, and \\(\\boldsymbol{\\beta}\\) is the vector containing all the coefficients. Remember that coefficients associated with alternative-invariant regressors are set to 0 for the baseline category, and the coefficients associated with the alternative-varying regressors are the same for all the categories. In addition, we assume \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) as the prior distribution. Thus, the posterior distribution is \\[ \\pi(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\boldsymbol{X}) \\propto p(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\boldsymbol{X}) \\times \\pi(\\boldsymbol{\\beta}). \\] As the multinomial logit model does not have a standard posterior distribution, Rossi, Allenby, and McCulloch (2012) propose a “tailored” independent Metropolis-Hastings algorithm where the proposal distribution is a multivariate Student’s \\(t\\) distribution with \\(v\\) degrees of freedom (tuning parameter), mean equal to the maximum likelihood estimator, and scale equal to the inverse of the Hessian matrix. Example: Simulation exercise Let’s conduct a simulation exercise to evaluate the performance of the Metropolis-Hastings algorithm for inference in the multinomial logit model. We consider a scenario with three alternatives, one alternative-invariant regressor (plus the intercept), and three alternative-varying regressors. The population parameters are given by \\(\\boldsymbol{\\beta}_1 = [1 \\ -2.5 \\ 0.5 \\ 0.8 \\ -3]^{\\top}\\), \\(\\boldsymbol{\\beta}_2 = [1 \\ -3.5 \\ 0.5 \\ 0.8 \\ -3]^{\\top}\\), and \\(\\boldsymbol{\\beta}_3 = [0 \\ 0 \\ 0.5 \\ 0.8 \\ -3]^{\\top}\\), where the first two elements of each vector correspond to the intercept and the alternative-invariant regressor, while the last three elements correspond to the alternative-varying regressors. The sample size is 1000, and all regressors are simulated from standard normal distributions. We can deploy our GUI using the command line at the beginning of this chapter. We should follow the next Algorithm to run multinomial logit models in our GUI (see Chapter 5 for details). Algorithm: Multinomial logit models Select Univariate Models on the top panel Select Multinomial Logit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Select the Base Alternative Select the Number of choice categorical alternatives Select the Number of alternative specific variables Select the Number of Non-alternative specific variables Click the Build formula button to generate the formula in R syntax Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors Select the tuning parameter for the Metropolis-Hastings algorithm, that is, the Degrees of freedom: Multivariate Student’s t distribution Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following code in R demonstrates how to implement the M-H algorithm from scratch. The first part simulates the dataset, the second part constructs the log-likelihood function, and the third part implements the M-H algorithm. We use vague priors centered at zero, with a covariance matrix of \\(1000\\mathbf{I}_7\\). We observe that the posterior estimates closely match the population parameters, and all 95% credible intervals contain the population parameters. remove(list = ls()) set.seed(12345) # Simulation of data N&lt;-1000 # Sample Size B&lt;-c(0.5,0.8,-3); B1&lt;-c(-2.5,-3.5,0); B2&lt;-c(1,1,0) # Alternative specific attributes of choice 1, for instance, price, quality and duration of choice 1 X1&lt;-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) # Alternative specific attributes of choice 2, for instance, price, quality and duration of choice 2 X2&lt;-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) # Alternative specific attributes of choice 3, for instance, price, quality and duration of choice 3 X3&lt;-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) X4&lt;-matrix(rnorm(N,1,1),N,1) V1&lt;-B2[1]+X1%*%B+B1[1]*X4; V2&lt;-B2[2]+X2%*%B+B1[2]*X4; V3&lt;-B2[3]+X3%*%B+B1[3]*X4 suma&lt;-exp(V1)+exp(V2)+exp(V3) p1&lt;-exp(V1)/suma; p2&lt;-exp(V2)/suma; p3&lt;-exp(V3)/suma p&lt;-cbind(p1,p2,p3) y&lt;- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE)) y1&lt;-y==1; y2&lt;-y==2; y3&lt;-y==3 # Log likelihood log.L&lt;- function(Beta){ V1&lt;-Beta[1]+Beta[3]*X4+X1%*%Beta[5:7] V2&lt;-Beta[2]+Beta[4]*X4+X2%*%Beta[5:7] V3&lt;- X3%*%Beta[5:7] suma&lt;-exp(V1)+exp(V2)+exp(V3) p11&lt;-exp(V1)/suma; p22&lt;-exp(V2)/suma; p33&lt;-exp(V3)/suma suma2&lt;-NULL for(i in 1:N){ suma1&lt;-y1[i]*log(p11[i])+y2[i]*log(p22[i])+y3[i]*log(p33[i]) suma2&lt;-c(suma2,suma1)} logL&lt;-sum(suma2) return(-logL) } # Parameters: Proposal k &lt;- 7 res.optim&lt;-optim(rep(0, k), log.L, method=&quot;BFGS&quot;, hessian=TRUE) MeanT &lt;- res.optim$par ScaleT &lt;- as.matrix(Matrix::forceSymmetric(solve(res.optim$hessian))) # Force this matrix to be symmetric # Hyperparameters: Priors B0 &lt;- 1000*diag(k); b0 &lt;- rep(0, k) MHfunction &lt;- function(iter, tuning){ Beta &lt;- rep(0, k); Acept &lt;- NULL BetasPost &lt;- matrix(NA, iter, k) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = iter, width = 300) for(s in 1:iter){ LogPostBeta &lt;- -log.L(Beta) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE) BetaC &lt;- c(LaplacesDemon::rmvt(n=1, mu = MeanT, S = ScaleT, df = tuning)) LogPostBetaC &lt;- -log.L(BetaC) + mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE) alpha &lt;- min(exp((LogPostBetaC-mvtnorm::dmvt(BetaC, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))-(LogPostBeta-mvtnorm::dmvt(Beta, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))) ,1) u &lt;- runif(1) if(u &lt;= alpha){ Acepti &lt;- 1; Beta &lt;- BetaC }else{ Acepti &lt;- 0; Beta &lt;- Beta } BetasPost[s, ] &lt;- Beta; Acept &lt;- c(Acept, Acepti) setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),&quot;% done&quot;)) } close(pb); AcepRate &lt;- mean(Acept) Results &lt;- list(AcepRate = AcepRate, BetasPost = BetasPost) return(Results) } # MCMC parameters mcmc &lt;- 10000; burnin &lt;- 1000; thin &lt;- 5; iter &lt;- mcmc + burnin; keep &lt;- seq(burnin, iter, thin); tuning &lt;- 6 # Degrees of freedom ResultsPost &lt;- MHfunction(iter = iter, tuning = tuning) summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ])) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.9711 0.20162 0.004508 0.004508 ## [2,] 0.9742 0.20934 0.004681 0.004681 ## [3,] -2.4350 0.18950 0.004237 0.004137 ## [4,] -3.4195 0.24656 0.005513 0.005513 ## [5,] 0.5253 0.07396 0.001654 0.001654 ## [6,] 0.8061 0.08007 0.001790 0.001790 ## [7,] -3.0853 0.17689 0.003955 0.003955 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.5862 0.8367 0.9650 1.1017 1.3683 ## var2 0.5679 0.8310 0.9681 1.1151 1.3761 ## var3 -2.8239 -2.5607 -2.4291 -2.3050 -2.0812 ## var4 -3.9176 -3.5806 -3.4074 -3.2496 -2.9423 ## var5 0.3840 0.4761 0.5250 0.5759 0.6647 ## var6 0.6555 0.7494 0.8064 0.8616 0.9604 ## var7 -3.4476 -3.1991 -3.0777 -2.9641 -2.7500 References Rossi, Peter E, Greg M Allenby, and Rob McCulloch. 2012. Bayesian Statistics and Marketing. John Wiley &amp; Sons. The multinomial mixed logit model can be implemented as a conditional logit model.↩︎ "],["sec66.html", "6.6 Ordered probit model", " 6.6 Ordered probit model The ordered probit model is used when there is a natural order in the categorical response variable. In this case, there is a latent variable \\(y_i^* = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} + \\mu_i\\), where \\(\\mathbf{x}_i\\) is a \\(K\\)-dimensional vector of regressors, \\(\\mu_i \\stackrel{i.i.d.}{\\sim} N(0,1)\\), such that \\(y_i = l\\) if and only if \\(\\alpha_{l-1} &lt; y_i^* \\leq \\alpha_l\\), for \\(l \\in \\{1, 2, \\dots, L\\}\\), where \\(\\alpha_0 = -\\infty\\), \\(\\alpha_1 = 0\\), and \\(\\alpha_L = \\infty\\).6 Then, the probability of observing \\(y_i = l\\) is given by: \\[ p(y_i = l) = \\Phi(\\alpha_l - \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\Phi(\\alpha_{l-1} - \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}), \\] and the likelihood function is: \\[ p(\\boldsymbol{\\beta}, \\boldsymbol{\\alpha} \\mid \\mathbf{y}, \\mathbf{X}) = \\prod_{i=1}^{N} p(y_i = l \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\alpha}, \\mathbf{X}). \\] The priors in this model are independent, i.e., \\(\\pi(\\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) = \\pi(\\boldsymbol{\\beta}) \\times \\pi(\\boldsymbol{\\gamma})\\), where \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) and \\(\\boldsymbol{\\gamma} \\sim N(\\boldsymbol{\\gamma}_0, \\boldsymbol{\\Gamma}_0)\\), and \\(\\boldsymbol{\\gamma} = \\left[ \\gamma_2 \\ \\gamma_3 \\ \\dots \\ \\gamma_{L-1} \\right]^{\\top}\\), such that: \\[ \\boldsymbol{\\alpha} = \\left[ \\exp\\left\\{\\gamma_2\\right\\} \\ \\sum_{l=2}^{3} \\exp\\left\\{\\gamma_l\\right\\} \\ \\dots \\ \\sum_{l=2}^{L-1} \\exp\\left\\{\\gamma_l\\right\\} \\right]^{\\top}. \\] This structure imposes the ordinal condition on the cut-offs. This model does not have a standard conditional posterior distribution for \\(\\boldsymbol{\\gamma}\\) (\\(\\boldsymbol{\\alpha}\\)), but it does have a standard conditional distribution for \\(\\boldsymbol{\\beta}\\) once data augmentation is used. We can then use a Metropolis-within-Gibbs sampling algorithm. In particular, we use Gibbs sampling to draw \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{y}^*\\), where: \\[ \\boldsymbol{\\beta} \\mid \\boldsymbol{y}^*, \\boldsymbol{\\alpha}, \\boldsymbol{X} \\sim N(\\boldsymbol{\\beta}_n, \\boldsymbol{B}_n), \\] with \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\boldsymbol{X}^{\\top}\\boldsymbol{X})^{-1}\\), \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{X}^{\\top}\\boldsymbol{y}^*)\\), and: \\[ y_i^* \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\alpha}, \\boldsymbol{y}, \\boldsymbol{X} \\sim TN_{(\\alpha_{y_i-1}, \\alpha_{y_i})}(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}, 1). \\] We use a random-walk Metropolis–Hastings algorithm for \\(\\boldsymbol{\\gamma}\\) with a proposal distribution that is Gaussian with mean equal to the current value and covariance matrix \\(s^2(\\boldsymbol{\\Gamma}_0^{-1} + \\hat{\\boldsymbol{\\Sigma}}_{\\gamma}^{-1})^{-1}\\), where \\(s &gt; 0\\) is a tuning parameter and \\(\\hat{\\boldsymbol{\\Sigma}}_{\\gamma}\\) is the sample covariance matrix associated with \\(\\gamma\\) from the maximum likelihood estimation. Example: Determinants of preventive health care visits We used the file named 2HealthMed.csv in this application. In particular, the dependent variable is MedVisPrevOr, which is an ordered variable equal to 1 if the individual did not visit a physician for preventive reasons, 2 if the individual visited once in that year, and so on, until it is equal to 6 for visiting five or more times. The latter category represents 1.6% of the sample. Observe that the dependent variable has six categories. In this example, the set of regressors is given by SHI, which is an indicator of being in the subsidized health care system (1 means being in the system), sex (Female), age (both linear and squared), socioeconomic conditions indicator (Est2 and Est3), with the lowest being the baseline category, self-perception of health status (Fair, Good, and Excellent), where Bad is the baseline, and education level (PriEd, HighEd, VocEd, UnivEd), with no education as the baseline category. We ran this application with 50,000 MCMC iterations plus 10,000 as burn-in, and a thinning parameter equal to 5. This setting means 10,000 effective posterior draws. We set \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}_{11}\\), \\(\\boldsymbol{B}_0 = 1000\\boldsymbol{I}_{11}\\), \\(\\boldsymbol{\\gamma}_0 = \\boldsymbol{0}_4\\), \\(\\boldsymbol{\\Gamma}_0 = \\boldsymbol{I}_4\\), and the tuning parameter is 1. We can run the ordered probit models in our GUI following the steps in the next Algorithm. Algorithm: Ordered probit models Select Univariate Models on the top panel Select Ordered Probit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. Remember that this formula must have -1 to omit the intercept in the specification Set the hyperparameters: mean vectors and covariance matrices. This step is not necessary as by default our GUI uses non-informative priors Select the number of ordered alternatives Set the hyperparameters: mean and covariance matrix of the cutoffs. This step is not necessary as by default our GUI uses non-informative prior Select the tuning parameter for the Metropolis-Hastings algorithm Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following R code shows how to perform inference in this model using the command rordprobitGibbs from the bayesm library, which is the command that our GUI uses. rm(list = ls()) set.seed(010101) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(Data) ## The following objects are masked from mydata: ## ## Age, Age2, Bad, Est1, Est2, Est3, Excellent, Fair, Female, ## FemaleAge, Good, HighEd, Hosp, id, MedVisPrev, MedVisPrevOr, NoEd, ## PriEd, PTL, SHI, UnivEd, VocEd ## The following objects are masked from Data (pos = 4): ## ## Age, Age2 y &lt;- MedVisPrevOr # MedVisPrevOr: Oredered variable for preventive visits to doctors in one year: 1 (none), 2 (once), ... 6 (five or more) X &lt;- cbind(SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent, PriEd, HighEd, VocEd, UnivEd) k &lt;- dim(X)[2] L &lt;- length(table(y)) # Hyperparameters b0 &lt;- rep(0, k); c0 &lt;- 1000; B0 &lt;- c0*diag(k) gamma0 &lt;- rep(0, L-2); Gamma0 &lt;- diag(L-2) # MCMC parameters mcmc &lt;- 60000+1; thin &lt;- 5; tuningPar &lt;- 1/(L-2)^0.5 DataApp &lt;- list(y = y, X = X, k = L) Prior &lt;- list(betabar = b0, A = solve(B0), dstarbar = gamma0, Ad = Gamma0) mcmcpar &lt;- list(R = mcmc, keep = 5, s = tuningPar, nprint = 0) PostBeta &lt;- bayesm::rordprobitGibbs(Data = DataApp, Prior = Prior, Mcmc = mcmcpar) ## ## Starting Gibbs Sampler for Ordered Probit Model ## with 12975 observations ## ## Table of y values ## y ## 1 2 3 4 5 6 ## 1935 2837 1241 2043 4711 208 ## ## Prior Parms: ## betabar ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 ## ## A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [2,] 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [3,] 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [4,] 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [5,] 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [6,] 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## [7,] 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 ## [8,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 0.000 ## [9,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.000 ## [10,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 ## [11,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.000 ## [12,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 ## [13,] 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 ## [,13] ## [1,] 0.000 ## [2,] 0.000 ## [3,] 0.000 ## [4,] 0.000 ## [5,] 0.000 ## [6,] 0.000 ## [7,] 0.000 ## [8,] 0.000 ## [9,] 0.000 ## [10,] 0.000 ## [11,] 0.000 ## [12,] 0.000 ## [13,] 0.001 ## ## dstarbar ## [1] 0 0 0 0 ## ## Ad ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## ## MCMC parms: ## R= 60001 keep= 5 nprint= 0 s= 0.5 ## BetasPost &lt;- coda::mcmc(PostBeta[[&quot;betadraw&quot;]]) colnames(BetasPost) &lt;- c(&quot;SHI&quot;, &quot;Female&quot;, &quot;Age&quot;, &quot;Age2&quot;, &quot;Est2&quot;, &quot;Est3&quot;, &quot;Fair&quot;, &quot;Good&quot;, &quot;Excellent&quot;, &quot;PriEd&quot;, &quot;HighEd&quot;, &quot;VocEd&quot;, &quot;UnivEd&quot;) summary(BetasPost) ## ## Iterations = 1:12000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 12000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## SHI 0.0654824 2.281e-02 2.082e-04 3.357e-04 ## Female -0.0374788 1.908e-02 1.742e-04 1.742e-04 ## Age 0.0190336 1.869e-03 1.706e-05 4.576e-05 ## Age2 -0.0002328 2.438e-05 2.225e-07 6.690e-07 ## Est2 0.0949445 2.226e-02 2.032e-04 4.659e-04 ## Est3 -0.1383965 3.411e-02 3.114e-04 3.459e-04 ## Fair 0.6451828 5.375e-02 4.907e-04 3.924e-03 ## Good 0.7343932 4.955e-02 4.523e-04 4.491e-03 ## Excellent 0.9826531 6.393e-02 5.836e-04 5.261e-03 ## PriEd 0.0309418 2.376e-02 2.169e-04 2.221e-04 ## HighEd -0.1805753 2.910e-02 2.656e-04 3.456e-04 ## VocEd 0.1395760 9.640e-02 8.800e-04 9.291e-04 ## UnivEd -0.2218120 1.189e-01 1.086e-03 1.086e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## SHI 0.0209045 0.0499525 0.0654041 0.0808572 0.1102130 ## Female -0.0746364 -0.0504288 -0.0377778 -0.0245643 0.0002350 ## Age 0.0155088 0.0178114 0.0190228 0.0202305 0.0226864 ## Age2 -0.0002804 -0.0002479 -0.0002328 -0.0002168 -0.0001864 ## Est2 0.0514963 0.0800442 0.0948246 0.1096800 0.1393326 ## Est3 -0.2055927 -0.1614479 -0.1381575 -0.1156348 -0.0717986 ## Fair 0.5579955 0.6129584 0.6414878 0.6726800 0.7439563 ## Good 0.6669005 0.7080863 0.7303217 0.7540621 0.8106430 ## Excellent 0.8891998 0.9477048 0.9783661 1.0102615 1.0846084 ## PriEd -0.0158405 0.0149388 0.0310167 0.0471892 0.0773202 ## HighEd -0.2378212 -0.2003574 -0.1802174 -0.1607329 -0.1243538 ## VocEd -0.0491123 0.0747424 0.1381100 0.2041479 0.3333107 ## UnivEd -0.4538119 -0.3023902 -0.2219313 -0.1414801 0.0086323 # Convergence diagnostics coda::geweke.diag(BetasPost) ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## SHI Female Age Age2 Est2 Est3 Fair Good ## 1.9824 0.1488 1.6564 -1.5988 0.9782 -1.9159 1.2891 1.2890 ## Excellent PriEd HighEd VocEd UnivEd ## 1.3675 2.2458 -1.3570 1.0199 -0.6709 coda::raftery.diag(BetasPost,q=0.5,r=0.05,s = 0.95) ## ## Quantile (q) = 0.5 ## Accuracy (r) = +/- 0.05 ## Probability (s) = 0.95 ## ## Burn-in Total Lower bound Dependence ## (M) (N) (Nmin) factor (I) ## SHI 2 393 385 1.020 ## Female 2 382 385 0.992 ## Age 2 401 385 1.040 ## Age2 2 399 385 1.040 ## Est2 2 409 385 1.060 ## Est3 1 385 385 1.000 ## Fair 6 1227 385 3.190 ## Good 12 1792 385 4.650 ## Excellent 6 1233 385 3.200 ## PriEd 2 392 385 1.020 ## HighEd 2 392 385 1.020 ## VocEd 2 390 385 1.010 ## UnivEd 2 393 385 1.020 coda::heidel.diag(BetasPost) ## ## Stationarity start p-value ## test iteration ## SHI passed 1201 0.8026 ## Female passed 1 0.5041 ## Age passed 1201 0.0812 ## Age2 passed 1201 0.0928 ## Est2 passed 1201 0.1970 ## Est3 passed 1201 0.4047 ## Fair failed NA 0.0360 ## Good failed NA 0.0156 ## Excellent failed NA 0.0403 ## PriEd passed 1 0.1479 ## HighEd passed 1201 0.0870 ## VocEd passed 1 0.5223 ## UnivEd passed 1 0.6614 ## ## Halfwidth Mean Halfwidth ## test ## SHI passed 0.065098 4.19e-04 ## Female passed -0.037479 3.41e-04 ## Age passed 0.018970 3.38e-05 ## Age2 passed -0.000232 4.40e-07 ## Est2 passed 0.094472 4.10e-04 ## Est3 passed -0.138067 6.42e-04 ## Fair &lt;NA&gt; NA NA ## Good &lt;NA&gt; NA NA ## Excellent &lt;NA&gt; NA NA ## PriEd passed 0.030942 4.35e-04 ## HighEd passed -0.180255 5.47e-04 ## VocEd passed 0.139576 1.82e-03 ## UnivEd passed -0.221812 2.13e-03 # Cut offs Cutoffs &lt;- PostBeta[[&quot;cutdraw&quot;]] summary(Cutoffs) ## Summary of Posterior Marginal Distributions ## Moments ## mean std dev num se rel eff sam size ## 1 0.00 0.000 -1.0e+04 -9999 -9999 ## 2 0.71 0.013 1.3e-03 108 99 ## 3 0.96 0.014 1.3e-03 96 112 ## 4 1.37 0.015 1.3e-03 85 127 ## 5 3.20 0.030 1.8e-03 38 277 ## ## Quantiles ## 2.5% 5% 50% 95% 97.5% ## 1 0.00 0.00 0.00 0.00 0.00 ## 2 0.68 0.69 0.71 0.73 0.73 ## 3 0.93 0.94 0.96 0.98 0.98 ## 4 1.33 1.34 1.37 1.39 1.39 ## 5 3.14 3.15 3.20 3.25 3.26 ## based on 10800 valid draws (burn-in=1200) coda::geweke.diag(Cutoffs) ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## var1 var2 var3 var4 var5 ## NaN 0.5305 1.2222 1.2512 1.6850 coda::heidel.diag(Cutoffs) ## ## Stationarity start p-value ## test iteration ## [,1] failed NA NA ## [,2] passed 1201 0.2060 ## [,3] passed 1201 0.1192 ## [,4] passed 1201 0.1004 ## [,5] passed 1201 0.0681 ## ## Halfwidth Mean Halfwidth ## test ## [,1] &lt;NA&gt; NA NA ## [,2] passed 0.71 0.00399 ## [,3] passed 0.96 0.00349 ## [,4] passed 1.37 0.00300 ## [,5] passed 3.20 0.00295 coda::raftery.diag(Cutoffs[,-1],q=0.5,r=0.05,s = 0.95) ## ## Quantile (q) = 0.5 ## Accuracy (r) = +/- 0.05 ## Probability (s) = 0.95 ## ## Burn-in Total Lower bound Dependence ## (M) (N) (Nmin) factor (I) ## 390 49320 385 128.0 ## 340 43214 385 112.0 ## 224 28504 385 74.0 ## 64 7432 385 19.3 The results suggest that older individuals (at a decreasing rate) in the subsidized health program, characterized by being in the second socioeconomic status, with an increasing self-perception of good health, and not having high school as their highest education degree, have a higher probability of visiting a physician for preventive health purposes. Convergence diagnostics look well, except for the self-health perception draws. We also obtained the posterior estimates of the cutoffs in the ordered probit model. These estimates are necessary to calculate the probability that an individual is in a specific category of physician visits. Due to identification restrictions, the first cutoff is set equal to 0. This is why we observe NaN values in Geweke (1992) and Heidelberger and Welch (1983) tests, and only four values in the Raftery and Lewis (1992) test, which correspond to the remaining free cutoffs. It seems that these cutoff estimates have some convergence issues when using the Raftery and Lewis (1992) test as a diagnostic tool. Furthermore, their dependence factors are also very high. References Geweke, J. 1992. “Bayesian Statistics.” In. Clarendon Press, Oxford, UK. Heidelberger, P., and P. D. Welch. 1983. “Simulation Run Length Control in the Presence of an Initial Transient.” Operations Research 31 (6): 1109–44. Raftery, A. E., and S. M. Lewis. 1992. “One Long Run with Diagnostics: Implementation Strategies for Markov Chain Monte Carlo.” Statistical Science 7: 493–97. Identification issues necessitate setting the variance in this model equal to 1 and \\(\\alpha_1 = 0\\). Observe that multiplying \\(y_i^*\\) by a positive constant or adding a constant to all of the cut-offs and subtracting the same constant from the intercept does not affect \\(y_i\\).↩︎ "],["negative-binomial-model.html", "6.7 Negative binomial model", " 6.7 Negative binomial model The dependent variable in the negative binomial model is a nonnegative integer or count. In contrast to the Poisson model, the negative binomial model accounts for over-dispersion. The Poisson model assumes equi-dispersion, meaning the mean and variance are equal. We assume that \\(y_i \\stackrel{i.i.d.} {\\sim} \\text{NB}(\\gamma, \\theta_i)\\), where the density function for individual \\(i\\) is \\[ \\frac{\\Gamma(y_i + \\gamma)}{\\Gamma(\\gamma) y_i!} (1 - \\theta_i)^{y_i} \\theta_i^{\\gamma}, \\] with the success probability \\(\\theta_i = \\frac{\\gamma}{\\lambda_i + \\gamma}\\), where \\(\\lambda_i = \\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\}\\) is the mean, and \\(\\gamma = \\exp\\left\\{\\alpha \\right\\}\\) is the target for the number of successful trials, or the dispersion parameter, and \\(\\mathbf{x}_i\\) is a \\(K\\)-dimensional vector of regressors. We assume independent priors for this model: \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\mathbf{B}_0)\\) and \\(\\alpha \\sim G(\\alpha_0, \\delta_0)\\). This model does not have standard conditional posterior distributions. Therefore, Rossi, Allenby, and McCulloch (2012) propose using a random-walk Metropolis–Hastings algorithm where the proposal distribution for \\(\\boldsymbol{\\beta}\\) is Gaussian, centered at the current stage, with covariance matrix \\(s_{\\boldsymbol{\\beta}}^2 \\hat{\\mathbf{\\Sigma}}_{\\boldsymbol{\\beta}}\\), where \\(s_{\\boldsymbol{\\beta}}\\) is a tuning parameter and \\(\\hat{\\mathbf{\\Sigma}}_{\\boldsymbol{\\beta}}\\) is the maximum likelihood covariance estimator. Additionally, the proposal for \\(\\alpha\\) is normal, centered at the current value, with variance \\(s_{\\alpha}^2 \\hat{\\sigma}_{\\alpha}^2\\), where \\(s_{\\alpha}\\) is a tuning parameter and \\(\\hat{\\sigma}_{\\alpha}^2\\) is the maximum likelihood variance estimator. Example: Simulation exercise Let’s do a simulation exercise to check the performance of the M-H algorithms in the negative binomial model. There are two regressors, \\(x_{i1} \\sim U(0,1)\\) and \\(x_{i2} \\sim N(0,1)\\), and the intercept. The dispersion parameter is \\(\\gamma = \\exp\\left\\{1.2\\right\\}\\), and \\(\\boldsymbol{\\beta} = \\left[1 \\ 1 \\ 1\\right]^{\\top}\\). The sample size is 1,000. We run this simulation using 10,000 MCMC iterations, a burn-in equal to 1,000, and a thinning parameter equal to 5. We set vague priors for the location parameters, particularly, \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}_{3}\\) and \\(\\boldsymbol{B}_0 = 1000\\boldsymbol{I}_{3}\\), and \\(\\alpha_0 = 0.5\\) and \\(\\delta_0 = 0.1\\), which are the default values in the rnegbinRw command from the bayesm package in R. In addition, the tuning parameters of the Metropolis–Hastings algorithms are \\(s_{\\boldsymbol{\\beta}} = 2.93/k^{1/2}\\) and \\(s_{\\alpha} = 2.93\\), which are also the default parameters in rnegbinRw, where \\(k\\) is the number of location parameters. We can run the negative binomial models in our GUI following the steps in the next Algorithm. Algorithm: Negative binomial models Select Univariate Models on the top panel Select Negative Binomial (Poisson) model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Set the hyperparameters: mean vector, covariance matrix, shape, and scale parameters. This step is not necessary as by default our GUI uses non-informative priors Select the tuning parameters for the Metropolis-Hastings algorithms Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following R code shows how to perform inference in the negative binomial model programming the M-H algorithms from scratch. We ask to estimate this example using the rnegbinRw command in Exercise 8. We observe from the results that all 95% credible intervals encompass the population parameters, and the posterior means are very close to the population parameters. rm(list = ls()) set.seed(010101) N &lt;- 2000 # Sample size x1 &lt;- runif(N); x2 &lt;- rnorm(N) X &lt;- cbind(1, x1, x2); k &lt;- dim(X)[2]; B &lt;- rep(1, k) alpha &lt;- 1.2; gamma &lt;- exp(alpha); lambda &lt;- exp(X%*%B) y &lt;- rnbinom(N, mu = lambda, size = gamma) # log likelihood logLik &lt;- function(par){ alpha &lt;- par[1]; beta &lt;- par[2:(k+1)] gamma &lt;- exp(alpha) lambda &lt;- exp(X%*%beta) logLikNB &lt;- sum(sapply(1:N, function(i){dnbinom(y[i], size = gamma, mu = lambda[i], log = TRUE)})) return(-logLikNB) } # Parameters: Proposal par0 &lt;- rep(0.5, k+1) res.optim &lt;- suppressWarnings(optim(par0, logLik, method=&quot;BFGS&quot;, hessian=TRUE)) res.optim$par ## [1] 1.3173049 1.0267103 0.9981069 0.9669848 res.optim$convergence ## [1] 0 Covar &lt;- solve(res.optim$hessian) CovarBetas &lt;- Covar[2:(k+1),2:(k+1)] VarAlpha &lt;- Covar[1:1] # Hyperparameters: Priors B0 &lt;- 1000*diag(k); b0 &lt;- rep(0, k) alpha0 &lt;- 0.5; delta0 &lt;- 0.1 # Metropolis-Hastings function MHfunction &lt;- function(iter, sbeta, salpha){ Beta &lt;- rep(0, k); Acept1 &lt;- NULL; Acept2 &lt;- NULL BetasPost &lt;- matrix(NA, iter, k); alpha &lt;- 1 alphaPost &lt;- rep(NA, iter); par &lt;- c(alpha, Beta) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = iter, width = 300) for(s in 1:iter){ LogPostBeta &lt;- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE) BetaC &lt;- c(MASS::mvrnorm(1, mu = Beta, Sigma = sbeta^2*CovarBetas)) parC &lt;- c(alpha, BetaC) LogPostBetaC &lt;- -logLik(parC) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE) alpha1 &lt;- min(exp((LogPostBetaC - mvtnorm::dmvnorm(BetaC, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))-(LogPostBeta - mvtnorm::dmvnorm(Beta, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))),1) u1 &lt;- runif(1) if(u1 &lt;= alpha1){Acept1i &lt;- 1; Beta &lt;- BetaC}else{ Acept1i &lt;- 0; Beta &lt;- Beta } par &lt;- c(alpha, Beta) LogPostBeta &lt;- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE) alphaC &lt;- rnorm(1, mean = alpha, sd = salpha*VarAlpha^0.5) parC &lt;- c(alphaC, Beta) LogPostBetaC &lt;- -logLik(parC) + dgamma(alphaC, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE) alpha2 &lt;- min(exp((LogPostBetaC - dnorm(alphaC, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))-(LogPostBeta - dnorm(alpha, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))),1) u2 &lt;- runif(1) if(u2 &lt;= alpha2){Acept2i &lt;- 1; alpha &lt;- alphaC}else{ Acept2i &lt;- 0; alpha &lt;- alpha } BetasPost[s, ] &lt;- Beta; alphaPost[s] &lt;- alpha Acept1 &lt;- c(Acept1, Acept1i); Acept2 &lt;- c(Acept2, Acept2i) setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),&quot;% done&quot;)) } close(pb) AcepRateBeta &lt;- mean(Acept1); AcepRateAlpha &lt;- mean(Acept2) Results &lt;- list(AcepRateBeta = AcepRateBeta, AcepRateAlpha = AcepRateAlpha, BetasPost = BetasPost, alphaPost = alphaPost) return(Results) } # MCMC parameters mcmc &lt;- 10000 burnin &lt;- 1000 thin &lt;- 5 iter &lt;- mcmc + burnin keep &lt;- seq(burnin, iter, thin) sbeta &lt;- 2.93/sqrt(k); salpha &lt;- 2.93 # Run M-H ResultsPost &lt;- MHfunction(iter = iter, sbeta = sbeta, salpha = salpha) ResultsPost$AcepRateBeta ## [1] 0.3892727 ResultsPost$AcepRateAlpha ## [1] 0.4021818 summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ])) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 1.0269 0.04612 0.0010314 0.0014371 ## [2,] 0.9973 0.07593 0.0016978 0.0023584 ## [3,] 0.9676 0.02343 0.0005239 0.0006485 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.9429 0.9946 1.0276 1.0567 1.118 ## var2 0.8529 0.9454 0.9967 1.0525 1.138 ## var3 0.9221 0.9525 0.9677 0.9831 1.014 summary(coda::mcmc(ResultsPost$alphaPost[keep[-1]])) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 1.280156 0.058052 0.001298 0.001491 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 1.169 1.242 1.279 1.317 1.396 References Rossi, Peter E, Greg M Allenby, and Rob McCulloch. 2012. Bayesian Statistics and Marketing. John Wiley &amp; Sons. "],["sec68.html", "6.8 Tobit model", " 6.8 Tobit model The dependent variable is partially observed in Tobit models due to sampling schemes, whereas the regressors are completely observed. In particular, \\[\\begin{equation} y_i = \\begin{Bmatrix} L, &amp; \\quad y_i^* &lt; L, \\\\ y_i^*, &amp; \\quad L \\leq y_i^* &lt; U, \\\\ U, &amp; \\quad y_i^* \\geq U, \\end{Bmatrix} \\end{equation}\\] where \\(y_i^* \\stackrel{i.i.d.}{\\sim} N(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2)\\), \\(\\mathbf{x}_i\\) is a \\(K\\)-dimensional vector of regressors.7 We use conjugate independent priors \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\mathbf{B}_0)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\), and data augmentation using \\(\\mathbf{y}^*_C\\) such that \\(y_{C_i}^* \\stackrel{i.n.d.}{\\thicksim} N(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2)\\), \\(y_{C_i} = \\left\\{ y_{C_i^L}^* \\cup y_{C_i^U}^* \\right\\}\\) are lower and upper censored data. This allows implementing the Gibbs sampling algorithm (Chib 1992). Then, \\[\\begin{align} \\pi(\\boldsymbol{\\beta}, \\sigma^2, \\mathbf{y^*} \\mid \\mathbf{y}, \\mathbf{X}) &amp;\\propto \\prod_{i=1}^N \\left[ \\mathbb{1}({y_i = L}) \\mathbb{1}({y_{C_i^L}^* &lt; L}) + \\mathbb{1}({L \\leq y_i &lt; U}) + \\mathbb{1}({y_i = U}) \\mathbb{1}({y_{C_i^U}^* \\geq U}) \\right] \\\\ &amp;\\times N(y_i^* \\mid \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2) \\times N(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\beta}_0, \\mathbf{B}_0) \\times IG(\\sigma^2 \\mid \\alpha_0/2, \\delta_0/2) \\end{align}\\] The posterior distributions are: \\[\\begin{equation} y_{C_i}^* \\mid \\boldsymbol{\\beta}, \\sigma^2, \\mathbf{y}, \\mathbf{X} \\sim \\begin{Bmatrix} TN_{(-\\infty,L)}(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}, \\sigma^2) \\ , \\ y_i = L \\\\ TN_{[U, \\infty)}(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}, \\sigma^2) \\ \\ , \\ y_i = U \\end{Bmatrix} \\end{equation}\\] \\[\\begin{equation} \\boldsymbol{\\beta} \\mid \\sigma^2, \\mathbf{y}, \\mathbf{X} \\sim N(\\boldsymbol{\\beta}_n, \\sigma^2 \\mathbf{B}_n) \\end{equation}\\] \\[\\begin{equation} \\sigma^2 \\mid \\boldsymbol{\\beta}, \\mathbf{y}, \\mathbf{X} \\sim IG(\\alpha_n/2, \\delta_n/2) \\end{equation}\\] where \\(\\mathbf{B}_n = (\\mathbf{B}_0^{-1} + \\sigma^{-2} \\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\), \\(\\boldsymbol{\\beta}_n = \\mathbf{B}_n(\\mathbf{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\sigma^{-2} \\mathbf{X}^{\\top} \\mathbf{y}^*)\\), \\(\\alpha_n = \\alpha_0 + N\\), and \\(\\delta_n = \\delta_0 + (\\mathbf{y}^* - \\mathbf{X} \\boldsymbol{\\beta})^{\\top}(\\mathbf{y}^* - \\mathbf{X} \\boldsymbol{\\beta})\\). Example: The market value of soccer players in Europe continues We continue with the example of the market value of soccer players from Section 6.1. We specify the same equation but assume that the sample is censored from below, such that we only have information for soccer players whose market value is higher than one million euros. The dependent variable is log(ValueCens), and the left censoring point is 13.82. The following Algorithm illustrates how to estimate Tobit models in our GUI. Our GUI utilizes the MCMCtobit command from the MCMCpack package. Algorithm: Tobit models Select Univariate Models on the top panel Select Tobit model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Set the left and right censoring points. To censor above only, specify -Inf in the left censoring box, and to censor below only, specify Inf in the right censoring box Set the hyperparameters: mean vector, covariance matrix, shape, and scale parameters. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons We run this application using the same hyperparameters that we set in the example of Section 6.1. All results seem similar to those in the example of linear models. In addition, the posterior chains seem to achieve good diagnostics. rm(list = ls()); set.seed(010101) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(Data) ## The following objects are masked from Data (pos = 3): ## ## Age, Age2 ## The following objects are masked from mydata: ## ## Age, Age2 ## The following objects are masked from Data (pos = 5): ## ## Age, Age2, Assists, Exp, Exp2, Goals, Goals2, NatTeam, Perf, Perf2, ## Player, Value, ValueCens y &lt;- log(ValueCens) X &lt;- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2) k &lt;- dim(X)[2] N &lt;- dim(X)[1] # Hyperparameters d0 &lt;- 0.001; a0 &lt;- 0.001 b0 &lt;- rep(0, k); c0 &lt;- 1000; B0 &lt;- c0*diag(k) B0i &lt;- solve(B0) # MCMC parameters mcmc &lt;- 50000 burnin &lt;- 10000 tot &lt;- mcmc + burnin thin &lt;- 1 # Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix posterior &lt;- MCMCpack::MCMCtobit(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf) summary(coda::mcmc(posterior)) ## ## Iterations = 1:50000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 50000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 1.049702 2.639780 1.181e-02 1.676e-02 ## XPerf 0.033979 0.004512 2.018e-05 2.288e-05 ## XAge 1.024758 0.213319 9.540e-04 1.337e-03 ## XAge2 -0.021861 0.004002 1.790e-05 2.546e-05 ## XNatTeam 0.847758 0.126113 5.640e-04 6.463e-04 ## XGoals 0.010091 0.001649 7.376e-06 7.688e-06 ## XExp 0.174196 0.070237 3.141e-04 3.808e-04 ## XExp2 -0.005652 0.002977 1.331e-05 1.555e-05 ## sigma2 0.982768 0.095944 4.291e-04 6.698e-04 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X -4.192313 -0.713326 1.071600 2.841806 6.1771087 ## XPerf 0.025131 0.030948 0.033967 0.036976 0.0428691 ## XAge 0.609991 0.880679 1.023709 1.167147 1.4480407 ## XAge2 -0.029803 -0.024542 -0.021820 -0.019162 -0.0141026 ## XNatTeam 0.603262 0.762408 0.847351 0.932705 1.0950039 ## XGoals 0.006877 0.008975 0.010094 0.011193 0.0133502 ## XExp 0.037752 0.126722 0.173706 0.221290 0.3127266 ## XExp2 -0.011525 -0.007632 -0.005642 -0.003657 0.0001633 ## sigma2 0.811752 0.915417 0.976781 1.042729 1.1887701 # Gibbs sampling functions XtX &lt;- t(X)%*%X PostBeta &lt;- function(Yl, sig2){ Bn &lt;- solve(B0i + sig2^(-1)*XtX) bn &lt;- Bn%*%(B0i%*%b0 + sig2^(-1)*t(X)%*%Yl) Beta &lt;- MASS::mvrnorm(1, bn, Bn) return(Beta) } PostYl &lt;- function(Beta, sig2, L, U, i){ Ylmean &lt;- X[i,]%*%Beta if(y[i] == L){ Yli &lt;- truncnorm::rtruncnorm(1, a = -Inf, b = L, mean = Ylmean, sd = sig2^0.5) }else{ if(y[i] == U){ Yli &lt;- truncnorm::rtruncnorm(1, a = U, b = Inf, mean = Ylmean, sd = sig2^0.5) }else{ Yli &lt;- y[i] } } return(Yli) } PostSig2 &lt;- function(Beta, Yl){ an &lt;- a0 + length(y) dn &lt;- d0 + t(Yl - X%*%Beta)%*%(Yl - X%*%Beta) sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBetas &lt;- matrix(0, mcmc+burnin, k); Beta &lt;- rep(0, k) PostSigma2 &lt;- rep(0, mcmc+burnin); sig2 &lt;- 1 L &lt;- log(1000000); U &lt;- Inf # create progress bar in case that you want to see iterations progress pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ Yl &lt;- sapply(1:N, function(i){PostYl(Beta = Beta, sig2 = sig2, L = L, U = U, i)}) Beta &lt;- PostBeta(Yl = Yl, sig2 = sig2) sig2 &lt;- PostSig2(Beta = Beta, Yl = Yl) PostBetas[s,] &lt;- Beta; PostSigma2[s] &lt;- sig2 setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), &quot;% done&quot;)) } close(pb) ## NULL keep &lt;- seq((burnin+1), tot, thin) PosteriorBetas &lt;- PostBetas[keep,] colnames(PosteriorBetas) &lt;- c(&quot;Intercept&quot;, &quot;Perf&quot;, &quot;Age&quot;, &quot;Age2&quot;, &quot;NatTeam&quot;, &quot;Goals&quot;, &quot;Exp&quot;, &quot;Exp2&quot;) summary(coda::mcmc(PosteriorBetas)) ## ## Iterations = 1:50000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 50000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Intercept 1.013599 2.639018 1.180e-02 1.680e-02 ## Perf 0.033985 0.004520 2.021e-05 2.274e-05 ## Age 1.027092 0.213316 9.540e-04 1.341e-03 ## Age2 -0.021910 0.003999 1.789e-05 2.555e-05 ## NatTeam 0.849940 0.125638 5.619e-04 6.327e-04 ## Goals 0.010102 0.001652 7.389e-06 7.747e-06 ## Exp 0.175133 0.070469 3.151e-04 3.809e-04 ## Exp2 -0.005684 0.002988 1.336e-05 1.561e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Intercept -4.227386 -0.744950 1.046922 2.811236 6.1216253 ## Perf 0.025161 0.030902 0.034004 0.037041 0.0428490 ## Age 0.614097 0.881087 1.024736 1.169974 1.4502181 ## Age2 -0.029864 -0.024574 -0.021866 -0.019179 -0.0141818 ## NatTeam 0.604448 0.765333 0.849641 0.934294 1.0972474 ## Goals 0.006839 0.008996 0.010106 0.011223 0.0133239 ## Exp 0.036747 0.128063 0.174950 0.222325 0.3149495 ## Exp2 -0.011549 -0.007680 -0.005673 -0.003692 0.0002033 summary(coda::mcmc(PostSigma2[keep])) ## ## Iterations = 1:50000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 50000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 0.9855375 0.0957929 0.0004284 0.0006728 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 0.8154 0.9183 0.9793 1.0468 1.1902 References Chib, S. 1992. “Bayes Inference in the Tobit Censored Regression Model.” Journal of Econometrics 51: 79–99. We can set \\(L\\) or \\(U\\) equal to \\(-\\infty\\) or \\(\\infty\\) to model data censored on just one side.↩︎ "],["sec69.html", "6.9 Quantile regression", " 6.9 Quantile regression In quantile regression, the location parameters vary according to the quantile of the dependent variable. Let \\(q_{\\tau}(\\boldsymbol{x}_i) = \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}_{\\tau}\\) denote the \\(\\tau\\)-th quantile regression function of \\(y_i\\) given \\(\\boldsymbol{x}_i\\), where \\(\\boldsymbol{x}_i\\) is a \\(K\\)-dimensional vector of regressors, and \\(0 &lt; \\tau &lt; 1\\). Specifically, we have the model \\(y_i = \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}_{\\tau} + \\mu_i\\), with the condition \\(\\int_{-\\infty}^{0} f_{\\tau}(\\mu_i) \\, d\\mu_i = \\tau\\), meaning that the \\(\\tau\\)-th quantile of \\(\\mu_i\\) is 0. In particular, Kozumi and Kobayashi (2011) propose the asymmetric Laplace distribution for \\(f_{\\tau}(\\mu_i)\\), given by \\[ f_{\\tau}(\\mu_i) = \\tau(1 - \\tau) \\exp\\left\\{- \\mu_i(\\tau - \\mathbb{1}({\\mu_i &lt; 0})) \\right\\}, \\] where \\(\\mu_i(\\tau - \\mathbb{1}({\\mu_i &lt; 0}))\\) is the check (loss) function. These authors also propose a location-scale mixture of normals representation, given by \\[ \\mu_i = \\theta e_i + \\psi \\sqrt{e_i} z_i, \\] where \\(\\theta = \\frac{1 - 2\\tau}{\\tau(1 - \\tau)}\\), \\(\\psi^2 = \\frac{2}{\\tau(1 - \\tau)}\\), \\(e_i \\sim E(1)\\), and \\(z_i \\sim N(0,1)\\), with \\(e_i \\perp z_i\\).8 As a result of this representation and the fact that the sample is i.i.d., the likelihood function is \\[ p(\\boldsymbol{y} \\mid \\boldsymbol{\\beta}_{\\tau}, \\boldsymbol{e}, \\boldsymbol{X}) \\propto \\left( \\prod_{i=1}^{N} e_i^{-1/2} \\right) \\exp\\left\\{- \\sum_{i=1}^{N} \\frac{(y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}_{\\tau} - \\theta e_i)^2}{2 \\psi^2 e_i} \\right\\}. \\] Assuming a normal prior for \\(\\boldsymbol{\\beta}_{\\tau}\\), i.e., \\(\\boldsymbol{\\beta}_{\\tau} \\sim N(\\boldsymbol{\\beta}_{\\tau 0}, \\boldsymbol{B}_{\\tau 0})\\), and using data augmentation for \\(\\boldsymbol{e}\\), we can implement a Gibbs sampling algorithm for this model. The posterior distributions are as follows: \\[\\begin{equation*} \\boldsymbol{\\beta}_{\\tau} \\mid \\boldsymbol{e}, \\boldsymbol{y}, \\boldsymbol{X} \\sim N(\\boldsymbol{\\beta}_{n\\tau}, \\boldsymbol{B}_{n\\tau}), \\end{equation*}\\] \\[\\begin{equation*} e_i \\mid \\boldsymbol{\\beta}_{\\tau}, \\boldsymbol{y}, \\boldsymbol{X} \\sim \\text{GIG}\\left( \\frac{1}{2}, \\alpha_{ni}, \\delta_{ni} \\right), \\end{equation*}\\] where \\[\\begin{align*} \\boldsymbol{B}_{n\\tau} &amp;= \\left( \\boldsymbol{B}_{\\tau 0}^{-1} + \\sum_{i=1}^{N} \\frac{\\boldsymbol{x}_i \\boldsymbol{x}_i^{\\top}}{\\psi^2 e_i} \\right)^{-1}, \\\\ \\boldsymbol{\\beta}_{n\\tau} &amp;= \\boldsymbol{B}_{n\\tau} \\left( \\boldsymbol{B}_{\\tau 0}^{-1} \\boldsymbol{\\beta}_{\\tau 0} + \\sum_{i=1}^{N} \\frac{\\boldsymbol{x}_i (y_i - \\theta e_i)}{\\psi^2 e_i} \\right), \\\\ \\alpha_{ni} &amp;= \\frac{(y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}_{\\tau})^2}{\\psi^2}, \\quad \\delta_{ni} = 2 + \\frac{\\theta^2}{\\psi^2}. \\end{align*}\\] Example: The market value of soccer players in Europe continues We continue the example of the market value of soccer players from Section 6.1. Now, we want to examine whether the marginal effect of having been on the national team varies with the quantile of the market value of top soccer players in Europe. Thus, we use the same regressors as in the previous example, but analyze the effects at the 0.5-th and 0.9-th quantiles of NatTeam. The following Algorithm shows how to estimate quantile regression models in our GUI. Our GUI uses the command MCMCquantreg from the package MCMCpack. The following code demonstrates how to perform this analysis using the package. The results show that at the median market value (0.5-th quantile), the 95% credible interval for the coefficient associated with national team is (0.34, 1.02), with a posterior mean of 0.69. At the 0.9-th quantile, these values are (0.44, 1.59) and 1.03, respectively. It appears that being on the national team increases the market value of more expensive players more significantly on average, although there is some overlap in the credible intervals. Algorithm: Quantile regression Select Univariate Models on the top panel Select Quantile model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Set the quantile to be analyzed, by default it is 0.5 Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons rm(list = ls()); set.seed(010101) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(Data) ## The following objects are masked from Data (pos = 3): ## ## Age, Age2, Assists, Exp, Exp2, Goals, Goals2, NatTeam, Perf, Perf2, ## Player, Value, ValueCens ## The following objects are masked from Data (pos = 4): ## ## Age, Age2 ## The following objects are masked from mydata: ## ## Age, Age2 ## The following objects are masked from Data (pos = 6): ## ## Age, Age2, Assists, Exp, Exp2, Goals, Goals2, NatTeam, Perf, Perf2, ## Player, Value, ValueCens y &lt;- log(ValueCens) X &lt;- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2) k &lt;- dim(X)[2]; N &lt;- dim(X)[1] # Hyperparameters b0 &lt;- rep(0, k); c0 &lt;- 1000; B0 &lt;- c0*diag(k); B0i &lt;- solve(B0) # MCMC parameters mcmc &lt;- 50000; burnin &lt;- 10000 tot &lt;- mcmc + burnin; thin &lt;- 1 # Quantile q &lt;- 0.5 posterior05 &lt;- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf) summary(coda::mcmc(posterior05)) ## ## Iterations = 1:50000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 50000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 7.380881 2.922415 1.307e-02 2.305e-02 ## XPerf 0.029380 0.005972 2.671e-05 5.269e-05 ## XAge 0.550080 0.242058 1.083e-03 1.911e-03 ## XAge2 -0.012009 0.004607 2.061e-05 3.665e-05 ## XNatTeam 0.681238 0.170806 7.639e-04 1.564e-03 ## XGoals 0.010642 0.002415 1.080e-05 1.958e-05 ## XExp 0.091802 0.085777 3.836e-04 6.821e-04 ## XExp2 -0.002962 0.003874 1.733e-05 2.930e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X 1.721153 5.421451 7.355579 9.3156087 13.189612 ## XPerf 0.017526 0.025434 0.029410 0.0333849 0.040987 ## XAge 0.070621 0.389228 0.552940 0.7121539 1.020032 ## XAge2 -0.020951 -0.015103 -0.012073 -0.0089386 -0.002859 ## XNatTeam 0.341558 0.568023 0.682516 0.7949333 1.017583 ## XGoals 0.005849 0.009081 0.010586 0.0122120 0.015472 ## XExp -0.068241 0.032809 0.088512 0.1472835 0.269448 ## XExp2 -0.010951 -0.005446 -0.002856 -0.0003978 0.004471 q &lt;- 0.9 posterior09 &lt;- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf) summary(coda::mcmc(posterior09)) ## ## Iterations = 1:50000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 50000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 8.871556 5.825738 2.605e-02 6.605e-02 ## XPerf 0.019617 0.010110 4.521e-05 1.127e-04 ## XAge 0.531550 0.473922 2.119e-03 5.320e-03 ## XAge2 -0.012324 0.008689 3.886e-05 9.523e-05 ## XNatTeam 1.039941 0.295468 1.321e-03 3.453e-03 ## XGoals 0.008703 0.004295 1.921e-05 4.799e-05 ## XExp 0.136201 0.176328 7.886e-04 2.064e-03 ## XExp2 -0.002835 0.007613 3.405e-05 8.450e-05 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X -2.4546594 4.890189 8.838894 12.807759 20.296222 ## XPerf -0.0001067 0.012859 0.019580 0.026298 0.039655 ## XAge -0.3889773 0.210175 0.531298 0.850672 1.462811 ## XAge2 -0.0295151 -0.018080 -0.012278 -0.006447 0.004438 ## XNatTeam 0.4437112 0.845176 1.045862 1.241973 1.600731 ## XGoals 0.0018928 0.005572 0.008122 0.011280 0.018537 ## XExp -0.2241036 0.019995 0.142557 0.258915 0.461877 ## XExp2 -0.0163570 -0.008121 -0.003296 0.001949 0.013474 References Kozumi, Hideo, and Genya Kobayashi. 2011. “Gibbs Sampling Methods for Bayesian Quantile Regression.” Journal of Statistical Computation and Simulation 81 (11): 1565--1578. \\(E\\) denotes an exponential density. ↩︎ "],["sec610.html", "6.10 Bayesian bootstrap regression", " 6.10 Bayesian bootstrap regression We implement the Bayesian bootstrap (Rubin 1981) for linear regression models. In particular, the Bayesian bootstrap simulates the posterior distributions by assuming that the sample cumulative distribution function (CDF) is the population CDF (this assumption is also implicit in the frequentist bootstrap (Efron 1979)). Given \\(y_i \\stackrel{i.i.d.}{\\sim} \\mathcal{F}\\), where \\(\\mathcal{F}\\) does not specify a particular parametric family of distributions, but instead sets \\(\\mathbb{E}(y_i \\mid \\boldsymbol{x}_i) = \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}\\), with \\(\\boldsymbol{x}_i\\) being a \\(K\\)-dimensional vector of regressors and \\(\\boldsymbol{\\beta}\\) a \\(K\\)-dimensional vector of parameters, the Bayesian bootstrap generates posterior probabilities for each \\(y_i\\), where the values of \\(\\boldsymbol{y}\\) that are not observed have zero posterior probability. The algorithm to implement the Bayesian bootstrap is the following: Algorithm: Bayesian bootstrap Draw g ∼ Dir(α1, α2, ..., αN) such that αi=1 for all i g=(g1, g2, ..., gN) is the vector of probabilities to attach to (y1,x1), (y2,x2), ..., (yN,xN) for each Bayesian bootstrap replication. For \\( s = 1, \\dots, S \\): - Sample (yi,xi) N times with replacement and probabilities gi, i=1,2, ...,N. - Estimate β(s) using weighted least squares in the model E(y|X)=Xβ, y being a S1 dimensional vector, and X a S1 X K matrix from the previous stage. End for The distribution of β(s) is the Bayesian distribution of β. Example: Simulation exercise Let’s perform a simulation exercise to evaluate the performance of the previous Algorithm for inference using the Bayesian bootstrap. The data-generating process is defined by two regressors, each distributed as standard normal. The location vector is \\(\\boldsymbol{\\beta} = \\left[1 \\ 1 \\ 1\\right]^{\\top}\\), with a variance of \\(\\sigma^2 = 1\\), and the sample size is 1,000. The following Algorithm illustrates how to use our GUI to run the Bayesian bootstrap. Our GUI is based on the bayesboot command from the bayesboot package in R. Exercise 11 asks about using this package to perform inference in this simulation and compares the results with those obtained using our GUI with \\(S = 10000\\). Algorithm: Bayesian Bootstrap in Linear Regression Select Univariate Models on the top panel Select Bootstrap model using the left radio button Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend. You should see a preview of the dataset Select number of bootstrap replications using the Range sliders Select dependent and independent variables using the Formula builder table Click the Build formula button to generate the formula in R syntax. You can modify the formula in the Main equation box using valid arguments of the formula command structure in R Click the Go! button Analyze results Download posterior chains and diagnostic plots using the Download Posterior Chains and Download Posterior Graphs buttons The following R code shows how to program the Bayesian bootstrap from scratch. We observe from the results that all 95% credible intervals encompass the population parameters, and the posterior means are close to the population parameters. rm(list = ls()); set.seed(010101) #--- Data N &lt;- 1000; x1 &lt;- runif(N); x2 &lt;- rnorm(N) X &lt;- cbind(1, x1, x2); B &lt;- c(1, 1, 1); sig2 &lt;- 1 y &lt;- as.numeric(X %*% B + rnorm(N, 0, sqrt(sig2))) data &lt;- data.frame(y, x1, x2) #--- Bayesian bootstrap (Rubin) for OLS coefficients BB &lt;- function(S, df, alpha = 1) { N &lt;- nrow(df) Betas &lt;- matrix(NA_real_, nrow = S, ncol = 3) colnames(Betas) &lt;- c(&quot;(Intercept)&quot;, &quot;x1&quot;, &quot;x2&quot;) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = S, width = 300) for (s in 1:S) { # One Dirichlet weight vector over the N observations w &lt;- as.numeric(LaplacesDemon::rdirichlet(1, rep(alpha, N))) # Weighted least squares = Bayesian bootstrap draw of the OLS functional fit &lt;- lm(y ~ x1 + x2, data = df, weights = w) Betas[s, ] &lt;- coef(fit) setWinProgressBar(pb, s, title=paste( round(s/S*100, 0), &quot;% done&quot;)) } close(pb) return(Betas) } S &lt;- 10000 BBs &lt;- BB(S = S, df = data, alpha = 1) summary(coda::mcmc(BBs)) ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## (Intercept) 0.9169 0.06359 0.0006359 0.0006359 ## x1 1.1738 0.10698 0.0010698 0.0010698 ## x2 1.0136 0.03337 0.0003337 0.0003337 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## (Intercept) 0.7934 0.8732 0.917 0.9595 1.042 ## x1 0.9647 1.1019 1.173 1.2462 1.381 ## x2 0.9477 0.9911 1.014 1.0362 1.079 References Efron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7: 1–26. Rubin, Donnald B. 1981. “The Bayesian Bootstrap.” The Annals of Statistics 9 (1): 130–34. "],["sec611.html", "6.11 Summary", " 6.11 Summary In this chapter, we present the core univariate regression models and demonstrate how to perform Bayesian inference using Markov Chain Monte Carlo (MCMC) methods. Specifically, we cover a range of algorithms: Gibbs sampling, Metropolis-Hastings, nested Metropolis-Hastings, and Metropolis-Hastings-within-Gibbs. These algorithms form the foundation for performing Bayesian inference in more complex settings using cross-sectional datasets. "],["sec612.html", "6.12 Exercises", " 6.12 Exercises Get the posterior conditional distributions of the Gaussian linear model assuming independent priors \\(\\pi(\\boldsymbol{\\beta}, \\sigma^2) = \\pi(\\boldsymbol{\\beta}) \\times \\pi(\\sigma^2)\\), where \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\). Given the model \\(y_i \\sim N(\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}, \\sigma^2/\\tau_i)\\) (Gaussian linear model with heteroskedasticity) with independent priors, \\(\\pi(\\boldsymbol{\\beta}, \\sigma^2, \\boldsymbol{\\tau}) = \\pi(\\boldsymbol{\\beta}) \\times \\pi(\\sigma^2) \\times \\prod_{i=1}^N \\pi(\\tau_i)\\), where \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_0, \\boldsymbol{B}_0)\\), \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\), and \\(\\tau_i \\sim G(v/2, v/2)\\). Show that \\[ \\boldsymbol{\\beta} \\mid \\sigma^2, \\boldsymbol{\\tau}, \\boldsymbol{y}, \\boldsymbol{X} \\sim N(\\boldsymbol{\\beta}_n, \\boldsymbol{B}_n), \\quad \\sigma^2 \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\tau}, \\boldsymbol{y}, \\boldsymbol{X} \\sim IG(\\alpha_n/2, \\delta_n/2), \\] and \\[ \\tau_i \\mid \\boldsymbol{\\beta}, \\sigma^2, \\boldsymbol{y}, \\boldsymbol{X} \\sim G(v_{1n}/2, v_{2in}/2), \\] where \\(\\boldsymbol{\\tau} = [\\tau_1 \\dots \\tau_n]^{\\top}\\), \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\sigma^{-2} \\boldsymbol{X}^{\\top} \\Psi \\boldsymbol{X})^{-1}\\), \\[ \\boldsymbol{\\beta}_n = \\boldsymbol{B}_n (\\boldsymbol{B}_0^{-1} \\boldsymbol{\\beta}_0 + \\sigma^{-2} \\boldsymbol{X}^{\\top} \\Psi \\boldsymbol{y}), \\] \\(\\alpha_n = \\alpha_0 + N\\), \\(\\delta_n = \\delta_0 + (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})^{\\top} \\Psi (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})\\), \\(v_{1n} = v + 1\\), \\(v_{2in} = v + \\sigma^{-2}(y_i - \\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta})^2\\), and \\(\\Psi = \\text{diagonal}\\{\\tau_i\\}\\). The market value of soccer players in Europe continues Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the market value of soccer players in Europe, setting \\(v = 5\\) and the same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case? Example: Determinants of hospitalization continues Program a Gibbs sampling algorithm in the application of determinants of hospitalization. Choice of the fishing mode continues Run the Algorithm of the Multinomial Probit of the book to show the results of the Geweke (Geweke 1992), Raftery (Raftery and Lewis 1992), and Heidelberger (Heidelberger and Welch 1983) tests using our GUI. Use the command rmnpGibbs to do the example of the choice of the fishing mode. Simulation exercise of the multinomial logit model continues Perform inference in the simulation of the multinomial logit model using the command rmnlIndepMetrop from the bayesm package of R and using our GUI. Simulation of the ordered probit model Simulate an ordered probit model where the first regressor distributes \\(N(6, 5)\\) and the second distributes \\(G(1, 1)\\), the location vector is \\(\\boldsymbol{\\beta} = \\left[ 0.5, -0.25, 0.5 \\right]^{\\top}\\), and the cutoffs are in the vector \\(\\boldsymbol{\\alpha} = \\left[ 0, 1, 2.5 \\right]^{\\top}\\). Program from scratch a Metropolis-within-Gibbs sampling algorithm to perform inference in this simulation. Simulation of the negative binomial model continues Perform inference in the simulation of the negative binomial model using the bayesm package in R software. The market value of soccer players in Europe continues Perform the application of the value of soccer players with left censoring at one million Euros in our GUI using the Algorithm of the Tobit models, and the hyperparameters of the example. The market value of soccer players in Europe continues Program from scratch the Gibbs sampling algorithm in the example of the market value of soccer players at the 0.75 quantile. Use the bayesboot package to perform inference in the simulation exercise of Section 6.10, and compare the results with the ones that we get using our GUI, setting \\(S = 10000\\). References Geweke, J. 1992. “Bayesian Statistics.” In. Clarendon Press, Oxford, UK. Heidelberger, P., and P. D. Welch. 1983. “Simulation Run Length Control in the Presence of an Initial Transient.” Operations Research 31 (6): 1109–44. Raftery, A. E., and S. M. Lewis. 1992. “One Long Run with Diagnostics: Implementation Strategies for Markov Chain Monte Carlo.” Statistical Science 7: 493–97. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
