[["Chap8.html", "Chapter 8 Time series", " Chapter 8 Time series In this chapter, we provide a brief introduction to performing inference in time series models using a Bayesian framework. There is a large literature on time series in statistics and econometrics, making it impossible to present a thorough treatment in just a few pages of an introductory book. However, there are excellent books on Bayesian inference in time series; see, for instance, West and Harrison (2006), Petris, Petrone, and Campagnoli (2009), and Pole, West, and Harrison (2018). A time series is a sequence of observations collected in chronological order, allowing us to track how variables change over time. However, it also introduces technical challenges, as we must account for statistical features such as autocorrelation and stationarity. Since time series data is time-dependent, we adjust our notation. Specifically, we use \\(t\\) and \\(T\\) instead of \\(i\\) and \\(N\\) to explicitly indicate time. Our starting point in this chapter is the state-space representation of time series models. Much of the Bayesian inference literature in time series adopts this approach, as it allows dynamic systems to be modeled in a structured way. This representation provides modularity, flexibility, efficiency, and interpretability in complex models where the state evolves over time. It also enables the use of recursive estimation methods, such as the Kalman filter for dynamic Gaussian linear models and the particle filter (also known as sequential Monte Carlo) for non-Gaussian and nonlinear state-space models. The latter method is especially useful for online predictions or when there are data storage limitations. These inferential tools are based on the sequential updating process of Bayes’ rule, where the posterior at time \\(t\\) becomes the prior at time \\(t+1\\). Remember that we can run our GUI typing shiny::runGitHub(\"besmarter/BSTApp\", launch.browser=T) in the R console or any R code editor and execute it. However, users should see Chapter 5 for details. References "],["sec81.html", "8.1 State-space representation", " 8.1 State-space representation A state-space model consists of an unobservable state vector \\(\\boldsymbol{\\beta}_t \\in \\mathbb{R}^K\\) and an observed measure \\(\\boldsymbol{Y}_t \\in \\mathbb{R}^M\\), for \\(t=1,2,\\dots\\). These components satisfy two key properties: (i) \\(\\boldsymbol{\\beta}_t\\) follows a Markov process, meaning that \\(\\pi(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{\\beta}_{1:t-1})=\\pi(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{\\beta}_{t-1})\\). In other words, all information about \\(\\boldsymbol{\\beta}_t\\) is carried by \\(\\boldsymbol{\\beta}_{t-1}\\), and (ii) \\(\\boldsymbol{Y}_t\\) is independent of \\(\\boldsymbol{Y}_s\\) given \\(\\boldsymbol{\\beta}_t\\) for all \\(s &lt; t\\) (Petris, Petrone, and Campagnoli 2009, chap. 2). These assumptions imply that \\[ \\pi(\\boldsymbol{\\beta}_{0:t},\\boldsymbol{Y}_{1:t})=\\pi(\\boldsymbol{\\beta}_0)\\prod_{s=1}^{t}\\pi(\\boldsymbol{\\beta}_s\\mid \\boldsymbol{\\beta}_{s-1})\\pi(\\boldsymbol{Y}_s\\mid \\boldsymbol{\\beta}_s). \\] A state-space model where the states are discrete random variables is called a hidden Markov model. There are three key aims in state-space models: filtering, smoothing, and forecasting. - In filtering, we estimate the current state given observations up to time \\(t\\), obtaining the density \\(\\pi(\\boldsymbol{\\beta}_{s}\\mid \\boldsymbol{y}_{1:t})\\) for \\(s = t\\). - In smoothing, we analyze past states, obtaining \\(\\pi(\\boldsymbol{\\beta}_{s}\\mid \\boldsymbol{y}_{1:t})\\) for \\(s &lt; t\\). - In forecasting, we predict future observations by first computing \\(\\pi(\\boldsymbol{\\beta}_{s}\\mid \\boldsymbol{y}_{1:t})\\) as an intermediate step to obtain \\(\\pi(\\boldsymbol{Y}_{s}\\mid \\boldsymbol{y}_{1:t})\\) for \\(s &gt; t\\). A key advantage of these methods is that all these densities can be calculated recursively. Petris, Petrone, and Campagnoli (2009) provide the recursive equations in Propositions 2.1 (filtering), 2.3 (smoothing), and 2.5 (forecasting). 8.1.1 Gaussian linear state-space models An important class of state-space models is the Gaussian linear state-space model, also known as a dynamic linear model: \\[\\begin{align} \\boldsymbol{Y}_t &amp;= \\boldsymbol{X}_t\\boldsymbol{\\beta}_t+\\boldsymbol{\\mu}_t &amp; \\text{(Observation equations)} \\\\ \\boldsymbol{\\beta}_t &amp;= \\boldsymbol{G}_t\\boldsymbol{\\beta}_{t-1}+\\boldsymbol{w}_t &amp; \\text{(State equations)} \\end{align}\\] where \\(\\boldsymbol{\\beta}_0\\sim N(\\boldsymbol{b}_0,\\boldsymbol{B}_0)\\), \\(\\boldsymbol{\\mu}_t\\sim N(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_t)\\), and \\(\\boldsymbol{w}_t\\sim N(\\boldsymbol{0}, \\boldsymbol{\\Omega}_t)\\). The terms \\(\\boldsymbol{\\beta}_0\\), \\(\\boldsymbol{\\mu}_t\\), and \\(\\boldsymbol{w}_t\\) are independent, while \\(\\boldsymbol{X}_t\\) and \\(\\boldsymbol{G}_t\\) are known matrices of dimensions \\(M\\times K\\) and \\(K\\times K\\), respectively. These assumptions imply that \\[ \\boldsymbol{Y}_t\\mid \\boldsymbol{\\beta}_t \\sim N(\\boldsymbol{X}_t\\boldsymbol{\\beta}_t, \\boldsymbol{\\Sigma}_t), \\quad \\boldsymbol{\\beta}_t\\mid \\boldsymbol{\\beta}_{t-1} \\sim N(\\boldsymbol{G}_t\\boldsymbol{\\beta}_{t-1}, \\boldsymbol{\\Omega}_t). \\] A general state-space model is defined as \\(\\boldsymbol{Y}_t = \\boldsymbol{f}_t(\\boldsymbol{\\beta}_t, \\boldsymbol{\\mu}_t)\\) and \\(\\boldsymbol{\\beta}_t = \\boldsymbol{m}_t(\\boldsymbol{\\beta}_{t-1}, \\boldsymbol{w}_t)\\), where \\(\\boldsymbol{f}_t\\) and \\(\\boldsymbol{m}_t\\) are arbitrary functions with corresponding distributions for \\(\\boldsymbol{\\mu}_t\\) and \\(\\boldsymbol{w}_t\\), and a prior for \\(\\boldsymbol{\\beta}_0\\). Let \\(\\boldsymbol{\\beta}_{t-1}\\mid \\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{b}_{t-1},\\boldsymbol{B}_{t-1})\\), then, we can get the Kalman filter by obtaining: The one-step-ahead predictive distribution of \\(\\boldsymbol{\\beta}_t\\) given \\(\\boldsymbol{y}_{1:t-1}\\) is \\(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{a}_t, \\boldsymbol{R}_t)\\), where \\[\\boldsymbol{a}_t=\\boldsymbol{G}_t\\boldsymbol{b}_{t-1}, \\quad \\boldsymbol{R}_t=\\boldsymbol{G}_t\\boldsymbol{B}_{t-1}\\boldsymbol{G}_t^{\\top}+\\boldsymbol{\\Omega}_t.\\] The one-step-ahead predictive distribution of \\(\\boldsymbol{Y}_t\\) given \\(\\boldsymbol{y}_{1:t-1}\\) is \\(\\boldsymbol{Y}_t\\mid \\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{f}_t, \\boldsymbol{Q}_t)\\), where \\[\\boldsymbol{f}_t=\\boldsymbol{X}_t\\boldsymbol{a}_t, \\quad \\boldsymbol{Q}_t=\\boldsymbol{X}_t\\boldsymbol{R}_t\\boldsymbol{X}_t^{\\top}+\\boldsymbol{\\Sigma}_t.\\] The distribution of the one-step-ahead prediction error \\(\\boldsymbol{e}_t=\\boldsymbol{Y}_t-\\mathbb{E}[\\boldsymbol{Y}_t\\mid \\boldsymbol{y}_{1:t-1}]=\\boldsymbol{Y}_t-\\boldsymbol{f}_t\\) is \\(N(\\boldsymbol{0}, \\boldsymbol{Q}_t)\\) Shumway and Stoffer (2017), Chap. 6. The filtering distribution of \\(\\boldsymbol{\\beta}_t\\) given \\(\\boldsymbol{y}_{1:t}\\) is \\(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t}\\sim N(\\boldsymbol{b}_t, \\boldsymbol{B}_t)\\), where \\[\\boldsymbol{b}_t=\\boldsymbol{a}_t+\\boldsymbol{K}_t\\boldsymbol{e}_t, \\quad \\boldsymbol{K}_t=\\boldsymbol{R}_t\\boldsymbol{X}_t^{\\top}\\boldsymbol{Q}_t^{-1}\\] is the Kalman gain, and \\[\\boldsymbol{B}_t=\\boldsymbol{R}_t-\\boldsymbol{R}_t\\boldsymbol{X}_t^{\\top}\\boldsymbol{Q}_t^{-1}\\boldsymbol{X}_t\\boldsymbol{R}_t.\\] The formal proofs of these results can be found in Petris, Petrone, and Campagnoli (2009), Chap. 2. Just take into account that the logic of these results follows the Seemingly Unrelated Regression (SUR) model in 7.2 for a particular time period. In addition, we know that the posterior distribution using information up to \\(t-1\\) becomes the prior in \\(t\\), \\[\\pi(\\boldsymbol{\\theta}\\mid \\mathbf{y}_{1:t})\\propto p(y_{t}\\mid \\boldsymbol{y}_{1:t-1},\\boldsymbol{\\theta})\\times \\pi(\\boldsymbol{\\theta}\\mid \\boldsymbol{y}_{1:t-1}).\\] This is the updating process from \\(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{a}_t, \\boldsymbol{R}_t)\\) to \\(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t}\\sim N(\\boldsymbol{b}_t, \\boldsymbol{B}_t)\\). Moreover, the posterior mean and variance of the SUR model with independent conjugate priors for a particular time period can be written as \\[\\boldsymbol{a}_{t}+\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}+ \\boldsymbol{\\Sigma}_t)^{-1}(\\boldsymbol{y}_t-\\boldsymbol{X}_t\\boldsymbol{a}_{t})\\] and \\[\\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}+\\boldsymbol{\\Sigma}_t)^{-1} \\boldsymbol{X}_t\\boldsymbol{R}_{t}^{\\top},\\] respectively. Let’s see this, we know from 7.2 that \\[\\boldsymbol{B}_t=(\\boldsymbol{R}_t^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X}_t)^{-1}\\] and \\[\\boldsymbol{\\beta}_t=\\boldsymbol{B}_t(\\boldsymbol{R}_t^{-1}\\boldsymbol{a}_t+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{y}_t).\\] Thus, let’s show that both conditional posterior distributions are the same. In particular, the posterior mean in the state-space representation is \\[[\\boldsymbol{I}_K-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}+ \\boldsymbol{\\Sigma}_t)^{-1}\\boldsymbol{X}_t]\\boldsymbol{a}_{t}+\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}+ \\boldsymbol{\\Sigma}_t)^{-1}\\boldsymbol{y}_t,\\] where \\[\\begin{align*} \\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}+ \\boldsymbol{\\Sigma}_t)^{-1} &amp;=\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}[\\boldsymbol{\\Sigma}_t^{-1}-\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_t^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}]\\\\ &amp;=\\boldsymbol{R}_{t}[\\boldsymbol{I}_K-\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_t^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}]\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\\\ &amp;=\\boldsymbol{R}_{t}(\\boldsymbol{I}_K-[\\boldsymbol{I}_K-\\boldsymbol{R}_t^{-1}(\\boldsymbol{R}_t^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}])\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\\\ &amp;=(\\boldsymbol{R}_t^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}, \\end{align*}\\] where the first equality uses the Woodbury matrix identity (matrix inversion lemma), and the third equality uses \\(\\boldsymbol{D}(\\boldsymbol{D}+\\boldsymbol{E})^{-1}=\\boldsymbol{I}-\\boldsymbol{E}(\\boldsymbol{D}+\\boldsymbol{E})^{-1}\\). Thus, we have the following expression: \\[\\begin{align*} &amp;[\\mathbf{I}_K - \\mathbf{R}_t \\mathbf{X}_t^{\\top} (\\mathbf{X}_t \\mathbf{R}_t \\mathbf{X}_t^{\\top} + \\boldsymbol{\\Sigma}_t)^{-1} \\mathbf{X}_t] \\mathbf{a}_t + \\mathbf{R}_t \\mathbf{X}_t^{\\top} (\\mathbf{X}_t \\mathbf{R}_t \\mathbf{X}_t^{\\top} + \\boldsymbol{\\Sigma}_t)^{-1} \\mathbf{y}_t \\\\ &amp;= [\\mathbf{I}_K - (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t] \\mathbf{a}_t + (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{y}_t \\\\ &amp;= (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} \\mathbf{R}_t^{-1} \\mathbf{a}_t + (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{y}_t \\\\ &amp;= (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} (\\mathbf{R}_t^{-1} \\mathbf{a}_t + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{y}_t) \\\\ &amp;= (\\mathbf{R}_t^{-1} + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t)^{-1} (\\mathbf{R}_t^{-1} \\mathbf{a}_t + \\mathbf{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\mathbf{X}_t \\hat{\\boldsymbol{\\beta}}_t), \\end{align*}\\] where the second equality uses the identity: \\[ \\boldsymbol{I} - (\\boldsymbol{D} + \\boldsymbol{E})^{-1} \\boldsymbol{D} = (\\boldsymbol{D} + \\boldsymbol{E})^{-1} \\boldsymbol{E}, \\] and the estimator \\(\\hat{\\boldsymbol{\\beta}}_t\\) is defined as: \\[ \\hat{\\boldsymbol{\\beta}}_t = (\\boldsymbol{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\boldsymbol{X}_t)^{-1} \\boldsymbol{X}_t^{\\top} \\boldsymbol{\\Sigma}_t^{-1} \\boldsymbol{y}_t. \\] This shows that the posterior mean is a weighted average of the prior mean and the maximum likelihood estimator (which is the generalized least squares estimator). The weights are linked to the signal-to-noise ratio, that is, the proportion of the total variability (\\(\\boldsymbol{\\Omega}_t+\\boldsymbol{\\Sigma}_t\\)) due to the signal (\\(\\boldsymbol{\\Omega}_t\\)) versus the noise (\\(\\boldsymbol{\\Sigma}_t\\)). Note that in the simplest case where \\(M=K=1\\), and \\(\\boldsymbol{X}_t=\\boldsymbol{G}_t=1\\), then \\(\\boldsymbol{K}_t=\\boldsymbol{R}_t\\boldsymbol{Q}_t^{-1}=(B_{t-1}+\\Omega_t)/(B_{t-1}+\\Omega_t+\\Sigma_t)\\). Thus, the weight associated with the observations is equal to 1 if \\(\\Sigma_t=0\\), that is, the posterior mean is equal to the actual observation. On the other hand, if \\(\\Sigma_t\\) increases compare to \\(\\Omega_t\\), there is more weight to the prior information, and consequently, the posterior mean is smoother as it heavily dependents on the history. We ask in Exercise 1 to perform simulations with different signal-to-noise ratios to see the effects on the system. The equality of variances of both approaches is as follows: \\[\\begin{align*} Var[\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t}]&amp; = \\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\boldsymbol{X}_t^\\top+\\boldsymbol{\\Sigma}_t)^{-1} \\boldsymbol{X}_t\\boldsymbol{R}_{t}\\\\ &amp;=\\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}(\\boldsymbol{\\Sigma}_t^{-1}- \\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1})\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\\\ &amp;=\\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t\\boldsymbol{R}_{t}+ \\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t\\boldsymbol{R}_{t}\\\\ &amp;=\\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t\\boldsymbol{R}_{t}+ \\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t[\\boldsymbol{I}_K-(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\boldsymbol{R}_{t}^{-1}]\\boldsymbol{R}_{t}\\\\ &amp;=\\boldsymbol{R}_{t}-\\boldsymbol{R}_{t}\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}\\\\ &amp;=\\boldsymbol{R}_t[\\boldsymbol{I}_K-\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}]\\\\ &amp;=\\boldsymbol{R}_{t}[\\boldsymbol{I}_K-(\\boldsymbol{I}_K-\\boldsymbol{R}_{t}^{-1}(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1})]\\\\ &amp;=(\\boldsymbol{R}_{t}^{-1}+\\boldsymbol{X}_t^{\\top}\\boldsymbol{\\Sigma}_t^{-1}\\boldsymbol{X}_t)^{-1}, \\end{align*}\\] where the second equality uses the Woodbury matrix identity, the fourth equality uses \\((\\boldsymbol{D}+\\boldsymbol{E})^{-1}\\boldsymbol{D}=\\boldsymbol{I}-(\\boldsymbol{D}+\\boldsymbol{E})^{-1}\\boldsymbol{E}\\), and the seventh equality uses \\(\\boldsymbol{D}(\\boldsymbol{D}+\\boldsymbol{E})^{-1}=\\boldsymbol{I}-\\boldsymbol{E}(\\boldsymbol{D}+\\boldsymbol{E})^{-1}\\). The Kalman filter allows calculating recursively in a forward way \\(\\pi(\\boldsymbol{\\beta}_t\\mid \\boldsymbol{y}_{1:t})\\) from \\(\\pi(\\boldsymbol{\\beta}_{t-1}\\mid \\boldsymbol{y}_{1:t-1})\\) starting from \\(\\pi(\\boldsymbol{\\beta}_0)\\). Let \\(\\boldsymbol{\\beta}_{t+1} \\mid \\mathbf{y}_{1:T} \\sim N(\\boldsymbol{s}_{t+1}, \\mathbf{S}_{t+1})\\), then we can get the Kalman smoother by \\(\\boldsymbol{\\beta}_{t} \\mid \\mathbf{y}_{1:T} \\sim N(\\boldsymbol{s}_{t}, \\mathbf{S}_{t})\\), where \\[ \\boldsymbol{s}_t = \\mathbf{b}_t + \\mathbf{B}_t \\mathbf{G}_{t+1}^{\\top} \\mathbf{R}_{t+1}^{-1} (\\boldsymbol{s}_{t+1} - \\mathbf{a}_{t+1}) \\] and \\[ \\mathbf{S}_t = \\mathbf{B}_t - \\mathbf{B}_t \\mathbf{G}_{t+1}^{\\top} \\mathbf{R}_{t+1}^{-1} (\\mathbf{R}_{t+1} - \\mathbf{S}_{t+1}) \\mathbf{R}_{t+1}^{-1} \\mathbf{G}_{t+1} \\mathbf{B}_{t}. \\] The proof can be found in Petris, Petrone, and Campagnoli (2009), Chap. 2. Thus, we can calculate the Kalman smoother starting from \\(t = T-1\\), that is, \\(\\boldsymbol{\\beta}_{T} \\mid \\mathbf{y}_{1:T} \\sim N(\\boldsymbol{s}_{T}, \\mathbf{S}_{T})\\). However, this is the filtering distribution at \\(T\\), which means \\(\\boldsymbol{s}_{T} = \\mathbf{b}_{T}\\) and \\(\\mathbf{S}_{T} = \\mathbf{B}_{T}\\), and then, we should proceed recursively in a backward way. Finally, the forecasting recursion in the dynamic linear model, given \\(\\mathbf{a}_t(0) = \\mathbf{b}_t\\) and \\(\\mathbf{R}_t(0) = \\mathbf{B}_t\\), \\(h \\geq 1\\), is given by The forecasting distribution of \\(\\boldsymbol{\\beta}_{t+h} \\mid \\mathbf{y}_{1:t}\\) is \\(N(\\mathbf{a}_t(h), \\mathbf{R}_t(h))\\), where \\[ \\mathbf{a}_t(h) = \\mathbf{G}_{t+h} \\mathbf{a}_{t}(h-1), \\quad \\mathbf{R}_t(h) = \\mathbf{G}_{t+h} \\mathbf{R}_t(h-1) \\mathbf{G}_{t+h}^{\\top} + \\boldsymbol{\\Omega}_{t+h}. \\] The forecasting distribution \\(\\mathbf{Y}_{t+h} \\mid \\mathbf{y}_{1:t}\\) is \\(N(\\mathbf{f}_t(h), \\mathbf{Q}_t(h))\\), where \\[ \\mathbf{f}_t(h) = \\mathbf{X}_{t+h} \\mathbf{a}_t(h), \\quad \\mathbf{Q}_t(h) = \\mathbf{X}_{t+h} \\mathbf{R}_t(h) \\mathbf{X}_{t+h}^{\\top} + \\boldsymbol{\\Sigma}_{t+h}. \\] The proof can be found in Petris, Petrone, and Campagnoli (2009), Chap. 2. These recursive equations allow us to perform probabilistic forecasting \\(h\\)-steps-ahead for the state and observation equations. These results demonstrate how to use these recursive equations for filtering, smoothing, and forecasting in dynamic linear models (Gaussian linear state-space models). Although these algorithms appear simple, they suffer from numerical instability, which can lead to non-symmetric and negative-definite covariance matrices. Thus, special care must be taken when working with them. In addition, this setup assumes that \\(\\boldsymbol{\\Sigma}_t\\) and \\(\\boldsymbol{\\Omega}_t\\) are known. However, this is rarely the case in most situations. Therefore, we need to estimate them. One option is to perform maximum likelihood estimation. However, this approach does not account for the uncertainty associated with the fact that \\(\\boldsymbol{\\Sigma}_t\\) and \\(\\boldsymbol{\\Omega}_t\\) are unknown when their estimates are plugged into the state space recursions. On the other hand, we can use a Bayesian approach and perform the recursions associated with each posterior draw of the unknown parameters, thus taking their uncertainty into account. The point of departure is the posterior distribution, such that \\[ \\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\beta}_0, \\dots, \\boldsymbol{\\beta}_T \\mid \\mathbf{y}, \\mathbf{X}, \\mathbf{G}) \\propto \\pi(\\boldsymbol{\\beta}_0 \\mid \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) \\prod_{t=1}^{T} \\pi(\\boldsymbol{\\beta}_t \\mid \\boldsymbol{\\beta}_{t-1}, \\boldsymbol{\\theta}) \\pi(\\mathbf{y}_t \\mid \\boldsymbol{\\beta}_t, \\boldsymbol{\\theta}), \\] where \\(\\boldsymbol{\\theta}\\) is the vector of unknown parameters. We can compute \\[\\pi(\\boldsymbol{\\beta}_s, \\boldsymbol{\\theta} \\mid \\mathbf{y}_{1:t}) = \\pi(\\boldsymbol{\\beta}_s \\mid \\mathbf{y}_{1:t}, \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1:t}),\\] for \\(s=t\\) (filtering), \\(s&lt;t\\) (smoothing), and \\(s&gt;t\\) (forecasting). The marginal posterior distribution of the states is \\[ \\pi(\\boldsymbol{\\beta}_s \\mid \\mathbf{y}_{1:t}) = \\int_{\\boldsymbol{\\Theta}} \\pi(\\boldsymbol{\\beta}_s \\mid \\mathbf{y}_{1:t}, \\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}_{1:t}) d\\boldsymbol{\\theta}. \\] We can use the Gibbs sampling algorithm to get the posterior draws in the dynamic linear model assuming conjugate families. In particular, let’s see the univariate case with random walk states, \\[\\begin{align} y_t&amp;=\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}_t+\\mu_t \\tag{8.1}\\\\ \\boldsymbol{\\beta}_t&amp;=\\boldsymbol{\\beta}_{t-1}+\\boldsymbol{w}_t, \\tag{8.2} \\end{align}\\] where \\(\\mu_t\\sim N(0,\\sigma^2)\\) and \\(\\boldsymbol{w}_t\\sim N(\\boldsymbol{0},\\text{diag}\\left\\{\\omega_1^2,\\dots,\\omega_K^2\\right\\})\\). We assume that \\(\\pi(\\sigma^2,\\omega_1^2,\\dots,\\omega_K^2,\\boldsymbol{\\beta}_0)=\\pi(\\sigma^2)\\pi(\\omega_1^2),\\dots,\\pi(\\omega_K^2)\\pi(\\boldsymbol{\\beta}_0)\\) where \\(\\sigma^2\\sim IG(\\alpha_0/2,\\delta_0/2)\\), \\(\\omega_k^2\\sim IG(\\alpha_{k0}/2,\\delta_{k0}/2)\\), \\(k=1,\\dots,K\\), and \\(\\boldsymbol{\\beta}_0\\sim N(\\boldsymbol{b}_0,\\boldsymbol{B}_0)\\). Thus, the conditional posterior distributions are \\(\\sigma^2\\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{\\beta}_{1:T}\\sim IG(\\alpha_{n}/2,\\delta_n/2)\\), where \\(\\alpha_{n}=T+\\alpha_0\\) and \\(\\delta_n=\\sum_{t=1}^T(y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}_t)^2+\\delta_0\\), and \\(\\omega_k^2\\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{\\beta}_{0:T}\\sim IG(\\alpha_{kn}/2,\\delta_{kn}/2)\\), where \\(\\alpha_{kn}=T+\\alpha_{k0}\\) and \\(\\delta_{kn}=\\sum_{t=1}^T(\\boldsymbol{\\beta}_{t,k}-\\boldsymbol{\\beta}_{t-1,k})^2+\\delta_{k0}\\). The vector of the dependent variable is \\(\\boldsymbol{y}\\), and all regressors are in \\(\\boldsymbol{X}\\). We also need to sample the states from \\(\\pi(\\boldsymbol{\\beta}_{1:T}\\mid \\boldsymbol{y},\\boldsymbol{X},\\sigma^2,\\omega_1^2,\\dots,\\omega_K^2)\\). This can be done using the forward filtering backward sampling (FFBS) algorithm (Carter and Kohn 1994; Frühwirth-Schnatter 1994; Shephard 1994). This algorithm is basically a simulation version of the smoothing recursion, which allows getting draws of the states, even if we do not have analytical solutions, for instance, in non-linear settings. See below and Petris, Petrone, and Campagnoli (2009) Chap. 3 for details. A word of caution here, users should be careful to set non-informative priors in this setting, and in general, settings where there are a large number of parameters (see G. M. Koop (2003) Chap. 8 for details). Thus, it is useful to use empirical Bayes methods focusing on relevant hyperparameters, for instance, the hyperparameters of the inverse-gamma distributions which define the signal-to-noise ratio. We use the command dlmGibbsDIG from the dlm package in our GUI to perform Bayesian inference in the univariate dynamic linear model with random walk states. This function uses the FFBS algorithm, and assumes independent gamma priors for the precision (inverse of variance) parameters. In addition, this package uses the singular value decomposition to calculate the covariance matrices to avoid numerical instability. The following Algorithm shows how to perform inference in the univariate dynamic linear model with random walk states in our GUI. See also Chapter 5 for details regarding the dataset structure. Algorithm: Dynamic Linear Models Select Time series Model on the top panel Select Dynamic linear model using the left radio button Upload the dataset selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Set the hyperparameters of the precision of the observation equation: prior mean and variance Set the hyperparameters of the precision of the state equations: just one set of prior mean and variance parameters Click the Go! button Analyze results Download posterior chains of variances of observation and state equations, and posterior chains of states using the Download Posterior Chains of Variances and Download Posterior Chains of States buttons Example: Simulation exercise of the dynamic linear model We simulate the process \\(y_t = \\beta_{t1} + x_t \\beta_{t2} + \\mu_t\\) and \\(\\boldsymbol{\\beta}_t = \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t\\), \\(t = 1, 2, \\dots, 200\\), where \\(\\boldsymbol{\\beta}_t = [\\beta_{t1} \\ \\beta_{t2}]^{\\top}\\), \\(\\mu_t \\sim N(0, 0.5^2)\\), \\(\\boldsymbol{w}_t \\sim N(\\boldsymbol{0}, \\text{diag}\\{0.2, 0.1\\})\\), \\(x_t \\sim N(1, 1)\\), \\(\\boldsymbol{\\beta}_0\\) and \\(\\boldsymbol{B}_0\\) are the OLS estimates and variance of the recursive OLS estimates (see below), respectively. The following algorithm demonstrates how to perform inference using dlmGibbsDIG and compares the results to those of the maximum likelihood estimator, which is based on the dlmMLE function. We also use the dlmSvd2var function, which is based on singular value decomposition, to calculate the variance of the smoothing states. All these functions are from the dlm package in R. Users can observe that we employ a straightforward strategy for setting the hyperparameters. First, we recursively estimate the model using ordinary least squares (OLS), progressively increasing the sample size, and save the location parameters. Next, we compute the covariance matrix of this sequence and use it to set the priors: the prior mean of the precision of the state vector is set equal to the inverse of the maximum element of the main diagonal of this covariance matrix (a.theta), and the prior variance is set equal to ten times this value (b.theta). For the observation equation, the prior mean of the precision is set equal to the inverse of the OLS variance estimate (a.y), and the prior variance is set equal to ten times this value (b.y). We perform some sensitivity analysis of the results regarding the hyperparameters, and it seems that the results are robust. However, we encourage giving more consideration to empirical Bayes methods for setting hyperparameters in state-space models. rm(list = ls()); set.seed(010101) T &lt;- 200; sig2 &lt;- 0.5^2 x &lt;- rnorm(T, mean = 1, sd = 1) X &lt;- cbind(1, x); B0 &lt;- c(1, 0.5) K &lt;- length(B0) e &lt;- rnorm(T, mean = 0, sd = sig2^0.5) Omega &lt;- diag(c(0.2, 0.1)) w &lt;- MASS::mvrnorm(T, c(0, 0), Omega) Bt &lt;- matrix(NA, T, K); Bt[1,] &lt;- B0 yt &lt;- rep(NA, T) yt[1] &lt;- X[1,]%*%B0 + e[1] for(t in 1:T){ if(t == 1){ Bt[t,] &lt;- w[t,] }else{ Bt[t,] &lt;- Bt[t-1,] + w[t,] } yt[t] &lt;- X[t,]%*%Bt[t,] + e[t] } RegLS &lt;- lm(yt ~ x) SumRegLS &lt;- summary(RegLS) SumRegLS; SumRegLS$sigma^2 Bp &lt;- matrix(RegLS$coefficients, T, K, byrow = TRUE) S &lt;- 20 for(t in S:T){ RegLSt &lt;- lm(yt[1:t] ~ x[1:t]) Bp[t,] &lt;- RegLSt$coefficients } # plot(Bp[S:T,2], type = &quot;l&quot;) VarBp &lt;- var(Bp) # State space model ModelReg &lt;- function(par){ Mod &lt;- dlm::dlmModReg(x, dV = exp(par[1]), dW = exp(par[2:3]), m0 = RegLS$coefficients, C0 = VarBp) return(Mod) } outMLEReg &lt;- dlm::dlmMLE(yt, parm = rep(0, K+1), ModelReg) exp(outMLEReg$par) RegFilter &lt;- dlm::dlmFilter(yt, ModelReg(outMLEReg$par)) RegSmoth &lt;- dlm::dlmSmooth(yt, ModelReg(outMLEReg$par)) SmoothB2 &lt;- RegSmoth$s[-1,2] VarSmooth &lt;- dlm::dlmSvd2var(u = RegSmoth[[&quot;U.S&quot;]], RegSmoth[[&quot;D.S&quot;]]) SDVarSmoothB2 &lt;- sapply(2:(T+1), function(t){VarSmooth[[t]][K,K]^0.5}) LimInfB2 &lt;- SmoothB2 - qnorm(0.975)*SDVarSmoothB2 LimSupB2 &lt;- SmoothB2 + qnorm(0.975)*SDVarSmoothB2 # Gibbs MCMC &lt;- 2000; burnin &lt;- 1000 a.y &lt;- (SumRegLS$sigma^2)^(-1); b.y &lt;- 10*a.y; a.theta &lt;- (max(diag(VarBp)))^(-1); b.theta &lt;- 10*a.theta gibbsOut &lt;- dlm::dlmGibbsDIG(yt, mod = dlm::dlmModReg(x), a.y = a.y, b.y = b.y, a.theta = a.theta, b.theta = b.theta, n.sample = MCMC, thin = 5, save.states = TRUE) B2t &lt;- matrix(0, MCMC - burnin, T + 1) for(t in 1:(T+1)){ B2t[,t] &lt;- gibbsOut[[&quot;theta&quot;]][t,2,-c(1:burnin)] } Lims &lt;- apply(B2t, 2, function(x){quantile(x, c(0.025, 0.975))}) summary(coda::mcmc(gibbsOut[[&quot;dV&quot;]])) summary(coda::mcmc(gibbsOut[[&quot;dW&quot;]])) # Figure require(latex2exp) # LaTeX equations in figures xx &lt;- c(1:(T+1), (T+1):1) yy &lt;- c(Lims[1,], rev(Lims[2,])) plot (xx, yy, type = &quot;n&quot;, xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{t2}$&quot;)) polygon(xx, yy, col = &quot;lightblue&quot;, border = &quot;lightblue&quot;) xxML &lt;- c(1:T, T:1) yyML &lt;- c(LimInfB2, rev(LimSupB2)) polygon(xxML, yyML, col = &quot;blue&quot;, border = &quot;blue&quot;) lines(colMeans(B2t), col = &quot;red&quot;, lw = 2) lines(Bt[,2], col = &quot;black&quot;, lw = 2) lines(SmoothB2, col = &quot;green&quot;, lw = 2) title(&quot;State vector: Slope parameter&quot;) The Figure shows the comparison between maximum likelihood (ML) and Bayesian inference. The light blue (Bayesian) and dark blue (maximum likelihood) shadows show the credible and confidence intervals at 95% for the state slope parameter (\\(\\beta_{t2}\\)). We see that the Bayesian interval encompass the ML interval. This is a reflection of the extra uncertainty of the unknown variances. The black line is the actual trajectory of \\(\\beta_{t2}\\), the green and red lines are the smoothing recursions using the ML and Bayesian estimates (posterior mean), respectively. Example: Effects of inflation on interest rate I We use the dataset 16INTDEF.csv provided by Wooldridge (2016) to study the effects of inflation on the interest rate. The specification is \\[ \\Delta i_t = \\beta_{t1} + \\beta_{t2} \\Delta \\text{inf}_t + \\beta_{t3} \\Delta \\text{def}_t + \\mu_t \\] and \\[ \\boldsymbol{\\beta}_t = \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t, \\] where \\(\\Delta z_t = z_t - z_{t-1}\\) is the difference operator, \\(i_t\\) is the three-month T-bill rate, \\(\\text{inf}_t\\) is the annual inflation rate based on the consumer price index (CPI), and \\(\\text{def}_t\\) is the federal budget deficit as a percentage of gross domestic product (GDP) from 1948 to 2003 in the USA. In addition, \\(\\mu_t \\sim N(0, \\sigma^2)\\) and \\(\\boldsymbol{w}_t \\sim N(\\boldsymbol{0}, \\text{diag}\\{\\omega_1^2, \\omega_2^2\\})\\). We assume inverse-gamma distributions for the priors of the scale parameters and set 12,000 MCMC iterations, 2,000 as burn-in, and 10 as the thinning parameter. The following code shows how to perform this application. We use the variance of the recursive estimation of OLS to set the hyperparameters of the inverse-gamma distribution for the variances of \\(\\boldsymbol{w}_t\\), and the OLS estimate of the variance of the model to set the hyperparameters of the distribution of \\(\\sigma^2\\). Note that, as we are using the function dlmGibbsDIG from the dlm package, the hyperparameters are set in terms of precision parameters. The Figure shows the posterior results of the effect of inflation on the interest rate. This is a fan chart indicating deciles from 10% to 90%. The red shaded area shows the range around the median value, and the black line represents the mean value of the state associated with the annual change in inflation. We see that the annual changes in interest rates are weakly positively related to annual changes in inflation. rm(list = ls()); set.seed(010101) DataIntRate &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/16INTDEF.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(DataIntRate); Xt &lt;- cbind(diff(inf), diff(def)) K &lt;- dim(Xt)[2] + 1; yt &lt;- diff(i3) T &lt;- length(yt); RegLS &lt;- lm(yt ~ Xt) SumRegLS &lt;- summary(RegLS); SumRegLS; SumRegLS$sigma^2 # Recursive OLS Bp &lt;- matrix(RegLS$coefficients, T, K, byrow = TRUE) S &lt;- 20 for(t in S:T){ RegLSt &lt;- lm(yt[1:t] ~ Xt[1:t,]) Bp[t,] &lt;- RegLSt$coefficients } VarBp &lt;- var(Bp) # State space model ModelReg &lt;- function(par){ Mod &lt;- dlm::dlmModReg(Xt, dV = exp(par[1]), dW = exp(par[2:(K+1)]), m0 = RegLS$coefficients, C0 = diag(VarBp)) return(Mod) } MCMC &lt;- 12000; burnin &lt;- 2000; thin &lt;- 10 a.y &lt;- (SumRegLS$sigma^2)^(-1); b.y &lt;- 10*a.y; a.theta &lt;- (max(diag(VarBp)))^(-1); b.theta &lt;- 10*a.theta gibbsOut &lt;- dlm::dlmGibbsDIG(yt, mod = dlm::dlmModReg(Xt), a.y = a.y, b.y = b.y, a.theta = a.theta, b.theta = b.theta, n.sample = MCMC, thin = 5, save.states = TRUE) B2t &lt;- matrix(0, MCMC - burnin, T + 1) for(t in 1:(T+1)){ B2t[,t] &lt;- gibbsOut[[&quot;theta&quot;]][t,2,-c(1:burnin)] } dV &lt;- coda::mcmc(gibbsOut[[&quot;dV&quot;]][-c(1:burnin)]) dW &lt;- coda::mcmc(gibbsOut[[&quot;dW&quot;]][-c(1:burnin),]) summary(dV); summary(dW) plot(dV); plot(dW) library(fanplot); library(latex2exp) df &lt;- as.data.frame(B2t) plot(NULL, main=&quot;Percentiles&quot;, xlim = c(1, T+1), ylim = c(-1, 2), xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{t1}$&quot;)) fan(data = df); lines(colMeans(B2t), col = &quot;black&quot;, lw = 2) abline(h=0, col = &quot;blue&quot;) We can extend the dynamic linear model with random walk states to take into account time-invariant location parameters. In particular, we follow De Jong and Shephard (1995), who propose the simulation smoother. This algorithm overcomes some shortcomings of the FFBS algorithm, such as slow convergence and computational overhead. We focus on the case \\(M = 1\\), \\[\\begin{align} y_t &amp;= \\boldsymbol{z}_t^{\\top} \\boldsymbol{\\alpha} + \\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta}_t + \\boldsymbol{h}_t^{\\top} \\boldsymbol{\\epsilon}_t, &amp; t = 1, 2, \\dots, T. &amp; \\text{ (Observation equation)} \\tag{8.3} \\\\ \\boldsymbol{\\beta}_t &amp;= \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{H}_t \\boldsymbol{\\epsilon}_t, &amp; t = 1, 2, \\dots, T. &amp; \\text{ (States equation)} \\tag{8.4} \\end{align}\\] where \\(\\boldsymbol{z}_t\\) and \\(\\boldsymbol{x}_t\\) are \\(L\\)-dimensional and \\(K\\)-dimensional vectors of regressors associated with time-invariant and time-varying parameters, respectively, \\(\\boldsymbol{h}_t\\) is a vector of dimension \\(1+K\\), \\(\\boldsymbol{H}_t\\) is a matrix of dimension \\(K \\times (1+K)\\), \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}\\), and \\(\\boldsymbol{\\epsilon}_t \\sim N(\\boldsymbol{0}_{1+K}, \\sigma^2 \\boldsymbol{I}_{1+K})\\). Observe that this specification encompasses Equations (8.1) and (8.2) setting \\(\\boldsymbol{\\epsilon}_t = [\\mu_t \\ \\boldsymbol{w}_t^{\\top}]^{\\top}\\), \\(\\boldsymbol{h}_t = [1 \\ 0 \\ \\dots \\ 0]\\), \\(\\boldsymbol{H}_t = [\\boldsymbol{0}_K \\ \\boldsymbol{U}_{K \\times K}]\\) such that \\(\\text{diag}\\{\\omega_1^2 \\ \\dots \\ \\omega_K^2\\} = \\sigma^2 \\boldsymbol{U} \\boldsymbol{U}^{\\top}\\), \\(\\boldsymbol{\\alpha} = \\boldsymbol{0}\\), and \\(\\boldsymbol{h}_t \\boldsymbol{H}_t^{\\top} = \\boldsymbol{0}_K\\). The nice idea of De Jong and Shephard (1995) was to propose an efficient algorithm to get draws from \\(\\boldsymbol{\\eta}_t = \\boldsymbol{F}_t \\boldsymbol{\\epsilon}_t\\), where the most common choice is \\(\\boldsymbol{F}_t = \\boldsymbol{H}_t\\), which means drawing samples from the perturbations of the states, and then, recovering the states from Equation (8.4) with \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}\\). De Jong and Shephard (1995) present a more general version of the state space model than the one presented here. Using the system given by Equations (8.3) and (8.4), \\(\\boldsymbol{F}_t=\\boldsymbol{H}_t\\) and \\(\\boldsymbol{h}_t\\boldsymbol{H}_t^{\\top}=\\boldsymbol{0}_{K}\\), the filtering recursions are given by \\(e_t=Y_t-\\boldsymbol{z}_t^{\\top}\\boldsymbol{\\alpha}-\\boldsymbol{x}_t^{\\top}\\boldsymbol{b}_{t-1}\\), \\({q}_t=\\boldsymbol{x}_t^{\\top}\\boldsymbol{B}_{t-1}\\boldsymbol{x}_t+\\boldsymbol{h}_t^{\\top}\\boldsymbol{h}_t\\), \\(\\boldsymbol{K}_t=\\boldsymbol{B}_{t-1}\\boldsymbol{x}_tq_t^{-1}\\), \\(\\boldsymbol{b}_t=\\boldsymbol{b}_{t-1}+\\boldsymbol{K}_t e_t\\), and \\(\\boldsymbol{B}_t=\\boldsymbol{B}_{t-1}-\\boldsymbol{B}_{t-1}\\boldsymbol{x}_t\\boldsymbol{K}_t^{\\top}+\\boldsymbol{H}_t\\boldsymbol{H}_t^{\\top}\\), where \\(\\boldsymbol{b}_0=\\boldsymbol{0}\\) and \\(\\boldsymbol{B}_0=\\boldsymbol{H}_0\\boldsymbol{H}_0^{\\top}\\). See system 2 in De Jong and Shephard (1995) for a more general case. We should save \\(e_t\\) (innovation vector), \\(q_t\\) (scale innovation variance) and \\(\\boldsymbol{K}_t\\) (Kalman gain) from this recursion. Then, setting \\(\\boldsymbol{r}_T=0\\) and \\(\\boldsymbol{M}_T=\\boldsymbol{0}\\), we run backwards from \\(t=T-1, T-2, \\dots, 1\\), the following recursions: \\(\\boldsymbol{\\Lambda}_{t+1}=\\boldsymbol{H}_{t+1}\\boldsymbol{H}_{t+1}^{\\top}\\), \\(\\boldsymbol{C}_{t+1}=\\boldsymbol{\\Lambda}_{t+1}-\\boldsymbol{\\Lambda}_{t+1}\\boldsymbol{M}_{t+1}\\boldsymbol{\\Lambda}_{t+1}^{\\top}\\), \\(\\boldsymbol{\\xi}_{t+1}\\sim N(\\boldsymbol{0}_K,\\sigma^2\\boldsymbol{C}_{t+1})\\), \\(\\boldsymbol{L}_{t+1}=\\boldsymbol{I}_K-\\boldsymbol{K}_{t+1}\\boldsymbol{x}_{t+1}^{\\top}\\), \\(\\boldsymbol{V}_{t+1}=\\boldsymbol{\\Lambda}_{t+1}\\boldsymbol{M}_{t+1}\\boldsymbol{L}_{t+1}\\), \\(\\boldsymbol{r}_{t}=\\boldsymbol{x}_{t+1} e_{t+1}/q_{t+1} + \\boldsymbol{L}_{t+1}^{\\top}\\boldsymbol{r}_{t+1}-\\boldsymbol{V}_{t+1}^{\\top}\\boldsymbol{C}_{t+1}^{-1}\\boldsymbol{\\xi}_{t+1}\\), \\(\\boldsymbol{M}_{t}=\\boldsymbol{x}_{t+1}\\boldsymbol{x}_{t+1}^{\\top}/q_{t+1}+\\boldsymbol{L}_{t+1}^{\\top}\\boldsymbol{M}_{t+1}\\boldsymbol{L}_{t+1}+\\boldsymbol{V}_{t+1}^{\\top}\\boldsymbol{C}_{t+1}^{-1}\\boldsymbol{V}_{t+1}\\), and \\(\\boldsymbol{\\eta}_{t+1}=\\boldsymbol{\\Lambda}_{t+1}\\boldsymbol{r}_{t+1}+\\boldsymbol{\\xi}_{t+1}\\). De Jong and Shephard (1995) show that \\(\\boldsymbol{\\eta}=[\\boldsymbol{\\eta}_1^{\\top} \\ \\dots \\ \\boldsymbol{\\eta}_T^{\\top}]^{\\top}\\) is drawn from \\(p(\\boldsymbol{H}_t\\boldsymbol{\\epsilon}_t\\mid y_t,\\boldsymbol{x}_t,\\boldsymbol{z}_t,\\boldsymbol{h}_t,\\boldsymbol{H}_t,\\boldsymbol{\\alpha},\\sigma^2, t=1,2,\\dots,T)\\). Thus, we can recover \\(\\boldsymbol{\\beta}_t\\) using (8.4) and \\(\\boldsymbol{\\beta}_0=\\boldsymbol{0}_K\\). We assume in the model given by Equations (8.3) and (8.4) that \\(\\boldsymbol{h}_t=[1 \\ 0 \\ \\dots \\ 0]^{ \\top}\\) and \\(\\boldsymbol{H}_t=[\\boldsymbol{0}_K \\ \\text{diag}\\left\\{1/\\tau_1\\dots1/\\tau_K\\right\\}]\\), and then perform Bayesian inference assuming independent priors, that is, \\(\\pi(\\boldsymbol{\\beta}_0,\\boldsymbol{\\alpha},\\sigma^2,\\boldsymbol{\\tau})=\\pi(\\boldsymbol{\\beta}_0)\\pi(\\boldsymbol{\\alpha})\\pi(\\sigma^2)\\prod_{k=1}^K\\pi(\\tau_k^2)\\) where \\(\\sigma^2\\sim IG(\\alpha_0/2,\\delta_0/2)\\), \\(\\tau_k^2\\sim G(v_{0}/2,v_{0}/2)\\), \\(k=1,\\dots,K\\), \\(\\boldsymbol{\\alpha}\\sim N(\\boldsymbol{a}_0,\\boldsymbol{A}_0)\\) and \\(\\boldsymbol{\\beta}_0\\sim N(\\boldsymbol{b}_0,\\boldsymbol{B}_0)\\). The conditional posterior distributions are \\(\\sigma^2\\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{Z},\\boldsymbol{\\beta}_{0:T},\\boldsymbol{\\alpha},\\boldsymbol{\\tau}\\sim IG(\\alpha_{n}/2,\\delta_n/2)\\), where \\(\\delta_n=\\sum_{t=1}^T\\left[(\\boldsymbol{\\beta}_t-\\boldsymbol{\\beta}_{t-1})^{\\top}\\boldsymbol{\\Psi}(\\boldsymbol{\\beta}_t-\\boldsymbol{\\beta}_{t-1})+(y_t-\\boldsymbol{z}_t^{\\top}\\boldsymbol{\\alpha}-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}_t)^{\\top}(y_t-\\boldsymbol{z}_t^{\\top}\\boldsymbol{\\alpha}-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}_t)\\right]+\\delta_0\\) and \\(\\alpha_{n}=T(K+1)+\\alpha_0\\), \\(\\boldsymbol{\\tau}=[\\tau_1 \\ \\dots \\ \\tau_K]\\), \\(\\boldsymbol{\\Psi}=\\text{diag}\\left\\{\\tau_1^2,\\dots,\\tau_K^2\\right\\}\\), and \\(\\tau_k^2\\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{Z},\\boldsymbol{\\beta}_{0:T},\\sigma^2\\sim G(v_{1n}/2,v_{2kn}/2)\\), where \\(v_{1n}=T+v_{0}\\) and \\(v_{2kn}=\\sigma^{-2}\\sum_{t=1}^T(\\boldsymbol{\\beta}_{t,k}-\\boldsymbol{\\beta}_{t-1,k})^2+v_{0}\\), and \\(\\boldsymbol{\\alpha}\\mid \\boldsymbol{y},\\boldsymbol{X},\\boldsymbol{Z},\\sigma^2,\\boldsymbol{\\beta}_{1:T},\\boldsymbol{\\tau}\\sim N(\\boldsymbol{a}_n,\\boldsymbol{A}_n)\\), where \\(\\boldsymbol{A}_n=(\\boldsymbol{A}_0^{-1}+\\sigma^{-2}\\sum_{t=1}^T\\boldsymbol{z}_t\\boldsymbol{z}_t^{\\top})^{-1}\\) and \\(\\boldsymbol{a}_n=\\boldsymbol{A}_n(\\boldsymbol{A}_0^{-1}\\boldsymbol{a}_0+\\sigma^{-2}\\sum_{t=1}^T\\boldsymbol{z}_t(y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}_t))\\). The vector of the dependent variable is \\(\\boldsymbol{y}\\), and all regressors are in \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\). We can see that all the previous posterior distributions are conditional on the state vector \\(\\boldsymbol{\\beta}_{0:T}\\), which can be sampled using the simulation smoother algorithm, conditional on draws of the time-invariant parameters. Thus, the state space model provides an excellent illustration of the modular nature of the Bayesian framework, where performing inference on more complex models often simply involves adding new blocks to an MCMC algorithm. This means we can break down a complex inferential problem into smaller, more manageable parts, which is a “divide and conquer” approach. This is possible due to the structure of the conditional posterior distributions. Exercise 3 asks you to perform a simulation of the model given by Equations (8.3) and (8.4), and to program the MCMC algorithm, including the simulation smoother. References "],["sec82.html", "8.2 ARMA processes", " 8.2 ARMA processes Since the seminal work of Box and Jenkins (1976), autoregressive moving average (ARMA) models have become ubiquitous in time series analysis. Thus, we present a brief introduction to these models in this section. Let’s start with the linear Gaussian model with autoregressive errors: \\[\\begin{align} y_t &amp;= \\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta} + \\mu_t \\tag{8.5} \\\\ \\phi(L) \\mu_t &amp;= \\epsilon_t \\tag{8.6} \\end{align}\\] where \\(\\boldsymbol{x}_t\\) is a \\(K\\)-dimensional vector of regressors, \\(\\epsilon_t \\sim \\text{iid} \\, N(0, \\sigma^2)\\), and \\(\\phi(L) = 1 - \\phi_1 L - \\phi_2 L^2 - \\dots - \\phi_p L^p\\) is a polynomial in the lag operator \\(L\\), where \\(L z_t = z_{t-1}\\), and in general, \\(L^r z_t = z_{t-r}\\). Thus, we see that the stochastic error \\(\\mu_t\\) follows an autoregressive process of order \\(p\\), i.e., \\(\\mu_t \\sim AR(p)\\). It is standard practice to assume that \\(\\mu_t\\) is second-order stationary, meaning the mean, variance, and autocovariance of \\(\\mu_t\\) are finite and independent of \\(t\\) and \\(s\\), although \\(\\mathbb{E}[\\mu_t \\mu_s]\\) may depend on \\(|t - s|\\). Then, all roots of \\(\\phi(L)\\) lie outside the unit circle. For instance, for an \\(AR(1)\\), \\(1 - \\phi_1 L = 0\\), implying \\(L = 1/\\phi_1\\), such that \\(|\\phi_1| &lt; 1\\) for the process to be second-order stationary. The likelihood function conditional on the first \\(p\\) observations is: \\[\\begin{align*} p(y_{p+1}, \\dots, y_T \\mid y_{p}, \\dots, y_1, \\boldsymbol{\\theta}) &amp;= \\prod_{t=p+1}^{T} p(y_t \\mid \\mathcal{I}_{t-1}, \\boldsymbol{\\theta}) \\\\ &amp;\\propto \\sigma^{-(T-p)} \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\sum_{t=p+1}^T \\left(y_t - \\hat{y}_{t \\mid t-1, \\boldsymbol{\\theta}}\\right)^2 \\right\\} \\end{align*}\\] where \\(\\mathcal{I}_{t-1}\\) is the past information, \\(\\boldsymbol{\\theta}\\) collects all parameters \\((\\boldsymbol{\\beta}, \\phi_1, \\dots, \\phi_p, \\sigma^2)\\), and \\(\\hat{y}_{t \\mid t-1, \\boldsymbol{\\theta}} = (1 - \\phi(L)) y_t + \\phi(L) \\boldsymbol{x}^{\\top} \\boldsymbol{\\beta}\\). We can see that multiplying the first expression in Equation (8.5) by \\(\\phi(L)\\), we can express the model as \\[\\begin{align} y_t^*=\\boldsymbol{x}_t^{*\\top}\\boldsymbol{\\beta}+\\epsilon_t \\tag{8.7} \\end{align}\\] where \\(y_t^*=\\phi(L)Y_t\\) and \\(\\boldsymbol{x}_t^{*}=\\phi(L)\\boldsymbol{x}_t\\). Thus, collecting all observations \\(t=p+1,p+2,\\dots,T\\), we have \\(\\boldsymbol{y}^*=\\boldsymbol{X}^*\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\epsilon}\\sim N(\\boldsymbol{0},\\sigma^2\\boldsymbol{I}_{T-p})\\), \\(\\boldsymbol{y}^*\\) is a \\(T-p\\) dimensional vector, and \\(\\boldsymbol{X}^*\\) is a \\((T-p)\\times K\\) dimensional matrix. Assuming that \\(\\boldsymbol{\\beta}\\mid \\sigma\\sim N(\\boldsymbol{\\beta}_0,\\sigma^2\\boldsymbol{B}_0)\\), \\(\\sigma^2\\sim IG(\\alpha_0/2,\\delta_0/2)\\) and \\(\\boldsymbol{\\phi}\\sim N(\\boldsymbol{\\phi}_0,\\boldsymbol{\\Phi}_0)\\mathbb{1}(\\boldsymbol{\\phi}\\in S_{\\boldsymbol{\\phi}})\\), where \\(S_{\\boldsymbol{\\phi}}\\) is the stationary region of \\(\\boldsymbol{\\phi}=[\\phi_1 \\ \\dots \\ \\phi_p]^{\\top}\\). Then, Equation (8.7) implies that \\(\\boldsymbol{\\beta}\\mid \\sigma^2,\\boldsymbol{\\phi},\\boldsymbol{y},\\boldsymbol{X}\\sim N(\\boldsymbol{\\beta}_n, \\sigma^2{\\boldsymbol{B}}_n)\\), where \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\boldsymbol{X}^{*\\top}\\boldsymbol{X}^{*})^{-1}\\) and \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{X}^{*\\top}\\boldsymbol{y}^{*})\\). In addition, \\(\\sigma^2\\mid \\boldsymbol{\\beta},\\boldsymbol{\\phi},\\boldsymbol{y},\\boldsymbol{X}\\sim IG(\\alpha_n/2,\\delta_n/2)\\) where \\(\\alpha_n=\\alpha_0+T-p\\) and \\(\\delta_n=\\delta_0+(\\boldsymbol{y}^*-\\boldsymbol{X}^{*}\\boldsymbol{\\beta})^{\\top}(\\boldsymbol{y}^*-\\boldsymbol{X}^{*}\\boldsymbol{\\beta})+(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\boldsymbol{B}_0^{-1}(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_0)\\). Thus, the previous conditional posterior distributions imply that we can use a Gibbs sampling algorithm to perform inference of these parameters (Chib 1993). We know from Equation q(eq:eq1) that \\(\\mu_t=y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}\\), from Equation (8.6) that \\(\\mu_t=\\phi_1\\mu_{t-1}+\\dots+\\phi_p\\mu_{t-p}+\\epsilon_t\\), \\(t=p+1,\\dots,T\\). In matrix notation \\(\\boldsymbol{\\mu}=\\boldsymbol{U}\\boldsymbol{\\phi}+\\boldsymbol{\\epsilon}\\), where \\(\\boldsymbol{\\mu}\\) is a \\(T-p\\) dimensional vector, \\(\\boldsymbol{U}\\) is a \\((T-p)\\times p\\) matrix whose \\(t\\)-th row is \\([\\mu_{t-1} \\ \\dots \\ \\mu_{t-p}]\\). Thus, the posterior distribution of \\(\\boldsymbol{\\phi}\\mid \\boldsymbol{\\beta},\\sigma^2,\\boldsymbol{y},\\boldsymbol{X}\\) is \\(N(\\boldsymbol{\\phi}_n, \\boldsymbol{\\Phi}_n)\\mathbb{1}(\\boldsymbol{\\phi}\\in S_{\\boldsymbol{\\phi}})\\), where \\(\\boldsymbol{\\Phi}_n=(\\boldsymbol{\\Phi}_0^{-1}+\\sigma^{-2}\\boldsymbol{U}^{\\top}\\boldsymbol{U})\\) and \\(\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}_n(\\boldsymbol{\\Phi}_0^{-1}\\boldsymbol{\\phi}_0+\\sigma^{-2}\\boldsymbol{U}^{\\top}\\boldsymbol{\\mu})\\) (see Exercise 4). Drawing from the model under the stationarity restriction is straightforward: we simply sample from the multivariate normal distribution and discard draws that do not satisfy the stationarity condition. The proportion of draws that meet this restriction represents the conditional probability that the process is stationary. Example: Effects of inflation on interest rate II We specify a dynamic linear model in the example of the effects of inflation on interest rates to account for a potential dynamic relationship. However, we can introduce dynamics in this model by assuming \\[ \\Delta i_t = \\beta_{1} + \\beta_{2} \\Delta inf_t + \\beta_{3} \\Delta def_t + \\mu_t, \\] where \\(\\mu_t = \\phi \\mu_{t-1} + \\epsilon_t\\). This leads to the model: \\[ \\Delta i_t = \\beta_{1}(1-\\phi_1) + \\phi_1 \\Delta i_{t-1} + \\beta_{2}(\\Delta inf_t - \\phi_1 \\Delta inf_{t-1}) + \\beta_{3}(\\Delta def_t - \\phi_1 \\Delta def_{t-1}) + \\epsilon_t. \\] Thus, we again use the dataset 16INTDEF.csv provided by Wooldridge (2016) to illustrate linear regressions with \\(AR(1)\\) errors. The following code demonstrates how to implement this application using vague priors, assuming \\(\\alpha_0 = \\delta_0 = 0.01\\), \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}\\), \\(\\boldsymbol{B}_0 = \\boldsymbol{I}\\), \\(\\boldsymbol{\\phi}_0 = \\boldsymbol{0}\\), and \\(\\boldsymbol{\\Phi}_0 = \\boldsymbol{I}\\). We use 15,000 MCMC iterations, with a burn-in of 5,000 and a thinning parameter of 5. rm(list = ls()) set.seed(010101) DataIntRate &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/16INTDEF.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(DataIntRate) yt &lt;- diff(i3); ytlag &lt;- dplyr::lag(yt, n = 1) T &lt;- length(yt) Xt &lt;- cbind(diff(inf), diff(def)); Xtlag &lt;- dplyr::lag(Xt, n = 1) K &lt;- dim(Xt)[2] + 1 Reg &lt;- lm(yt ~ ytlag + I(Xt[,-1] - Xtlag)) SumReg &lt;- summary(Reg); SumReg ## ## Call: ## lm(formula = yt ~ ytlag + I(Xt[, -1] - Xtlag)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.85560 -0.86022 0.04917 0.95966 2.85684 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07375 0.19570 0.377 0.7081 ## ytlag 0.13495 0.15530 0.869 0.3897 ## I(Xt[, -1] - Xtlag)1 -0.13756 0.07594 -1.811 0.0771 . ## I(Xt[, -1] - Xtlag)2 -0.14807 0.09212 -1.607 0.1153 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.331 on 43 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1506, Adjusted R-squared: 0.09137 ## F-statistic: 2.542 on 3 and 43 DF, p-value: 0.06878 PostSig2 &lt;- function(Beta, Phi){ Xstar&lt;- matrix(NA, T-1, K - 1) ystar &lt;- matrix(NA, T-1, 1) for(t in 2:T){ Xstar[t-1,] &lt;- Xt[t,] - Phi*Xt[t-1,] ystar[t-1,] &lt;- yt[t] - Phi*yt[t-1] } Xstar &lt;- cbind(1, Xstar) an &lt;- T - 1 + a0 dn &lt;- d0 + t(ystar - Xstar%*%Beta)%*%(ystar - Xstar%*%Beta) + t(Beta - b0)%*%B0i%*%(Beta - b0) sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2, Phi){ Xstar&lt;- matrix(NA, T-1, K - 1) ystar &lt;- matrix(NA, T-1, 1) for(t in 2:T){ Xstar[t-1,] &lt;- Xt[t,] - Phi*Xt[t-1,] ystar[t-1,] &lt;- yt[t] - Phi*yt[t-1] } Xstar &lt;- cbind(1, Xstar) XtXstar &lt;- t(Xstar)%*%Xstar Xtystar &lt;- t(Xstar)%*%ystar Bn &lt;- solve(B0i + XtXstar) bn &lt;- Bn%*%(B0i%*%b0 + Xtystar) Beta &lt;- MASS::mvrnorm(1, bn, sig2*Bn) return(Beta) } PostPhi &lt;- function(sig2, Beta){ u &lt;- yt - cbind(1,Xt)%*%Beta U &lt;- u[-T] ustar &lt;- u[-1] UtU &lt;- t(U)%*%U Utu &lt;- t(U)%*%ustar Phin &lt;- solve(Phi0i + sig2^(-1)*UtU) phin &lt;- Phin%*%(Phi0i%*%phi0 + sig2^(-1)*Utu) Phi &lt;- truncnorm::rtruncnorm(1, a = -1, b = 1, mean = phin, sd = Phin^0.5) return(Phi) } # Hyperparameters d0 &lt;- 0.01; a0 &lt;- 0.01 b0 &lt;- rep(0, K); c0 &lt;- 1; B0 &lt;- c0*diag(K); B0i &lt;- solve(B0) phi0 &lt;- 0; Phi0 &lt;- 1; Phi0i &lt;- 1/Phi0 # MCMC parameters mcmc &lt;- 15000 burnin &lt;- 5000 tot &lt;- mcmc + burnin thin &lt;- 1 PostBetas &lt;- matrix(0, mcmc+burnin, K) PostSigma2s &lt;- rep(0, mcmc+burnin) PostPhis &lt;- rep(0, mcmc+burnin) Beta &lt;- rep(0, K); Phi &lt;- 0 sig2 &lt;- SumReg$sigma^2; Phi &lt;- SumReg$coefficients[2,1] Beta &lt;- SumReg$coefficients[c(1,3,4),1] pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ sig2 &lt;- PostSig2(Beta = Beta, Phi = Phi) PostSigma2s[s] &lt;- sig2 Beta &lt;- PostBeta(sig2 = sig2, Phi = Phi) PostBetas[s,] &lt;- Beta Phi &lt;- PostPhi(sig2 = sig2, Beta = Beta) PostPhis[s] &lt;- Phi setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), &quot;% done&quot;)) } close(pb) ## NULL keep &lt;- seq((burnin+1), tot, thin) PosteriorBetas &lt;- coda::mcmc(PostBetas[keep,]) summary(PosteriorBetas) ## ## Iterations = 1:15000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 15000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.0663 0.1904 0.0015545 0.001554 ## [2,] 0.2293 0.1171 0.0009563 0.001163 ## [3,] -0.1451 0.1704 0.0013913 0.001391 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 -0.307875 -0.05914 0.06597 0.19276 0.4392 ## var2 -0.004462 0.15191 0.22912 0.30839 0.4564 ## var3 -0.475203 -0.25696 -0.14711 -0.03156 0.1918 PosteriorSigma2 &lt;- coda::mcmc(PostSigma2s[keep]) summary(PosteriorSigma2) ## ## Iterations = 1:15000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 15000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 1.698250 0.385326 0.003146 0.003462 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 1.110 1.423 1.643 1.909 2.611 PosteriorPhi &lt;- coda::mcmc(PostPhis[keep]) summary(PosteriorPhi) ## ## Iterations = 1:15000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 15000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 0.020208 0.163454 0.001335 0.001705 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## -0.28977 -0.09105 0.01602 0.12843 0.34971 dfBinf &lt;- as.data.frame(PosteriorBetas[,2]) # Basic density library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3 p &lt;- ggplot(dfBinf, aes(x=var1)) + geom_density(color=&quot;darkblue&quot;, fill=&quot;lightblue&quot;) + geom_vline(aes(xintercept=mean(var1)), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, linewidth=1) + geom_vline(aes(xintercept=quantile(var1, 0.025)), color=&quot;red&quot;, linetype=&quot;dashed&quot;, linewidth=1) + geom_vline(aes(xintercept=quantile(var1, 0.975)), color=&quot;red&quot;, linetype=&quot;dashed&quot;, linewidth=1) + labs(title=&quot;Density effect of inflation on interest rate&quot;, x=&quot;Effect of inflation&quot;, y = &quot;Density&quot;) p This figure shows the posterior density plot of the effects of inflation rate on interest rate. The posterior mean of this coefficient is approximately 0.25, and the credible interval at 95% is (0, 0.46), which indicates again that the annual changes in interest rate are weakly positive related to annual changes in inflation. Observe that the previous setting encompasses the particular relevant case \\(y_t\\sim AR(p)\\), it is just omitting the covariates such that \\(y_t=\\mu_t\\). Chib and Greenberg (1994) extend the Bayesian inference of linear regression with \\(AR(p)\\) errors to \\(ARMA(p,q)\\) errors using a state-space representation. Setting \\(y_t=\\mu_t\\) such that \\(y_t=\\sum_{s=1}^{p}\\phi_jy_{t-s}+\\sum_{s=1}^{q}\\theta_s \\epsilon_{t-s}+\\epsilon_t\\), letting \\(r=\\max \\left\\{p,q+1\\right\\}\\), \\(\\phi_s=0\\) for \\(s&gt;p\\) and \\(\\theta_s=0\\) for \\(s&gt;q\\), and defining \\(\\boldsymbol{x}^{\\top}=[1 \\ 0 \\ \\dots \\ 0]\\), and \\(\\boldsymbol{H}=[1 \\ \\psi_1 \\ \\dots \\ \\psi_{r-1}]^{\\top}\\) \\(r\\)-dimensional vectors, and \\[\\begin{align*} \\boldsymbol{G}=\\begin{bmatrix} \\phi_1 &amp; 1 &amp; 0 &amp; \\dots &amp; 0\\\\ \\phi_2 &amp; 0 &amp; 1 &amp; \\dots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; &amp;\\\\ \\phi_{r-1} &amp; 0 &amp; 0 &amp; \\dots &amp; 1\\\\ \\phi_r &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\ \\end{bmatrix} = \\begin{bmatrix} \\phi_1 &amp; \\vdots &amp; &amp; &amp; \\\\ \\phi_2 &amp; \\vdots &amp; &amp; \\boldsymbol{I}_{r-1} &amp; \\\\ \\vdots &amp; \\vdots &amp; &amp; &amp;\\\\ \\dots &amp; \\dots &amp; \\dots &amp; \\dots &amp; \\dots\\\\ \\phi_r &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\ \\end{bmatrix}, \\end{align*}\\] which is a \\(r\\times r\\) dimensional matrix, and give the state vector \\(\\boldsymbol{\\beta}_t=[\\beta_{1,t} \\ \\beta_{2,t} \\ \\dots \\ \\beta_{r,t}]^{\\top}\\), the \\(ARMA\\) model has the following representation: \\[\\begin{align*} y_t&amp;=\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}_t\\\\ \\boldsymbol{\\beta}_t &amp;= \\boldsymbol{G}\\boldsymbol{\\beta}_{t-1}+\\boldsymbol{H}\\epsilon_{t}. \\end{align*}\\] This is a dynamic linear model where \\(\\boldsymbol{\\Sigma}_t=0\\), and \\(\\boldsymbol{\\Omega}_t=\\sigma^2\\boldsymbol{H}\\boldsymbol{H}^{\\top}\\) (see Petris, Petrone, and Campagnoli (2009) and Chib and Greenberg (1994)). A notable advantage of the state-space representation of the \\(ARMA\\) model is that the evaluation of the likelihood can be performed efficiently using the recursive laws. Extensions to autoregressive integrated moving average \\(ARIMA(p,d,q)\\) models are discussed in Petris, Petrone, and Campagnoli (2009). In \\(ARIMA(p,d,q)\\) models, \\(d\\) refers to the level of integration (or differencing) required to eliminate the stochastic trend in a time series (see Enders (2014) for details). Example: \\(AR(2)\\) process Let’s see the state-space representation of a stationary \\(AR(2)\\) process with intercept, that is, \\(y_t=\\mu+\\phi_1y_{t-1}+\\phi_2y_{t-2}+\\epsilon_t\\), where \\(\\epsilon_t\\sim N(0,\\sigma^2)\\). Thus, \\(\\mathbb{E}[y_t]=\\frac{\\mu}{1-\\phi_1-\\phi_2}\\), and variance \\(Var[y_t]=\\frac{\\sigma^2(1-\\phi_2)}{1-\\phi_2-\\phi_1^2-\\phi_1^2\\phi_2-\\phi_2^2+\\phi_2^3}\\). In addition, we can proof that setting \\(z_t=Y_t-\\bar{\\mu}\\), we have \\(z_t=\\phi_1z_{t-1}+\\phi_2z_{t-2}+\\epsilon_t\\) where \\(\\mathbb{E}[z_t]=0\\), and these are equivalent representations (see Exercise 5). Then, setting \\(\\boldsymbol{x}^{\\top}=[1 \\ 0]\\), \\(\\boldsymbol{H}=[1 \\ 0]^{\\top}\\), \\(\\boldsymbol{G}=\\begin{bmatrix} \\phi_1 &amp; 1\\\\ \\phi_2 &amp; 0 \\\\ \\end{bmatrix}\\), \\(\\boldsymbol{\\beta}_t=[\\beta_{t1} \\ \\beta_{t2}]^{\\top}\\), \\(\\boldsymbol{\\Sigma}_t=0\\) and \\(\\boldsymbol{\\Omega}_t=\\sigma^2\\) we have \\[\\begin{align*} z_t&amp;=\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}_t&amp; \\text{(Observation equation)}\\\\ \\boldsymbol{\\beta}_t&amp;=\\boldsymbol{G}\\boldsymbol{\\beta}_{t-1}+\\boldsymbol{H}{\\epsilon}_t &amp; \\text{(States equations)}. \\end{align*}\\] We use the function stan_sarima from the package bayesforecast to perform Bayesian inference in \\(ARMA\\) models in our GUI. The following code shows how to simulate an \\(AR(2)\\) process, and perform Bayesian inference using this function. We perform 10000 MCMC iterations plus a burn-in equal 5000 assuming \\(\\sigma^2\\sim IG(0.01/2, 0.01/2)\\), \\(\\mu\\sim N(0, 1)\\) and \\(\\phi_k\\sim N(0, 1)\\), \\(k=1,2\\). The trace plots look well, and all 95% credible intervals encompass the population values. rm(list = ls()); set.seed(010101) T &lt;- 200; mu &lt;- 0.5 phi1 &lt;- 0.5; phi2 &lt;- 0.3; sig &lt;- 0.5 Ey &lt;- mu/(1-phi1-phi2); Sigy &lt;- sig*((1-phi2)/(1-phi2-phi1^2-phi2*phi1^2-phi2^2+phi2^3))^0.5 y &lt;- rnorm(T, mean = Ey, sd = Sigy) e &lt;- rnorm(T, mean = 0, sd = sig) for(t in 3:T){ y[t] &lt;- mu + phi1*y[t-1] + phi2*y[t-2] + e[t] } mean(y); sd(y) y &lt;- ts(y, start=c(1820, 1), frequency=1) plot(y) iter &lt;- 10000; burnin &lt;- 5000; thin &lt;- 1; tot &lt;- iter + burnin library(bayesforecast) sf1 &lt;- bayesforecast::stan_sarima(y, order = c(2, 0, 0), prior_mu0 = normal(0, 1), prior_ar = normal(0, 1), prior_sigma0 = inverse.gamma(0.01/2, 0.01/2), seasonal = c(0, 0, 0), iter = tot, warmup = burnin, chains = 1) keep &lt;- seq(burnin+1, tot, thin) Postmu &lt;- sf1[[&quot;stanfit&quot;]]@sim[[&quot;samples&quot;]][[1]][[&quot;mu0&quot;]][keep] Postsig &lt;- sf1[[&quot;stanfit&quot;]]@sim[[&quot;samples&quot;]][[1]][[&quot;sigma0&quot;]][keep] Postphi1 &lt;- sf1[[&quot;stanfit&quot;]]@sim[[&quot;samples&quot;]][[1]][[&quot;ar0[1]&quot;]][keep] Postphi2 &lt;- sf1[[&quot;stanfit&quot;]]@sim[[&quot;samples&quot;]][[1]][[&quot;ar0[2]&quot;]][keep] Postdraws &lt;- coda::mcmc(cbind(Postmu, Postsig, Postphi1, Postphi2)) summary(Postdraws) The following Algorithm shows how to perform inference in \\(ARMA(p,q)\\) models using our GUI. See also Chapter 5 for details regarding the dataset structure. Algorithm: Autoregressive Moving Average (ARMA) Models Select Time series Model on the top panel Select ARMA using the left radio button Upload the dataset selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Set the order of the ARMA model, p and q parameters Set the frequency: annual (1), quarterly (4), monthly (12), etc. Set the location and scale hyperparameters of the intercept, autoregressive (AR), moving average (MA) and standard deviation. Take into account that there is just one set of hyperparameters for AR and MA coefficients. This step is not necessary as by default our GUI uses non-informative priors Click the Go! button Analyze results Download posterior chains using the Download Posterior Chains button The function stan_sarima uses the Stan software (Stan Development Team 2024), which in turn employs Hamiltonian Monte Carlo (HMC). The following code illustrates how to perform Bayesian inference in the \\(AR(2)\\) model by programming the HMC from scratch. It is important to note that this is only an illustration, as HMC is less efficient than the Gibbs sampler in this example. However, HMC can outperform traditional MCMC algorithms in more complex models, particularly when dealing with high-dimensional probability distributions or when MCMC struggles with poor mixing due to posterior correlation. In the first block, we perform the simulation by setting \\(\\mu=0.5\\), \\(\\phi_1=0.5\\), \\(\\phi_2=0.3\\), \\(\\sigma=0.25\\), and a sample size of 200. We then set the hyperparameters and define the function to calculate the logarithm of the posterior distribution. The model is parametrized using \\(\\tau = \\log(\\sigma^2)\\), such that \\(\\sigma^2=\\exp(\\tau)\\), which avoids issues related to the non-negativity restriction of \\(\\sigma^2\\). As a result, we need to account for the Jacobian due to this transformation, specifically \\(d\\sigma^2/d\\tau = \\exp(\\tau)\\). Next, we define the function to compute the gradient vector of the log posterior distribution. It is preferable to calculate the gradient vector analytically, as using finite differences can be computationally expensive. However, it is a good practice to check the analytical calculations by evaluating the function at the maximum posterior estimate, where the function should return values close to 0, or by comparing the results with finite differences at a few evaluation points. The posterior distribution is given by1 \\[\\begin{align*} \\pi(\\mu,\\phi_1,\\phi_2,\\tau\\mid \\boldsymbol{y})&amp;\\propto \\prod_{t=3}^T(\\exp(\\tau))^{-1/2}\\exp\\left\\{-\\frac{1}{2\\exp(\\tau)}(y_t-\\mu-\\phi_1y_{t-1}-\\phi_2y_{t-2})^2\\right\\}\\\\ &amp;\\times\\exp\\left\\{-\\frac{1}{2\\sigma^2_{\\mu}}(\\mu-\\mu_0)^2\\right\\}\\times\\exp\\left\\{-\\frac{1}{2\\sigma^2_{\\phi_1}}(\\phi_1-\\phi_{10})^2\\right\\}\\\\ &amp;\\times\\exp\\left\\{-\\frac{1}{2\\sigma^2_{\\phi_2}}(\\phi_2-\\phi_{20})^2\\right\\}\\times\\exp\\left\\{-(\\alpha_0/2+1)\\tau\\right\\}\\exp\\left\\{-\\delta_0/(2\\exp(\\tau))\\right\\}\\exp(\\tau). \\end{align*}\\] The components of the gradient vector of the log posterior distribution are given by \\[\\begin{align*} \\frac{\\partial \\log(\\pi(\\mu,\\phi_1,\\phi_2,\\tau\\mid \\boldsymbol{y}))}{\\partial\\mu}&amp;=\\frac{\\sum_{t=3}^T(y_t-\\mu-\\phi_1y_{t-1}-\\phi_2y_{t-2})}{\\exp(\\tau)}-\\frac{1}{\\sigma_{\\mu}^2}(\\mu-\\mu_0)\\\\ \\frac{\\partial\\log(\\pi(\\mu,\\phi_1,\\phi_2,\\tau\\mid \\boldsymbol{y}))}{\\partial\\phi_1}&amp;=\\frac{\\sum_{t=3}^T(y_t-\\mu-\\phi_1y_{t-1}-\\phi_2y_{t-2})y_{t-1}}{\\exp(\\tau)}-\\frac{1}{\\sigma_{\\phi_1}^2}(\\phi_1-\\phi_{10})\\\\ \\frac{\\partial\\log(\\pi(\\mu,\\phi_1,\\phi_2,\\tau\\mid \\boldsymbol{y}))}{\\partial\\phi_2}&amp;=\\frac{\\sum_{t=3}^T(y_t-\\mu-\\phi_1y_{t-1}-\\phi_2y_{t-2})y_{t-2}}{\\exp(\\tau)}-\\frac{1}{\\sigma_{\\phi_2}^2}(\\phi_2-\\phi_{20})\\\\ \\frac{\\partial\\log(\\pi(\\mu,\\phi_1,\\phi_2,\\tau\\mid \\boldsymbol{y}))}{\\partial\\tau}&amp;=-\\frac{(T-2)}{2}+\\frac{\\sum_{t=3}^T(y_t-\\mu-\\phi_1y_{t-1}-\\phi_2y_{t-2})^2}{2\\exp(\\tau)}\\\\ &amp;-(\\alpha_0/2+1)+\\delta_0/(2\\exp(\\tau))+1.\\\\ \\end{align*}\\] Next, we provide the code for the Hamiltonian Monte Carlo, as outlined in Chapter 4. The initial values are set as follows: \\(\\mu=\\bar{y}=\\frac{1}{T-2}\\sum_{t=3}^T y_t\\), \\(\\phi_1=\\phi_2=0\\), and \\(\\tau=\\exp\\left(\\frac{1}{T-2}\\sum_{t=3}^T(y_t-\\bar{y})^2\\right)\\), with \\(M\\) being the inverse covariance matrix of the posterior distribution evaluated at its maximum. Additionally, \\(\\epsilon\\) is randomly drawn from a uniform distribution between 0 and \\(2\\epsilon_0\\), and \\(L\\) is set to the highest integer near \\(1/\\epsilon\\), in order to approximately satisfy \\(L\\epsilon=1\\), where \\(\\epsilon_0=0.1\\). We can verify that all 95% credible intervals encompass the population values, and the posterior means are close to the population values. The acceptance rate averages above 65%, so we should consider increasing the base step (\\(\\epsilon_0\\)). Furthermore, we do not impose the stationarity conditions on \\(\\phi_1\\) and \\(\\phi_2\\). Exercise 6 asks to program an HMC that takes these requirements into account. # Simulation AR(2) rm(list = ls()); set.seed(010101); T &lt;- 1000; K &lt;- 4 mu &lt;- 0.5; phi1 &lt;- 0.5; phi2 &lt;- 0.3; sig &lt;- 0.5 Ey &lt;- mu/(1-phi1-phi2); Sigy &lt;- sig*((1-phi2)/(1-phi2-phi1^2-phi2*phi1^2-phi2^2+phi2^3))^0.5 y &lt;- rnorm(T, mean = Ey, sd = Sigy); e &lt;- rnorm(T, mean = 0, sd = sig) for(t in 3:T){ y[t] &lt;- mu + phi1*y[t-1] + phi2*y[t-2] + e[t] } # Hyperparameters d0 &lt;- 0.01; a0 &lt;- 0.01; mu0 &lt;- 0; MU0 &lt;- 1 phi0 &lt;- c(0, 0); Phi0 &lt;- diag(2) # Log posterior multiply by -1 to use optim LogPost &lt;- function(theta, y){ mu &lt;- theta[1]; phi1 &lt;- theta[2]; phi2 &lt;- theta[3] tau &lt;- theta[4]; sig2 &lt;- exp(tau); logLik &lt;- NULL for(t in 3:T){ logLikt &lt;- dnorm(y[t], mean = mu + phi1*y[t-1] + phi2*y[t-2], sd = sig2^0.5, log = TRUE) logLik &lt;- c(logLik, logLikt) } logLik &lt;- sum(logLik) logPrior &lt;- dnorm(mu, mean = mu0, sd = MU0^0.5, log = TRUE) + dnorm(phi1, mean = phi0[1], sd = Phi0[1,1]^0.5, log = TRUE) + dnorm(phi2, mean = phi0[2], sd = Phi0[2,2]^0.5, log = TRUE) + invgamma::dinvgamma(sig2, shape = a0/2, rate = d0/2, log = TRUE) logPosterior &lt;- logLik + logPrior + tau return(-logPosterior) # Multiply by -1 to minimize using optim } theta0 &lt;- c(mean(y), 0, 0, var(y)) Opt &lt;- optim(theta0, LogPost, y = y, hessian = TRUE) theta0 &lt;- Opt$par; VarPost &lt;- solve(Opt$hessian) # Gradient log posterior GradientTheta &lt;- function(theta, y){ mu &lt;- theta[1]; phi1 &lt;- theta[2]; phi2 &lt;- theta[3] tau &lt;- theta[4]; sig2 &lt;- exp(tau); SumLik &lt;- matrix(0, 3, 1) SumLik2 &lt;- NULL for(t in 3:T){ xt &lt;- matrix(c(1, y[t-1], y[t-2]), 3, 1) SumLikt &lt;- (y[t] - (mu + phi1*y[t-1] + phi2*y[t-2]))*xt SumLik2t &lt;- (y[t] - (mu + phi1*y[t-1] + phi2*y[t-2]))^2 SumLik &lt;- rowSums(cbind(SumLik, SumLikt)) SumLik2 &lt;- sum(SumLik2, SumLik2t) } Grad_mu &lt;- SumLik[1]/sig2 - (1/MU0)*(mu - mu0) Grad_phi1 &lt;- SumLik[2]/exp(tau) - 1/Phi0[1,1]*(phi1 - phi0[1]) Grad_phi2 &lt;- SumLik[3]/exp(tau) - 1/Phi0[2,2]*(phi2 - phi0[2]) Grad_tau &lt;- -(T-2)/2 + SumLik2/(2*exp(tau)) - (a0/2 + 1) + d0/(2*exp(tau)) + 1 Grad &lt;- c(Grad_mu, Grad_phi1, Grad_phi2, Grad_tau) return(Grad) } # Hamiltonian Monte Carlo function HMC &lt;- function(theta, y, epsilon, M){ L &lt;- ceiling(1/epsilon) Minv &lt;- solve(M); thetat &lt;- theta K &lt;- length(thetat) mom &lt;- t(mvtnorm::rmvnorm(1, rep(0, K), M)) logPost_Mom_t &lt;- -LogPost(thetat, y) + mvtnorm::dmvnorm(t(mom), rep(0, K), M, log = TRUE) for(l in 1:L){ if(l == 1 | l == L){ mom &lt;- mom + 0.5*epsilon*GradientTheta(theta, y) theta &lt;- theta + epsilon*Minv%*%mom }else{ mom &lt;- mom + epsilon*GradientTheta(theta, y) theta &lt;- theta + epsilon*Minv%*%mom } } logPost_Mom_star &lt;- -LogPost(theta, y) + mvtnorm::dmvnorm(t(mom), rep(0, K), M, log = TRUE) alpha &lt;- min(1, exp(logPost_Mom_star-logPost_Mom_t)) u &lt;- runif(1) if(u &lt;= alpha){ thetaNew &lt;- c(theta) }else{ thetaNew &lt;- thetat } rest &lt;- list(theta = thetaNew, Prob = alpha) return(rest) } # Posterior draws S &lt;- 1000; burnin &lt;- 1000; thin &lt;- 2; tot &lt;- S + burnin thetaPost &lt;- matrix(NA, tot, K) ProbAccept &lt;- rep(NA, tot) theta0 &lt;- c(mean(y), 0, 0, exp(var(y))) M &lt;- solve(VarPost); epsilon0 &lt;- 0.1 pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ epsilon &lt;- runif(1, 0, 2*epsilon0) L &lt;- ceiling(1/epsilon) HMCs &lt;- HMC(theta = theta0, y, epsilon, M) theta0 &lt;- HMCs$theta thetaPost[s,] &lt;- HMCs$theta ProbAccept[s] &lt;- HMCs$Prob setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), &quot;% done&quot;)) } close(pb) keep &lt;- seq((burnin+1), tot, thin) thetaF &lt;- coda::mcmc(thetaPost[keep,]) summary(thetaF) summary(exp(thetaF[,K])) ProbAcceptF &lt;- coda::mcmc(ProbAccept[keep]) summary(ProbAcceptF) References "],["sec83.html", "8.3 Stochastic volatility models", " 8.3 Stochastic volatility models A notable example of non-linear and non-Gaussian state-space models is stochastic volatility models (SVMs), which are widely used to model the volatility of financial returns. SVMs have gained significant attention due to their flexibility, ability to capture complex dynamics such as asymmetries, and ease of generalization to simultaneously model multiple returns, making them advantageous over generalized autoregressive conditional heteroskedasticity (GARCH) models proposed by Bollerslev (1986). However, estimating SVMs is more challenging than estimating GARCH models. This is because GARCH models set variance in a deterministic manner, whereas SVMs do so stochastically. Consequently, GARCH models are typically estimated using maximum likelihood methods, while SVMs require Bayesian approaches, adding complexity to the estimation process. The specification of the stochastic volatility model is given by \\[\\begin{align} y_t&amp;=\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta}+\\exp\\left\\{0.5h_t\\right\\}\\mu_t&amp; \\text{(Observation equation)} \\tag{8.8}\\\\ h_t&amp;=\\mu+\\phi(h_{t-1}-\\mu)+\\sigma w_t&amp; \\text{(State equation)} \\tag{8.9}, \\end{align}\\] where \\(y_t\\) are the log-returns, \\(\\boldsymbol{x}_t\\) are controls, \\(\\boldsymbol{\\beta}\\) are time-invariant location parameters, \\(\\mu_t\\sim N(0,1)\\), \\(w_t\\sim N(0,1)\\), \\(\\mu_t\\perp w_t\\), the initial log-variance process \\(h_0\\sim N(\\mu, \\sigma^2/(1-\\phi^2))\\), \\(\\mu\\), \\(\\phi\\) and \\(\\sigma\\) are the level, persistence and standard deviation of the log-variance, respectively. Given the specification in Equations (8.8) and (8.9), we can write the observation equation as \\[ \\log\\left\\{(y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta})^2\\right\\} = h_t + \\log(\\mu_t^2), \\] which leads to a linear, but non-Gaussian, state-space model. Kastner and Frühwirth-Schnatter (2014) approximate the distribution of \\(\\log(\\mu_t^2)\\) by a mixture of normal distributions, that is, \\[ \\log(\\mu_t^2)\\mid l_t \\sim N(m_{l_t},s_{l_t}^2), \\] where \\(l_t \\in \\{1, 2, \\dots, 10\\}\\) defines the mixture component indicator at time \\(t\\). Thus, the model can be written as \\[ \\log\\left\\{(y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta})^2\\right\\} = h_t + \\log(\\mu_t^2), \\] and \\[ h_t = \\mu + \\phi(h_{t-1} - \\mu) + \\sigma w_t. \\] This forms a linear and conditionally Gaussian state-space model, where \\[ \\log\\left\\{(y_t-\\boldsymbol{x}_t^{\\top}\\boldsymbol{\\beta})^2\\right\\} = m_{l_t} + h_t + \\mu_t^2, \\] and \\[ \\mu_t \\sim N(0, s_{l_t}^2). \\] We use the stochvol package in our GUI to perform MCMC inference in the SVMs (Hosszejni and Kastner 2021); this package is based on the MCMC algorithms proposed by Kastner and Frühwirth-Schnatter (2014). The default prior distributions in the stochvol package are: \\[ \\boldsymbol{\\beta} \\sim N(\\boldsymbol{b}_0, \\boldsymbol{B}_0), \\quad \\mu \\sim N(\\mu_0, \\sigma_{\\mu0}^2), \\quad \\frac{\\phi+1}{2} \\sim B(\\alpha_0, \\beta_0), \\quad \\sigma^2 \\sim G\\left(\\frac{1}{2}, \\frac{1}{2\\sigma^2_{\\sigma^2}}\\right). \\] The prior distribution for \\(\\phi\\) is set to ensure stationarity of the process (\\(\\phi \\in (-1,1)\\)). In most applications, \\(\\phi \\approx 1\\), so the authors of the package recommend setting \\(\\alpha_0 \\gtrsim 5\\) and \\(\\beta_0 \\approx 1.5\\). The prior distribution for \\(\\sigma\\) is \\(|N(0, \\sigma^2_{\\sigma^2})|\\) (a half-normal distribution). This is recommended by the authors since the conjugate inverse-gamma distribution does not work well in this case, as it bounds \\(\\sigma\\) away from 0, which is undesirable when modeling the log-variance of log-returns. The following Algorithm shows how to perform inference in stochastic volatility models using our GUI. See also Chapter 5 for details regarding the dataset structure. Algorithm: Stochastic Volatility Models Select Time series Model on the top panel Select Stochastic volatility using the left radio button Upload the dataset selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Set the hyperparameters: the mean and standard deviation of the Gaussian prior for the regression parameters, mean and standard deviation for the Gaussian prior distribution of the level of the log-volatility, shape parameters for the Beta prior distribution of the transformed persistence parameter, and the positive real number, which stands for the scaling of the transformed volatility of log-volatility. This step is not necessary as by default our GUI uses default values in the stochvol package Click the Go! button Analyze results Download posterior chains of the fixed coefficients, and the states using the Download Posterior Chains Fixed and Download Posterior States buttons Example: Simulation exercise of the stochastic volatility model The following code shows how to simulate and perform Bayesian inference in the stochastic volatility model using the function svsample from the stochvol package. We set the stochastic volatility parameters to \\(\\mu = -10\\), \\(\\phi = 0.95\\), and \\(\\sigma = 0.3\\). We assume two regressors, which are distributed as standard normal, with \\(\\boldsymbol{\\beta} = [0.5 \\ 0.3]^{\\top}\\), and the sample size is 1250, which corresponds to approximately 5 years of daily returns. We use the default hyperparameters: 10000 MCMC iterations, a burn-in of 5000, and a thinning parameter of 5. The summary statistics of the posterior draws show that all 95% credible intervals encompass the population parameters, and the posterior chains appear to have converged. The Figure displays the posterior results for the volatility (\\(h_t\\)). The posterior mean (blue) follows the “observed” series (black), and the 95% credible intervals (light blue) typically encompass the “observed” series. rm(list = ls()); set.seed(010101) T &lt;- 1250; K &lt;- 2 X &lt;- matrix(rnorm(T*K), T, K) B &lt;- c(0.5, 0.3); mu &lt;- -10; phi &lt;- 0.95; sigma &lt;- 0.3 h &lt;- numeric(T); y &lt;- numeric(T) h[1] &lt;- rnorm(1, mu, sigma / sqrt(1 - phi^2)) # Initial state y[1] &lt;- X[1,]%*%B + rnorm(1, 0, exp(h[1] / 2)) # Initial observation for (t in 2:T) { h[t] &lt;- mu + phi*(h[t-1]-mu) + rnorm(1, 0, sigma) y[t] &lt;- X[t,]%*%B + rnorm(1, 0, sd = exp(0.5*h[t])) } df &lt;- as.data.frame(cbind(y, X)) colnames(df) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;) MCMC &lt;- 10000; burnin &lt;- 10000; thin &lt;- 5 res &lt;- stochvol::svsample(y, designmatrix = X, draws = MCMC, burnin = burnin, thin = thin, priormu = c(0, 100), priorsigma = c(1), priorphi = c(5, 1.5), priorbeta = c(0, 10000)) summary(res[[&quot;para&quot;]][[1]][,-c(4,5)]) summary(res[[&quot;beta&quot;]]) ht &lt;- res[[&quot;latent&quot;]][[1]] library(dplyr) library(ggplot2) library(latex2exp) ggplot2::theme_set(theme_bw()) x_means &lt;- colMeans(ht) x_quantiles &lt;- apply(ht, 2, function(x) quantile(x, probs = c(0.025, 0.975))) df &lt;- tibble(t = seq(1, T), mean = x_means, lower = x_quantiles[1, ], upper = x_quantiles[2, ], x_true = h, observations = y) plot_filtering_estimates &lt;- function(df) { p &lt;- ggplot(data = df, aes(x = t)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1, fill = &quot;lightblue&quot;) + geom_line(aes(y = x_true), colour = &quot;black&quot;, alpha = 1, linewidth = 0.5) + geom_line(aes(y = mean), colour = &quot;blue&quot;, linewidth = 0.5) + ylab(TeX(&quot;$h_{t}$&quot;)) + xlab(&quot;Time&quot;) print(p) } plot_filtering_estimates(df) So far, we have used MCMC algorithms to perform inference in state-space models. These algorithms require all observations to estimate the unknown parameters, a process referred to as offline or batch inference. However, this approach has limitations when online inference is needed, as every new observation requires simulating a new posterior chain. This is because MCMC algorithms do not naturally adapt to sequential updates. In contrast, particle filter algorithms, which are a subset of sequential Monte Carlo (SMC) methods, are specifically designed for sequential use, making them suitable for online inference. Remember from Chapter 4 that particle filters (sequential Monte Carlo) are algorithms that allow computing a numerical approximation to the filtering distribution \\(\\pi(\\boldsymbol{\\theta}_{1:t}\\mid \\boldsymbol{y}_{1:t})\\) sequentially in time. This is particularly relevant in non-linear and non-Gaussian models where there is no analytical solution for the filtering distribution. The following code shows how to perform particle filtering in the vanilla stochastic volatility model assuming that the proposal distribution is the conditional prior distribution, that is, \\(q(h_t\\mid h_{t-1},y_t)=\\pi(h_t\\mid h_{t-1})\\), which is normal with mean \\(\\mu+\\phi(h_{t-1}-\\mu)\\) and variance \\(\\sigma^2\\). This choice implies that the incremental importance weights are equal to \\(p(y_t\\mid h_t)\\), which is \\(N(0,\\exp(h_t))\\). Therefore, the weights are proportional to the likelihood function. We perform multinomial resampling every time period in the code, and start the algorithm in the stationary distribution of \\(h_t\\). Remember that there are other resampling approaches that are more efficient, for instance, residual resampling. We ask in Exercise 7 to modify this code to perform resampling when the effective sample size is lower than 50% of the initial number of particles. In addition, we ask to program a sequential importance sampling, and check why is important to perform resampling in this simple example. rm(list = ls()); set.seed(010101) T &lt;- 1250; mu &lt;- -10; phi &lt;- 0.95; sigma &lt;- 0.3 h &lt;- numeric(T); y &lt;- numeric(T) h[1] &lt;- rnorm(1, mu, sigma / sqrt(1 - phi^2)) y[1] &lt;- rnorm(1, 0, exp(h[1] / 2)) for (t in 2:T) { h[t] &lt;- mu + phi*(h[t-1]-mu) + rnorm(1, 0, sigma) y[t] &lt;- rnorm(1, 0, sd = exp(0.5*h[t])) } N &lt;- 10000 log_Weights &lt;- matrix(NA, N, T) # Log weights Weights &lt;- matrix(NA, N, T) # Weights WeightsST &lt;- matrix(NA, N, T) # Normalized weights WeightsSTT &lt;- matrix(1/N, N, T) # Normalized weights bar particles &lt;- matrix(NA, N, T) # Particles particlesT &lt;- matrix(NA, N, T) # Particles bar logalphas &lt;- matrix(NA, N, T) # Incremental importance particles[, 1] &lt;- rnorm(N, mu, sigma / sqrt(1 - phi^2)) # Stationary prior log_Weights[, 1] &lt;- dnorm(y[1], 0, sd = exp(0.5*particles[,1]), log = TRUE) # Likelihood Weights[, 1] &lt;- exp(log_Weights[, 1]) WeightsST[, 1] &lt;- Weights[, 1] / sum(Weights[, 1]) ind &lt;- sample(1:N, size = N, replace = TRUE, prob = WeightsST[, 1]) # Resample particles[, 1] &lt;- particles[ind, 1] # Resampled particles particlesT[, 1] &lt;- particles[, 1] # Resampled particles WeightsST[, 1] &lt;- rep(1/N, N) # Resampled weights pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = T, width = 300) for (t in 2:T) { particles[, t] &lt;- rnorm(N, mu + phi*(particles[, t - 1] - mu), sigma) # Sample from proposal logalphas[, t] &lt;- dnorm(y[t], 0, sd = exp(0.5*particles[,t]), log = TRUE) Weights[, t] &lt;- exp(logalphas[, t]) WeightsST[, t] &lt;- Weights[, t] / sum(Weights[, t]) if(t &lt; T){ ind &lt;- sample(1:N, size = N, replace = TRUE, prob = WeightsST[, t]) particles[, 1:t] &lt;- particles[ind, 1:t] }else{ ind &lt;- sample(1:N, size = N, replace = TRUE, prob = WeightsST[, t]) particlesT[, 1:t] &lt;- particles[ind, 1:t] } setWinProgressBar(pb, t, title=paste( round(t/T*100, 0), &quot;% done&quot;)) } close(pb) FilterDist &lt;- colSums(particles * WeightsST) SDFilterDist &lt;- (colSums(particles^2 * WeightsST) - FilterDist^2)^0.5 FilterDistT &lt;- colSums(particlesT * WeightsSTT) SDFilterDistT &lt;- (colSums(particlesT^2 * WeightsSTT) - FilterDistT^2)^0.5 MargLik &lt;- colMeans(Weights) plot(MargLik, type = &quot;l&quot;) library(dplyr) library(ggplot2) require(latex2exp) ggplot2::theme_set(theme_bw()) Tfig &lt;- 250 keepFig &lt;- 1:Tfig df &lt;- tibble(t = keepFig, mean = FilterDist[keepFig], lower = FilterDist[keepFig] - 2*SDFilterDist[keepFig], upper = FilterDist[keepFig] + 2*SDFilterDist[keepFig], meanT = FilterDistT[keepFig], lowerT = FilterDistT[keepFig] - 2*SDFilterDistT[keepFig], upperT = FilterDistT[keepFig] + 2*SDFilterDistT[keepFig], x_true = h[keepFig]) plot_filtering_estimates &lt;- function(df) { p &lt;- ggplot(data = df, aes(x = t)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1, fill = &quot;lightblue&quot;) + geom_line(aes(y = x_true), colour = &quot;black&quot;, alpha = 1, linewidth = 0.5) + geom_line(aes(y = mean), colour = &quot;blue&quot;, linewidth = 0.5) + geom_line(aes(y = meanT), colour = &quot;purple&quot;, linewidth = 0.5) + ylab(TeX(&quot;$h_{t}$&quot;)) + xlab(&quot;Time&quot;) print(p) } plot_filtering_estimates(df) The Figure illustrates the filtering recursion using SMC with uneven weights (blue line), even weights (purple line), bands corresponding to plus/minus two standard deviations (light blue shaded area), and the true state (black line).2 The results indicate that SMC performs well even with a simple implementation, with no significant differences between using even and uneven weights (see Chapter 4). In this example, we use the population parameters to perform the filtering recursion. However, this is not the case in practice, as we must estimate the time-invariant parameters. Therefore, more elaborate algorithms are required to achieve this. For instance, Andrieu, Doucet, and Holenstein (2010) propose particle Markov chain Monte Carlo, a family of methods that combines MCMC and SMC. See Dahlin and Schön (2019) for a tutorial on particle Metropolis-Hastings in R. A potential practical solution for applications that require sequential updating of a posterior distribution over an unbounded time horizon is to estimate the time-invariant parameters offline using MCMC algorithms up to a specific time period, and then update the state vector sequentially online during subsequent time periods, iterating this process. This is not optimal, but it can be practical. References "],["sec84.html", "8.4 Vector Autoregressive models", " 8.4 Vector Autoregressive models Another widely used methodological approach in time series analysis is the vector autoregressive (VAR) model, which extends AR(p) models to the multivariate case. Since the seminal work by Sims (1980) (Sims 1980), these models have become a cornerstone of macroeconomic research to perform forecasts, and impulse-response (structural) analysis. This chapter provides an introduction to Bayesian inference in VAR models, with detailed discussions available in G. Koop, Korobilis, et al. (2010), Del Negro and Schorfheide (2011), Woźniak (2016), and Chan et al. (2019). The reduced-form VAR(p) model can be written as \\[\\begin{align} \\boldsymbol{y}_t=\\boldsymbol{v} + \\sum_{j=1}^p\\boldsymbol{A}_{j}\\boldsymbol{y}_{t-j}+\\boldsymbol{\\mu}_t, \\tag{8.10} \\end{align}\\] where \\(\\boldsymbol{y}_t\\) is a \\(M\\)-dimensional vector having information of \\(M\\) time series variables, \\(\\boldsymbol{v}\\) is a \\(M\\)-dimensional vector of intercepts, \\(\\boldsymbol{A}_{j}\\) are \\(M\\times M\\) matrices of coefficients, and \\(\\boldsymbol{\\mu}_t \\stackrel{iid}{\\sim} N_M(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\) are stochastic errors, \\(t=1,2,\\dots,T\\) and \\(j=1,2,\\dots,p\\). Other deterministic terms and exogenous variables can be added to the specification without main difficulty, we do not do this to keep simply the notation. In addition, we assume that the stability condition is satisfied such that the stochastic process is stationary (see Helmut (2005) Chap. 2 for details), and we have available \\(p\\) presample values for each variable. Following the matrix-form notation of the multivariate regression model (see sections 3.4 and 7.1), we can set \\(\\boldsymbol{Y}=\\left[{\\boldsymbol{y_{1}}} \\ {\\boldsymbol{y_{2}}} \\ \\ldots \\ {\\boldsymbol{y_{M}}}\\right]\\), which is an \\(T \\times M\\) matrix, \\(\\boldsymbol{x}_t=[1 \\ \\boldsymbol{y}_{t-1}^{\\top} \\ \\dots \\ \\boldsymbol{y}_{t-p}^{\\top}]\\) is a \\((1+Mp)\\)-dimensional row vector, we define \\(K=1+Mp\\) to facilitate notation, and set \\[\\begin{align*} \\boldsymbol{X}=\\begin{bmatrix} \\boldsymbol{x}_1\\\\ \\boldsymbol{x}_2\\\\ \\vdots \\\\ \\boldsymbol{x}_T\\\\ \\end{bmatrix}, \\end{align*}\\] which is a \\(T\\times K\\) matrix, \\(\\boldsymbol{B}=\\left[\\boldsymbol{v} \\ \\boldsymbol{A}_{1} \\ \\boldsymbol{A}_{2} \\ldots \\boldsymbol{A}_{P}\\right]^{\\top}\\) is a \\(K \\times M\\) matrix of parameters, and \\(\\boldsymbol{U}=\\left[\\boldsymbol{\\mu}_{1} \\ \\boldsymbol{\\mu}_{2}\\ldots \\boldsymbol{\\mu}_{M}\\right]\\) is a \\(T\\times M\\)-dimensional matrix of stochastic random errors such that \\(\\boldsymbol{U}\\sim N_{T\\times M}(\\boldsymbol{0}_{T\\times M},\\boldsymbol{\\Sigma}\\otimes \\boldsymbol{I}_T)\\). Thus, we can express the VAR(p) model in the form of a multivariate regression model, \\[\\begin{align*} \\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{B}+\\boldsymbol{U}. \\end{align*}\\] We can assume conjugate priors to facilitate computation, that is, \\[ \\pi({\\boldsymbol{B}}, {\\boldsymbol{\\Sigma}}) = \\pi({\\boldsymbol{B}} \\mid {\\boldsymbol{\\Sigma}}) \\pi({\\boldsymbol{\\Sigma}}), \\] where \\({\\boldsymbol{B}} \\mid {\\boldsymbol{\\Sigma}} \\sim N_{K \\times M}({\\boldsymbol{B}}_{0}, {\\boldsymbol{V}}_{0}, {\\boldsymbol{\\Sigma}})\\) and \\({\\boldsymbol{\\Sigma}} \\sim IW({\\boldsymbol{\\Psi}}_{0}, \\alpha_{0})\\). Thus, \\[ \\pi({\\boldsymbol{B}}, {\\boldsymbol{\\Sigma}} \\mid {\\boldsymbol{Y}}, {\\boldsymbol{X}}) = \\pi({\\boldsymbol{B}} \\mid {\\boldsymbol{\\Sigma}}, {\\boldsymbol{Y}}, {\\boldsymbol{X}}) \\pi({\\boldsymbol{\\Sigma}} \\mid {\\boldsymbol{Y}}, {\\boldsymbol{X}}), \\] where \\({\\boldsymbol{B}} \\mid {\\boldsymbol{\\Sigma}}, {\\boldsymbol{Y}}, {\\boldsymbol{X}} \\sim N_{K \\times M}({\\boldsymbol{B}}_n, {\\boldsymbol{V}}_n, {\\boldsymbol{\\Sigma}})\\) and \\({\\boldsymbol{\\Sigma}} \\mid {\\boldsymbol{Y}}, {\\boldsymbol{X}} \\sim IW({\\boldsymbol{\\Psi}}_n, \\alpha_n)\\). The quantities \\({\\boldsymbol{B}}_n\\), \\({\\boldsymbol{V}}_n\\), \\({\\boldsymbol{\\Psi}}_n\\), and \\(\\alpha_n\\) are given by the following expressions: \\[ {\\boldsymbol{B}}_n = ({\\boldsymbol{V}}_{0}^{-1} + {\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}})^{-1}({\\boldsymbol{V}}_{0}^{-1}{\\boldsymbol{B}}_{0} + {\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}} \\widehat{\\boldsymbol{B}}), \\] \\[ {\\boldsymbol{V}}_n = ({\\boldsymbol{V}}_{0}^{-1} + {\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}})^{-1}, \\] \\[ {\\boldsymbol{\\Psi}}_n = {\\boldsymbol{\\Psi}}_{0} + {\\boldsymbol{S}} + {\\boldsymbol{B}}_{0}^{\\top}{\\boldsymbol{V}}_{0}^{-1}{\\boldsymbol{B}}_{0} + \\widehat{\\boldsymbol{B}}^{\\top}{\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}} \\widehat{\\boldsymbol{B}} - {\\boldsymbol{B}}_n^{\\top} {\\boldsymbol{V}}_n^{-1} {\\boldsymbol{B}}_n, \\] \\[ {\\boldsymbol{S}} = ({\\boldsymbol{Y}} - {\\boldsymbol{X}} \\widehat{\\boldsymbol{B}})^{\\top}({\\boldsymbol{Y}} - {\\boldsymbol{X}} \\widehat{\\boldsymbol{B}}), \\] \\[ \\widehat{\\boldsymbol{B}} = ({\\boldsymbol{X}}^{\\top}{\\boldsymbol{X}})^{-1}{\\boldsymbol{X}}^{\\top}{\\boldsymbol{Y}}, \\] and \\[ \\alpha_n = T + \\alpha_0. \\] Thus, we see that once we express a VAR(p) model in the correct form, we can perform Bayesian inference as we did in the multivariate regression model. However, assuming conjugate priors has some limitations. First, VAR(p) models have many parameters. For instance, with 4 lags and 6 variables, we would have 150 location parameters (\\((1 + (6 \\times 4)) \\times 6\\)) and 21 scale parameters (\\(6 \\times (6 + 1)/2\\)) for the covariance matrix. This can lead to a loss of precision, especially when using macroeconomic data, due to the typical lack of large sample sizes. Therefore, it is desirable to impose prior restrictions on the model specification, which cannot be achieved using conjugate priors. Second, natural conjugate priors do not allow for flexible extensions, such as having different regressors in different equations. Third, the prior structure implies that the prior covariance of the coefficients in any two equations must be proportional to each other. This is because the prior covariance form is \\(\\boldsymbol{\\Sigma} \\otimes \\boldsymbol{V}_0\\). However, this does not always make sense in certain applications. For example, imposing zero prior restrictions on some coefficients would imply that the prior variance of these coefficients should be near zero, but this does not need to be true for all coefficients in the model. To address the first issue, we can think of the VAR(p) specification in a similar way to the seemingly unrelated regression (SUR) model, where we have different regressors in different equations and account for unobserved dependence. This approach allows us to impose zero restrictions on the VAR(p) model, thereby improving its parsimony. Following the setup in Section 7.2, we have \\[ \\boldsymbol{y}_{m} = \\boldsymbol{Z}_{m} \\boldsymbol{\\beta}_m + \\boldsymbol{\\mu}_m, \\] where \\(\\boldsymbol{y}_m\\) is a \\(T\\)-dimensional vector corresponding to the \\(m\\)-th time series variable, \\(\\boldsymbol{Z}_m\\) is a \\(T \\times K_m\\) matrix of regressors, \\(\\boldsymbol{\\beta}_m\\) is a \\(K_m\\)-dimensional vector of location parameters, and \\(\\boldsymbol{\\mu}_m\\) is a \\(T\\)-dimensional vector of stochastic errors, for \\(m = 1, 2, \\dots, M\\). Stacking the \\(M\\) equations, we can write \\(\\boldsymbol{y}=\\boldsymbol{Z}\\boldsymbol{\\beta}+\\boldsymbol{\\mu}\\) where \\(\\boldsymbol{y}=\\left[\\boldsymbol{y}_{1}^{\\top} \\ \\boldsymbol{y}_{2}^{\\top} \\dots \\boldsymbol{y}_{M}^{\\top}\\right]^{\\top}\\) is a \\(MT\\)-dimensional vector, \\(\\boldsymbol{\\beta}=\\left[\\boldsymbol{\\beta}_{1}^{\\top} \\ \\boldsymbol{\\beta}_{2}^{\\top} \\ldots \\boldsymbol{\\beta}_{M}^{\\top}\\right]^{\\top}\\) is a \\(K\\) dimensional vector, \\(K=\\sum_{m=1}^{M} K_m\\), \\(\\boldsymbol{Z}\\) is an \\(MT\\times K\\) block diagonal matrix composed of \\(\\boldsymbol{Z}_{m}\\), that is, \\[\\begin{align*} \\boldsymbol{Z}&amp;=\\begin{bmatrix} \\boldsymbol{Z}_1 &amp; \\boldsymbol{0} &amp; \\dots &amp; \\boldsymbol{0}\\\\ \\boldsymbol{0} &amp; \\boldsymbol{Z}_2 &amp; \\dots &amp; \\boldsymbol{0}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\boldsymbol{0} &amp; \\boldsymbol{0} &amp; \\dots &amp; \\boldsymbol{Z}_M \\end{bmatrix}, \\end{align*}\\] and \\(\\boldsymbol{\\mu}=\\left[\\boldsymbol{\\mu}_{1}^{\\top} \\ \\boldsymbol{\\mu}_{2}^{\\top} \\dots \\ \\boldsymbol{\\mu}_{M}^{\\top}\\right]^{\\top}\\) is a \\(MT\\)-dimensional vector of stochastic errors such that \\(\\boldsymbol{\\mu}\\sim{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}\\otimes \\boldsymbol{I}_T)\\). We can use independent priors in this model to overcome the limitations of the conjugate prior, that is, \\(\\pi(\\boldsymbol{\\beta})\\sim{N}(\\boldsymbol{\\beta}_0,\\boldsymbol{B}_0)\\) and \\(\\pi(\\boldsymbol{\\Sigma}^{-1})\\sim{W}(\\alpha_0,\\boldsymbol{\\Psi}_0)\\). Thus, we know from Section 7.2 that the posterior distributions are \\[\\begin{equation*} \\boldsymbol{\\beta}\\mid \\boldsymbol{\\Sigma}, \\boldsymbol{y}, \\boldsymbol{Z} \\sim {N}(\\boldsymbol{\\beta}_n, \\boldsymbol{B}_n), \\end{equation*}\\] \\[\\begin{equation*} \\boldsymbol{\\Sigma}^{-1}\\mid \\boldsymbol{\\beta}, \\boldsymbol{y}, \\boldsymbol{Z} \\sim {W}(\\alpha_n, \\boldsymbol{\\Psi}_n), \\end{equation*}\\] where \\(\\boldsymbol{B}_n=(\\boldsymbol{Z}^{\\top}(\\boldsymbol{\\Sigma}^{-1}\\otimes \\boldsymbol{I}_T )\\boldsymbol{Z}+\\boldsymbol{B}_0^{-1})^{-1}\\), \\(\\boldsymbol{\\beta}_n=\\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{Z}^{\\top}(\\boldsymbol{\\Sigma}^{-1}\\otimes \\boldsymbol{I}_T)\\boldsymbol{y})\\), \\(\\alpha_n = \\alpha_0 + T\\) and \\(\\boldsymbol{\\Psi}_n = (\\boldsymbol{\\Psi}_0^{-1} + \\boldsymbol{U}^{\\top}\\boldsymbol{U})^{-1}\\), where \\(\\boldsymbol{U}\\) is an \\(T\\times M\\) matrix whose columns are \\(\\boldsymbol{y}_m-\\boldsymbol{Z}_m\\boldsymbol{\\beta}_m\\).3 Observe that we have standard conditional posteriors, thus, we can employ a Gibbs sampling algorithm to get the posterior draws. We can calculate the prediction \\(\\boldsymbol{y}_{T+1}=[y_{1T+1} \\ y_{2T+1} \\ \\dots \\ y_{MT+1}]^{\\top}\\) knowing that \\(\\boldsymbol{y}_{T+1}\\sim N(\\boldsymbol{Z}_{T}\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})\\), where \\[\\begin{align*} \\boldsymbol{Z}_T&amp;=\\begin{bmatrix} \\boldsymbol{z}_{1T}^{\\top} &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; \\boldsymbol{z}_{2T}^{\\top} &amp; \\dots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0&amp; \\dots &amp; \\boldsymbol{z}_{MT}^{\\top} \\end{bmatrix}, \\end{align*}\\] and using the posterior draws of \\(\\boldsymbol{\\beta}^{(s)}\\) and \\(\\boldsymbol{\\Sigma}^{(s)}\\), \\(s=1,2,\\dots,S\\). We can also perform inference of functions of the parameters that are of main interest when using VAR models. Note that independent priors offer more flexibility regarding prior information. For instance, we can set \\(\\boldsymbol{\\Psi}_0 = \\boldsymbol{S}^{-1}\\), \\(\\alpha_0 = T\\), \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}\\), and \\(\\boldsymbol{B}_0\\) as a diagonal matrix, where the variance of the components associated with the coefficients in the \\(m\\)-th equation is such that the prior variance of the coefficients for the own lags is \\(a_1/l^2\\), the variances for lag \\(l\\) of variable \\(m \\neq j\\) are \\(a_2s_{m}^2/(l^2 s_{j}^2)\\), and the variance of the intercepts is set to \\(a_3 s_{m}^2\\), with \\(l = 1, 2, \\dots, p\\), where \\(s_m\\) is the estimated standard error of the residuals from an unrestricted univariate autoregression of variable \\(m\\) against a constant and its \\(p\\) lags (Litterman 1986; G. Koop, Korobilis, et al. 2010). Note that setting \\(a_1 &gt; a_2\\) implies that own lags are more important as predictors than lags of other variables, and dividing by \\(l^2\\) implies that more recent lags are more relevant than those further in the past. The specific choices of \\(a_1\\), \\(a_2\\), and \\(a_3\\) (\\(a_k &gt; 0\\), \\(k = 1, 2, 3\\)) depend on the specific application, but it is generally easier to elicit these parameters rather than the \\(K(K+1)/2\\) different components of \\(\\boldsymbol{B}_0\\).4 This setting is known as the Minnesota prior, as it is based on the seminal proposals for Bayesian VAR models by researchers at the University of Minnesota and the Federal Reserve Bank of Minneapolis (Doan, Litterman, and Sims 1984; Litterman 1986).5 An important non-linear function of parameters when performing VAR analysis is the impulse response function, which is, the response of one variable to an impulse in another variable in the model. The impulse response function can be deduced using the \\(MA\\) representation of the VAR model. In particular, we can write Equation (8.10) using the lag operator (see Section 8.2), \\[\\begin{align} \\boldsymbol{y}_t=\\boldsymbol{v} + (\\boldsymbol{A}_{1}L+\\boldsymbol{A}_{2}L^2+\\dots+\\boldsymbol{A}_{p}L^p)\\boldsymbol{y}_t+\\boldsymbol{\\mu}_t, \\tag{8.11} \\end{align}\\] thus \\(\\boldsymbol{A}(L)\\boldsymbol{y}_t=\\boldsymbol{v}+\\boldsymbol{\\mu}_t\\), where \\(\\boldsymbol{A}(L)=\\boldsymbol{I}_M-\\boldsymbol{A}_{1}L-\\boldsymbol{A}_{2}L^2-\\dots-\\boldsymbol{A}_{p}L^p\\). Let \\(\\boldsymbol{\\Phi}(L):= \\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_sL^s\\) an operator such that \\(\\boldsymbol{\\Phi}(L)\\boldsymbol{A}(L)=\\boldsymbol{I}_M\\). Thus, we have that \\(\\boldsymbol{\\Phi}(L)\\boldsymbol{A}(L)\\boldsymbol{y}_t=\\left(\\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_sL^s\\right)\\boldsymbol{v}+\\left(\\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_sL^s\\right)\\boldsymbol{\\mu}_{t}=\\boldsymbol{\\mu}+\\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_s\\boldsymbol{\\mu}_{t-s}\\). Note that \\(L^s\\boldsymbol{v}=\\boldsymbol{v}\\) because \\(\\boldsymbol{v}\\) is constant, thus we set \\(\\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_sL^s\\boldsymbol{v}=\\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_s\\boldsymbol{v}=\\boldsymbol{\\Phi}(1)\\boldsymbol{v}=(\\boldsymbol{I}_M-\\boldsymbol{A}_{1}-\\boldsymbol{A}_{2}-\\dots-\\boldsymbol{A}_{p})^{-1}\\boldsymbol{v}:=\\boldsymbol{\\mu}\\), which is the mean of the process (Helmut 2005). Therefore, the MA representation of the VAR is \\[\\begin{align} \\boldsymbol{y}_t=\\boldsymbol{\\mu} + \\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_s\\boldsymbol{\\mu}_{t-s}, \\tag{8.12} \\end{align}\\] where \\(\\boldsymbol{\\Phi}_0=\\boldsymbol{I}_M\\), and we can get the coefficients in \\(\\boldsymbol{\\Phi}_s\\) by the recursion \\(\\boldsymbol{\\Phi}_s=\\sum_{l=1}^s\\boldsymbol{\\Phi}_{s-l}\\boldsymbol{A}_l\\), \\(\\boldsymbol{A}_l=\\boldsymbol{0}\\), \\(l&gt;p\\) and \\(s=1,2,..\\) (Helmut 2005). This impulse response function is called forecast error impulse response function. The MA coefficients contain the impulse responses of the system. In particular, \\(\\phi_{mj,s}\\), which is the \\(mj\\)-th element of the matrix \\(\\boldsymbol{\\Phi}_s\\), represents the response of the \\(m\\)-th variable to a unit shock of the variable \\(j\\) in the system, \\(s\\) periods ago, provided that the effect is not contaminated by other shocks in the system. The long-term effects (total multipliers) are given by \\(\\boldsymbol{\\Psi}_{\\infty}:=\\sum_{s=1}^{\\infty}\\boldsymbol{\\Phi}_s=(\\boldsymbol{I}_M-\\boldsymbol{A}_{1}-\\boldsymbol{A}_{2}-\\dots-\\boldsymbol{A}_{p})^{-1}\\). An assumption in these impulse response functions is that a shock occurs in only one variable at a time. This can be questionable as different shocks may be correlated, consequently, occurring simultaneously. Thus, the impulse response analysis can be performed based on the alternative MA representation, \\(\\boldsymbol{y}_t=\\boldsymbol{\\mu} + \\sum_{s=0}^{\\infty}\\boldsymbol{\\Phi}_s\\boldsymbol{P}\\boldsymbol{P}^{-1}\\boldsymbol{\\mu}_{t-s}=\\boldsymbol{\\mu} + \\sum_{s=0}^{\\infty}\\boldsymbol{\\Theta}_s\\boldsymbol{w}_{t-s}\\), where \\(\\boldsymbol{\\Theta}_s=\\boldsymbol{\\Phi}_s\\boldsymbol{P}\\) and \\(\\boldsymbol{w}_{t}=\\boldsymbol{P}^{-1}\\boldsymbol{\\mu}_{t}\\), \\(\\boldsymbol{P}\\) is a lower triangular matrix such that \\(\\boldsymbol{\\Sigma}=\\boldsymbol{P}\\boldsymbol{P}^{\\top}\\) (Cholesky factorization/decomposition). Note that the covariance matrix of \\(\\boldsymbol{w}_t\\) is \\(\\boldsymbol{I}_M\\) due to \\(\\mathbb{E}[\\boldsymbol{w}_t\\boldsymbol{w}_t^{\\top}]=\\mathbb{E}[\\boldsymbol{P}^{-1}\\boldsymbol{\\mu}_{t}\\boldsymbol{\\mu}_t^{\\top}(\\boldsymbol{P}^{-1})^{\\top}]=\\boldsymbol{P}^{-1}\\boldsymbol{\\Sigma}(\\boldsymbol{P}^{-1})^{\\top}=\\boldsymbol{P}^{-1}\\boldsymbol{P}\\boldsymbol{P}^{\\top}(\\boldsymbol{P}^{-1})^{\\top}=\\boldsymbol{I}_M\\). In this representation is sensible to assume that each shock occurs independently due to the covariance matrix of \\(\\boldsymbol{w}_t\\) being an identity. In addition, a unit shock is a shock of size one standard deviation due the result of the covariance matrix. This is named the ortogonalized impulse response, where \\(\\theta_{mj,s}\\), which is the \\(mj\\)-th element of the matrix \\(\\boldsymbol{\\Theta}_s\\), represents the response of the \\(m\\)-th variable to a standard deviation shock of the variable \\(j\\) in the system, \\(s\\) periods ago. The critical point with the ortogonalized impulse responses is that the order of the variables in the VAR is really important because implicitly establishes a recursive model, that is, the \\(m\\)-th equation in the system may contain \\(y_{1t}, y_{2t}, \\dots, y_{m-1t}\\), but not \\(y_{mt}, y_{m+1t}, \\dots, y_{Mt}\\) on the hand-right side of its equation. Thus, \\(y_{mt}\\) cannot have an instantaneous impact on \\(y_{jt}\\) for \\(j&lt;m\\) (Helmut 2005). Beyond the fascinating macroeconomic implications embedded in the specification of VAR models, the key point for this section is that we can infer impulse response functions using the posterior draws. Example: US fiscal system Let’s use the dataset provided by Woźniak (2024) of the US fiscal system, where ttr is the quarterly total tax revenue, gs is the quarterly total government spending, and gdp is the quarterly gross domestic product, all expressed in log, real, per person terms, and the period is 1948q1 to 2024q2. This dataset is the 18USAfiscal.csv file. Mertens and Ravn (2014) analyze the US fiscal policy shocks using these variables. Let’s estimate a VAR model where \\(\\boldsymbol{y}_t=[\\Delta(ttr_t) \\ \\Delta(gs_t) \\ \\Delta(gdp_t)]^{\\top}\\), that is, we work with the log differences (variation rates), and we set \\(p=1\\). We use the package bvartools to estimate the forecast error and ortogonalized impulse response functions. We use vague independent priors setting \\(\\boldsymbol{\\beta}_0=\\boldsymbol{0}\\), \\(\\boldsymbol{B}_0=100\\boldsymbol{I}\\), \\(\\boldsymbol{V}_0=5^{-1}\\boldsymbol{I}\\) and \\(\\alpha_0=3\\), and the Minnesota prior setting \\(a_1=2\\), \\(\\kappa_2=0.5\\) and \\(\\kappa_3=5\\) (default values).6 The following code shows how to do this, take into account that we use the first 301 observations to estimate the model, and keep the last 4 observations to check the forecasting performance. The first and second figures show the impulse response functions of gs with respect to gs, the forecast error impulse response using vague independent priors, and the orthogonalized impulse response using the Minnesota prior, respectively. We see that the effect of the Minnesota prior is to decrease uncertainty. In addition, the forecasting exercise results indicate that these assumptions have same effects in this example. In particular, the third Figure shows that the mean forecasts using the vague prior (green line) and the Minnesota prior (red line) are indistinguishable from the true observations (black line). However, the Minnesota prior enhances forecast precision, as its 95% predictive interval (blue shaded area) is narrower and fully contained within the 95% predictive interval obtained using vague priors (light blue shaded area). This improvement is attributable to the shrinkage properties of the Minnesota prior. rm(list = ls()); set.seed(010101) DataUSfilcal &lt;- read.csv(&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/18USAfiscal.csv&quot;, sep = &quot;,&quot;, header = TRUE, quote = &quot;&quot;) attach(DataUSfilcal) # upload data Y &lt;- cbind(diff(as.matrix(DataUSfilcal[,-c(1:2)]))) T &lt;- dim(Y)[1]-1; K &lt;- dim(Y)[2] Ynew &lt;- Y[-c((T-2):(T+1)), ] # Use 4 last observations to check forecast y1 &lt;- Ynew[-1, 1]; y2 &lt;- Ynew[-1, 2]; y3 &lt;- Ynew[-1, 3] X1 &lt;- cbind(1, lag(Ynew)); X1 &lt;- X1[-1,] X2 &lt;- cbind(1, lag(Ynew)); X2 &lt;- X2[-1,] X3 &lt;- cbind(1, lag(Ynew)); X3 &lt;- X3[-1,] M &lt;- dim(Y)[2]; K1 &lt;- dim(X1)[2]; K2 &lt;- dim(X2)[2]; K3 &lt;- dim(X3)[2] K &lt;- K1 + K2 + K3 # Hyperparameters b0 &lt;- 0; c0 &lt;- 100; V0 &lt;- 5^(-1); a0 &lt;- M #Posterior draws library(tibble) MCMC &lt;- 10000; burnin &lt;- 1000; H &lt;- 10; YnewPack &lt;- ts(Ynew) model &lt;- bvartools::gen_var(YnewPack, p = 1, deterministic = &quot;const&quot;, iterations = MCMC, burnin = burnin) # Create model model &lt;- bvartools::add_priors(model, coef = list(v_i = c0^-1, v_i_det = c0^-1, const = b0), sigma = list(df = a0, scale = V0/a0), coint_var = FALSE) # Add priors object &lt;- bvartools::draw_posterior(model) # Posterior draws ir &lt;- bvartools::irf.bvar(object, impulse = &quot;gs&quot;, response = &quot;gs&quot;, n.ahead = H, type = &quot;feir&quot;, cumulative = FALSE) # Calculate IR # Plot IR plot_IR &lt;- function(df) { p &lt;- ggplot(data = df, aes(x = t)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1, fill = &quot;lightblue&quot;) + geom_line(aes(y = mean), colour = &quot;blue&quot;, linewidth = 0.5) + ylab(&quot;Impulse response&quot;) + xlab(&quot;Time&quot;) + xlim(0,H) print(p) } dfNew &lt;- tibble(t = 0:H, mean = as.numeric(ir[,2]), lower = as.numeric(ir[,1]), upper = as.numeric(ir[,3])) FigNew &lt;- plot_IR(dfNew) # Using Minnesota prior modelMin &lt;- bvartools::gen_var(YnewPack, p = 1, deterministic = &quot;const&quot;, iterations = MCMC, burnin = burnin) modelMin &lt;- bvartools::add_priors(modelMin, minnesota = list(kappa0 = 2, kappa1 = 0.5, kappa3 = 5), coint_var = FALSE) # Minnesota prior objectMin &lt;- bvartools::draw_posterior(modelMin) # Posterior draws irMin &lt;- bvartools::irf.bvar(objectMin, impulse = &quot;gs&quot;, response = &quot;gs&quot;, n.ahead = H, type = &quot;feir&quot;, cumulative = FALSE) # Calculate IR dfNewMin &lt;- tibble(t = 0:H, mean = as.numeric(irMin[,2]), lower = as.numeric(irMin[,1]), upper = as.numeric(irMin[,3])) FigNewMin &lt;- plot_IR(dfNewMin) ### Forecasting bvar_pred &lt;- predict(object, n.ahead = 4, new_d = rep(1, 4)) bvar_predOR &lt;- predict(objectMin, n.ahead = 4, new_d = rep(1, 4)) dfFore &lt;- tibble(t = c((T-2):(T+1)), mean = as.numeric(bvar_pred[[&quot;fcst&quot;]][[&quot;gs&quot;]][,2]), lower = as.numeric(bvar_pred[[&quot;fcst&quot;]][[&quot;gs&quot;]][,1]), upper = as.numeric(bvar_pred[[&quot;fcst&quot;]][[&quot;gs&quot;]][,3]), mean1 = as.numeric(bvar_predOR[[&quot;fcst&quot;]][[&quot;gs&quot;]][,2]), lower1 = as.numeric(bvar_predOR[[&quot;fcst&quot;]][[&quot;gs&quot;]][,1]), upper1 = as.numeric(bvar_predOR[[&quot;fcst&quot;]][[&quot;gs&quot;]][,3]), true = as.numeric(Y[c((T-2):(T+1)),2])) plot_FORE &lt;- function(df) { p &lt;- ggplot(data = dfFore, aes(x = t)) + geom_ribbon(aes(ymin = lower1, ymax = upper1), alpha = 1, fill = &quot;blue&quot;) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1, fill = &quot;lightblue&quot;) + geom_line(aes(y = mean), colour = &quot;green&quot;, linewidth = 0.5) + geom_line(aes(y = mean1), colour = &quot;red&quot;, linewidth = 0.5) + geom_line(aes(y = true), colour = &quot;black&quot;, linewidth = 0.5) + ylab(&quot;Forecast&quot;) + xlab(&quot;Time&quot;) + xlim(c((T-2),(T+1))) print(p) } FigFore &lt;- plot_FORE(dfFore) The following Algorithm shows how to do perform inference in VAR models using our GUI. See also Chapter 5 for details regarding the dataset structure. Algorithm: Vector Autoregressive Models Select Time series Model on the top panel Select VAR models using the left radio button Upload the dataset selecting first if there is a header in the file, and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MCMC iterations, burn-in, and thinning parameters using the Range sliders Set the number of lags (p) Set the hyperparameters for the Minnesota prior: a1, κ2 and κ3. This step is not necessary as by default our GUI uses default values in the bvartools package Select the type of impulse response functions: forecast error or orthogonalized, and ordinary or cumulative Set the time horizon for the impulse response functions and the forecasts Click the Go! button Analyze results Download impulse responses and forecasts using the Download Impulse Responses, and Forecast buttons There are other good packages in R to perform Bayesian inference in VAR models. For instance, bayesianVARs package implements inference of reduced-form VARs with stochastic volatility (Luis and Gregor 2024), BVAR package performs inference using hierarchical priors (Nikolas et al. 2022), bvarsv implements time-varying parameters models (Krueger 2022), bsvars performs estimation of structural VAR models (Woźniak 2024), and bsvarSIGNs to estimating structural VAR models with sign restrictions (Xiaolei and Woźniak 2024). References "],["sec85.html", "8.5 Summary", " 8.5 Summary We present a brief review of Bayesian inference in time series models. In particular, we introduce the state-space representation and demonstrate how to perform inferential analysis for these models, focusing on the dynamic linear model and the stochastic volatility model. Additionally, we show how ARMA(p,q) processes can be expressed in state-space form and provide methods for estimating such models. We also include code for implementing computational inference algorithms, such as sequential Monte Carlo (SMC), Hamiltonian Monte Carlo (HMC), and various Markov chain Monte Carlo (MCMC) methods. Finally, we introduce VAR(p) models, detailing how to perform impulse-response analysis and forecasting within this framework. Time series analysis is a highly active research area with remarkable methodological developments and applications. Interested readers can refer to excellent materials in chapters 7 and 9 of Geweke, Koop, and Dijk (2011), and chapters 17 to 20 of Chan et al. (2019), along with the references therein. References "],["sec86.html", "8.6 Exercises", " 8.6 Exercises Simulate the dynamic linear model assuming \\(X_t \\sim N(1, 0.1\\sigma^2)\\), \\(w_t \\sim N(0, 0.5\\sigma^2)\\), \\(\\mu_t \\sim N(0, \\sigma^2)\\), \\(\\beta_0 = 1\\), \\(B_0 = 0.5\\sigma^2\\), \\(\\sigma^2 = 0.25\\), and \\(G_t = 1\\), for \\(t = 1, \\dots, 100\\). Then, perform the filtering recursion fixing \\(\\Sigma = 25 \\times 0.25\\), \\(\\Omega_1 = 0.5\\Sigma\\) (high signal-to-noise ratio) and \\(\\Omega_2 = 0.1\\Sigma\\) (low signal-to-noise ratio). Plot and compare the results. Simulate the dynamic linear model \\(y_t = \\beta_t x_t + \\mu_t\\), \\(\\beta_t = \\beta_{t-1} + w_t\\), where \\(x_t \\sim N(1, 0.1\\sigma^2)\\), \\(w_t \\sim N(0, 0.5\\sigma^2)\\), \\(\\mu_t \\sim N(0, \\sigma^2)\\), \\(\\beta_0 = 0\\), \\(B_0 = 0.5\\sigma^2\\), and \\(\\sigma^2 = 1\\), for \\(t = 1, \\dots, 100\\). Perform the filtering and smoothing recursions from scratch. Simulate the process \\(y_t = \\alpha z_t + \\beta_t x_t + \\boldsymbol{h}^{\\top}\\boldsymbol{\\epsilon}_t\\), \\(\\beta_t = \\beta_{t-1} + \\boldsymbol{H}^{\\top}\\boldsymbol{\\epsilon}_t\\), where \\(\\boldsymbol{h}^{\\top} = [1 \\ 0]\\), \\(\\boldsymbol{H}^{\\top} = [0 \\ 1/\\tau]\\), \\(\\boldsymbol{v}_t \\sim N(\\boldsymbol{0}_2, \\sigma^2 \\boldsymbol{I}_2)\\), \\(x_t \\sim N(1, 2\\sigma^2)\\), \\(z_t \\sim N(0, 2\\sigma^2)\\), \\(\\alpha = 2\\), \\(\\tau^2 = 5\\), and \\(\\sigma^2 = 0.1\\), for \\(t = 1, \\dots, 200\\). Assume \\(\\pi({\\beta}_0, {\\alpha}, \\sigma^2, {\\tau}) = \\pi({\\beta}_0)\\pi({\\alpha})\\pi(\\sigma^2)\\pi(\\tau^2)\\) where \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\), \\(\\tau^2 \\sim G(v_{0}/2, v_{0}/2)\\), \\({\\alpha} \\sim N({a}_0, {A}_0)\\), and \\({\\beta}_0 \\sim N({b}_0, {B}_0)\\) such that \\(\\alpha_0 = \\delta_0 = 1\\), \\(v_0 = 5\\), \\(a_0 = 0\\), \\(A_0 = 1\\), \\(\\beta_0 = 0\\), \\(B_0 = \\sigma^2/\\tau^2\\). Program the MCMC algorithm including the simulation smoother. Show that the posterior distribution of \\(\\boldsymbol{\\phi} \\mid \\boldsymbol{\\beta}, \\sigma^2, \\boldsymbol{y}, \\boldsymbol{X}\\) in the model \\(y_t = \\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta} + \\mu_t\\) where \\(\\phi(L) \\mu_t = \\epsilon_t\\) and \\(\\epsilon_t \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) is \\(N(\\boldsymbol{\\phi}_n, \\boldsymbol{\\Phi}_n)\\mathbb{1}(\\boldsymbol{\\phi} \\in S_{\\boldsymbol{\\phi}})\\), where \\(\\boldsymbol{\\Phi}_n = (\\boldsymbol{\\Phi}_0^{-1} + \\sigma^{-2} \\boldsymbol{U}^{\\top} \\boldsymbol{U})\\), \\(\\boldsymbol{\\phi}_n = \\boldsymbol{\\Phi}_n (\\boldsymbol{\\Phi}_0^{-1} \\boldsymbol{\\phi}_0 + \\sigma^{-2} \\boldsymbol{U}^{\\top} \\boldsymbol{\\mu})\\), and \\(S_{\\boldsymbol{\\phi}}\\) is the stationary region of \\(\\boldsymbol{\\phi}\\). Show that in the \\(AR(2)\\) stationary process, \\(y_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\\), where \\(\\epsilon_t \\sim N(0, \\sigma^2)\\), \\(\\mathbb{E}[y_t] = \\frac{\\mu}{1 - \\phi_1 - \\phi_2}\\), and \\(\\text{Var}[y_t] = \\frac{\\sigma^2(1 - \\phi_2)}{1 - \\phi_2 - \\phi_1^2 - \\phi_1^2 \\phi_2 - \\phi_2^2 + \\phi_2^3}\\). Program a Hamiltonian Monte Carlo taking into account the stationary restrictions on \\(\\phi_1\\) and \\(\\phi_2\\), and \\(\\epsilon_0\\) such that the acceptance rate is near 65%. Stochastic volatility model Program a sequential importance sampling (SIS) from scratch in the vanilla stochastic volatility model setting \\(\\mu = -10\\), \\(\\phi = 0.95\\), \\(\\sigma = 0.3\\), and \\(T = 250\\). Check what happens with its performance. Modify the sequential Monte Carlo (SMC) to perform multinomial resampling when the effective sample size is lower than 50% the initial number of particles. Estimate the vanilla stochastic volatility model using the dataset 17ExcRate.csv, provided by Ramı́rez-Hassan and Frazier (2024), which contains the exchange rate log daily returns for USD/EUR, USD/GBP, and GBP/EUR from one year before and after the WHO declared the COVID-19 pandemic on 11 March 2020. Simulate the VAR(1) process: \\[ \\begin{bmatrix} y_{1t}\\\\ y_{2t}\\\\ y_{3t}\\\\ \\end{bmatrix} = \\begin{bmatrix} 2.8\\\\ 2.2\\\\ 1.3\\\\ \\end{bmatrix} + \\begin{bmatrix} 0.5 &amp; 0 &amp; 0\\\\ 0.1 &amp; 0.1 &amp; 0.3\\\\ 0 &amp; 0.2 &amp; 0.3\\\\ \\end{bmatrix} \\begin{bmatrix} y_{1t-1}\\\\ y_{2t-1}\\\\ y_{3t-1}\\\\ \\end{bmatrix} + \\begin{bmatrix} \\mu_{1t}\\\\ \\mu_{2t}\\\\ \\mu_{3t}\\\\ \\end{bmatrix}, \\] where \\(\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2.25 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0.5\\\\ 0 &amp; 0.5 &amp; 0.74\\\\ \\end{bmatrix}\\). Use vague independent priors setting \\(\\boldsymbol{\\beta}_0 = \\boldsymbol{0}\\), \\(\\boldsymbol{B}_0 = 100\\boldsymbol{I}\\), \\(\\boldsymbol{V}_0 = 5\\boldsymbol{I}\\), \\(\\alpha_0 = 3\\), and estimate a VAR(1) model using the rsurGibbs function from the package bayesm. Then, program from scratch References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
