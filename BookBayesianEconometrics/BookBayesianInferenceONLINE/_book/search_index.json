[["Chap10.html", "Chapter 10 Bayesian model averaging in variable selection", " Chapter 10 Bayesian model averaging in variable selection We outline in this chapter a framework for addressing model uncertainty and averaging across different models in a probabilistically consistent manner. The discussion tackles two major computational challenges in Bayesian model averaging: the vast space of possible models and the absence of analytical solutions for the marginal likelihood. We begin by illustrating the approach within the Gaussian linear model, assuming exogeneity of the regressors, and extend the analysis to cases with endogenous regressors, and dynamic models. Additionally, we adapt the framework to generalized linear models, including the logit, gamma, and Poisson families. Lastly, we explore alternative methods for computing marginal likelihoods, especially when the Bayesian information criterion’s asymptotic approximation proves inadequate. Remember that we can run our GUI typing shiny::runGitHub(\"besmarter/BSTApp\", launch.browser=T) in the R console or any R code editor and execute it. However, users should see Chapter 5 for details. "],["sec10_1.html", "10.1 Foundation", " 10.1 Foundation Remember from Chapter 1 that Bayesian model averaging (BMA) is an approach which takes into account model uncertainty. In particular, we consider uncertainty in the regressors (variable selection) in a regression framework where there are \\(K\\) possible explanatory variables.1 This implies \\(2^K\\) potential models indexed by parameters \\(\\boldsymbol{\\theta}_m\\), \\(m=1,2,\\dots,2^K\\). Following Simmons et al. (2010), the posterior model probability is \\[\\begin{equation*} \\pi(\\mathcal{M}_j |\\boldsymbol{y})=\\frac{p(\\boldsymbol{y} | \\mathcal{M}_j)\\pi(\\mathcal{M}_j)}{\\sum_{m=1}^{2^K}p(\\boldsymbol{y} | \\mathcal{M}_m)\\pi(\\mathcal{M}_m)}, \\end{equation*}\\] where \\(\\pi(\\mathcal{M}_j)\\) is the prior model probability,2 \\[\\begin{equation*} p(\\boldsymbol{y} | \\mathcal{M}_j)=\\int_{\\boldsymbol{\\Theta}_j} p(\\boldsymbol{y}| \\boldsymbol{\\theta}_j,\\mathcal{M}_j)\\pi(\\boldsymbol{\\theta}_j | \\mathcal{M}_j) d\\boldsymbol{\\theta}_{j} \\end{equation*}\\] is the marginal likelihood, and \\(\\pi(\\boldsymbol{\\theta}_j | \\mathcal{M}_j)\\) is the prior distribution of \\(\\boldsymbol{\\theta}_j\\) conditional on model \\(\\mathcal{M}_j\\). Following Adrian E. Raftery (1993), the posterior distribution of \\(\\boldsymbol{\\theta}\\) is \\[\\begin{equation*} \\pi(\\boldsymbol{\\theta}|\\boldsymbol{y})= \\sum_{m=1}^{2^K}\\pi(\\boldsymbol{\\theta}_m|\\boldsymbol{y},\\mathcal{M}_m) \\pi(\\mathcal{M}_m|\\boldsymbol{y}) \\end{equation*}\\] The posterior distribution of the parameter vector \\(\\boldsymbol{\\theta}\\) under model \\(\\mathcal{M}_m\\) is denoted as \\(\\pi(\\boldsymbol{\\theta}_m|\\boldsymbol{y}, \\mathcal{M}_m)\\). The posterior mean of \\(\\boldsymbol{\\theta}\\) is given by: \\[ \\mathbb{E}[\\boldsymbol{\\theta}|\\boldsymbol{y}] = \\sum_{m=1}^{2^K} \\hat{\\boldsymbol{\\theta}}_m \\, \\pi(\\mathcal{M}_m|\\boldsymbol{y}), \\] where \\(\\hat{\\boldsymbol{\\theta}}_m\\) represents the posterior mean under model \\(\\mathcal{M}_m\\). The variance of the \\(k\\)-th element of \\(\\boldsymbol{\\theta}\\) given the data \\(\\boldsymbol{y}\\) is: \\[ \\text{Var}(\\theta_{km}|\\boldsymbol{y}) = \\sum_{m=1}^{2^K} \\pi(\\mathcal{M}_m|\\boldsymbol{y}) \\, \\widehat{\\text{Var}}(\\theta_{km}|\\boldsymbol{y}, \\mathcal{M}_m) + \\sum_{m=1}^{2^K} \\pi(\\mathcal{M}_m|\\boldsymbol{y}) \\left( \\hat{\\theta}_{km} - \\mathbb{E}[\\theta_{km}|\\boldsymbol{y}] \\right)^2, \\] where \\(\\widehat{\\text{Var}}(\\theta_{km}|\\boldsymbol{y}, \\mathcal{M}_m)\\) denotes the posterior variance of the \\(k\\)-th element of \\(\\boldsymbol{\\theta}\\) under model \\(\\mathcal{M}_m\\). The posterior variance highlights how BMA accounts for model uncertainty. The first term represents the weighted variance of each model, averaged across all potential models, while the second term reflects the stability of the estimates across models. The greater the variation in estimates between models, the higher the posterior variance. The posterior predictive distribution is \\[\\begin{equation*} \\pi(\\boldsymbol{y}_0|\\boldsymbol{y})= \\sum_{m=1}^{2^K}p_m(\\boldsymbol{y}_0|\\boldsymbol{y},\\mathcal{M}_m) \\pi(M_m|\\boldsymbol{y}) \\end{equation*}\\] where \\(p_m(\\boldsymbol{y}_0|\\boldsymbol{y},\\mathcal{M}_m)=\\int_{\\boldsymbol{\\Theta}_m} p(\\boldsymbol{y}_0|\\boldsymbol{y},\\boldsymbol{\\theta}_m,\\mathcal{M}_m)\\pi(\\boldsymbol{\\theta}_m |\\boldsymbol{y}, \\mathcal{M}_m) d\\boldsymbol{\\theta}_{m}\\) is the posterior predictive distribution under model \\(\\mathcal{M}_m\\). Another important statistic in BMA is the posterior inclusion probability associated with variable \\(\\boldsymbol{x}_k\\), \\(k=1,2,\\dots,K\\), which is \\[\\begin{equation*} PIP(\\boldsymbol{x}_k)=\\sum_{m=1}^{2^K}\\pi(\\mathcal{M}_m|\\boldsymbol{y})\\times \\mathbb{1}_{k,m}, \\end{equation*}\\] where \\(\\mathbb{1}_{k,m}= \\left\\{ \\begin{array}{lcc} 1&amp; if &amp; \\boldsymbol{x}_{k}\\in \\mathcal{M}_m \\\\ \\\\ 0 &amp; if &amp; \\boldsymbol{x}_{k}\\not \\in \\mathcal{M}_m \\end{array} \\right\\}.\\) Kass and Raftery (1995) suggest that posterior inclusion probabilities (PIP) less than 0.5 are evidence against the regressor, \\(0.5\\leq PIP&lt;0.75\\) is weak evidence, \\(0.75\\leq PIP&lt;0.95\\) is positive evidence, \\(0.95\\leq PIP&lt;0.99\\) is strong evidence, and \\(PIP\\geq 0.99\\) is very strong evidence. There are two main computational issues in implementing BMA based on variable selection. First, the number of models in the model space is \\(2^K\\), which sometimes can be enormous. For instance, three regressors imply just eight models, see the next Table, but 40 regressors implies approximately 1.1e+12 models. Take into account that models always include the intercept, and all regressors should be standardized to avoid scale issues.3 The second computational issue is calculating the marginal likelihood \\(p(\\boldsymbol{y} | \\mathcal{M}_j)=\\int_{\\boldsymbol{\\Theta}_j} p(\\boldsymbol{y}| \\boldsymbol{\\theta}_j,\\mathcal{M}_j)\\pi(\\boldsymbol{\\theta}_j | \\mathcal{M}_j) d\\boldsymbol{\\theta}_{j}\\), which most of the time does not have an analytic solution. Table 10.1: Space of models Variable \\(M_{1}\\) \\(M_{2}\\) \\(M_{3}\\) \\(M_{4}\\) \\(M_{5}\\) \\(M_{6}\\) \\(M_{7}\\) \\(M_{8}\\) \\(x_1\\) 1 1 1 1 0 0 0 0 \\(x_2\\) 1 1 0 0 1 1 0 0 \\(x_3\\) 1 0 1 0 1 0 1 0 The first computational issue is basically a problem of ranking models. This can be tackled using different approaches, such as Occam’s window criterion (D. Madigan and Raftery 1994; Adrian E. Raftery, Madigan, and Hoeting 1997), reversible jump Markov chain Monte Carlo computation (Green 1995), Markov chain Monte Carlo model composition (D. Madigan, York, and Allard 1995), and multiple testing using intrinsic priors (Casella and Moreno 2006) or nonlocal prior densities (Jhonson and Rossell 2012). We focus on Occam’s window and Markov chain Monte Carlo model composition in our GUI.4 In Occam’s window, a model is discarded if its predictive performance is much worse than that of the best model (D. Madigan and Raftery 1994; Adrian E. Raftery, Madigan, and Hoeting 1997). Thus, models not belonging to \\(\\mathcal{M}&#39;=\\left\\{\\mathcal{M}_j:\\frac{\\max_m {\\pi(\\mathcal{M}_m|\\boldsymbol{y})}}{\\pi(\\mathcal{M}_j|\\boldsymbol{y})}\\leq c\\right\\}\\) should be discarded, where \\(c\\) is chosen by the user (D. Madigan and Raftery (1994) propose \\(c=20\\)). In addition, complicated models than are less supported by the data than simpler models are also discarded, that is, \\(\\mathcal{M}&#39;&#39;=\\left\\{\\mathcal{M}_j:\\exists \\mathcal{M}_m\\in\\mathcal{M}&#39;,\\mathcal{M}_m\\subset \\mathcal{M}_j,\\frac{\\pi(\\mathcal{M}_m|\\boldsymbol{y})}{\\pi(\\mathcal{M}_j|\\boldsymbol{y})}&gt;1\\right\\}\\). Then, the set of models used in BMA is \\(\\mathcal{M}^*=\\mathcal{M}&#39;\\cap \\mathcal{M}&#39;&#39;^c\\in\\mathcal{M}\\). Adrian E. Raftery, Madigan, and Hoeting (1997) find that the number of models in \\(\\mathcal{M}^*\\) is normally less than 25. However, the previous theoretical framework requires finding the model with the maximum a posteriori model probability (\\(\\max_m {\\pi(\\mathcal{M}_m|\\boldsymbol{y})}\\)), which implies calculating all possible models in \\(\\mathcal{M}\\). This is computationally burdensome. Hence, a heuristic approach is proposed by Adrian Raftery et al. (2012) based on ideas of D. Madigan and Raftery (1994). The search strategy is based on a series of nested comparisons of ratios of posterior model probabilities. Let \\(\\mathcal{M}_0\\) be a model with one regressor less than model \\(\\mathcal{M}_1\\), then: If \\(\\log(\\pi(\\mathcal{M}_0|\\boldsymbol{y})/\\pi(\\mathcal{M}_1|\\boldsymbol{y}))&gt;\\log(O_R)\\), then \\(\\mathcal{M}_1\\) is rejected and \\(\\mathcal{M}_0\\) is considered. If \\(\\log(\\pi(\\mathcal{M}_0|\\boldsymbol{y})/\\pi(\\mathcal{M}_1|\\boldsymbol{y}))\\leq -\\log(O_L)\\), then \\(\\mathcal{M}_0\\) is rejected, and \\(\\mathcal{M}_1\\) is considered. If \\(\\log(O_L)&lt;\\log(\\pi(\\mathcal{M}_0|\\boldsymbol{y})/\\pi(\\mathcal{M}_1|\\boldsymbol{y}))\\leq \\log(O_R\\)), \\(\\mathcal{M}_0\\) and \\(\\mathcal{M}_1\\) are considered. Here \\(O_R\\) is a number specifying the maximum ratio for excluding models in Occam’s window, and \\(O_L=1/O_R^{2}\\) is defined by default in Adrian Raftery et al. (2012). The search strategy can be “up’‘, adding one regressor, or “down’’, dropping one regressor (see D. Madigan and Raftery (1994) for details about the down and up algorithms). The leaps and bounds algorithm (Furnival and Wilson 1974) is implemented to improve the computational efficiency of this search strategy (Adrian Raftery et al. 2012). Once the set of potentially acceptable models is defined, we discard all the models that are not in \\(\\mathcal{M}&#39;\\), and the models that are in \\(\\mathcal{M}&#39;&#39;\\) where 1 is replaced by \\(\\exp\\left\\{O_R\\right\\}\\) due to the leaps and bounds algorithm giving an approximation to BIC, so as to ensure that no good models are discarded. The second approach that we consider in our GUI to tackle the model space size issue is Markov chain Monte Carlo model composition (MC3) (David Madigan, York, and Allard 1995). In particular, given the space of models \\(\\mathcal{M}_m\\), we simulate a chain of \\(\\mathcal{M}_s\\) models, \\(s = 1, 2, ..., S&lt;&lt;2^K\\), where the algorithm randomly extracts a candidate model \\(\\mathcal{M}_c\\) from a neighborhood of models (\\(nbd(\\mathcal{M}_m)\\)) that consists of the actual model itself and the set of models with either one variable more or one variable less (Adrian E. Raftery, Madigan, and Hoeting 1997). Therefore, there is a transition kernel in the space of models \\(q(\\mathcal{M}_m\\rightarrow \\mathcal{M}_c)\\), such that \\(q(\\mathcal{M}_m\\rightarrow \\mathcal{M}_{c})=0 \\ \\forall \\mathcal{M}_{c}\\notin nbd(\\mathcal{M}_m)\\) and \\(q(\\mathcal{M}_m\\rightarrow \\mathcal{M}_{c})=\\frac{1}{|nbd(\\mathcal{M}_m)|} \\ \\forall \\mathcal{M}_m\\in nbd(\\mathcal{M}_m)\\), \\(|nbd(\\mathcal{M}_m)|\\) being the number of neighbors of \\(\\mathcal{M}_m\\). This candidate model is accepted with probability \\[\\begin{equation*} \\alpha (\\mathcal{M}_{s-1},\\mathcal{M}_{c})=\\min \\bigg \\{ \\frac{|nbd(\\mathcal{M}_m)|p(\\boldsymbol{y} | \\mathcal{M}_c)\\pi(\\mathcal{M}_c)}{|nbd(\\mathcal{M}^{c})|p(\\boldsymbol{y}| \\mathcal{M}_{(s-1)})\\pi(\\mathcal{M}_{(s-1)})},1 \\bigg \\}. \\end{equation*}\\] Observe that by construction \\(|nbd(\\mathcal{M}_m)|=|nbd(\\mathcal{M}_c)|=k\\), except in extreme cases where a model has only one regressor or has all regressors. The Bayesian information criterion is a possible solution for the second computational issue in BMA, that is, calculating the marginal likelihood when there is no an analytic solution. Defining \\(h(\\boldsymbol{\\theta}|\\mathcal{M}_j)=-\\frac{\\log(p(\\boldsymbol{y}| \\boldsymbol{\\theta}_j,\\mathcal{M}_j)\\pi(\\boldsymbol{\\theta}_j | \\mathcal{M}_j))}{N}\\), then \\(p(\\boldsymbol{y} | \\mathcal{M}_j)=\\int_{\\boldsymbol{\\Theta}_j} \\exp\\left\\{-N h(\\boldsymbol{\\theta}|\\mathcal{M}_j)\\right\\} d\\boldsymbol{\\theta}_{j}\\). If \\(N\\) is sufficiently large (technically \\(N\\to \\infty\\)), we can make the following assumptions (Hoeting et al. 1999): We can use the Laplace method for approximating integrals (Tierney and Kadane 1986). The posterior mode is reached at the same point as the maximum likelihood estimator (MLE), denoted by \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\). We get the following results under these assumptions: \\[\\begin{align*} p(\\boldsymbol{y} | \\mathcal{M}_j)\\approx&amp;\\left( \\frac{2\\pi}{N}\\right)^{K_j/2}|\\boldsymbol{\\Sigma}|^{-1/2} \\exp\\left\\{-N h(\\boldsymbol{\\hat{\\theta}}_j^{MLE}|\\mathcal{M}_j)\\right\\}, \\ N\\rightarrow\\infty, \\end{align*}\\] where \\(\\boldsymbol{\\Sigma}\\) is the inverse of the Hessian matrix of \\(h(\\boldsymbol{\\hat{\\theta}}_j^{MLE}|\\mathcal{M}_j)\\), and \\(K_j=dim\\left\\{\\boldsymbol{\\theta}_j\\right\\}\\). This implies \\[\\begin{align*} \\log\\left(p(\\boldsymbol{y} | \\mathcal{M}_j)\\right)\\approx&amp; \\frac{K_j}{2}\\log(2\\pi)- \\frac{K_j}{2}\\log(N) -\\frac{1}{2}\\log(|\\boldsymbol{\\Sigma}|) + \\log(p(\\boldsymbol{y}| \\boldsymbol{\\hat{\\theta}}_j^{MLE},\\mathcal{M}_j))+\\log(\\pi(\\boldsymbol{\\hat{\\theta}}_j^{MLE} | \\mathcal{M}_j)), \\ N\\rightarrow\\infty. \\end{align*}\\] Since \\(\\frac{K_j}{2}\\log(2\\pi)\\) and \\(\\log(\\pi(\\boldsymbol{\\hat{\\theta}}_j^{MLE} | \\mathcal{M}_j))\\) are constants as functions of \\(\\boldsymbol{y}\\), and \\(|\\boldsymbol{\\Sigma}|\\) is bounded by a finite constant, we have \\[\\begin{align*} log\\left(p(\\boldsymbol{y} | \\mathcal{M}_j)\\right)\\approx&amp; -\\frac{K_j}{2}\\log(N)+\\log(p(\\boldsymbol{y}| \\boldsymbol{\\hat{\\theta}}_j^{MLE},\\mathcal{M}_j))= -\\frac{BIC}{2}, \\ N \\rightarrow \\infty. \\end{align*}\\] The marginal likelihood thus asymptotically converges to a linear transformation of the Bayesian Information Criterion (BIC), significantly simplifying its calculation. In addition, the BIC is consistent, that is, the probability of uncovering the population statistical model converges to one as the sample size converges to infinity given a \\(\\mathcal{M}\\)-closed view (Bernardo and Smith 1994), that is, one of the models in consideration is the population statistical model (data generating process) (Schwarz 1978; Burnham and Anderson 2004). In case that there is an \\(\\mathcal{M}\\)-completed view of nature, that is, there is a true data generating process, but the space of models that we are comparing does not include it, the BIC asymptotically selects the model that minimizes the Kullback-Leiber (KL) divergence to the true (population) model (Claeskens and Hjort 2008). References Bernardo, J., and A. Smith. 1994. Bayesian Theory. Chichester: Wiley. Burnham, Kenneth P, and David R Anderson. 2004. “Multimodel Inference: Understanding AIC and BIC in Model Selection.” Sociological Methods &amp; Research 33 (2): 261–304. Casella, G., and E. Moreno. 2006. “Objective Bayesian Variable Selection.” Journal of the American Statistical Association 101 (473): 157–67. Claeskens, Gerda, and Nils Lid Hjort. 2008. “Model Selection and Model Averaging.” Cambridge Books. Furnival, George M., and Robert W. Wilson. 1974. “Regressions by Leaps and Bounds.” Technometrics 16 (4): 499–511. George, E., and R. McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89. ———. 1997. “Approaches for Bayesian Variable Selection.” Statistica Sinica 7: 339–73. Green, P. J. 1995. “Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination.” Biometrika 82: 711–32. Hoeting, J., D. Madigan, A. Raftery, and C. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14 (4): 382–417. Jhonson, V. E., and D. Rossell. 2012. “Bayesian Model Selection in High-Dimensional Settings.” Journal of the American Statistical Association 107 (498): 649–60. Kass, R E, and A E Raftery. 1995. “Bayes factors.” Journal of the American Statistical Association 90 (430): 773–95. Ley, Eduardo, and Mark FJ Steel. 2009. “On the Effect of Prior Assumptions in Bayesian Model Averaging with Applications to Growth Regression.” Journal of Applied Econometrics 24 (4): 651–74. Madigan, David, Jeremy York, and Denis Allard. 1995. “Bayesian Graphical Models for Discrete Data.” International Statistical Review 63: 215–32. Madigan, D., and A. E. Raftery. 1994. “Model selection and accounting for model uncertainty in graphical models using Occam’s window.” Journal of the American Statistical Association 89 (428): 1535–46. Madigan, D., J. C. York, and D. Allard. 1995. “Bayesian Graphical Models for Discrete Data.” International Statistical Review 63 (2): 215–32. Park, T., and G. Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. Raftery, Adrian E. 1993. “Bayesian Model Selection in Structural Equation Models.” Sage Focus Editions 154: 163–63. Raftery, Adrian E., David Madigan, and Jennifer A. Hoeting. 1997. “Bayesian Model Averaging for Linear Regression Models.” Journal of the American Statistical Association 92 (437): 179–91. Raftery, Adrian, Jennifer Hoeting, Chris Volinsky, Ian Painter, and Ka Yee Yeung. 2012. Package BMA. https://CRAN.R-project.org/package=BMA. Schwarz, Gideon. 1978. “Estimating the Dimension of a Model.” The Annals of Statistics, 461–64. Simmons, Susan J, Fang Fang, Qijun Fang, and Karl Ricanek. 2010. “Markov Chain Monte Carlo Model Composition Search Strategy for Quantitative Trait Loci in a Bayesian Hierarchical Model.” World Academy of Science, Engineering and Technology 63: 58–61. Tierney, Luke, and Joseph B Kadane. 1986. “Accurate Approximations for Posterior Moments and Marginal Densities.” Journal of the American Statistical Association 81 (393): 82–86. Take into account that \\(K\\) can increase when interaction terms and/or polynomial terms of the original control variables are included.↩︎ We attach equal prior probabilities to each model in our GUI. However, this choice gives more prior probability to the set of models of medium size (think about the \\(k\\)-th row of Pascal’s triangle). An interesting alternative is to use the Beta-Binomial prior proposed by Ley and Steel (2009).↩︎ Scaling variables is always an important step in variable selection.↩︎ Variable selection (model selection or regularization) is a topic related to model uncertainty. Approaches such as stochastic search variable selection (spike and slab) (George and McCulloch 1993, 1997) and Bayesian Lasso (Park and Casella 2008) are good examples of how to tackle this issue. See Chapter 12.↩︎ "],["sec102.html", "10.2 The Gaussian linear model", " 10.2 The Gaussian linear model The Gaussian linear model specifies \\(\\boldsymbol{y}=\\alpha\\boldsymbol{i}_N+\\boldsymbol{X}_m\\boldsymbol{\\beta}_m+\\boldsymbol{\\mu}_m\\) such that \\(\\boldsymbol{\\mu}_m\\sim{N}(\\boldsymbol{0},\\sigma^2\\boldsymbol{I}_n)\\), and \\(\\boldsymbol{X}_m\\) does not have the column of ones. Following G. M. Koop (2003), the conjugate prior for the location parameters is \\(\\boldsymbol{\\beta}_m|\\sigma^2 \\sim {N}(\\boldsymbol{\\beta}_{m0}, \\sigma^2 \\boldsymbol{B}_{m0})\\), and the priors for \\(\\sigma^2\\) and \\(\\alpha\\) can be improper, as these parameters are common to all models \\(\\mathcal{M}_m\\). Particularly, \\(\\pi(\\sigma^2)\\propto 1/\\sigma^2\\) (Jeffreys’ prior for the linear Gaussian model, see Ibrahim and Laud (1991)) and \\(\\pi(\\alpha)\\propto 1\\). The selection of the hyperparameters of \\(\\boldsymbol{\\beta}_m\\) is more critical, as these parameters are not common to all models. A very common prior for the location parameters in the BMA literature is the Zellner’s prior (Zellner 1986), where \\(\\boldsymbol{\\beta}_{m0}=\\boldsymbol{0}_m\\) and \\(\\boldsymbol{B}_{m0}=(g_m\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1}\\). Observe that this covariance matrix is similar to the covariance matrix of the ordinary least squares estimator of the location parameters. This suggests that there is compatibility between the prior information and the sample information, and the only parameter to elicit is \\(g_m\\geq 0\\), which facilitates the elicitation process, as eliciting covariance matrices is a very hard endeavor. Following same steps as in Section 3.3, the posterior conditional distribution of \\(\\boldsymbol{\\beta}_m\\) has covariance matrix \\(\\sigma^2\\boldsymbol{B}_{mn}\\), where \\(\\boldsymbol{B}_{mn}=((1+g_m)\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1}\\) (Exercise 1), which means that \\(g_m=0\\) implies a non-informative prior, whereas \\(g_m=1\\) implies that prior and data information have same weights. We follow Fernandez, Ley, and Steel (2001), who recommend \\[\\begin{align*} g_m &amp; = \\begin{Bmatrix} 1/K^2, &amp; N \\leq K^2\\\\ 1/N, &amp; N&gt;K^2 \\end{Bmatrix}. \\end{align*}\\] Given the likelihood function, \\[\\begin{equation*} p(\\boldsymbol{\\beta}_m, \\sigma^2|\\boldsymbol{y}, \\boldsymbol{X}_m) = (2\\pi\\sigma^2)^{-\\frac{N}{2}} \\exp \\left\\{-\\frac{1}{2\\sigma^2} (\\boldsymbol{y} - \\alpha\\boldsymbol{i}_N - \\boldsymbol{X}_m\\boldsymbol{\\beta}_m)^{\\top}(\\boldsymbol{y} - \\alpha\\boldsymbol{i}_N - \\boldsymbol{X}_m\\boldsymbol{\\beta}_m) \\right\\}, \\end{equation*}\\] the marginal likelihood associated with model \\(\\mathcal{M}_m\\) is proportional to (Exercise 1) \\[\\begin{align*} p(\\boldsymbol{y}|\\mathcal{M}_m)&amp;\\propto \\left(\\frac{g_m}{1+g_m}\\right)^{k_m/2} \\left[(\\boldsymbol{y}-\\bar{y}\\boldsymbol{i}_N)^{\\top}(\\boldsymbol{y}-\\bar{y}\\boldsymbol{i}_N)-\\frac{1}{1+g_m}(\\boldsymbol{y}^{\\top}\\boldsymbol{P}_{X_m}\\boldsymbol{y})\\right]^{-(N-1)/2}, \\end{align*}\\] where all parameters are indexed to model \\(\\mathcal{M}_m\\), \\(\\boldsymbol{P}_{X_m}=\\boldsymbol{X}_m(\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1}\\boldsymbol{X}_m\\) is the projection matrix on the space generated by the columns of \\(\\boldsymbol{X}_m\\), and \\(\\bar{y}\\) is the sample mean of \\(\\boldsymbol{y}\\). We implement in our GUI four approaches to perform BMA in the Gaussian linear model: the BIC approximation using the Occam’s window approach, the MC3 algorithm using the analytical expression for calculating the marginal likelihood, an instrumental variable approach based on conditional likelihoods, and dynamic variable selection. Example: Simulation exercise Let’s perform a simulation exercise to assess the performance of the BIC approximation using the Occam’s window, and the Markov chain Monte Carlo model composition approaches. Let’s set a model where the computational burden is low and we know the data generating process (population statistical model). In particular, we set 10 regressors such that \\(x_k\\sim N(1, 1)\\), \\(k =1,\\dots,6\\), and \\(x_k\\sim B(0.5)\\), \\(k=7,\\dots,10\\). We set \\(\\boldsymbol{\\beta}=[1 \\ 0 \\ 0 \\ 0 \\ 0.5 \\ 0, 0, 0, 0, -0.7]^{\\top}\\) such that just \\(x_1\\), \\(x_5\\) and \\(x_{10}\\) are relevant to drive \\(y_i=1+\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}+\\mu_i\\), \\(\\mu_i\\sim N(0,0.5^2)\\). Observe that we just have \\(2^{10}=1024\\) models in this setting, thus, we can calculate the posterior model probability for each model. Our GUI uses the commands bicreg and MC3.REG from the package BMA to perform Bayesian model averaging in the linear regression model using the BIC approximation and MC3, respectively. These commands in turn are based on A. Raftery (1995) and Adrian E. Raftery, Madigan, and Hoeting (1997). The following code shows how to perform the simulation and get the posterior mean and standard deviation using these commands with the default values of hyperparameters and tuning parameters. rm(list = ls()); set.seed(010101) N &lt;- 1000 K1 &lt;- 6; K2 &lt;- 4; K &lt;- K1 + K2 X1 &lt;- matrix(rnorm(N*K1,1 ,1), N, K1) X2 &lt;- matrix(rbinom(N*K2, 1, 0.5), N, K2) X &lt;- cbind(X1, X2); e &lt;- rnorm(N, 0, 0.5) B &lt;- c(1,0,0,0,0.5,0,0,0,0,-0.7) y &lt;- 1 + X%*%B + e BMAglm &lt;- BMA::bicreg(X, y, strict = FALSE, OR = 50) summary(BMAglm) ## ## Call: ## BMA::bicreg(x = X, y = y, strict = FALSE, OR = 50) ## ## ## 8 models were selected ## Best 5 models (cumulative posterior probability = 0.9176 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## Intercept 100.0 1.0321222 0.031290 1.032e+00 1.047e+00 1.041e+00 ## X1 100.0 1.0018025 0.015274 1.002e+00 1.002e+00 1.002e+00 ## X2 3.0 -0.0002625 0.002997 . . -8.826e-03 ## X3 2.6 0.0001208 0.002530 . . . ## X4 2.9 0.0002341 0.002910 . . . ## X5 100.0 0.4976248 0.015668 4.976e-01 4.975e-01 4.976e-01 ## X6 3.9 -0.0005920 0.004256 . -1.509e-02 . ## X7 2.8 0.0004292 0.005739 . . . ## X8 2.9 0.0004508 0.005860 . . . ## X9 2.9 0.0004729 0.005914 . . . ## X10 100.0 -0.7035270 0.030939 -7.036e-01 -7.036e-01 -7.031e-01 ## ## nVar 3 4 4 ## r2 0.855 0.855 0.855 ## BIC -1.912e+03 -1.906e+03 -1.906e+03 ## post prob 0.791 0.039 0.030 ## model 4 model 5 ## Intercept 1.024e+00 1.024e+00 ## X1 1.002e+00 1.002e+00 ## X2 . . ## X3 . . ## X4 8.150e-03 . ## X5 4.975e-01 4.976e-01 ## X6 . . ## X7 . . ## X8 . 1.569e-02 ## X9 . . ## X10 -7.029e-01 -7.034e-01 ## ## nVar 4 4 ## r2 0.855 0.855 ## BIC -1.906e+03 -1.906e+03 ## post prob 0.029 0.029 BMAreg &lt;- BMA::MC3.REG(y, X, num.its=500) ## Warning in covMcd(X, alpha = alpha, use.correction = use.correction): The 505-th order statistic of the absolute deviation of variable 7 is ## zero. ## There are 505 observations (in the entire dataset of 1000 obs.) lying ## on the hyperplane with equation a_1*(x_i1 - m_1) + ... + a_p*(x_ip - ## m_p) = 0 with (m_1, ..., m_p) the mean of these observations and ## coefficients a_i from the vector a &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0) Models &lt;- unique(BMAreg[[&quot;variables&quot;]]) nModels &lt;- dim(Models)[1] nVistModels &lt;- dim(BMAreg[[&quot;variables&quot;]])[1] PMP &lt;- NULL for(m in 1:nModels){ idModm &lt;- NULL for(j in 1:nVistModels){ if(sum(Models[m,] == BMAreg[[&quot;variables&quot;]][j,]) == K){ idModm &lt;- c(idModm, j) }else{ idModm &lt;- idModm } } PMPm &lt;- sum(BMAreg[[&quot;post.prob&quot;]][idModm]) PMP &lt;- c(PMP, PMPm) } PIP &lt;- NULL for(k in 1:K){ PIPk &lt;- sum(PMP[which(Models[,k] == 1)]) PIP &lt;- c(PIP, PIPk) } plot(PIP) Means &lt;- matrix(0, nModels, K) Vars &lt;- matrix(0, nModels, K) for(m in 1:nModels){ idXs &lt;- which(Models[m,] == 1) if(length(idXs) == 0){ Regm &lt;- lm(y ~ 1) }else{ Xm &lt;- X[, idXs] Regm &lt;- lm(y ~ Xm) SumRegm &lt;- summary(Regm) Means[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,1] Vars[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,2]^2 } } BMAmeans &lt;- colSums(Means*PMP) BMAsd &lt;- (colSums(PMP*Vars) + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5 BMAmeans ## [1] 1.001771e+00 -5.322016e-05 6.635422e-06 3.721457e-07 4.976335e-01 ## [6] -1.271339e-04 1.000932e-08 2.107441e-05 6.578654e-06 -7.035557e-01 BMAsd ## [1] 1.527261e-02 1.353624e-03 5.936816e-04 1.163947e-04 1.566698e-02 ## [6] 1.987360e-03 2.778896e-05 1.270579e-03 6.997305e-04 3.093389e-02 BMAmeans/BMAsd ## [1] 6.559266e+01 -3.931680e-02 1.117673e-02 3.197272e-03 3.176320e+01 ## [6] -6.397124e-02 3.601905e-04 1.658647e-02 9.401697e-03 -2.274385e+01 We can see from the results that the BIC approximation with the Occam’s window, and the MC3 algorithm perform a good job finding the relevant regressors, and their posterior BMA means are very close to the population values. We also see that the BMA results are very similar in the two approaches. We can perform Bayesian model averaging in our GUI for linear Gaussian models using the BIC approximation and MC3 using the following Algorithms. We ask in Exercise 2 to perform BMA using the dataset 10ExportDiversificationHHI.csv from Jetter and Ramírez Hassan (2015). Algorithm: Bayesian Model Averaging in Linear Gaussian Models using the Bayesian Information Criterion Select Bayesian Model Averaging on the top panel Select Normal data model using the left radio button Select BIC using the right radio button under Which type do you want to perform? Upload the dataset, selecting first if there is a header in the file and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Type the OR number of the Occam’s window in the box under OR: Number between 5 and 50 (this step is optional, as the default value is 50) Click the Go! button Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset, the PIP (posterior inclusion probability, p!=0), the BMA posterior mean (EV), the BMA standard deviation (SD), and the posterior mean for models with the highest PMP. At the bottom of the table, for the models with the largest PMP, the number of variables (nVar), the coefficient of determination (r2), the BIC, and the PMP (post prob) are displayed Download posterior results using the Download results using BIC button. Two files are provided: The first file contains the best models by row according to the PMP (last column), indicating variable inclusion with a 1 (0 indicates no inclusion) The second file contains the PIP, the BMA expected value, and the standard deviation for each variable in the dataset Algorithm: Bayesian Model Averaging in Linear Gaussian Models using Markov Chain Monte Carlo Model Composition Select Bayesian Model Averaging on the top panel Select Normal data model using the left radio button Select MC3 using the right radio button under Which type do you want to perform? Upload the dataset, selecting first if there is a header in the file and the kind of separator in the csv file of the dataset (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Select MC3 iterations using the Range slider under the label MC3 iterations: Click the Go! button Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset, the PIP (posterior inclusion probability, p!=0), the BMA posterior mean (EV), the BMA standard deviation (SD), and the posterior mean for models with the highest PMP. At the bottom of the table, for the models with the largest PMP, the number of variables (nVar), the coefficient of determination (r2), the BIC, and the PMP (post prob) are displayed Download posterior results using the Download results using BIC button. Two files are provided: The first file contains the best models by row according to the PMP (last column), indicating variable inclusion with a 1 (0 indicates no inclusion) The second file contains the PIP, the BMA expected value, and the standard deviation for each variable in the dataset We show in the following code how to program a MC3 algorithm from scratch to perform BMA using the setting from the previous simulation exercise. The first part of the code is the function to calculate the log marginal likelihood. This is a small simulation setting, thus we can calculate the marginal likelihood for all 1024 models, and then calculate the posterior model probability standardizing using the model with the largest log marginal likelihood. We see from the results that this model is the data generating process (population statistical model). We also find that the posterior inclusion probabilities for \\(x_{1}\\), \\(x_{5}\\) and \\(x_{10}\\) are 1, whereas the PIP for the other variables are less than 0.05. Although BMA allows incorporating model uncertainty in a regression framework, sometimes it is desirable to select just one model. Two compelling alternatives are the model with the largest posterior model probability, and the median probability model. The latter is the model which includes every predictor that has posterior inclusion probability higher than 0.5. The first model is the best alternative for prediction in the case of a 0–1 loss function (Clyde and George 2004), whereas the second is the best alternative when there is a quadratic loss function in prediction (Barbieri and Berger 2004). In this simulation, the two criteria indicate selection of the data generating process. We also show how to estimate the posterior mean and standard deviation based on BMA in this code. We see that the posterior means are very close to the population parameters. rm(list = ls()); set.seed(010101) N &lt;- 1000 K1 &lt;- 6; K2 &lt;- 4; K &lt;- K1 + K2 X1 &lt;- matrix(rnorm(N*K1,1 ,1), N, K1) X2 &lt;- matrix(rbinom(N*K2, 1, 0.5), N, K2) X &lt;- cbind(X1, X2); e &lt;- rnorm(N, 0, 0.5) B &lt;- c(1,0,0,0,0.5,0,0,0,0,-0.7) y &lt;- 1 + X%*%B + e LogMLfunt &lt;- function(Model){ indr &lt;- Model == 1 kr &lt;- sum(indr) if(kr &gt; 0){ gr &lt;- ifelse(N &gt; kr^2, 1/N, kr^(-2)) Xr &lt;- matrix(Xnew[ , indr], ncol = kr) PX &lt;- Xr%*%solve(t(Xr)%*%Xr)%*%t(Xr) s2pos &lt;- c((t(y - mean(y))%*%(y - mean(y))) - t(y)%*%PX%*%y/(1 + gr)) mllMod &lt;- (kr/2)*log(gr/(1+gr))-(N-1)/2*log(s2pos) }else{ gr &lt;- ifelse(N &gt; kr^2, 1/N, kr^(-2)) s2pos &lt;- c((t(y - mean(y))%*%(y - mean(y)))) mllMod &lt;- (kr/2)*log(gr/(1+gr))-(N-1)/2*log(s2pos) } return(mllMod) } combs &lt;- expand.grid(c(0,1), c(0,1), c(0,1), c(0,1), c(0,1),c(0,1), c(0,1), c(0,1), c(0,1), c(0,1)) Xnew &lt;- apply(X, 2, scale) mll &lt;- sapply(1:2^K, function(s){LogMLfunt(matrix(combs[s,], 1, K))}) MaxPMP &lt;- which.max(mll); StMarLik &lt;- exp(mll-max(mll)) PMP &lt;- StMarLik/sum(StMarLik) PMP[MaxPMP] ## [1] 0.7705196 combs[MaxPMP,] ## Var1 Var2 Var3 Var4 Var5 Var6 Var7 Var8 Var9 Var10 ## 530 1 0 0 0 1 0 0 0 0 1 PIP &lt;- NULL for(k in 1:K){ PIPk &lt;- sum(PMP[which(combs[,k] == 1)]); PIP &lt;- c(PIP, PIPk) } PIP ## [1] 1.00000000 0.03617574 0.03208369 0.03516743 1.00000000 0.04795509 ## [7] 0.03457102 0.03468819 0.03510209 1.00000000 nModels &lt;- dim(combs)[1]; Means &lt;- matrix(0, nModels, K) Vars &lt;- matrix(0, nModels, K) for(m in 1:nModels){ idXs &lt;- which(combs[m,] == 1) if(length(idXs) == 0){ Regm &lt;- lm(y ~ 1) }else{ Xm &lt;- X[, idXs]; Regm &lt;- lm(y ~ Xm) SumRegm &lt;- summary(Regm) Means[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,1] Vars[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,2]^2 } } BMAmeans &lt;- colSums(Means*PMP) BMAmeans ## [1] 1.0018105888 -0.0003196423 0.0001489711 0.0002853524 0.4976225353 ## [6] -0.0007229563 0.0005342718 0.0005441905 0.0005758708 -0.7035206822 BMAsd &lt;- (colSums(PMP*Vars) + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5 BMAsd ## [1] 0.015274980 0.003304115 0.002814491 0.003214722 0.015668278 0.004694003 ## [7] 0.006400541 0.006435695 0.006528471 0.030940753 BMAmeans/BMAsd ## [1] 65.58506579 -0.09674068 0.05293002 0.08876427 31.75987477 ## [6] -0.15401700 0.08347292 0.08455816 0.08820914 -22.73767175 #### MC3 Algorithm #### M &lt;- 100 Models &lt;- matrix(rbinom(K*M, 1, p = 0.5), ncol=K, nrow = M) mllnew &lt;- sapply(1:M,function(s){LogMLfunt(matrix(Models[s,], 1, K))}) oind &lt;- order(mllnew, decreasing = TRUE) mllnew &lt;- mllnew[oind]; Models &lt;- Models[oind, ]; iter &lt;- 1000 pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = iter, width = 300); s &lt;- 1 while(s &lt;= iter){ ActModel &lt;- Models[M,]; idK &lt;- which(ActModel == 1) Kact &lt;- length(idK) if(Kact &lt; K &amp; Kact &gt; 1){ CardMol &lt;- K; opt &lt;- sample(1:3, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ if(opt == 2){ # Add All &lt;- 1:K; NewX &lt;- sample(All[-idK], 1) CandModel &lt;- ActModel; CandModel[NewX] &lt;- 1 }else{ # Subtract LessX &lt;- sample(idK, 1); CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } } }else{ CardMol &lt;- K + 1 if(Kact == K){ opt &lt;- sample(1:2, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ # Subtract LessX &lt;- sample(1:K, 1); CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } }else{ if(K == 1){ opt &lt;- sample(1:3, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ if(opt == 2){ # Add All &lt;- 1:K; NewX &lt;- sample(All[-idK], 1) CandModel &lt;- ActModel; CandModel[NewX] &lt;- 1 }else{ # Subtract LessX &lt;- sample(idK, 1); CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } } }else{ # Add NewX &lt;- sample(1:K, 1); CandModel &lt;- ActModel CandModel[NewX] &lt;- 1 } } } LogMLact &lt;- LogMLfunt(matrix(ActModel, 1, K)) LogMLcand &lt;- LogMLfunt(matrix(CandModel, 1, K)) alpha &lt;- min(1, exp(LogMLcand-LogMLact)) u &lt;- runif(1) if(u &lt;= alpha){ mllnew[M] &lt;- LogMLcand; Models[M, ] &lt;- CandModel oind &lt;- order(mllnew, decreasing = TRUE) mllnew &lt;- mllnew[oind]; Models &lt;- Models[oind, ] }else{ mllnew &lt;- mllnew; Models &lt;- Models } s &lt;- s + 1 setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),&quot;% done&quot;)) } close(pb) ## NULL ModelsUni &lt;- unique(Models) mllnewUni &lt;- sapply(1:dim(ModelsUni)[1], function(s){LogMLfunt(matrix(ModelsUni[s,], 1, K))}) StMarLik &lt;- exp(mllnewUni-mllnewUni[1]) PMP &lt;- StMarLik/sum(StMarLik) # PMP based on unique selected models nModels &lt;- dim(ModelsUni)[1] StMarLik &lt;- exp(mllnew-mllnew[1]) PMPold &lt;- StMarLik/sum(StMarLik) # PMP all selected models PMPot &lt;- NULL PMPap &lt;- NULL FreqMod &lt;- NULL for(m in 1:nModels){ idModm &lt;- NULL for(j in 1:M){ if(sum(ModelsUni[m,] == Models[j,]) == K){ idModm &lt;- c(idModm, j) }else{ idModm &lt;- idModm } } PMPm &lt;- sum(PMPold[idModm]) # PMP unique models using sum of all selected models PMPot &lt;- c(PMPot, PMPm) PMPapm &lt;- length(idModm)/M # PMP using relative frequency in all selected models PMPap &lt;- c(PMPap, PMPapm) FreqMod &lt;- c(FreqMod, length(idModm)) } PIP &lt;- NULL for(k in 1:K){ PIPk &lt;- sum(PMP[which(ModelsUni[,k] == 1)]) PIP &lt;- c(PIP, PIPk) } Means &lt;- matrix(0, nModels, K) Vars &lt;- matrix(0, nModels, K) for(m in 1:nModels){ idXs &lt;- which(ModelsUni[m,] == 1) if(length(idXs) == 0){ Regm &lt;- lm(y ~ 1) }else{ Xm &lt;- X[, idXs] Regm &lt;- lm(y ~ Xm) SumRegm &lt;- summary(Regm) Means[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,1] Vars[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,2]^2 } } BMAmeans &lt;- colSums(Means*PMP) BMAsd &lt;- (colSums(PMP*Vars) + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5 BMAmeans; BMAsd; BMAmeans/BMAsd ## [1] 1.001787e+00 -1.099674e-05 5.928440e-06 1.237330e-06 4.976289e-01 ## [6] -7.240575e-04 5.317222e-04 4.590382e-05 5.316641e-04 -7.035531e-01 ## [1] 0.0152733978 0.0006149476 0.0005564602 0.0002155678 0.0156674028 ## [6] 0.0046971545 0.0063886626 0.0018639899 0.0062682419 0.0309357255 ## [1] 65.590349277 -0.017882401 0.010653843 0.005739863 31.762056368 ## [6] -0.154148108 0.083229033 0.024626646 0.084818691 -22.742415397 The second part of the code demonstrates how to perform the MC3 algorithm. While this algorithm is not strictly necessary for this small-dimensional problem, it serves as a useful pedagogical exercise. The starting point is to set \\(S=100\\) random models and order their log marginal likelihoods. The logic of the algorithm is to select the worst model among the \\(S\\) models and propose a candidate model to compete against it. We repeat this process for 1000 iterations (as shown in the code). Note that 1000 iterations is fewer than the number of potential models (1024). This is the essence of the MC3 algorithm: performing fewer iterations than the number of models in the space. In our algorithm, we analyze all model scenarios using different conditionals and reasonably assume the same prior model probability for all models, with the same cardinality for both the actual and candidate models. The posterior model probability (PMP) can be calculated in several ways. One method is to recover the unique models from the final set of \\(S\\) models, calculate the log marginal likelihood for these models, and then standardize by the best model among them. Another method involves calculating the PMP using the complete set of \\(S\\) final models, accounting for the fact that some models may appear multiple times in the set, which requires summing the PMPs of repeated models. A third method is to calculate the PMP based on the relative frequency with which a model appears in the final set of \\(S\\) models. These three methods can yield different PMPs, particularly when the number of MC3 iterations is small. In our example, using 1000 MC3 iterations, the data-generating process receives the highest PMP across all three methods. A noteworthy aspect of this algorithm is that we can obtain a single model after significantly increasing the number of iterations (for example, try using 10,000 iterations). This can be advantageous if we require only one model. However, this approach neglects model uncertainty, which could be a desirable characteristic in some cases. As a challenge, we suggest programming an algorithm that yields \\(S\\) different models after completing the MC3 iterations (Exercise 3). An important issue to account for regressors (model) uncertainty in the identification of causal effects, rather than finding good predictors (association relationships), is endogeneity. Thus, we also implement the instrumental variable approach of Section 7.3 to tackle this issue in BMA. We assume that \\(\\boldsymbol{\\gamma}\\sim {N}(\\boldsymbol{0},\\boldsymbol{I})\\), \\(\\boldsymbol{\\beta}\\sim {N}(\\boldsymbol{0},\\boldsymbol{I})\\), and \\(\\boldsymbol{\\Sigma}^{-1} \\sim {W}(3,\\boldsymbol{I})\\) (Karl and Lenkoski 2012). Lenkoski, Karl, and Neudecker (2013) propose an algorithm based on conditional Bayes factors (J. M. Dickey and Gunel 1978) that allows embedding MC3 within a Gibbs sampling algorithm. Given the candidate (\\(M_{c}^{2nd}\\)) and actual (\\(M_{s-1}^{2nd}\\)) models for the iteration \\(s\\) in the second stage, the conditional Bayes factor is \\[\\begin{equation*} CBF^{2nd}=\\frac{p(\\boldsymbol{y}|M_{c}^{2nd},\\boldsymbol{\\gamma},\\boldsymbol{\\Sigma})}{p(\\boldsymbol{y}|M_{s-1}^{2nd},\\boldsymbol{\\gamma},\\boldsymbol{\\Sigma})}, \\end{equation*}\\] where \\[\\begin{align*} p(\\boldsymbol{y}|M_{c}^{2nd},\\boldsymbol{\\gamma},\\boldsymbol{\\Sigma})&amp;=\\int_{\\mathcal{M}^{2nd}}p(\\boldsymbol{y}|\\boldsymbol{\\beta},\\boldsymbol{\\gamma},\\boldsymbol{\\Sigma})\\pi(\\boldsymbol{\\beta}|M_{c}^{2nd})d\\boldsymbol{\\beta}\\\\ &amp;\\propto |\\boldsymbol{B}_n|^{-1/2} \\exp\\left\\{\\frac{1}{2}{\\boldsymbol{\\beta}_n}^{\\top}\\boldsymbol{B}_n^{-1}\\boldsymbol{\\beta}_n\\right\\} . \\end{align*}\\] In the first stage, \\[\\begin{equation*} CBF^{1st}=\\frac{p(\\boldsymbol{y}|M_{c}^{1st},\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})}{p(\\boldsymbol{y}|M_{s-1}^{1st},\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})}, \\end{equation*}\\] where \\[\\begin{align*} p(\\boldsymbol{y}|M_{c}^{1st},\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})&amp;=\\int_{\\mathcal{M}^{1st}}p(\\boldsymbol{y}|\\boldsymbol{\\gamma},\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})\\pi(\\boldsymbol{\\gamma}|M_{c}^{1st})d\\boldsymbol{\\gamma}\\\\ &amp;\\propto |\\boldsymbol{G}_n|^{-1/2} \\exp\\left\\{\\frac{1}{2}{\\boldsymbol{\\gamma}_n}^{\\top}\\boldsymbol{G}_n^{-1}\\boldsymbol{\\gamma}_n\\right\\}. \\end{align*}\\] These conditional Bayes factors assume \\(\\pi(M^{1st},M^{2sd})\\propto 1\\). See Lenkoski, Karl, and Neudecker (2013) for more details of the instrumental variable BMA algorithm.5 We perform instrumental variable BMA in our GUI using the package ivbma. The following Algorithm shows how to perform this in our GUI. Algorithm: Instrumental Variable Bayesian Model Averaging in Linear Gaussian Models Select Bayesian Model Averaging on the top panel Select Normal data model using the left radio button Select Instrumental variable using the right radio button under Which type do you want to perform? Upload the dataset containing the dependent variable, endogenous regressors, and exogenous regressors (including the constant). The user should first select if there is a header in the file and the kind of separator in the csv file (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Upload the dataset containing the instruments. The user should first select if there is a header in the file and the kind of separator in the csv file (comma, semicolon, or tab). Then, use the Browse button under the Choose File (Instruments) legend Write down the number of endogenous regressors in the box labeled Number of Endogenous variables Select MCMC iterations and burn-in using the Range slider under the labels MCMC iterations: and Burn-in Sample: Click the Go! button Analyze results: After a few seconds or minutes, two tables appear showing, for each regressor in the dataset, the PIP (posterior inclusion probability, p!=0), and the BMA posterior mean (EV). The top table shows the results of the second stage (main equation), and the bottom table shows the results of the first stage (auxiliary equations) Download posterior results using the Download results using IV button. Three files are provided: The first file contains the posterior inclusion probabilities of each variable and the BMA posterior means of the coefficients in the first stage equations The second file contains these results for the second stage (main equation) The third file contains the posterior chains of all parameters by iteration Example: Simulation exercise Let’s assume that \\(y_i = 2 + 0.5x_{i1} - x_{i2} + x_{i3} + \\mu_i\\), where \\(x_{i1} = 4z_{i1} - z_{i2} + 2z_{i3} + \\epsilon_{i1}\\) and \\(x_{i2} = -2z_{i1} + 3z_{i2} - z_{i3} + \\epsilon_{i2}\\), such that \\([\\epsilon_{i1} \\ \\epsilon_{i2} \\ \\mu_i]^{\\top} \\sim N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{\\Sigma} = \\begin{bmatrix} 1 &amp; 0 &amp; 0.8 \\\\ 0 &amp; 1 &amp; 0.5 \\\\ 0.8 &amp; 0.5 &amp; 1 \\end{bmatrix}\\), for \\(i = 1, 2, \\dots, 1000\\). The endogeneity arises due to the correlation between \\(\\mu_i\\) and \\(x_{i1}\\) and \\(x_{i2}\\) through the stochastic errors. In addition, there are three instruments, \\(z_{il} \\sim U(0,1)\\), for \\(l = 1, 2, 3\\), and another 18 regressors believed to influence \\(y_i\\), which are distributed according to a standard normal distribution. The following code shows how to perform IV BMA using the ivbma package. We see from the results that the PIP of \\(x_{i1}\\), \\(x_{i2}\\), intercept and \\(x_{i3}\\) are equal to 1, whereas the remaining PIP are close to 0. In addition, the BMA means are also close to the population values. The PIP of the first stage equations, as well as their BMA posterior means, are very close to the populations values. The same happens with the covariance matrix. rm(list = ls()) set.seed(010101) simIV &lt;- function(delta1,delta2,beta0,betas1,betas2,beta2,Sigma,n,z) { eps &lt;- matrix(rnorm(3*n),ncol=3) %*% chol(Sigma) xs1 &lt;- z%*%delta1 + eps[,1] xs2 &lt;- z%*%delta2 + eps[,2] x2 &lt;- rnorm(dim(z)[1]) y &lt;- beta0+betas1*xs1+betas2*xs2+beta2*x2 + eps[,3] X &lt;- as.matrix(cbind(xs1,xs2,1,x2)) colnames(X) &lt;- c(&quot;x1en&quot;,&quot;x2en&quot;,&quot;cte&quot;,&quot;xex&quot;) y &lt;- matrix(y,dim(z)[1],1) colnames(y) &lt;- c(&quot;y&quot;) list(X=X,y=y) } n &lt;- 1000 ; p &lt;- 3 z &lt;- matrix(runif(n*p),ncol=p) rho31 &lt;- 0.8; rho32 &lt;- 0.5; Sigma &lt;- matrix(c(1,0,rho31,0,1,rho32,rho31,rho32,1),ncol=3) delta1 &lt;- c(4,-1,2); delta2 &lt;- c(-2,3,-1); betas1 &lt;- .5; betas2 &lt;- -1; beta2 &lt;- 1; beta0 &lt;- 2 simiv &lt;- simIV(delta1,delta2,beta0,betas1,betas2,beta2,Sigma,n,z) nW &lt;- 18 W &lt;- matrix(rnorm(nW*dim(z)[1]),dim(z)[1],nW) YXW&lt;-cbind(simiv$y, simiv$X, W) y &lt;- YXW[,1]; X &lt;- YXW[,2:3]; W &lt;- YXW[,-c(1:3)] S &lt;- 10000; burnin &lt;- 1000 regivBMA &lt;- ivbma::ivbma(Y = y, X = X, Z = z, W = W, s = S+burnin, b = burnin, odens = S, print.every = round(S/10), run.diagnostics = FALSE) ## [1] &quot;Running IVBMA for 11000 iterations 2025-07-24 14:03:35.04785&quot; ## [1] &quot;On Iteration 1000 2025-07-24 14:03:40.048142&quot; ## [1] &quot;On Iteration 2000 2025-07-24 14:03:45.125475&quot; ## [1] &quot;On Iteration 3000 2025-07-24 14:03:50.205975&quot; ## [1] &quot;On Iteration 4000 2025-07-24 14:03:55.070597&quot; ## [1] &quot;On Iteration 5000 2025-07-24 14:04:00.024637&quot; ## [1] &quot;On Iteration 6000 2025-07-24 14:04:05.070484&quot; ## [1] &quot;On Iteration 7000 2025-07-24 14:04:10.302642&quot; ## [1] &quot;On Iteration 8000 2025-07-24 14:04:15.672808&quot; ## [1] &quot;On Iteration 9000 2025-07-24 14:04:20.695435&quot; ## [1] &quot;On Iteration 10000 2025-07-24 14:04:25.729681&quot; ## [1] &quot;On Iteration 11000 2025-07-24 14:04:30.959768&quot; PIPmain &lt;- regivBMA[[&quot;L.bar&quot;]] # PIP outcome PIPmain ## [1] 1.0000 1.0000 1.0000 1.0000 0.0140 0.0243 0.0116 0.0309 0.0305 0.0145 ## [11] 0.0105 0.0103 0.0233 0.0402 0.0048 0.0349 0.0210 0.0343 0.0044 0.0137 ## [21] 0.0175 0.0223 EVmain &lt;- regivBMA[[&quot;rho.bar&quot;]] # Posterior mean outcome EVmain ## [1] 5.074816e-01 -9.873671e-01 2.004880e+00 1.005181e+00 -1.849026e-04 ## [6] -3.951743e-04 -8.087320e-05 6.401508e-04 6.699469e-04 -1.456550e-04 ## [11] 7.932297e-05 -5.582272e-05 -1.913875e-04 6.135038e-04 2.458307e-05 ## [16] -5.885881e-04 -2.431515e-05 -1.170588e-04 1.793956e-05 7.936547e-05 ## [21] -2.579314e-05 -2.026556e-04 PIPaux &lt;- regivBMA[[&quot;M.bar&quot;]] # PIP auxiliary EVaux &lt;- regivBMA[[&quot;lambda.bar&quot;]] # Posterior mean auxiliary plot(EVaux[,1]) plot(EVaux[,2]) EVsigma &lt;- regivBMA[[&quot;Sigma.bar&quot;]] # Posterior mean variance matrix EVsigma ## [,1] [,2] [,3] ## [1,] 1.0345164 0.848730076 0.515301058 ## [2,] 0.8487301 1.081309693 0.008180855 ## [3,] 0.5153011 0.008180855 1.028434857 Bayesian model averaging has been also extended to state-space models. The point of departure is the univariate random walk state-space model (see Chapter 8) conditional on model \\(\\mathcal{M}_m\\), \\(m=1,2\\dots,M\\). \\[\\begin{align} y_t&amp;=\\boldsymbol{x}_{mt}^{\\top}\\boldsymbol{\\beta}_{mt}+\\mu_{mt}\\\\ \\boldsymbol{\\beta}_{mt}&amp;=\\boldsymbol{\\beta}_{mt-1}+\\boldsymbol{w}_{mt}, \\end{align}\\] where \\(\\mu_{mt}\\sim N(0,\\sigma^2)\\) and \\(\\boldsymbol{w}_{mt}\\sim N(\\boldsymbol{0},\\boldsymbol{\\Omega}_{mt})\\). Given \\(\\boldsymbol{\\beta}_{mt-1}|\\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{b}_{mt-1},\\boldsymbol{B}_{mt-1})\\), then, we know from Chapter 8 that \\(\\boldsymbol{\\beta}_{mt}|\\boldsymbol{y}_{1:t-1}\\sim N(\\boldsymbol{b}_{mt-1}, \\boldsymbol{R}_{mt})\\), \\(\\boldsymbol{R}_{mt}=\\boldsymbol{B}_{mt-1}+\\boldsymbol{\\Omega}_{mt}\\). Specification of \\(\\boldsymbol{\\Omega}_t\\) can be highly demanding. Thus, a common approach is to express \\(\\boldsymbol{\\Omega}_{mt}=\\frac{1-\\lambda}{\\lambda}\\boldsymbol{B}_{mt-1}\\), where \\(\\lambda\\) is called the forgetting parameter or discount factor, because it discounts the matrix \\(\\boldsymbol{B}_{mt-1}\\) that we would have with a deterministic state evolution into the matrix \\(\\boldsymbol{R}_{mt}\\) (Petris, Petrone, and Campagnoli 2009). This parameter is typically slightly below 1, and implies that \\(\\boldsymbol{R}_{mt}=\\lambda^{-1}\\boldsymbol{B}_{mt-1}\\). (\\(\\lambda^{-1}&gt;1\\)). Adrian E. Raftery, Kárnỳ, and Ettler (2010) assume that the model changes infrequently, and its evolution is given by the transition matrix \\(\\boldsymbol{T}=[t_{ml}]\\), where \\(t_{ml}=P(\\mathcal{M}_t=\\mathcal{M}_m|\\mathcal{M}_{t-1}=\\mathcal{M}_l)\\). Then, the aim is to calculate the filtering distribution \\(p(\\boldsymbol{\\beta}_{mt},\\mathcal{M}_t|y_t)=\\sum_{m=1}^Mp(\\boldsymbol{\\beta}_{mt}|\\mathcal{M}_t=\\mathcal{M}_m,y_t)p(\\mathcal{M}_t=\\mathcal{M}_m|y_t)\\). Thus, given the conditional distribution of the state at time \\(t-1\\), \\(p(\\boldsymbol{\\beta}_{mt-1},\\mathcal{M}_{t-1}|{y}_{t-1})=\\sum_{m=1}^Mp(\\boldsymbol{\\beta}_{mt-1}|\\mathcal{M}_{t-1}=\\mathcal{M}_m,{y}_{t-1})p(\\mathcal{M}_{t-1}=\\mathcal{M}_m|{y}_{t-1})\\), where the conditional distribution of \\(\\boldsymbol{\\beta}_{mt-1}\\) is approximated by a Gaussian distribution, \\(\\boldsymbol{\\beta}_{mt-1}|\\mathcal{M}_{t-1}=\\mathcal{M}_{m},y_{t-1}\\sim N(\\boldsymbol{b}_{mt-1},\\boldsymbol{B}_{mt-1})\\), then the first step to get the one-step-ahead predictive distribution is getting the prediction of the model indicator, \\[\\begin{align*} p(\\mathcal{M}_t=\\mathcal{M}_l|y_{t-1})&amp;=\\sum_{m=1}^M p(\\mathcal{M}_{t-1}=\\mathcal{M}_m|y_{t-1})\\times t_{lm}\\\\ &amp;\\approx \\frac{p(\\mathcal{M}_{t-1}=\\mathcal{M}_l|y_{t-1})^{\\delta}+c}{\\sum_{m=1}^M p(\\mathcal{M}_{t-1}=\\mathcal{M}_m|y_{t-1})^{\\delta}+c}, \\end{align*}\\] where the second equality is used to avoid dealing with the \\(M^2\\) elements of the transition matrix \\(\\boldsymbol{T}\\) such that the forgetting parameter \\(\\delta\\) is used, this parameter is slightly less than 1, and \\(c=0.001/M\\) is introduced to handle a model probability being brought to computational zero by outliers. Then, we get the one-step-ahead predictive distribution of the state vector, \\(\\boldsymbol{\\beta}_{mt}|\\mathcal{M}_{t}=\\mathcal{M}_{m},y_{t-1}\\sim N(\\boldsymbol{b}_{mt-1},\\lambda^{-1}\\boldsymbol{B}_{mt-1})\\) Now, we consider the filtering stage, where the model filtering equation is \\[\\begin{align*} p(\\mathcal{M}_t=\\mathcal{M}_l|y_{t})=\\frac{p(\\mathcal{M}_t=\\mathcal{M}_l|y_{t-1})p_l(y_t|y_{t-1})}{\\sum_{m=1}^M p(\\mathcal{M}_t=\\mathcal{M}_m|y_{t-1})p_m(y_t|y_{t-1})}, \\end{align*}\\] where \\(p_m(y_t|y_{t-1})\\) is the one-step-ahead predictive distribution of \\(y_t|{y}_{t-1}\\), which is \\(N(f_t,Q_t)\\), where \\(f_t=\\boldsymbol{x}_t^{\\top}\\boldsymbol{b}_{t-1}\\) and \\(Q_t=\\boldsymbol{x}_{mt}^{\\top}\\lambda^{-1}\\boldsymbol{B}_{mt-1}\\boldsymbol{x}_{mt}+\\sigma^2\\) (see Chapter 8). The states filtering equation is \\(\\boldsymbol{\\beta}_{mt}|\\mathcal{M}_{t}=\\mathcal{M}_{m},y_{t}\\sim N(\\boldsymbol{b}_{mt},\\boldsymbol{B}_{mt})\\) where \\(\\boldsymbol{b}_{mt}\\) and \\(\\boldsymbol{B}_{mt}\\) are given in the Kalman filtering recursion of Chapter 8. Adrian E. Raftery, Kárnỳ, and Ettler (2010) initiate their algorithm assuming equal prior model probabilities, and \\(\\sigma^2\\) is estimated using a recursive method of moments estimator.6 We implement dynamic Bayesian model averaging in our GUI using the function dma from the package dma. The next Algorithm shows how to perform inference using our GUI. Algorithm: Dynamic Bayesian Model Averaging Select Bayesian Model Averaging on the top panel Select Normal data model using the left radio button Select Dynamic Bayesian Model Averaging using the right radio button under Which type do you want to perform? Upload the dataset, selecting first whether there is a header in the file and the type of separator in the csv file (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Upload the matrix of models, selecting first whether there is a header in the file and the type of separator in the csv file (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Type the forgetting parameters in the boxes under Lambda: Number slightly below 1 and Delta: Number slightly below 1. This is not necessary, as the default values are 0.99 for both Click the Go! button Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset, the dynamic Bayesian average filtering recursions for each state (Mean and Standard deviation), the posterior model probability (PMP), and the Bayesian model averaging prediction (Prediction) Download posterior results using the Download results DBMA button. Two files are provided: The first file contains the dynamic Bayesian average filtering recursions for each state The second file contains the PMP of each model and the dynamic Bayesian model averaging prediction Example: Dynamic Bayesian model averaging We perform a simulation exercise where there are 8 (\\(2^3\\)) competing models originating from 3 regressors: \\(x_{tk} \\sim N(0.5, 0.8^2)\\) for \\(k = 2, 3, 4\\), with \\(\\beta_1 = 0.5\\). The sequence \\(\\beta_{2t}\\) ranges from 1 to 2 in steps of \\(1/T\\), and \\(\\beta_{3t}\\) is given by: \\[ \\beta_{3t} = \\begin{cases} -1, &amp; 1 &lt; t \\leq 0.75T \\\\ 0, &amp; 0.75T &lt; t \\leq T \\end{cases} \\] and \\(\\beta_4 = 1.2\\). Then, we have the model: \\[ y_t = \\beta_1 + \\beta_{2t} x_{2t} + \\beta_{3t} x_{3t} + \\beta_4 x_{4t} + \\mu_t, \\] where \\(\\mu_t \\sim N(0,1)\\) for \\(t = 1, 2, \\dots, 500\\). This setting implies that during the first 75% of the period, the model with all 3 regressors is the data-generating process, while after this, the model with regressors 2 and 4 is the data-generating process. The following code shows the simulation exercise and the results of the dynamic Bayesian model averaging, setting \\(\\lambda = \\delta = 0.99\\). rm(list = ls()); set.seed(010101) T &lt;- 500; K &lt;- 3 X &lt;- matrix(rnorm(T*K, mean = 0.5, sd = 0.8), T, K) combs &lt;- expand.grid(c(0,1), c(0,1), c(0,1)) B1 &lt;- 0.5; B2t &lt;- seq(1, 2, length.out=T ) a &lt;- 0.75; B3t &lt;- c(rep(-1,round(a*T)), rep(0,round((1-a)*T))) B4 &lt;- 1.2; sigma &lt;- 1; mu &lt;- rnorm(T, 0, sigma) y &lt;- B1 + X[,1]*B2t + X[,2]*B3t + X[,3]*B4 + mu T0 &lt;- 50 dma.test &lt;- dma::dma(X, y, combs, lambda=.99, gamma=.99, initialperiod = T0) plot(dma.test[[&quot;pmp&quot;]][-c(1:T0),8], type = &quot;l&quot;, col = &quot;green&quot;, main = &quot;Posterior model probability&quot;, xlab = &quot;Time&quot;, ylab = &quot;PMP&quot;) lines(dma.test[[&quot;pmp&quot;]][-c(1:T0),6], col = &quot;red&quot;) legend(x = 0, y = 1, legend = c(&quot;Model: All regressors&quot;, &quot;Model: Regressors 2 and 4&quot;), col = c(&quot;green&quot;, &quot;red&quot;), lty=1:1, cex=0.8) require(latex2exp) ## Loading required package: latex2exp plot(dma.test[[&quot;thetahat.ma&quot;]][-c(1:T0),1], type = &quot;l&quot;, col = &quot;green&quot;, main = &quot;Bayesian model average filtering recursion&quot;, xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{1}$&quot;)) abline(h = B1, col = &quot;red&quot;) legend(x = 0, y = 0.4, legend = c(&quot;State filtering&quot;, &quot;State population&quot;), col = c(&quot;green&quot;, &quot;red&quot;), lty=1:1, cex=0.8) plot(dma.test[[&quot;thetahat.ma&quot;]][-c(1:T0),2], type = &quot;l&quot;, col = &quot;green&quot;, main = &quot;Bayesian model average filtering recursion&quot;, xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{2t}$&quot;), ylim = c(0.5,2)) lines(B2t[-c(1:T0)], col = &quot;red&quot;) legend(x = 0, y = 0.8, legend = c(&quot;State filtering&quot;, &quot;State population&quot;), col = c(&quot;green&quot;, &quot;red&quot;), lty=1:1, cex=0.8) plot(dma.test[[&quot;thetahat.ma&quot;]][-c(1:T0),3], type = &quot;l&quot;, col = &quot;green&quot;, main = &quot;Bayesian model average filtering recursion&quot;, xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{3t}$&quot;)) lines(B3t[-c(1:T0)], col = &quot;red&quot;) legend(x = 0, y = -0.4, legend = c(&quot;State filtering&quot;, &quot;State population&quot;), col = c(&quot;green&quot;, &quot;red&quot;), lty=1:1, cex=0.8) plot(dma.test[[&quot;thetahat.ma&quot;]][-c(1:T0),4], type = &quot;l&quot;, col = &quot;green&quot;, main = &quot;Bayesian model average filtering recursion&quot;, xlab = &quot;Time&quot;, ylab = TeX(&quot;$\\\\beta_{4t}$&quot;)) abline(h = B4, col = &quot;red&quot;) legend(x = 0, y = 1.3, legend = c(&quot;State filtering&quot;, &quot;State population&quot;), col = c(&quot;green&quot;, &quot;red&quot;), lty=1:1, cex=0.8) The first Figure shows the posterior model probabilities for the model with all the regressors (green line) and the model with regressors 2 and 4 (red line). On one hand, we see that the model with all regressors, which is the data-generating process in the first period (\\(t \\leq 0.75T\\)), has a PMP close to 1, and then its PMP decreases. On the other hand, the model with regressors 2 and 4 has a PMP close to 0 in the first part of the period, and then its PMP increases to values higher than 60% on average, when this model becomes the data-generating process. These results suggest that, in this particular simulation exercise, the dynamic Bayesian model averaging method works relatively well in calculating the PMPs. The following four Figures show a comparison between the Bayesian model averaging filtering recursions of the states (green lines) and their population values (red lines). We observe that the filtering recursions follow the general pattern of the population values. However, the values are not perfectly aligned. This discrepancy arises because the posterior model probabilities (PMPs) of the models that match the data-generating process are not equal to 1, which in turn affects the performance of the filtering recursions. Dynamic Bayesian model averaging was extended to logit models by McCormick et al. (2012). We ask in Exercise 12 to perform a simulation of this model, and perform BMA using the function logistic.dma from the dma package. References Barbieri, M., and J. Berger. 2004. “Optimal Predictive Model Selection.” The Annals of Statistics 32 (3): 870–97. Clyde, M., and E. George. 2004. “Model Uncertatinty.” Statistical Science 19 (1): 81–94. Dickey, J. M., and E. Gunel. 1978. “Bayes Factors from Mixed Probabilities.” Journal of the Royal Statistical Society: Series B (Methodology) 40: 43–46. Fernandez, Carmen, Eduardo Ley, and Mark FJ Steel. 2001. “Benchmark Priors for Bayesian Model Averaging.” Journal of Econometrics 100 (2): 381–427. Ibrahim, Joseph G., and Purushottam W. Laud. 1991. “On Bayesian Analysis of Generalized Linear Models Using Jeffreys’s Prior.” Journal of the American Statistical Association 86 (416): 981–86. Jetter, M., and A. Ramírez Hassan. 2015. “Want Export Diversification? Educate the Kids First.” Economic Inquiry 53 (4): 1765–82. Karl, A., and A. Lenkoski. 2012. “Instrumental Variable Bayesian Model Averaging via Conditional Bayes Factor.” Heidelberg University. Koop, Gary M. 2003. Bayesian Econometrics. John Wiley &amp; Sons Inc. Koop, G, R León-Gonzalez, and R Strachan. 2012. “Bayesian Model Averaging in the Instrumental Variable Regression Model.” Journal of Econometrics 171: 237–50. Lenkoski, Alex, Theo S. Eicher, and Adrian Raftery. 2014. “Two-Stage Bayesian Model Averaging in Endogeneous Variable Models.” Econometric Reviews 33. Lenkoski, Alex, Anna Karl, and Andreas Neudecker. 2013. Package ivbma. https://CRAN.R-project.org/package=ivbma. McCormick, Tyler H, Adrian E Raftery, David Madigan, and Randall S Burd. 2012. “Dynamic Logistic Regression and Dynamic Model Averaging for Binary Classification.” Biometrics 68 (1): 23–30. Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. “Dynamic Linear Models.” In Dynamic Linear Models with r, 31–84. Springer. Raftery, A. 1995. “Bayesian Model Selection in Social Research.” Sociological Methodology 25: 111–63. Raftery, Adrian E, Miroslav Kárnỳ, and Pavel Ettler. 2010. “Online Prediction Under Model Uncertainty via Dynamic Model Averaging: Application to a Cold Rolling Mill.” Technometrics 52 (1): 52–66. Raftery, Adrian E., David Madigan, and Jennifer A. Hoeting. 1997. “Bayesian Model Averaging for Linear Regression Models.” Journal of the American Statistical Association 92 (437): 179–91. Ramı́rez-Hassan, Andrés. 2020. “Dynamic Variable Selection in Dynamic Logistic Regression: An Application to Internet Subscription.” Empirical Economics 59 (2): 909–32. Zellner, Arnold. 1986. “On Assessing Prior Distributions and Bayesian Regression Analysis with g-Prior Distributions.” Bayesian Inference and Decision Techniques. G. Koop, León-Gonzalez, and Strachan (2012) and Lenkoski, Eicher, and Raftery (2014) propose other frameworks for BMA taking into account endogeneity.↩︎ Ramı́rez-Hassan (2020) extends this approach to Markov chain Monte Carlo model composition↩︎ "],["sec103.html", "10.3 Generalized linear models", " 10.3 Generalized linear models Generalized linear models (GLMs) were introduced by Nelder and Wedderburn (1972), extending the concept of linear regression to a more general setting. These models are characterized by: i) a dependent variable \\(y_i\\) whose probability distribution function belongs to the exponential family (see Section 3.1, ii) a linear predictor \\(\\eta = \\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}\\), and iii) a link function such that \\(\\mathbb{E}[y|\\boldsymbol{x}] = g^{-1}(\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta})\\), which implies that \\(g(\\mathbb{E}[y|\\boldsymbol{x}]) = \\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}\\). GLMs can be extended to the overdispersed exponential family (McCullagh and Nelder 1989). As we know from Section 3.1, the Poisson distribution belongs to the exponential family, such that \\(p(y|\\lambda) = \\frac{\\exp(-\\lambda)\\exp(y\\log(\\lambda))}{y!}\\), or in the canonical form \\(p(y|\\eta) = \\frac{\\exp(\\eta y - \\exp(\\eta))}{y!}\\), where \\(\\eta = \\log(\\lambda)\\), which means that \\(\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta} = \\log(\\lambda)\\). Consequently, \\(\\mathbb{E}[y|\\boldsymbol{x}] = \\nabla(\\exp(\\eta)) = \\exp(\\eta) = \\lambda = \\exp(\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta})\\). Therefore, the link function in the Poisson case is the log function. In Exercise 6, we ask you to show that the link function in the Bernoulli case is the logit function. Other examples include the identity function in the case of the Gaussian distribution and the negative inverse in the case of the gamma distribution. We can use the GLM framework to perform Bayesian model averaging (BMA) using the BIC approximation, following A. Raftery (1995). Specifically, the BIC is given by \\(BIC = k_m \\log(N) - 2 \\log(p(\\hat{\\boldsymbol{\\theta}}_m | \\boldsymbol{y}))\\), where \\(\\hat{\\boldsymbol{\\theta}}_m\\) is the maximum likelihood estimator. Thus, we simply need to calculate the likelihood function at the maximum likelihood estimator. Example: Simulation exercises Let’s perform some simulation exercises to assess the performance of the BIC approximation using the Occam’s window in GLMs. There are 27 regressors, where \\(x_{i1}\\) and \\(x_{i2}\\) are just the relevant regressors in all exercises, \\(i=1,2,\\dots,1000\\). Logit: \\(x_k\\sim N(0, 1)\\), \\(k =1,\\dots,27\\), and \\(p(y_i=1|\\boldsymbol{x}_i)=\\exp(0.5+0.8x_{i1}-1.2x_{i2})/(1+\\exp(0.5+0.8x_{i1}-1.2x_{i2}))\\). Gamma: \\(x_k\\sim N(0, 0.5^2)\\), \\(k =1,\\dots,27\\), and \\(y_i\\sim G(\\alpha,\\delta)\\) where \\(\\alpha=-(0.5+0.2x_{i1}0.1x_{i2})^{-1}\\) and \\(\\delta=1\\). Poisson: \\(x_k\\sim N(0, 1)\\), \\(k =1,\\dots,27\\), and \\(\\mathbb{E}[y_i|\\boldsymbol{x}_i]=\\lambda_i=\\exp(0.5+1.1x_{i1}+0.7x_{i2})\\). Our GUI uses the command bic.glm from the BMA package to perform BMA using the BIC approximation with the Occam’s window in GLMs. The next Algorithm shows how to do this in our GUI, and the following code shows how to perform BMA in logit models using the simulation setting. Algorithm: Bayesian Model Averaging in Generalized Linear Models using BIC Select Bayesian Model Averaging on the top panel Select the Generalized Linear Model using the left radio button. Options: Binomial data (Logit) Real positive data (Gamma) Count data (Poisson) Upload the dataset, selecting first whether there is a header in the file and the type of separator in the csv file (comma, semicolon, or tab). Then, use the Browse button under the Choose File legend Type the OR number of Occam’s window in the box under OR: Number between 5 and 50. This is not necessary, as the default value is 50 Type the OL number of Occam’s window in the box under OL: Number between 0.0001 and 1. This is not necessary, as the default value is 0.0025 Click the Go! button Analyze results: After a few seconds or minutes, a table appears showing, for each regressor in the dataset: The posterior inclusion probability (p!=0) The BMA posterior mean (EV) The BMA standard deviation (SD) The PMP for the most relevant models (highest PMPs) Download posterior results using the Download results using BIC button. Two files are provided: The first file contains the best models by row according to the PMP (last column), indicating variable inclusion with 1 (0 indicates no inclusion) The second file contains the PIP, the BMA expected value, and the standard deviation for each variable in the dataset The results show that the PIPs of \\(x_{i1}\\) and \\(x_{i2}\\) are equal 1 in all three settings, the data generating process gets the highest PMP, and the BMA posterior means are close to the population values in each simulation setting. The other variables get PIPs close to 0, except a few exceptions, and the BMA posterior means are also close to 0. This suggests that the BIC approximation does a good job finding the data generating process in generalized linear models. We can take advantage of the glm function in R to perform BMA by programming an MC3 algorithm. The following code illustrates how to do this in the Poisson simulation. First, we simulate the data; second, we define a function to compute the log marginal likelihood approximation using the results from the glm function. Then, we initialize the models to begin the MC3 algorithm. After that, we implement the MC3 algorithm, which involves small modifications of the code used for MC3 in Gaussian linear models. We can calculate the posterior model probabilities (PMPs), posterior inclusion probabilities (PIPs), BMA means, and standard deviations as we did previously. The simulation setting involves \\(2^{27}\\) models, which corresponds to approximately 135 million models in the model space. We run our MC3 algorithm using the BIC approximation with 50,000 iterations. This takes considerably more time than the BIC approximation from the BMA package, but it seems to perform well in identifying the data-generating process, as the PMP of this model equals 1. The posterior inclusion probabilities (PIPs) for \\(x_{i1}\\) and \\(x_{i2}\\) are also 1, and the posterior means are 1.1 and 0.7, respectively, which are equal to the population values. The t-ratios are far greater than 2. However, running 50,000 iterations results in mass concentration in one model, in this case, the data-generating process. If we run 25,000 MC3 iterations, the highest PMP is 0.8, but it is not associated with the data-generating process. Nonetheless, the PIP is equal to 1 for \\(x_{i1}\\) and \\(x_{i2}\\), and other regressors also have high PIPs. The BMA means for \\(x_{i1}\\) and \\(x_{i2}\\) are equal to the population values, and the BMA means for the other regressors are equal to 0. The t-ratios of the regressors in the population statistical model are much greater than 2, whereas the t-ratios of the other regressors are equal to 0. This exercise demonstrates that 25,000 iterations were not sufficient to uncover the data-generating process. However, it also emphasizes an important point: we need to analyze all the relevant results from the BMA analysis, not just the PMPs and/or PIPs. In Exercise 10, we ask you to use this approach to perform a BMA algorithm in the logit regression, using the simulation setting for logit models from this section. ### Logit ### rm(list = ls()); set.seed(010101) n&lt;-1000; B&lt;-c(0.5,0.8,-1.2) X&lt;-matrix(cbind(rep(1,n),rnorm(n,0,1),rnorm(n,0,1)),n,length(B)) p &lt;- exp(X%*%B)/(1+exp(X%*%B)); y &lt;- rbinom(n, 1, p) nXgar&lt;-25; Xgar&lt;-matrix(rnorm(nXgar*n),n,nXgar) df&lt;-as.data.frame(cbind(y,X[,-1],Xgar)) colnames(df) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;x10&quot;, &quot;x11&quot;, &quot;x12&quot;, &quot;x13&quot;, &quot;x14&quot;, &quot;x15&quot;, &quot;x16&quot;, &quot;x17&quot;, &quot;x18&quot;, &quot;x19&quot;, &quot;x20&quot;, &quot;x21&quot;, &quot;x22&quot;, &quot;x23&quot;, &quot;x24&quot;, &quot;x25&quot;, &quot;x26&quot;, &quot;x27&quot;) BMAglmLogit &lt;- BMA::bic.glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14+x15+x16+x17+x18+x19+x20+x21+x22+x23+x24+x25+x26+x27, data = df, glm.family = binomial(link=&quot;logit&quot;), strict = FALSE, OR = 50) summary(BMAglmLogit) ## ## Call: ## bic.glm.formula(f = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25 + x26 + x27, data = df, glm.family = binomial(link = &quot;logit&quot;), strict = FALSE, OR = 50) ## ## ## 40 models were selected ## Best 5 models (cumulative posterior probability = 0.6196 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## Intercept 100 4.231e-01 0.076191 0.4212 0.4182 0.4296 ## x1.x 100.0 8.512e-01 0.084649 0.8467 0.8541 0.8549 ## x2.x 100.0 -1.105e+00 0.089964 -1.1029 -1.1079 -1.1023 ## x3.x 2.9 2.382e-03 0.018557 . . . ## x4.x 24.5 -4.203e-02 0.082651 . . -0.1699 ## x5.x 0.8 1.304e-04 0.006885 . . . ## x6.x 1.8 1.240e-03 0.013934 . . . ## x7.x 14.8 2.273e-02 0.061659 . . . ## x8.x 0.8 -6.231e-05 0.006677 . . . ## x9.x 0.9 -2.308e-04 0.007548 . . . ## x10.x 0.9 -3.683e-04 0.008215 . . . ## x11.x 1.1 -6.107e-04 0.009791 . . . ## x12.x 0.8 -1.680e-05 0.006267 . . . ## x13.x 1.1 -6.397e-04 0.009839 . . . ## x14.x 0.8 1.517e-04 0.007144 . . . ## x15.x 0.9 -1.923e-04 0.007157 . . . ## x16.x 1.0 4.411e-04 0.008425 . . . ## x17.x 1.8 1.298e-03 0.013796 . . . ## x18.x 0.8 4.661e-05 0.006742 . . . ## x19.x 1.0 4.311e-04 0.008746 . . . ## x20.x 0.8 -1.571e-04 0.007049 . . . ## x21.x 7.5 8.922e-03 0.037260 . . . ## x22.x 0.8 4.378e-05 0.006687 . . . ## x23.x 24.3 -4.202e-02 0.083002 . -0.1718 . ## x24.x 0.8 -1.384e-04 0.007200 . . . ## x25.x 0.8 -1.363e-05 0.006489 . . . ## x26.x 1.8 1.336e-03 0.014274 . . . ## x27.x 1.9 1.354e-03 0.014298 . . . ## ## nVar 2 3 3 ## BIC -5812.0652 -5810.3862 -5810.3482 ## post prob 0.260 0.112 0.110 ## model 4 model 5 ## Intercept 0.4221 0.4271 ## x1.x 0.8481 0.8627 ## x2.x -1.1016 -1.1087 ## x3.x . . ## x4.x . -0.1784 ## x5.x . . ## x6.x . . ## x7.x 0.1575 . ## x8.x . . ## x9.x . . ## x10.x . . ## x11.x . . ## x12.x . . ## x13.x . . ## x14.x . . ## x15.x . . ## x16.x . . ## x17.x . . ## x18.x . . ## x19.x . . ## x20.x . . ## x21.x . . ## x22.x . . ## x23.x . -0.1801 ## x24.x . . ## x25.x . . ## x26.x . . ## x27.x . . ## ## nVar 3 4 ## BIC -5809.6482 -5809.1451 ## post prob 0.078 0.060 ## ## 1 observations deleted due to missingness. ### Gamma ### rm(list = ls()); set.seed(010101) n&lt;-1000; B&lt;- c(0.5, 0.2, 0.1) X&lt;-matrix(cbind(rep(1,n),rnorm(n,0,0.5),rnorm(n,0,0.5)),n,length(B)) y1 &lt;- (X%*%B)^(-1) y &lt;- rgamma(n,y1,scale=1) nXgar&lt;-25; Xgar&lt;-matrix(rnorm(nXgar*n),n,nXgar) df&lt;-as.data.frame(cbind(y,X[,-1],Xgar)) colnames(df) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;x10&quot;, &quot;x11&quot;, &quot;x12&quot;, &quot;x13&quot;, &quot;x14&quot;, &quot;x15&quot;, &quot;x16&quot;, &quot;x17&quot;, &quot;x18&quot;, &quot;x19&quot;, &quot;x20&quot;, &quot;x21&quot;, &quot;x22&quot;, &quot;x23&quot;, &quot;x24&quot;, &quot;x25&quot;, &quot;x26&quot;, &quot;x27&quot;) BMAglmGamma &lt;- BMA::bic.glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14+x15+x16+x17+x18+x19+x20+x21+x22+x23+x24+x25+x26+x27, data = df, glm.family = Gamma(link=&quot;inverse&quot;), strict = FALSE, OR = 50) summary(BMAglmGamma) ## ## Call: ## bic.glm.formula(f = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25 + x26 + x27, data = df, glm.family = Gamma(link = &quot;inverse&quot;), strict = FALSE, OR = 50) ## ## ## 36 models were selected ## Best 5 models (cumulative posterior probability = 0.6264 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## Intercept 100 4.890e-01 0.0117343 4.885e-01 4.895e-01 4.902e-01 ## x1.x 100.0 2.026e-01 0.0187327 2.021e-01 2.033e-01 2.042e-01 ## x2.x 99.2 7.552e-02 0.0210472 7.577e-02 7.595e-02 7.820e-02 ## x3.x 1.1 3.661e-05 0.0011044 . . . ## x4.x 1.2 7.240e-05 0.0012692 . . . ## x5.x 1.0 1.013e-06 0.0010648 . . . ## x6.x 1.0 -7.413e-06 0.0009672 . . . ## x7.x 2.1 1.787e-04 0.0019443 . . . ## x8.x 2.7 3.187e-04 0.0025652 . . . ## x9.x 2.8 3.287e-04 0.0026193 . . . ## x10.x 2.5 -2.618e-04 0.0023160 . . . ## x11.x 2.2 -1.909e-04 0.0018748 . . . ## x12.x 1.3 -1.021e-04 0.0014787 . . . ## x13.x 1.1 5.812e-05 0.0012447 . . . ## x14.x 1.2 -8.201e-05 0.0013340 . . . ## x15.x 1.1 5.983e-05 0.0012278 . . . ## x16.x 1.0 4.335e-06 0.0010045 . . . ## x17.x 1.0 1.127e-05 0.0010351 . . . ## x18.x 1.0 1.195e-05 0.0010612 . . . ## x19.x 1.2 -8.152e-05 0.0013225 . . . ## x20.x 25.2 5.858e-03 0.0112689 . 2.329e-02 . ## x21.x 11.5 -2.200e-03 0.0069431 . . -1.938e-02 ## x22.x 1.1 -3.714e-05 0.0011170 . . . ## x23.x 10.4 1.998e-03 0.0067118 . . . ## x24.x 1.0 8.960e-06 0.0009762 . . . ## x25.x 1.5 1.462e-04 0.0017640 . . . ## x26.x 2.1 -1.856e-04 0.0019576 . . . ## x27.x 1.0 5.144e-06 0.0010106 . . . ## ## nVar 2 3 3 ## BIC -5.849e+03 -5.848e+03 -5.846e+03 ## post prob 0.316 0.149 0.072 ## model 4 model 5 ## Intercept 4.893e-01 4.903e-01 ## x1.x 2.019e-01 2.027e-01 ## x2.x 7.641e-02 7.639e-02 ## x3.x . . ## x4.x . . ## x5.x . . ## x6.x . . ## x7.x . . ## x8.x . . ## x9.x . . ## x10.x . . ## x11.x . . ## x12.x . . ## x13.x . . ## x14.x . . ## x15.x . . ## x16.x . . ## x17.x . . ## x18.x . . ## x19.x . . ## x20.x . 2.364e-02 ## x21.x . . ## x22.x . . ## x23.x 1.914e-02 1.948e-02 ## x24.x . . ## x25.x . . ## x26.x . . ## x27.x . . ## ## nVar 3 4 ## BIC -5.846e+03 -5.845e+03 ## post prob 0.060 0.030 ## ## 1 observations deleted due to missingness. ### Poisson ### rm(list = ls()); set.seed(010101) n&lt;-1000; B&lt;-c(2,1.1,0.7) X&lt;-matrix(cbind(rep(1,n),rnorm(n,0,1),rnorm(n,0,1)),n,length(B)) y1&lt;-exp(X%*%B); y&lt;-rpois(n,y1) nXgar&lt;-25; Xgar&lt;-matrix(rnorm(nXgar*n),n,nXgar) df&lt;-as.data.frame(cbind(y,X[,-1],Xgar)) colnames(df) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;x10&quot;, &quot;x11&quot;, &quot;x12&quot;, &quot;x13&quot;, &quot;x14&quot;, &quot;x15&quot;, &quot;x16&quot;, &quot;x17&quot;, &quot;x18&quot;, &quot;x19&quot;, &quot;x20&quot;, &quot;x21&quot;, &quot;x22&quot;, &quot;x23&quot;, &quot;x24&quot;, &quot;x25&quot;, &quot;x26&quot;, &quot;x27&quot;) BMAglmPoisson &lt;- BMA::bic.glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12+x13+x14+x15+x16+x17+x18+x19+x20+x21+x22+x23+x24+x25+x26+x27, data = df, glm.family = poisson(link=&quot;log&quot;), strict = FALSE, OR = 50) summary(BMAglmPoisson) ## ## Call: ## bic.glm.formula(f = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 + x23 + x24 + x25 + x26 + x27, data = df, glm.family = poisson(link = &quot;log&quot;), strict = FALSE, OR = 50) ## ## ## 26 models were selected ## Best 5 models (cumulative posterior probability = 0.6228 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## Intercept 100 2.004e+00 0.0120917 2.004e+00 2.002e+00 2.003e+00 ## x1.x 100.0 1.093e+00 0.0071406 1.093e+00 1.096e+00 1.095e+00 ## x2.x 100.0 7.020e-01 0.0076175 7.020e-01 7.023e-01 7.011e-01 ## x3.x 1.9 -1.202e-04 0.0013752 . . . ## x4.x 1.4 -2.783e-05 0.0009363 . . . ## x5.x 5.8 7.628e-04 0.0036096 . . 1.326e-02 ## x6.x 1.9 -1.311e-04 0.0014441 . . . ## x7.x 1.5 5.797e-05 0.0010305 . . . ## x8.x 1.4 -2.484e-05 0.0009678 . . . ## x9.x 1.6 -7.084e-05 0.0011074 . . . ## x10.x 3.7 -4.232e-04 0.0026514 . . . ## x11.x 1.4 -3.190e-05 0.0010090 . . . ## x12.x 1.5 4.617e-05 0.0009407 . . . ## x13.x 1.4 2.822e-05 0.0009560 . . . ## x14.x 1.4 2.029e-05 0.0010251 . . . ## x15.x 2.1 -1.445e-04 0.0014510 . . . ## x16.x 1.6 7.135e-05 0.0011593 . . . ## x17.x 1.5 -5.180e-05 0.0010208 . . . ## x18.x 1.6 -8.503e-05 0.0012462 . . . ## x19.x 3.2 -3.442e-04 0.0023704 . . . ## x20.x 5.8 -7.149e-04 0.0033497 . -1.223e-02 . ## x21.x 1.4 -2.393e-05 0.0009079 . . . ## x22.x 2.1 1.578e-04 0.0015657 . . . ## x23.x 1.7 9.252e-05 0.0012586 . . . ## x24.x 1.5 -4.766e-05 0.0010436 . . . ## x25.x 2.1 -1.453e-04 0.0014830 . . . ## x26.x 3.4 -3.780e-04 0.0025203 . . . ## x27.x 4.1 -4.169e-04 0.0024626 . . . ## ## nVar 2 3 3 ## BIC -5.798e+03 -5.794e+03 -5.794e+03 ## post prob 0.429 0.058 0.058 ## model 4 model 5 ## Intercept 2.004e+00 2.004e+00 ## x1.x 1.094e+00 1.093e+00 ## x2.x 7.024e-01 7.024e-01 ## x3.x . . ## x4.x . . ## x5.x . . ## x6.x . . ## x7.x . . ## x8.x . . ## x9.x . . ## x10.x . -1.139e-02 ## x11.x . . ## x12.x . . ## x13.x . . ## x14.x . . ## x15.x . . ## x16.x . . ## x17.x . . ## x18.x . . ## x19.x . . ## x20.x . . ## x21.x . . ## x22.x . . ## x23.x . . ## x24.x . . ## x25.x . . ## x26.x . . ## x27.x -1.027e-02 . ## ## nVar 3 3 ## BIC -5.793e+03 -5.793e+03 ## post prob 0.041 0.037 ## ## 1 observations deleted due to missingness. ######################################## rm(list = ls()); set.seed(010101) n&lt;-1000; B&lt;-c(2,1.1,0.7) X&lt;-matrix(cbind(rep(1,n),rnorm(n,0,1),rnorm(n,0,1)),n,length(B)) y1&lt;-exp(X%*%B); y&lt;-rpois(n,y1) nXgar&lt;-25; Xgar&lt;-matrix(rnorm(nXgar*n),n,nXgar) df&lt;-as.data.frame(cbind(y,X[,-1],Xgar)) colnames(df) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;x10&quot;, &quot;x11&quot;, &quot;x12&quot;, &quot;x13&quot;, &quot;x14&quot;, &quot;x15&quot;, &quot;x16&quot;, &quot;x17&quot;, &quot;x18&quot;, &quot;x19&quot;, &quot;x20&quot;, &quot;x21&quot;, &quot;x22&quot;, &quot;x23&quot;, &quot;x24&quot;, &quot;x25&quot;, &quot;x26&quot;, &quot;x27&quot;) Xnew &lt;- apply(df[,-1], 2, scale) BICfunt &lt;- function(Model){ indr &lt;- Model == 1; kr &lt;- sum(indr) if(kr &gt; 0){ Xr &lt;- as.matrix(Xnew[ , indr]) model &lt;- glm(y ~ Xr, family = poisson(link = &quot;log&quot;)) model_bic &lt;- BIC(model) mllMod &lt;- -model_bic/2 }else{ model &lt;- glm(y ~ 1, family = poisson(link = &quot;log&quot;)) model_bic &lt;- BIC(model); mllMod &lt;- -model_bic/2 } return(mllMod) } M &lt;- 500; K &lt;- dim(df)[2] - 1 Models &lt;- matrix(rbinom(K*M, 1, p = 0.5), ncol = K, nrow = M) mllnew &lt;- sapply(1:M, function(s){BICfunt(matrix(Models[s,], 1, K))}) oind &lt;- order(mllnew, decreasing = TRUE) mllnew &lt;- mllnew[oind]; Models &lt;- Models[oind, ] # Hyperparameters MC3 iter &lt;- 25000 pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = iter, width = 300) s &lt;- 1 while(s &lt;= iter){ ActModel &lt;- Models[M,] idK &lt;- which(ActModel == 1) Kact &lt;- length(idK) if(Kact &lt; K &amp; Kact &gt; 1){ CardMol &lt;- K opt &lt;- sample(1:3, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ if(opt == 2){ # Add All &lt;- 1:K NewX &lt;- sample(All[-idK], 1) CandModel &lt;- ActModel CandModel[NewX] &lt;- 1 }else{ # Subtract LessX &lt;- sample(idK, 1) CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } } }else{ CardMol &lt;- K + 1 if(Kact == K){ opt &lt;- sample(1:2, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ # Subtract LessX &lt;- sample(1:K, 1) CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } }else{ if(K == 1){ opt &lt;- sample(1:3, 1) if(opt == 1){ # Same CandModel &lt;- ActModel }else{ if(opt == 2){ # Add All &lt;- 1:K NewX &lt;- sample(All[-idK], 1) CandModel &lt;- ActModel CandModel[NewX] &lt;- 1 }else{ # Subtract LessX &lt;- sample(idK, 1) CandModel &lt;- ActModel CandModel[LessX] &lt;- 0 } } }else{ # Add NewX &lt;- sample(1:K, 1) CandModel &lt;- ActModel CandModel[NewX] &lt;- 1 } } } LogMLact &lt;- BICfunt(matrix(ActModel, 1, K)) LogMLcand &lt;- BICfunt(matrix(CandModel, 1, K)) alpha &lt;- min(1, exp(LogMLcand-LogMLact)) u &lt;- runif(1) if(u &lt;= alpha){ mllnew[M] &lt;- LogMLcand Models[M, ] &lt;- CandModel oind &lt;- order(mllnew, decreasing = TRUE) mllnew &lt;- mllnew[oind] Models &lt;- Models[oind, ] }else{ mllnew &lt;- mllnew Models &lt;- Models } s &lt;- s + 1 setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),&quot;% done&quot;)) } close(pb) ## NULL ModelsUni &lt;- unique(Models) mllnewUni &lt;- sapply(1:dim(ModelsUni)[1], function(s){BICfunt(matrix(ModelsUni[s,], 1, K))}) StMarLik &lt;- exp(mllnewUni-mllnewUni[1]) PMP &lt;- StMarLik/sum(StMarLik) # PMP based on unique selected models plot(PMP) ModelsUni[1,] ## [1] 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 PIP &lt;- NULL for(k in 1:K){ PIPk &lt;- sum(PMP[which(ModelsUni[,k] == 1)]) PIP &lt;- c(PIP, PIPk) } plot(PIP) Xnew &lt;- df[,-1] nModels &lt;- dim(ModelsUni)[1] Means &lt;- matrix(0, nModels, K) Vars &lt;- matrix(0, nModels, K) for(m in 1:nModels){ idXs &lt;- which(ModelsUni[m,] == 1) if(length(idXs) == 0){ Regm &lt;- glm(y ~ 1, family = poisson(link = &quot;log&quot;)) }else{ Xm &lt;- as.matrix(Xnew[, idXs]) Regm &lt;- glm(y ~ Xm, family = poisson(link = &quot;log&quot;)) SumRegm &lt;- summary(Regm) Means[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,1] Vars[m, idXs] &lt;- SumRegm[[&quot;coefficients&quot;]][-1,2]^2 } } BMAmeans &lt;- colSums(Means*PMP) BMAsd &lt;- (colSums(PMP*Vars) + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5 plot(BMAmeans) plot(BMAsd) plot(BMAmeans/BMAsd) References McCullagh, P., and J. A. Nelder. 1989. Generalized Linear Models. London: Chapman; Hall. Nelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84. Raftery, A. 1995. “Bayesian Model Selection in Social Research.” Sociological Methodology 25: 111–63. "],["sec10_4.html", "10.4 Calculating the marginal likelihood", " 10.4 Calculating the marginal likelihood The BIC is an asymptotic approximation of the marginal likelihood, and consequently, it is used to obtain the Bayes factors. However, this method has limitations in applications with moderate and small sample sizes (Gelfand and Dey 1994). Therefore, other methods are available to calculate the Bayes factors when there is no analytical solution for the marginal likelihood. Observe that calculating the Bayes factor with respect to a reference model (\\(\\mathcal{M}_0\\)) helps to obtain the posterior model probabilities, \\[\\begin{align*} \\pi(\\mathcal{M}_j |\\boldsymbol{y})&amp;=\\frac{p(\\boldsymbol{y} | \\mathcal{M}_j)\\pi(\\mathcal{M}_j)}{\\sum_{m=1}^{M}p(\\boldsymbol{y} | \\mathcal{M}_m)\\pi(\\mathcal{M}_m)}\\\\ &amp;=\\frac{p(\\boldsymbol{y} | \\mathcal{M}_j)\\pi(\\mathcal{M}_j)/p(\\boldsymbol{y} | \\mathcal{M}_0)}{\\sum_{m=1}^{M}p(\\boldsymbol{y} | \\mathcal{M}_m)\\pi(\\mathcal{M}_m)/p(\\boldsymbol{y} | \\mathcal{M}_0)}\\\\ &amp;=\\frac{BF_{j0}\\times\\pi(\\mathcal{M}_j)}{\\sum_{m=1}^{M}BF_{l0}\\times\\pi(\\mathcal{M}_l)}. \\end{align*}\\] Thus, \\(\\pi(\\mathcal{M}_j |\\boldsymbol{y})=\\frac{BF_{j0}}{\\sum_{m=1}^{M}BF_{l0}}\\) assuming equal prior model probabilities. In addition, it has been established in many settings that the Bayes factor is consistent. That is, the probability of identifying the true data generating process converges to 1 as the sample size increases to infinity. Alternatively, it asymptotically identifies the model that minimizes the Kullback-Leibler divergence with respect to the data generating process when this process is not part of the models under consideration (Chib and Kuffner 2016; Stephen G. Walker 2004b; Stephen G. Walker 2004a).7 10.4.1 Savage-Dickey density ratio The Savage-Dickey density ratio is a way to calculate the Bayes factors when we compare nested models with particular priors (James M. Dickey 1971; Verdinelli and Wasserman 1995). In particular, given the parameter space \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\omega}^{\\top}, \\boldsymbol{\\psi}^{\\top})^{\\top}\\in \\boldsymbol{\\Theta}=\\boldsymbol{\\Omega}\\times \\boldsymbol{\\Psi}\\), where we wish to test the null hypothesis \\(H_0:\\boldsymbol{\\omega}=\\boldsymbol{\\omega}_0\\) (model \\(\\mathcal{M}_1\\)) versus \\(H_1:\\boldsymbol{\\omega}\\neq \\boldsymbol{\\omega}_0\\) (model \\(\\mathcal{M}_2\\)), if \\(\\pi(\\boldsymbol{\\psi}|\\boldsymbol{\\omega}_0,\\mathcal{M}_2)=\\pi(\\boldsymbol{\\psi}|\\mathcal{M}_1)\\),8 then the Bayes factor comparing \\(\\mathcal{M}_1\\) versus \\(\\mathcal{M}_2\\) is \\[\\begin{equation} BF_{12}=\\frac{\\pi(\\boldsymbol{\\omega}=\\boldsymbol{\\omega}_0|\\boldsymbol{y},\\mathcal{M}_2)}{\\pi(\\boldsymbol{\\omega}=\\boldsymbol{\\omega}_0|\\mathcal{M}_2)}, \\tag{10.1} \\end{equation}\\] where \\(\\pi(\\boldsymbol{\\omega}=\\boldsymbol{\\omega}_0|\\boldsymbol{y},\\mathcal{M}_2)\\) and \\(\\pi(\\boldsymbol{\\omega}=\\boldsymbol{\\omega}_0|\\mathcal{M}_2)\\) are the posterior and prior densities of \\(\\boldsymbol{\\omega}\\) under \\(\\mathcal{M}_2\\) evaluated at \\(\\boldsymbol{\\omega}_0\\) (see Verdinelli and Wasserman (1995)). Equation (10.1) is called the Savage-Dickey density ratio. A nice feature is that just requires estimation of model \\(\\mathcal{M}_2\\), and evaluation of the prior and posterior densities. This means no evaluation of the marginal likelihood (G. M. Koop 2003). 10.4.2 Chib’s methods Another popular method to calculate the marginal likelihood is given by Chib (1995) and Chib and Jeliazkov (2001). The former is an algorithm to calculate the marginal likelihood from the posterior draws of the Gibbs sampling algorithm, and the latter calculates the marginal likelihood from the posterior draws of the Metropolis-Hastings algorithm. The point of departure in Chib (1995) is the identity \\[\\begin{align*} \\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m)=\\frac{p(\\boldsymbol{y}|\\boldsymbol{\\theta}^*,\\mathcal{M}_m)\\times\\pi(\\boldsymbol{\\theta}^*|\\mathcal{M}_m)}{p(\\boldsymbol{y}|\\mathcal{M}_m)}, \\end{align*}\\] where \\(\\boldsymbol{\\theta}^*\\) is a particular value of \\(\\boldsymbol{\\theta}\\) of high probability, for instance, the mode. This implies that \\[\\begin{align*} p(\\boldsymbol{y}|\\mathcal{M}_m)=\\frac{p(\\boldsymbol{y}|\\boldsymbol{\\theta}^*,\\mathcal{M}_m)\\times\\pi(\\boldsymbol{\\theta}^*|\\mathcal{M}_m)}{\\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m)}. \\end{align*}\\] We can easily calculate the numerator of this expression. However, the critical point in this expression is to calculate the denominator as we know \\(\\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m)\\) up to a normalizing constant. We can calculate this from the posterior draws. Assume that \\(\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}^{\\top}_1 \\ \\boldsymbol{\\theta}^{\\top}_2]^{\\top}\\), then \\(\\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m)=\\pi(\\boldsymbol{\\theta}^*_1|\\boldsymbol{\\theta}^*_2,\\boldsymbol{y},\\mathcal{M}_m)\\times \\pi(\\boldsymbol{\\theta}^*_2|\\boldsymbol{y},\\mathcal{M}_m)\\). We have the first term because in the Gibbs sampling algorithm the posterior conditional distributions are available. The second is \\[\\begin{align*} \\pi(\\boldsymbol{\\theta}^*_2|\\boldsymbol{y},\\mathcal{M}_m)&amp;=\\int_{\\boldsymbol{\\Theta}_1}\\pi(\\boldsymbol{\\theta}_1,\\boldsymbol{\\theta}^*_2|\\boldsymbol{y},\\mathcal{M}_m)d\\boldsymbol{\\theta}_1\\\\ &amp;=\\int_{\\boldsymbol{\\Theta}_1}\\pi(\\boldsymbol{\\theta}^*_2|\\boldsymbol{\\theta}_1,\\boldsymbol{y},\\mathcal{M}_m)\\pi(\\boldsymbol{\\theta}_1|\\boldsymbol{y},\\mathcal{M}_m)d\\boldsymbol{\\theta}_1\\\\ &amp;\\approx \\frac{1}{S}\\sum_{s=1}^S \\pi(\\boldsymbol{\\theta}^*_2|\\boldsymbol{\\theta}^{(s)}_1,\\boldsymbol{y},\\mathcal{M}_m), \\end{align*}\\] where \\(\\boldsymbol{\\theta}^{(s)}_1\\) are the posterior draws of \\(\\boldsymbol{\\theta}_1\\) from the Gibbs sampling algorithm. The generalization to more blocks can be seen in Chib (1995) and Greenberg (2012). In addition, the extension to the Metropolis-Hastings algorithm can be seen in Chib and Jeliazkov (2001), and Greenberg (2012). 10.4.3 Gelfand-Dey method We can use the Gelfand-Dey method (Gelfand and Dey 1994) when we want to calculate the Bayes factor to compare non-nested models, models where the Savage-Dickey density ratio is hard to calculate, or the Chib’s methods are difficult to implement. The Gelfand-Dey method is very general, and can be used in virtually any model (G. M. Koop 2003). Given a probability density function \\(q(\\boldsymbol{\\theta})\\), whose support is in \\(\\boldsymbol{\\Theta}\\), then \\[\\begin{align*} \\mathbb{E}\\left[\\frac{q(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{\\theta}|\\mathcal{M}_m)p(\\boldsymbol{y}|\\boldsymbol{\\theta}_m,\\mathcal{M}_m)}\\biggr\\rvert \\boldsymbol{y},\\mathcal{M}_m\\right]&amp;=\\frac{1}{p(\\boldsymbol{y}|\\mathcal{M}_m)}, \\end{align*}\\] where the expected value is with respect to the posterior distribution given the model \\(\\mathcal{M}_m\\) (see Exercise 12). The critical point is to select a good \\(q(\\boldsymbol{\\theta})\\). Geweke (1999) recommends to use \\(q(\\boldsymbol{\\theta})\\) equal to a truncated multivariate normal density function with mean and variance equal to the posterior mean (\\(\\hat{\\boldsymbol{\\theta}}\\)) and variance (\\(\\hat{\\boldsymbol{\\Sigma}}\\)) of \\(\\boldsymbol{\\theta}\\). The truncation region is \\(\\hat{\\boldsymbol{\\Theta}}=\\left\\{\\boldsymbol{\\theta}:(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})^{\\top}\\hat{\\boldsymbol{\\Sigma}}^{-1}(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}})\\leq \\chi_{1-\\alpha}^2(K)\\right\\}\\), where \\(\\chi_{1-\\alpha}^2(K)\\) is the \\((1-\\alpha)\\) percentile of the Chi-squared distribution with \\(K\\) degrees of freedom, \\(K\\) is the dimension of \\(\\boldsymbol{\\theta}\\). We can pick small values of \\(\\alpha\\), for instance, \\(\\alpha=0.01\\). Observe that \\[\\begin{align*} \\mathbb{E}\\left[\\frac{q(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{\\theta}|\\mathcal{M}_m)p(\\boldsymbol{y}|\\boldsymbol{\\theta}_m,\\mathcal{M}_m)}\\biggr\\rvert \\boldsymbol{y},\\mathcal{M}_m\\right]&amp;\\approx \\frac{1}{S}\\sum_{s=1}^S \\left[\\frac{q(\\boldsymbol{\\theta}^{(s)})}{\\pi(\\boldsymbol{\\theta}^{(s)}|\\mathcal{M}_m)p(\\boldsymbol{y}|\\boldsymbol{\\theta}^{(s)}_m,\\mathcal{M}_m)}\\right], \\end{align*}\\] where \\(\\boldsymbol{\\theta}^{(s)}_m\\) are draws from the posterior distribution. Observe that we can calculate the marginal likelihoods of the models in Chapters 6, 7, 8 and 9 using the Chib’s methods and the Gelfand-Dickey method. Example: Simulation exercise Let’s check the performance of the Savage-Dickey density ratio, Chib’s method and the Gelfand-Dey method to calculate the Bayes factor in a setting where we can obtain the analytical solution for the marginal likelihood. In particular, we will consider the Gaussian linear model with a conjugate prior (see Section 3.3). Assume that the data generating process is given by \\[ y_{i} = 0.7 + 0.3x_{i1} + 0.7x_{i2} - 0.2x_{i3} + 0.2x_{i4} + \\mu_i, \\] where \\(x_{i1} \\sim B(0.3)\\), \\(x_{ik} \\sim N(0,1)\\), for \\(k = 2, \\dots, 4\\), and \\(\\mu_i \\sim N(0, 1)\\), for \\(i = 1, 2, \\dots, 500\\). Let us set \\(H_0: \\beta_5 = 0\\) (model \\(\\mathcal{M}_1\\)) versus \\(H_1: \\beta_5 \\neq 0\\) (model \\(\\mathcal{M}_2\\)). We assume that \\(\\boldsymbol{\\beta}_{m0} = \\boldsymbol{0}_{m0}\\), \\(\\boldsymbol{B}_{m0} = 0.5 \\boldsymbol{I}_{m}\\), \\(\\alpha_0 = \\delta_0 = 4\\). The dimensions of \\(\\boldsymbol{0}_{m0}\\) and \\(\\boldsymbol{I}_m\\) are 4 for model \\(\\mathcal{M}_1\\) and 5 for model \\(\\mathcal{M}_2\\). In addition, we assume equal prior probabilities for both models. We know from Section 3.3 that the marginal likelihood is \\[\\begin{align*} p(\\bf{y}|\\mathcal{M}_m)&amp;=\\frac{\\delta_{m0}^{\\alpha_{m0}/2}}{\\delta_{mn}^{\\alpha_{mn}/2}}\\frac{|{\\bf{B}}_{mn}|^{1/2}}{|{\\bf{B}}_{m0}|^{1/2}}\\frac{\\Gamma(\\alpha_{mn}/2)}{\\Gamma(\\alpha_{m0}/2)}, \\end{align*}\\] where \\({{\\boldsymbol{B}}}_{mn} = ({\\boldsymbol{B}}_{m0}^{-1} + {\\boldsymbol{X}}_m^{\\top}{\\boldsymbol{X}}_m)^{-1}\\), \\(\\boldsymbol{\\beta}_{mn} = {{\\bf{B}}}_{mn}({\\boldsymbol{B}}_{m0}^{-1}\\boldsymbol{\\beta}_{m0} + {\\boldsymbol{X}}_m^{\\top}{\\boldsymbol{X}}_m\\hat{\\boldsymbol{\\beta}}_m)\\), \\(\\alpha_{mn}=\\alpha_{m0}+N\\), and \\(\\delta_{mn}=\\delta_{m0}+({\\boldsymbol{y}}-{\\boldsymbol{X}}_m\\hat{\\boldsymbol{\\beta}}_m)^{\\top}({\\boldsymbol{y}}-{\\boldsymbol{X}}_m\\hat{\\boldsymbol{\\beta}}_m)+(\\hat{\\boldsymbol{\\beta}}_m-\\boldsymbol{\\beta}_{m0})^{\\top}(({\\boldsymbol{X}_m}^{\\top}{\\boldsymbol{X}_m})^{-1}+{\\boldsymbol{B}}_{m0})^{-1}(\\hat{\\boldsymbol{\\beta}}_m-\\boldsymbol{\\beta}_{m0})\\), \\(m=1,2\\) are the indices of the models. The log marginal likelihoods for models \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\) are -751.72 and -740.79, respectively. This implies a \\(2\\times\\log(BF_{21})=21.85\\) which means positive evidence against model \\(\\mathcal{M}_1\\). We have different ways to calculate the Bayes factor using the Savage-Dickey density ratio in this example because we know that the marginal prior and marginal posterior distributions of \\(\\beta_5\\) have analytical solutions. In addition, we can use the posterior draws of \\(\\sigma^2\\) to evaluate the conditional prior and conditional posterior distributions at \\(\\beta_5=0\\). We show in the following code the latter approach, as it is more general than using analytical solutions, which are not always available. We know that the conditional posterior distribution of \\(\\beta_5\\) is \\(N(\\beta_{5n}, \\sigma \\boldsymbol{B}_{55n})\\), where \\(\\beta_{5n}\\) is the 5th element of \\(\\boldsymbol{\\beta}_n\\), and \\(\\boldsymbol{B}_{55n}\\) is the element 5,5 of \\(\\boldsymbol{B}_n\\). Then, \\[\\begin{align*} \\pi(\\beta_5=0|\\boldsymbol{y}, \\mathcal{M}_2) &amp;= \\int_{\\mathcal{R}^+} \\pi(\\beta_5=0|\\boldsymbol{y}, \\sigma^2) \\pi(\\sigma^2|\\boldsymbol{y}) \\, d\\sigma^2 \\\\ &amp;\\approx \\frac{1}{S} \\sum_{s=1}^S \\pi(\\beta_5=0|\\boldsymbol{y}, \\sigma^{2(s)}), \\end{align*}\\] where \\(\\sigma^{2(s)}\\) are draws from the posterior distribution of \\(\\sigma^2\\). We can follow the same logic to obtain an approximation to \\(\\pi(\\beta_5=0|\\mathcal{M}_2)\\) by sampling draws from the prior distribution of \\(\\sigma^2\\). We obtain \\(2 \\times \\log(BF_{21}) = 21.85\\) using the Savage-Dickey density ratio, which is the same value as the analytic solution using the marginal likelihoods. We calculate the log marginal likelihood using the Chib’s method taking into account that \\[\\begin{align*} \\log(p(\\boldsymbol{y}|\\mathcal{M}_m))&amp;=\\log(p(\\boldsymbol{y}|\\boldsymbol{\\theta}^*,\\mathcal{M}_m))+\\log(\\pi(\\boldsymbol{\\theta}^*|\\mathcal{M}_m))-\\log(\\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m)),\\\\ \\end{align*}\\] where \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta}^*,\\mathcal{M}_m)\\) is the value of a normal density with mean \\(\\boldsymbol{X}_m\\boldsymbol{\\beta}_{m}^*\\) and variance \\(\\sigma^{2*}_m\\boldsymbol{I}_N\\) evaluated at \\(\\boldsymbol{y}\\). In addition, \\(\\log(\\pi(\\boldsymbol{\\theta}^*|\\mathcal{M}_m))=\\log(\\pi(\\boldsymbol{\\beta}_m^*|\\sigma^{2*}_m))+\\log(\\pi(\\sigma^{2*}_m))\\), where the first term is the density of a normal with mean \\(\\boldsymbol{\\beta}_{m0}\\) and variance matrix \\(\\sigma^{2*}\\boldsymbol{B}_{m0}\\) evaluated at \\(\\boldsymbol{\\beta}_m^*\\), and the second term is the density of an inverse-gamma with parameters \\(\\alpha_{m0}/2\\) and \\(\\delta_{m0}/2\\) evaluated at \\(\\sigma^{2*}_m\\). Finally, the third term in the right hand of the previous expression is \\(\\log(\\pi(\\boldsymbol{\\theta}^*|\\boldsymbol{y},\\mathcal{M}_m))=\\log(\\pi(\\boldsymbol{\\beta}_m^*|\\sigma^{2*}_m,\\boldsymbol{y}))+\\log(\\pi(\\sigma^{2*}_m|\\boldsymbol{y}))\\), where the first term is the density of a normal with mean \\(\\boldsymbol{\\beta}_{mn}\\) and variance matrix \\(\\sigma^{2*}_m\\boldsymbol{B}_{mn}\\) evaluated at \\(\\boldsymbol{\\beta}_m^*\\), and the second term is the density of an inverse-gamma with parameters \\(\\alpha_{mn}/2\\) and \\(\\delta_{mn}/2\\) evaluated at \\(\\sigma^{2*}_m\\). We use the modes of the posterior draws of \\(\\boldsymbol{\\beta}_m\\) and \\(\\sigma^2_m\\) as reference values. We get the same value, up to two decimals, for the log marginal likelihood of the restricted and unrestricted models using the Chib’s method and the analytical expression. Thus, \\(2\\times\\log(BF_{21})=21.85\\), that is, positive evidence against model \\(\\mathcal{M}_1\\). We calculate the log marginal likelihood using the Gelfand-Dey method taking into account that \\[\\begin{align*} \\log\\left[\\frac{q(\\boldsymbol{\\theta}^{(s)})}{\\pi(\\boldsymbol{\\theta}^{(s)}|\\mathcal{M}_m)p(\\boldsymbol{y}|\\boldsymbol{\\theta}^{(s)}_m,\\mathcal{M}_m)}\\right]&amp;=\\log(q(\\boldsymbol{\\theta}^{(s)}))-\\log(\\pi(\\boldsymbol{\\theta}^{(s)}|\\mathcal{M}_m))-\\log(p(\\boldsymbol{y}|\\boldsymbol{\\theta}^{(s)}_m,\\mathcal{M}_m)), \\end{align*}\\] where \\(q(\\boldsymbol{\\theta}^{(s)})\\) is the truncated multivariate normal density of Subsection @ref(sec10_4_3) evaluated at \\(\\boldsymbol{\\theta}^{(s)}=[\\boldsymbol{\\beta}^{(s)\\top} \\ \\sigma^{2(s)}]^{\\top}\\), which is the \\(s\\)-th posterior draw of the Gibbs sampling algorithm, such that \\(\\boldsymbol{\\theta}^{(s)}\\) satisfies the truncation restriction. \\(\\log(\\pi(\\boldsymbol{\\theta}^{(s)}|\\mathcal{M}_m))=\\log(\\pi(\\boldsymbol{\\beta}_m^{(s)}|\\sigma^{2(s)}_m))+\\log(\\pi(\\sigma^{2(s)}_m))\\), where the first term is the density of a normal with mean \\(\\boldsymbol{\\beta}_{m0}\\) and variance matrix \\(\\sigma^{2(s)}\\boldsymbol{B}_{m0}\\) evaluated at \\(\\boldsymbol{\\beta}_m^{(s)}\\), and the second term is the density of an inverse-gamma with parameters \\(\\alpha_{m0}/2\\) and \\(\\delta_{m0}/2\\) evaluated at \\(\\sigma^{2(s)}_m\\). The third term \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta}^{(s)},\\mathcal{M}_m)\\) is the value of a normal density with mean \\(\\boldsymbol{X}_m\\boldsymbol{\\beta}_{m}^{(s)}\\) and variance \\(\\sigma^{2(s)}_m\\boldsymbol{I}_N\\) evaluated at \\(\\boldsymbol{y}\\). The log marginal likelihoods of the restricted and unrestricted models using the Gelfand-Dey method are -751.79 and -740.89, respectively. This implies \\(2\\times \\log(BF_{21})=21.81\\), which is positive evidence in favor of the unrestricted model. We see in this example that these methods give very good approximations to the true marginal likelihoods. However, the Savage-Dickey density ratio and Chib’s method performed slightly better than the Gelfand-Dey method. In addition, the computational demand of the Gelfand-Dey method is by far the largest. This is because the Gelfand-Dey method requires many evaluations based on the posterior draws. However, we should keep in mind that the Gelfand-Dey method is more general. The following code shows how to do all these calculations. rm(list = ls()); set.seed(010101) N &lt;- 500; K &lt;- 5; K2 &lt;- 3 B &lt;- c(0.7, 0.3, 0.7, -0.2, 0.2) X1 &lt;- rbinom(N, 1, 0.3) X2 &lt;- matrix(rnorm(K2*N), N, K2) X &lt;- cbind(1, X1, X2) Y &lt;- X%*%B + rnorm(N, 0, sd = 1) # Hyperparameters d0 &lt;- 4; a0 &lt;- 4 b0 &lt;- rep(0, K); cOpt &lt;- 0.5 an &lt;- N + a0; B0 &lt;- cOpt*diag(K) Bn &lt;- solve(solve(B0)+t(X)%*%X); bhat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y bn &lt;- Bn%*%(solve(B0)%*%b0+t(X)%*%X%*%bhat) dn &lt;- as.numeric(d0 + t(Y-X%*%bhat)%*%(Y-X%*%bhat)+t(bhat - b0)%*%solve(solve(t(X)%*%X)+B0)%*%(bhat - b0)) Hn &lt;- as.matrix(Matrix::forceSymmetric(dn*Bn/an)) S &lt;- 10000 LogMarLikLM &lt;- function(X, c0){ K &lt;- dim(X)[2] N &lt;- dim(X)[1] # Hyperparameters B0 &lt;- c0*diag(K) b0 &lt;- rep(0, K) # Posterior parameters bhat &lt;- solve(t(X)%*%X)%*%t(X)%*%Y # Force this matrix to be symmetric Bn &lt;- as.matrix(Matrix::forceSymmetric(solve(solve(B0) + t(X)%*%X))) bn &lt;- Bn%*%(solve(B0)%*%b0 + t(X)%*%X%*%bhat) dn &lt;- as.numeric(d0 + t(Y)%*%Y+t(b0)%*%solve(B0)%*%b0-t(bn)%*%solve(Bn)%*%bn) an &lt;- a0 + N # Log marginal likelihood logpy &lt;- (N/2)*log(1/pi)+(a0/2)*log(d0)-(an/2)*log(dn) + 0.5*log(det(Bn)/det(B0)) + lgamma(an/2)-lgamma(a0/2) return(-logpy) } LogMarM2 &lt;- -LogMarLikLM(X = X, c0 = cOpt) LogMarM1 &lt;- -LogMarLikLM(X = X[,1:4], c0 = cOpt) BF12 &lt;- exp(LogMarM1-LogMarM2) BF12; 1/BF12 ## [1] 1.79514e-05 ## [1] 55705.95 2*log(1/BF12) ## [1] 21.85568 # Savage-Dickey density ratio # Posterior evaluation Brest &lt;- 0 sig2P &lt;- invgamma::rinvgamma(S, shape = an/2, rate = dn/2) PostRestCom &lt;- mean(sapply(sig2P, function(x){dnorm(Brest, mean = bn[5], sd = (x*Bn[5,5])^0.5, log = FALSE)})) # Prior evaluation sig2 &lt;- invgamma::rinvgamma(S, shape = a0/2, rate = d0/2) PriorRestCom &lt;- mean(sapply(sig2, function(x){dnorm(Brest, mean = 0, sd = (x*cOpt)^0.5, log = FALSE)})) # Bayes factor BF12SD &lt;- PostRestCom/PriorRestCom 2*log(1/BF12SD) ## [1] 21.85275 # Chib&#39;s method sig2Post &lt;- MCMCpack::rinvgamma(S,an/2,dn/2) BetasGibbs &lt;- sapply(1:S, function(s){MASS::mvrnorm(n = 1, mu = bn, Sigma = sig2Post[s]*Bn)}) # Mode function for continuous data mode_continuous &lt;- function(x){ density_est &lt;- density(x) mode_value &lt;- density_est$x[which.max(density_est$y)] return(mode_value) } # Unrestricted model BetasMode &lt;- apply(BetasGibbs, 1, mode_continuous) Sigma2Mode &lt;- mode_continuous(sig2Post) VarModel &lt;- Sigma2Mode*diag(N) MeanModel &lt;- X%*%BetasMode LogLik &lt;- mvtnorm::dmvnorm(c(Y), mean = MeanModel, sigma = VarModel, log = TRUE, checkSymmetry = TRUE) LogPrior &lt;- mvtnorm::dmvnorm(BetasMode, mean = rep(0, K), sigma = Sigma2Mode*cOpt*diag(K), log = TRUE, checkSymmetry = TRUE)+log(MCMCpack::dinvgamma(Sigma2Mode, a0/2, d0/2)) LogPost1 &lt;- mvtnorm::dmvnorm(BetasMode, mean = bn, sigma = Sigma2Mode*Bn, log = TRUE, checkSymmetry = TRUE) LogPost2 &lt;- log(MCMCpack::dinvgamma(Sigma2Mode, an/2, dn/2)) LogMarLikChib &lt;- LogLik + LogPrior -(LogPost1 + LogPost2) # Restricted model anRest &lt;- N + a0; XRest &lt;- X[,-5] KRest &lt;- dim(XRest)[2]; B0Rest &lt;- cOpt*diag(KRest) BnRest &lt;- solve(solve(B0Rest)+t(XRest)%*%XRest) bhatRest &lt;- solve(t(XRest)%*%XRest)%*%t(XRest)%*%Y b0Rest &lt;- rep(0, KRest) bnRest &lt;- BnRest%*%(solve(B0Rest)%*%b0Rest+t(XRest)%*%XRest%*%bhatRest) dnRest &lt;- as.numeric(d0 + t(Y-XRest%*%bhatRest)%*%(Y-XRest%*%bhatRest)+t(bhatRest - b0Rest)%*%solve(solve(t(XRest)%*%XRest)+B0Rest)%*%(bhatRest - b0Rest)) sig2PostRest &lt;- MCMCpack::rinvgamma(S,anRest/2,dnRest/2) BetasGibbsRest &lt;- sapply(1:S, function(s){MASS::mvrnorm(n = 1, mu = bnRest, Sigma = sig2PostRest[s]*BnRest)}) BetasModeRest &lt;- apply(BetasGibbsRest, 1, mode_continuous) Sigma2ModeRest &lt;- mode_continuous(sig2PostRest) VarModelRest &lt;- Sigma2ModeRest*diag(N) MeanModelRest &lt;- XRest%*%BetasModeRest LogLikRest &lt;- mvtnorm::dmvnorm(c(Y), mean = MeanModelRest, sigma = VarModelRest, log = TRUE, checkSymmetry = TRUE) LogPriorRest &lt;- mvtnorm::dmvnorm(BetasModeRest, mean = rep(0, KRest), sigma = Sigma2ModeRest*cOpt*diag(KRest), log = TRUE, checkSymmetry = TRUE)+log(MCMCpack::dinvgamma(Sigma2ModeRest, a0/2, d0/2)) LogPost1Rest &lt;- mvtnorm::dmvnorm(BetasModeRest, mean = bnRest, sigma = Sigma2ModeRest*BnRest, log = TRUE, checkSymmetry = TRUE) LogPost2Rest &lt;- log(MCMCpack::dinvgamma(Sigma2ModeRest, anRest/2, dnRest/2)) LogMarLikChibRest &lt;- LogLikRest + LogPriorRest -(LogPost1Rest + LogPost2Rest) BFChibs &lt;- exp(LogMarLikChibRest-LogMarLikChib) BFChibs; 1/BFChibs; 2*log(1/BFChibs) ## [1] 1.79514e-05 ## [1] 55705.95 ## [1] 21.85568 # Gelfand-Dey method GDmarglik &lt;- function(ids, X, Betas, MeanThetas, VarThetas, sig2Post){ K &lt;- dim(X)[2]; Thetas &lt;- c(Betas[ids,], sig2Post[ids]) Lognom &lt;- (1/(1-alpha))*mvtnorm::dmvnorm(Thetas, mean = MeanThetas, sigma = VarThetas, log = TRUE, checkSymmetry = TRUE) Logden1 &lt;- mvtnorm::dmvnorm(Betas[ids,], mean = rep(0, K), sigma = sig2Post[ids]*cOpt*diag(K), log = TRUE, checkSymmetry = TRUE) + log(MCMCpack::dinvgamma(sig2Post[ids], a0/2, d0/2)) VarModel &lt;- sig2Post[ids]*diag(N) MeanModel &lt;- X%*%Betas[ids,] Logden2 &lt;- mvtnorm::dmvnorm(c(Y), mean = MeanModel, sigma = VarModel, log = TRUE, checkSymmetry = TRUE) LogGDid &lt;- Lognom - Logden1 - Logden2 return(LogGDid) } sig2Post &lt;- MCMCpack::rinvgamma(S,an/2,dn/2) Betas &lt;- LaplacesDemon::rmvt(S, bn, Hn, an) Thetas &lt;- cbind(Betas, sig2Post) MeanThetas &lt;- colMeans(Thetas); VarThetas &lt;- var(Thetas) iVarThetas &lt;- solve(VarThetas) ChiSQ &lt;- sapply(1:S, function(s){(Thetas[s,]-MeanThetas)%*%iVarThetas%*%(Thetas[s,]-MeanThetas)}) alpha &lt;- 0.01; criticalval &lt;- qchisq(1-alpha, K + 1) idGoodThetas &lt;- which(ChiSQ &lt;= criticalval) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = S, width = 300) InvMargLik2 &lt;- NULL for(s in idGoodThetas){ LogInvs &lt;- GDmarglik(ids = s, X = X, Betas = Betas, MeanThetas = MeanThetas, VarThetas = VarThetas, sig2Post = sig2Post) InvMargLik2 &lt;- c(InvMargLik2, LogInvs) setWinProgressBar(pb, s, title=paste( round(s/S*100, 0),&quot;% done&quot;)) } close(pb); mean(InvMargLik2) ## NULL ## [1] 740.8927 # Restricted model anRest &lt;- N + a0; XRest &lt;- X[,-5] KRest &lt;- dim(XRest)[2]; B0Rest &lt;- cOpt*diag(KRest) BnRest &lt;- solve(solve(B0Rest)+t(XRest)%*%XRest) bhatRest &lt;- solve(t(XRest)%*%XRest)%*%t(XRest)%*%Y b0Rest &lt;- rep(0, KRest) bnRest &lt;- BnRest%*%(solve(B0Rest)%*%b0Rest+t(XRest)%*%XRest%*%bhatRest) dnRest &lt;- as.numeric(d0 + t(Y-XRest%*%bhatRest)%*%(Y-XRest%*%bhatRest)+t(bhatRest - b0Rest)%*%solve(solve(t(XRest)%*%XRest)+B0Rest)%*%(bhatRest - b0Rest)) HnRest &lt;- as.matrix(Matrix::forceSymmetric(dnRest*BnRest/anRest)) sig2PostRest &lt;- MCMCpack::rinvgamma(S,anRest/2,dnRest/2) BetasRest &lt;- LaplacesDemon::rmvt(S, bnRest, HnRest, anRest) ThetasRest &lt;- cbind(BetasRest, sig2PostRest) MeanThetasRest &lt;- colMeans(ThetasRest) VarThetasRest &lt;- var(ThetasRest) iVarThetasRest &lt;- solve(VarThetasRest) ChiSQRest &lt;- sapply(1:S, function(s){(ThetasRest[s,]-MeanThetasRest)%*%iVarThetasRest%*%(ThetasRest[s,]-MeanThetasRest)}) idGoodThetasRest &lt;- which(ChiSQRest &lt;= criticalval) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = S, width = 300) InvMargLik1 &lt;- NULL for(s in idGoodThetasRest){ LogInvs &lt;- GDmarglik(ids = s, X = XRest, Betas = BetasRest, MeanThetas = MeanThetasRest, VarThetas = VarThetasRest, sig2Post = sig2PostRest) InvMargLik1 &lt;- c(InvMargLik1, LogInvs) setWinProgressBar(pb, s, title=paste( round(s/S*100, 0),&quot;% done&quot;)) } close(pb); summary(coda::mcmc(InvMargLik1)) ## NULL ## ## Iterations = 1:9951 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 9951 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 7.518e+02 1.275e-01 1.278e-03 1.278e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 751.6 751.7 751.8 751.9 752.0 mean(InvMargLik1) ## [1] 751.7977 BFFD &lt;- exp(mean(InvMargLik2)-mean(InvMargLik1)) BFFD; mean(1/BFFD); 2*log(1/BFFD) ## [1] 1.836687e-05 ## [1] 54445.85 ## [1] 21.80992 References Chib, Siddhartha. 1995. “Marginal Likelihood from the Gibbs Output.” Journal of the American Statistical Association 90 (432): 1313–21. Chib, Siddhartha, and Ivan Jeliazkov. 2001. “Marginal Likelihood from the Metropolis–Hastings Output.” Journal of the American Statistical Association 96 (453): 270–81. Chib, Siddhartha, and Todd A Kuffner. 2016. “Bayes Factor Consistency.” arXiv Preprint arXiv:1607.00292. Dickey, James M. 1971. “The Weighted Likelihood Ratio, Linear Hypotheses on Normal Location Parameters.” The Annals of Mathematical Statistics, 204–23. Gelfand, Alan E, and Dipak K Dey. 1994. “Bayesian Model Choice: Asymptotics and Exact Calculations.” Journal of the Royal Statistical Society: Series B (Methodological) 56 (3): 501–14. Geweke, John. 1999. “Using Simulation Methods for Bayesian Econometric Models: Inference, Development, and Communication.” Econometric Reviews 18 (1): 1–73. Greenberg, Edward. 2012. Introduction to Bayesian Econometrics. Cambridge University Press. Jhonson, V. E., and D. Rossell. 2012. “Bayesian Model Selection in High-Dimensional Settings.” Journal of the American Statistical Association 107 (498): 649–60. Koop, Gary M. 2003. Bayesian Econometrics. John Wiley &amp; Sons Inc. Verdinelli, Isabella, and Larry Wasserman. 1995. “Computing Bayes Factors Using a Generalization of the Savage-Dickey Density Ratio.” Journal of the American Statistical Association 90 (430): 614–18. Walker, Stephen G. 2004a. “Modern Bayesian Asymptotics.” Statistical Science, 111–17. Walker, Stephen G. 2004b. “New Approaches to Bayesian Consistency.” Annals of Statistics 32 (5): 2028–43. https://doi.org/10.1214/009053604000000409. Jhonson and Rossell (2012) highlight the important distinction between pairwise consistency and model selection consistency. The latter requires the consistency of a sequence of pairwise nested comparisons.↩︎ Note that a sufficient condition for this assumption is to assume the same prior for the parameters that are the same in each model. Verdinelli and Wasserman (1995) incorporate a correction factor when this assumption is not satisfied.↩︎ "],["sec10_5.html", "10.5 Summary", " 10.5 Summary In this chapter, we introduced Bayesian model averaging (BMA) in generalized linear models. For linear Gaussian models, we perform BMA using three approaches: the Bayesian Information Criterion (BIC) approximation with Occam’s window, the Markov Chain Monte Carlo Model Composition (MC3) algorithm, and conditional Bayes factors, which account for endogeneity. Additionally, we show how to perform dynamic Bayesian model averaging in state-space models, where forgetting parameters are used to facilitate computation. For other generalized linear models, such as logit, gamma, and Poisson, we demonstrate how to use the BIC approximation to perform BMA. Finally, we present alternative methods for calculating the marginal likelihood: the Savage-Dickey density ratio, Chib’s method, and the Gelfand-Dey method. These methods are particularly useful when the BIC approximation does not perform well due to small or moderate sample sizes. However, a limitation of standard BMA is its implicit assumption that one of the candidate models is the true data-generating process. As Box famously noted, “Since all models are wrong, the scientist cannot obtain a ‘correct’ one by excessive elaboration. On the contrary, following William of Occam, he should seek an economical description of natural phenomena” (Box 1976). This perspective has motivated new developments in BMA that relax the “true model” assumption and instead treat all models as misspecified. In these approaches, the Bayesian average of predictive densities is constructed using leave-one-out (LOO) predictive performance, and model weights are chosen to minimize a predictive loss function. One prominent example is stacking of predictive distributions, which, rather than weighting models by marginal likelihood, uses cross-validation (LOO) to assign weights that maximize out-of-sample predictive accuracy (Yao et al. 2018). For a comprehensive review of Bayesian methods for aggregating predictive distributions, see (yao2021ayesianaggregation?). References Box, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99. https://doi.org/10.1080/01621459.1976.10480949. Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using Stacking to Average Bayesian Predictive Distributions.” Bayesian Analysis 13 (3): 917–1003. https://doi.org/10.1214/17-BA1091. "],["sec10_6.html", "10.6 Exercises", " 10.6 Exercises The Gaussian linear model specifies \\(\\mathbf{y} = \\alpha\\boldsymbol{i}_N + \\boldsymbol{X}_m\\boldsymbol{\\beta}_m + \\boldsymbol{\\mu}_m\\) such that \\(\\boldsymbol{\\mu}_m \\sim N(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I}_n)\\), and \\(\\boldsymbol{X}_m\\) does not have the column of ones. Assuming that \\(\\pi(\\sigma^2) \\propto 1/{\\sigma^2}\\), \\(\\pi(\\alpha) \\propto 1\\), and \\(\\boldsymbol{\\beta}_m | \\sigma^2 \\sim N(\\boldsymbol{0}_{k_m}, \\sigma^2 (g_m\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1})\\): Show that the posterior conditional distribution of \\(\\boldsymbol{\\beta}_m\\) is \\(N(\\boldsymbol{\\beta}_{mn}, \\sigma^2\\boldsymbol{B}_{mn})\\), where \\(\\boldsymbol{\\beta}_{mn} = \\boldsymbol{B}_{mn}\\boldsymbol{X}_m^{\\top}\\mathbf{y}\\) and \\(\\boldsymbol{B}_{mn} = ((1+g_m)\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1}\\). Show that the marginal likelihood associated with model \\(\\mathcal{M}_m\\) is proportional to \\[ p(\\mathbf{y} | \\mathcal{M}_m) \\propto \\left(\\frac{g_m}{1+g_m}\\right)^{k_m/2} \\left[(\\mathbf{y} - \\bar{y}\\boldsymbol{i}_N)^{\\top}(\\mathbf{y} - \\bar{y}\\boldsymbol{i}_N) - \\frac{1}{1+g_m}(\\mathbf{y}^{\\top}\\boldsymbol{P}_{X_m}\\mathbf{y})\\right]^{-(N-1)/2}, \\] where all parameters are indexed to model \\(\\mathcal{M}_m\\), \\(\\boldsymbol{P}_{X_m} = \\boldsymbol{X}_m(\\boldsymbol{X}_m^{\\top}\\boldsymbol{X}_m)^{-1}\\boldsymbol{X}_m\\) is the projection matrix on the space generated by the columns of \\(\\boldsymbol{X}_m\\), and \\(\\bar{y}\\) is the sample mean of \\(\\mathbf{y}\\). Hint: Take into account that \\(\\boldsymbol{i}_N^{\\top}\\boldsymbol{X}_m = \\boldsymbol{0}_{k_m}\\) due to all columns being centered with respect to their means. Determinants of export diversification I Jetter and Ramírez Hassan (2015) use BMA to study the determinants of export diversification. Use the dataset 10ExportDiversificationHHI.csv to perform BMA using the BIC approximation and MC3 to check if these two approaches agree. Simulation exercise of the Markov Chain Monte Carlo model composition Program an algorithm to perform MC3 where the final \\(S\\) models are unique. Use the simulation setting of Section 10.2 increasing the number of regressors to 40, which implies approximately \\(1.1 \\times 10^{12}\\) models. Simulation exercise of IV BMA Use the simulation setting with endogeneity in Section 10.2 to perform BMA based on the BIC approximation and MC3. Determinants of export diversification II Use the datasets 11ExportDiversificationHHI.csv and 12ExportDiversificationHHIInstr.csv to perform IV BMA assuming that the log of per capita gross domestic product is endogenous (avglgdpcap). See Jetter and Ramírez Hassan (2015) for details. Show that the link function in the case of the Bernoulli distribution is \\(\\log(\\theta / (1 - \\theta))\\). Ramı́rez-Hassan (2020), Ramı́rez-Hassan and Carvajal-Rendón (2021) perform variable selection using the file 13InternetMed.csv. In this dataset, the dependent variable is an indicator of Internet adoption (internet) for 5,000 households in Medellín (Colombia) during 2006–2014. This dataset contains 18 potential determinants, implying 262,144 (\\(2^{18}\\)) potential models. Perform BMA using the logit link function with this dataset. Serna Rodríguez, Ramírez Hassan, and Coad (2019) use 14ValueFootballPlayers.csv to analyze the market value of soccer players in Europe’s top leagues. There are 26 potential determinants of the market value of 335 soccer players. Use this dataset to perform BMA using the gamma distribution, setting default values for Occam’s window. Use the dataset 15Fertile2.csv from Wooldridge (2012) to perform BMA using the Poisson model with the log link. The dataset contains 1,781 women from Botswana in 1988. The dependent variable is the number of children ever born (ceb), modeled as a function of 19 potential determinants. Perform BMA in the logit model using MC3 and the BIC approximation using the simulation setting of Section 10.3. Use 19ExchangeRateCOPUSD.csv to perform dynmaic BMA using four state-space models explaining annual variations in the COP to USD exchange rate: Interest rate parity \\(\\Delta e_t = \\beta_{1t}^{IRP} + \\beta_{2t}^{IRP} (i_{t-1}^{Col}-i_{t-1}^{USA})+\\mu_{t}^{IRP}\\) Purchasing power parity \\(\\Delta e_t = \\beta_{1t}^{PPP} + \\beta_{2t}^{PPP} (\\pi_{t-1}^{Col}-\\pi_{t-1}^{USA})+\\mu_{t}^{PPP}\\) Taylor rule \\(\\Delta e_t = \\beta_{1t}^{Taylor} + \\beta_{2t}^{Taylor} (\\pi_{t-1}^{Col}-\\pi_{t-1}^{USA})+\\beta_{2t}^{Taylor} (g_{t-1}^{Col}-g_{t-1}^{USA})+\\mu_{t}^{IRP}\\) Money supply \\(\\Delta e_t = \\beta_{1t}^{Money} + \\beta_{2t}^{Money} (g_{t-1}^{Col}-g_{t-1}^{USA})+\\beta_{2t}^{Money} (m_{t-1}^{Col}-m_{t-1}^{USA})+\\mu_{t}^{Money}\\) where varTRM (\\(\\Delta e_t\\)) represents the annual variation rate of the exchange rate from COP to USD, TES_COL10 (\\(i_{t}^{Col}\\)) and TES_USA10 (\\(i_{t}^{USA}\\)) denote the annual return rates of Colombian and U.S. public debts over 10 years, inflation_COL (\\(\\pi_{t}^{Col}\\)) and inflation_USA (\\(\\pi_{t}^{USA}\\)) are the annual inflation rates for Colombia and the U.S., varISE_COL (\\(g_{t}^{Col}\\)) and varISE_USA (\\(g_{t}^{USA}\\)) represent the annual variations of economic activity indices, and varCOL_M3 (\\(m_{t}^{Col}\\)) and varUSA_M3 (\\(m_{t}^{USA}\\)) are the annual variations of the money supply. In addition, \\(\\mu_{t}^{\\cdot}\\) is the stochastic error. The dataset includes monthly variations from January 2006 to November 2023. Perform Bayesian model averaging using these models, calculate posterior model probabilities, and plot the posterior mean and credible interval of \\(\\beta_{2t}^{Money}\\). Perform a simulation of the dynamic logistic model, where there are 7 (\\(2^3 - 1\\), excluding the model without regressors) competing models originating from 3 regressors: \\(x_{tk} \\sim N(0.5, 0.8^2)\\), \\(k = 2, 3, 4\\), and \\(\\beta_1 = 0.5\\), \\(\\beta_{2t}\\) is a sequence from 1 to 2 in steps given by \\(1/T\\), and \\(\\beta_{3t} = \\begin{Bmatrix} -1, &amp; 1 &lt; t \\leq 0.5T \\\\ 0, &amp; 0.5T &lt; t \\leq T \\end{Bmatrix}\\), with \\(\\beta_4 = 1.2\\). Then, \\(\\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta}_t = \\beta_1 + \\beta_{2t} x_{2t} + \\beta_{3t} x_{3t} + \\beta_4 x_{4t}\\), where \\[ P[Y_t = 1 | \\boldsymbol{x}_t, \\boldsymbol{\\beta}_t] = \\frac{\\exp(\\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta}_t)}{1 + \\exp(\\boldsymbol{x}_t^{\\top} \\boldsymbol{\\beta}_t)}, \\quad t = 1, 2, \\dots, 1100. \\] Use the function logistic.dma from the dma package to obtain the posterior model probabilities, first setting the forgetting parameter of the models to 0.99, and then to 0.95. Compare the results. Show that \\[ \\mathbb{E}\\left[\\frac{q(\\boldsymbol{\\theta})}{\\pi(\\boldsymbol{\\theta} | \\mathcal{M}_m)p(\\mathbf{y} | \\boldsymbol{\\theta}_m, \\mathcal{M}_m)} \\bigg\\rvert \\mathbf{y}, \\mathcal{M}_m\\right] = \\frac{1}{p(\\mathbf{y} | \\mathcal{M}_m)}, \\] where the expected value is with respect to the posterior distribution given model \\(\\mathcal{M}_m\\), and \\(q(\\boldsymbol{\\theta})\\) is the proposal distribution whose support is \\(\\boldsymbol{\\Theta}\\). References Jetter, M., and A. Ramírez Hassan. 2015. “Want Export Diversification? Educate the Kids First.” Economic Inquiry 53 (4): 1765–82. Ramı́rez-Hassan, Andrés. 2020. “Dynamic Variable Selection in Dynamic Logistic Regression: An Application to Internet Subscription.” Empirical Economics 59 (2): 909–32. Ramı́rez-Hassan, Andrés, and Daniela A. Carvajal-Rendón. 2021. “Specification Uncertainty in Modeling Internet Adoption: A Developing City Case Analysis.” Utilities Policy 70: 101218. Serna Rodríguez, M., A. Ramírez Hassan, and A. Coad. 2019. “Uncovering Value Drivers of High Performance Soccer Players.” Journal of Sport Economics 20 (6): 819–49. Wooldridge, Jeffrey M. 2012. Introductory Econometrics: A Modern Approach. Fifth. Mason, Ohio: South-Western: Cengage Learning. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
