[["Chap11.html", "Chapter 11 Semi-parametric and non-parametric models", " Chapter 11 Semi-parametric and non-parametric models Non-parametric models are characterized by making minimal assumptions about the data-generating process. Unlike parametric models, which have a finite-dimensional parameter space, non-parametric models often involve infinite-dimensional parameter spaces. A major challenge in non-parametric modeling is the curse of dimensionality, as these models require dense data coverage, necessitating large datasets to achieve reliable estimates. Semi-parametric methods, on the other hand, combine parametric assumptions for part of the model with non-parametric assumptions for the rest. This approach offers a balance between flexibility, tractability and applicability. In this chapter, we introduce finite Gaussian mixture models (GMM) and Dirichlet mixture processes (DMP), the latter representing an infinite mixture. Both can be used to specify an entire statistical model (nonparametric specification) or to model stochastic error distributions in a semiparametric framework. Additionally, we present spline models, where the outcome depends linearly on smooth nonparametric functions. To address the curse of dimensionality, we introduce partially linear models, which mitigate this issue while remaining interpretable and flexible for practical applications. We let other useful Bayesian non-parametric approaches like Bayesian additive random trees (BART) and Gaussian process (GP) for Chapter 12. "],["sec11_1.html", "11.1 Mixture models", " 11.1 Mixture models Mixture models naturally arise in situations where a sample consists of draws from different subpopulations (clusters) that cannot be easily distinguished based on observable characteristics. However, performing inference on specific identified subpopulations can be misleading if the assumed distribution for each cluster is misspecified. Even when distinct subpopulations do not exist, finite and infinite mixture models provide a useful framework for semi-parametric inference. They effectively approximate distributions with skewness, excess kurtosis, and multimodality, making them useful for modeling stochastic errors. In addition, mixture models help capture unobserved heterogeneity. That is, as data modelers, we may observe individuals with identical sets of observable variables but entirely different response variables. These differences cannot be explained solely by sampling variability; rather, they suggest the presence of an unobserved underlying process, independent of the observable features, that accounts for this pattern. 11.1.1 Finite Gaussian mixtures A finite Gaussian mixture model for regression with \\(H\\) known components assumes that a sample \\(\\boldsymbol{y}=\\left[y_1 \\ y_2 \\ \\dots \\ y_N\\right]^{\\top}\\) consists of observations \\(y_i\\), for \\(i=1,2,\\dots,N\\), where each \\(y_i\\) is generated from one of the \\(H\\) components, \\(h=1,2,\\dots,H\\), conditional on the regressors \\(\\boldsymbol{x}_i\\). Specifically, we assume \\[ y_i \\mid \\boldsymbol{x}_i \\sim N(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h, \\sigma_h^2). \\] Thus, the sampling distribution of \\(y_i\\) is given by \\[ p(y_i \\mid \\{\\lambda_h, \\boldsymbol{\\beta}_h, \\sigma_h^2\\}_{h=1}^H, \\boldsymbol{x}_i) = \\sum_{h=1}^H \\lambda_h \\phi(y_i \\mid \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h, \\sigma_h^2), \\] where \\(\\phi(y_i \\mid \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h, \\sigma_h^2)\\) is the Gaussian density with mean \\(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h\\) and variance \\(\\sigma_h^2\\), \\(0 &lt; \\lambda_h &lt; 1\\) represents the proportion of the population belonging to subpopulation \\(h\\), and the weights satisfy \\(\\sum_{h=1}^H \\lambda_h = 1\\). Then, we allow cross-sectional units to differ according to unobserved clusters (subpopulations) that exhibit homogeneous behavior within each cluster. To model a finite Gaussian mixture, we introduce an individual cluster indicator or latent class \\(\\psi_{ih}\\) such that \\[ \\psi_{ih}= \\begin{cases} 1, &amp; \\text{if the } i\\text{-th unit is drawn from the } h\\text{-th cluster}, \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] Thus, \\(P(\\psi_{ih}=1) = \\lambda_h\\) for all clusters \\(h=1,2,\\dots,H\\) and units \\(i=1,2,\\dots,N\\). Note that a high probability of individuals belonging to the same cluster suggests that these clusters capture similar sources of unobserved heterogeneity. This setting implies that \\[ \\boldsymbol{\\psi}_i = [\\psi_{i1} \\ \\psi_{i2} \\ \\dots \\ \\psi_{iH}]^{\\top} \\sim \\text{Categorical}(\\boldsymbol{\\lambda}), \\] where \\(\\boldsymbol{\\lambda} = [\\lambda_1 \\ \\lambda_2 \\ \\dots \\ \\lambda_H]^{\\top}\\) represents the event probabilities. We know from Subsection 3.2 that the Dirichlet prior distribution is conjugate to the multinomial distribution, where the categorical distribution is a special case in which the number of trials is one. Thus, we assume that \\[ \\pi(\\boldsymbol{\\lambda}) \\sim \\text{Dir}(\\boldsymbol{\\alpha}_0), \\] where \\(\\boldsymbol{\\alpha}_0 = [\\alpha_{10} \\ \\alpha_{20} \\ \\dots \\ \\alpha_{H0}]^{\\top}\\), \\(\\alpha_{h0}=1/H\\) is recommended by Gelman et al. (2021). Observe that we are using a hierarchical structure, as we specify a prior on \\(\\boldsymbol{\\lambda}\\), which serves as the hyperparameter for the cluster indicators. In addition, we can assume conjugate families for the location and scale parameters to facilitate computation, that is, \\(\\boldsymbol \\beta_h\\sim N(\\boldsymbol{\\beta}_{h0},\\boldsymbol{B}_{h0})\\) and \\(\\sigma_h^2\\sim IG(\\alpha_{h0}/2,\\delta_{h0}/2)\\). This setting allows to obtain standard conditional posterior distributions: \\[\\boldsymbol{\\beta}_{h}\\sim N(\\boldsymbol{\\beta}_{hn},\\boldsymbol{B}_{hn}),\\] where \\(\\boldsymbol{B}_{hn}=(\\boldsymbol{B}_{h0}^{-1}+\\sigma_h^{-2}\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\top})^{-1}\\) and \\(\\boldsymbol{\\beta}_{hn}=\\boldsymbol{B}_{hn}(\\boldsymbol{B}_{h0}^{-1}\\boldsymbol{\\beta}_{h0}+\\sigma_h^{-2}\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}\\boldsymbol{x}_iy_i)\\). \\[\\sigma_h^2\\sim IG(\\alpha_{hn}/2,\\delta_{hn}/2),\\] where \\(\\alpha_{hn}=\\alpha_{h0}+N_h\\), \\(\\delta_{hn}=\\delta_{h0}+\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}(y_i-\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h)^2\\), and \\(N_h\\) is the number of units in cluster \\(h\\). \\[\\boldsymbol{\\lambda}\\sim \\text{Dir}(\\boldsymbol{\\alpha}_n),\\] where \\(\\boldsymbol{\\alpha}_n=[\\alpha_{1n} \\ \\alpha_{2n} \\ \\dots \\ \\alpha_{Hn}]^{\\top}\\), and \\(\\alpha_{hn}=\\alpha_{h0}+N_h\\). \\[\\boldsymbol{\\psi}_{in}\\sim \\text{Categorical}(\\boldsymbol{\\lambda}_n),\\] where \\(P(\\psi_{ih}=1)=\\frac{\\lambda_{h}\\phi(y_i \\mid \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h,\\sigma_h^2)}{\\sum_{j=1}^H\\lambda_{j}\\phi(y_i \\mid \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_j,\\sigma_j^2)}\\). In general, it is always safer to perform inference in mixture models using informative priors, as non-informative priors may have unintended consequences on posterior inference. One way to facilitate prior elicitation is to work with standardized data, as this removes dependence on measurement units. Another useful approach is to specify the model in log-log form so that the coefficients can be interpreted as elasticities or semi-elasticities. As always in Bayesian inference, it makes sense to perform a sensitivity analysis with respect to the hyperparameters. Mixture models have the label-switching identification problem, meaning they are nonidentifiable because the distribution remains unchanged if the group labels are permuted (Van Hasselt 2011). For instance, a mixture model with two components can be characterized by \\(\\left\\{\\lambda_1,\\boldsymbol{\\beta}_1,\\sigma_1^2\\right\\}\\) for the first cluster and \\(\\left\\{1-\\lambda_1,\\boldsymbol{\\beta}_2,\\sigma_2^2\\right\\}\\) for the second. However, an alternative characterization is \\(\\left\\{1-\\lambda_1,\\boldsymbol{\\beta}_2,\\sigma_2^2\\right\\}\\) for cluster 1 and \\(\\left\\{\\lambda_1,\\boldsymbol{\\beta}_1,\\sigma_1^2\\right\\}\\) for cluster 2. This parametrization yields exactly the same likelihood as the first one, meaning any permutation of the cluster labels leaves the likelihood unchanged. Consequently, the posterior draws of each component-specific parameter target the same distribution. Label switching may pose challenges when performing inference on specific mixture components, such as in the regression analysis presented here. However, it is not an issue when inference on specific components is unnecessary, as in cases where mixtures are used to model stochastic errors in semi-parametric settings. In the former case, post-processing strategies can mitigate the issue, such as random permutation of latent classes (see the simulation exercise below, Gelman et al. (2021) and Algorithm 3.5 in Frühwirth-Schnatter (2006)). A semi-parametric regression imposes a specific structure in part of the model and uses flexible assumptions in another part, for instance \\[\\begin{align*} y_i&amp;=\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}+\\mu_i,\\\\ p(\\mu_i \\mid \\left\\{\\lambda_h,\\mu_h,\\sigma_h^2\\right\\}_{h=1}^H)&amp;=\\sum_{h=1}^H\\lambda_h\\phi(\\mu_i\\mid \\mu_h,\\sigma_h^2). \\end{align*}\\] Thus, the distribution of the stochastic error is a finite Gaussian mixture. Note that the mean of the stochastic error is not equal to zero; consequently, the intercept in the regression should be removed, as these two parameters are not separately identifiable (Van Hasselt 2011). Additionally, this approach allows for multiple modes and asymmetric distributions of the stochastic errors, providing greater flexibility. We can use a Gibbs sampling algorithm in this semi-parametric specification if we assume conjugate families. The difference from the previous setting is that we have the same slope parameters; thus, \\(\\boldsymbol{\\beta} \\sim N(\\boldsymbol{\\beta}_{0},\\boldsymbol{B}_{0})\\). Additionally, we must specify the prior distribution for the means of the stochastic errors, given by \\(\\mu_h \\sim N(\\mu_{h0},\\sigma^2_{\\mu 0})\\). Then, the posterior distributions are: \\[\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\beta}_{n},\\boldsymbol{B}_{n}),\\] where \\(\\boldsymbol{B}_{n}=(\\boldsymbol{B}_{0}^{-1}+\\sum_{h=1}^H\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}\\sigma_h^{-2}\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\top})^{-1}\\) and \\(\\boldsymbol{\\beta}_{n}=\\boldsymbol{B}_{n}(\\boldsymbol{B}_{0}^{-1}\\boldsymbol{\\beta}_{0}+\\sum_{h=1}^H\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}\\sigma_h^{-2}\\boldsymbol{x}_i(y_i-\\mu_h))\\). \\[\\sigma_h^2\\sim IG(\\alpha_{hn}/2,\\delta_{hn}/2),\\] where \\(\\alpha_{hn}=\\alpha_{h0}+N_h\\), \\(\\delta_{hn}=\\delta_{h0}+\\sum_{\\left\\{i: \\psi_{ih}=1\\right\\}}(y_i-\\mu_h-\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h)^2\\), and \\(N_h\\) is the number of units in cluster \\(h\\). \\[\\mu_h\\sim N(\\mu_{hn},\\sigma_{hn}^2),\\] where \\(\\sigma_{hn}^2=\\left(\\frac{1}{\\sigma_{h}^{2}}+\\frac{N_h}{\\sigma_{h}^2}\\right)^{-1}\\) and \\(\\mu_{hn}=\\sigma_{hn}^2\\left(\\frac{\\mu_{h0}}{\\sigma_{\\mu0}^2}+\\frac{\\sum_{\\left\\{i:\\psi_{ih}=1\\right\\}} (y_i-\\boldsymbol{x}_i\\boldsymbol{\\beta})}{\\sigma_h^2}\\right)\\). \\[\\boldsymbol{\\lambda}\\sim \\text{Dir}(\\boldsymbol{\\alpha}_n),\\] where \\(\\boldsymbol{\\alpha}_n=[\\alpha_{1n} \\ \\alpha_{2n} \\ \\dots \\ \\alpha_{Hn}]^{\\top}\\), and \\(\\alpha_{hn}=\\alpha_{h0}+N_h\\). \\[\\boldsymbol{\\psi}_{in}\\sim \\text{Categorical}(\\boldsymbol{\\lambda}_n),\\] where \\(P(\\psi_{ih}=1)=\\frac{\\lambda_{h}\\phi(y_i-\\mu_h-\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} \\mid \\mu_h,\\sigma_h^2,\\boldsymbol{\\beta})}{\\sum_{j=1}^H\\lambda_{j}\\phi(y_i-\\mu_j-\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} \\mid \\mu_j,\\sigma_j^2,\\boldsymbol{\\beta})}\\). A potential limitation of finite mixture models is the need to specify the number of components in advance. One approach is to estimate the model for different values of \\(H\\) and then compute the marginal likelihood to select the model best supported by the data. However, this procedure can be tedious. A simpler strategy is to set \\(H\\) large enough (e.g., 10 components), assign \\(\\alpha_{h0} = 1/H\\), and perform an initial run of the algorithm. If we are not interested in the specific composition of clusters, this approach is sufficient. Otherwise, the posterior distribution of \\(H\\) can be obtained by tracking the number of nonempty clusters in each iteration. In a second run, \\(H\\) can then be fixed at the mode of this posterior distribution. However, the previous approaches ultimately fix the number of components. Consequently, finite mixtures cannot be considered a non-parametric method (Rossi 2014), as they lack an automatic mechanism to increase \\(H\\) as the sample size grows. An alternative is to avoid pre-specifying the number of components altogether by using a Dirichlet process mixture (DPM). This is the topic of the next section. Example: Simulation exercises First, let’s illustrate the label-switching issue using a simple model without regressors, assuming the same known variance. Consider the following distribution: \\[p(y_i) = 0.75 \\phi(y_i \\mid \\beta_{01},1^2) + 0.25 \\phi(y_i \\mid \\beta_{02},1^2), \\quad i = 1,2,\\dots,500.\\] Initially, we set \\(\\beta_{01} = 0.5\\) and \\(\\beta_{02} = 2.5\\). We perform 1,000 MCMC iterations, with a burn-in period of 500 and a thinning factor of 2. The following code demonstrates how to implement the Gibbs sampler using a prior normal distribution with mean 0 and variance 10, with the hyperparameters of the Dirichlet distribution set to \\(1/2\\). rm(list = ls()); set.seed(010101); library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 500 z &lt;- rbinom(n, 1, 0.75) # Latent class indicator y &lt;- ifelse(z == 0, rnorm(n, 0.5, 1), rnorm(n, 2.5, 1)) data &lt;- data.frame(y) # Plot ggplot(data, aes(x = y)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(title = &quot;Density Plot&quot;, x = &quot;y&quot;, y = &quot;Density&quot;) + theme_minimal() # Hyperparameters mu0 &lt;- 0; sig2mu0 &lt;- 10; H &lt;- 2; a0h &lt;- rep(1/H, H) # MCMC parameters mcmc &lt;- 1000; burnin &lt;- 500; tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions Postmu &lt;- function(yh){ Nh &lt;- length(yh) sig2mu &lt;- (1/sig2mu0 + Nh)^(-1) mun &lt;- sig2mu*(mu0/sig2mu0 + sum(yh)) mu &lt;- rnorm(1, mun, sig2mu^0.5) return(mu) } PostPsi &lt;- matrix(NA, tot, n); PostMu &lt;- matrix(NA, tot, H) PostLambda &lt;- rep(NA, tot) Id1 &lt;- which(y &lt;= 1) # 1 is from inspection of the density plot of y Id2 &lt;- which(y &gt; 1) N1 &lt;- length(Id1); N2 &lt;- length(Id2) Lambda &lt;- c(N1/n, N2/n) MU &lt;- c(mean(y[Id1]), mean(y[Id2])); Psi &lt;- rep(NA, n) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:n){ lambdai &lt;- NULL for(h in 1:H){ lambdaih &lt;- Lambda[h]*dnorm(y[i], MU[h], 1) lambdai &lt;- c(lambdai, lambdaih) } Psi[i] &lt;- sample(1:H, 1, prob = lambdai) } PostPsi[s, ] &lt;- Psi for(h in 1:H){ idh &lt;- which(Psi == h); MU[h] &lt;- Postmu(yh = y[idh]) } PostMu[s,] &lt;- MU; Lambda &lt;- sort(MCMCpack::rdirichlet(1, a0h + table(Psi)), decreasing = TRUE) PostLambda[s] &lt;- Lambda[1] setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq(burnin, tot, thin) PosteriorMUs &lt;- coda::mcmc(PostMu[keep,]) summary(PosteriorMUs); plot(PosteriorMUs) dfMU &lt;- data.frame(mu1 = PostMu[keep,1], mu2 = PostMu[keep,2]) # Plot require(latex2exp) ggplot(dfMU) + geom_density(aes(x = mu1, color = &quot;mu1&quot;), linewidth = 1) + geom_density(aes(x = mu2, color = &quot;mu2&quot;), linewidth = 1) + labs(title = &quot;Density Plot&quot;, x = TeX(&quot;$\\\\mu$&quot;), y = &quot;Density&quot;, color = &quot;Variable&quot;) + theme_minimal() + scale_color_manual(values = c(&quot;mu1&quot; = &quot;blue&quot;, &quot;mu2&quot; = &quot;red&quot;)) The figure shows the posterior densities of the location parameters. The posterior means are 0.42 and 2.50, with 95% credible intervals of (0.07, 0.71) and (2.34, 2.65), respectively. The posterior mean of the probability is 0.27, with a 95% credible interval of (0.19, 0.35). Note that in this simple simulation exercise, we did not observe unintended consequences from using non-informative priors and not standardizing the data. However, real-world applications should take these aspects into account. We perform the same exercise assuming \\(\\beta_{01}=0.5\\) and \\(\\beta_{02}=1\\). The figure shows the posterior densities, where we observe significant overlap. The posterior means are 0.77 in both cases, with 95% credible intervals of (0.40, 1.05) and (-0.44, 1.71). The posterior mean of the probability is 0.84. rm(list = ls()); set.seed(010101); library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 500 z &lt;- rbinom(n, 1, 0.75) # Latent class indicator y &lt;- ifelse(z == 0, rnorm(n, 0.5, 1), rnorm(n, 1, 1)) data &lt;- data.frame(y) # Plot ggplot(data, aes(x = y)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(title = &quot;Density Plot&quot;, x = &quot;y&quot;, y = &quot;Density&quot;) + theme_minimal() # Hyperparameters mu0 &lt;- 0; sig2mu0 &lt;- 10; H &lt;- 2; a0h &lt;- rep(1/H, H) # MCMC parameters mcmc &lt;- 1000; burnin &lt;- 500; tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions Postmu &lt;- function(yh){ Nh &lt;- length(yh) sig2mu &lt;- (1/sig2mu0 + Nh)^(-1) mun &lt;- sig2mu*(mu0/sig2mu0 + sum(yh)) mu &lt;- rnorm(1, mun, sig2mu^0.5) return(mu) } PostPsi &lt;- matrix(NA, tot, n); PostMu &lt;- matrix(NA, tot, H) PostLambda &lt;- rep(NA, tot) Id1 &lt;- which(y &lt;= 1) # 1 is from inspection of the density plot of y Id2 &lt;- which(y &gt; 1) N1 &lt;- length(Id1); N2 &lt;- length(Id2) Lambda &lt;- c(N1/n, N2/n) MU &lt;- c(mean(y[Id1]), mean(y[Id2])); Psi &lt;- rep(NA, n) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:n){ lambdai &lt;- NULL for(h in 1:H){ lambdaih &lt;- Lambda[h]*dnorm(y[i], MU[h], 1) lambdai &lt;- c(lambdai, lambdaih) } Psi[i] &lt;- sample(1:H, 1, prob = lambdai) } PostPsi[s, ] &lt;- Psi for(h in 1:H){ idh &lt;- which(Psi == h); MU[h] &lt;- Postmu(yh = y[idh]) } PostMu[s,] &lt;- MU; Lambda &lt;- sort(MCMCpack::rdirichlet(1, a0h + table(Psi)), decreasing = TRUE) PostLambda[s] &lt;- Lambda[1] setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq(burnin, tot, thin) PosteriorMUs &lt;- coda::mcmc(PostMu[keep,]) summary(PosteriorMUs); plot(PosteriorMUs) dfMU &lt;- data.frame(mu1 = PostMu[keep,1], mu2 = PostMu[keep,2]) # Plot require(latex2exp) ggplot(dfMU) + geom_density(aes(x = mu1, color = &quot;mu1&quot;), linewidth = 1) + geom_density(aes(x = mu2, color = &quot;mu2&quot;), linewidth = 1) + labs(title = &quot;Density Plot&quot;, x = TeX(&quot;$\\\\mu$&quot;), y = &quot;Density&quot;, color = &quot;Variable&quot;) + theme_minimal() + scale_color_manual(values = c(&quot;mu1&quot; = &quot;blue&quot;, &quot;mu2&quot; = &quot;red&quot;)) In the second setting, the posterior draws of the Gibbs sampler can switch between the two means because they are relatively close. This situation contrasts with the first example, where there is a relatively large separation between the means, resulting in a region of the parameter space with zero probability (see the flat region between the two posterior distributions in that example). The key point is that, given a sufficiently large number of Gibbs sampler iterations, the algorithm should eventually explore the entire parameter space and encounter the label-switching issue. This occurs because both posterior chains should exhibit similar behavior, as they are targeting the same distribution. We can implement random permutation of latent classes to address this issue. This involves sampling a random permutation of the labels at each iteration of the MCMC algorithm. For example, with three clusters, there are \\(3! = 6\\) possible label permutations. Let the permutations be labeled as \\(\\boldsymbol{p}_k=\\left\\{p_k(1),p_k(2),\\dots,p_k(H)\\right\\}, k=1,2,\\dots,H!\\). At the end of each iteration in the MCMC algorithm, we randomly select one of the permutations \\(\\boldsymbol{p}_k\\) and replace the cluster probabilities \\(\\lambda_1^{(s)},\\dots,\\lambda_H^{(s)}\\) with \\(\\lambda_{p_k(1)}^{(s)},\\dots,\\lambda_{p_k(H)}^{(s)}\\). We apply the same permutation to \\(\\boldsymbol{\\beta}^{(s)}\\), \\(\\sigma^{2(s)}\\), and \\(\\boldsymbol{\\psi}_{i}^{(s)}\\), for \\(i=1,2,\\dots,n\\). The following algorithm illustrates how to implement this in our simple example. ###### Permutations ###### rm(list = ls()); set.seed(010101); library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 500 z &lt;- rbinom(n, 1, 0.75) # Latent class indicator y &lt;- ifelse(z == 0, rnorm(n, 0.5, 1), rnorm(n, 2.5, 1)) # Hyperparameters mu0 &lt;- 0; sig2mu0 &lt;- 10; H &lt;- 2; a0h &lt;- rep(1/H, H) # MCMC parameters mcmc &lt;- 2000; burnin &lt;- 500 tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions Postmu &lt;- function(yh){ Nh &lt;- length(yh) sig2mu &lt;- (1/sig2mu0 + Nh)^(-1) mun &lt;- sig2mu*(mu0/sig2mu0 + sum(yh)) mu &lt;- rnorm(1, mun, sig2mu^0.5) return(mu) } PostPsi &lt;- matrix(NA, tot, n); PostMu &lt;- matrix(NA, tot, H) PostLambda &lt;- rep(NA, tot) Id1 &lt;- which(y &lt;= 1); Id2 &lt;- which(y &gt; 1) N1 &lt;- length(Id1); N2 &lt;- length(Id2) Lambda &lt;- c(N1/n, N2/n); MU &lt;- c(mean(y[Id1]), mean(y[Id2])) Psi &lt;- rep(NA, n); per1 &lt;- c(1,2); per2 &lt;- c(2,1) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:n){ lambdai &lt;- NULL for(h in 1:H){ lambdaih &lt;- Lambda[h]*dnorm(y[i], MU[h], 1) lambdai &lt;- c(lambdai, lambdaih) } Psi[i] &lt;- sample(1:H, 1, prob = lambdai) } for(h in 1:H){ idh &lt;- which(Psi == h) MU[h] &lt;- Postmu(yh = y[idh]) } Lambda &lt;- MCMCpack::rdirichlet(1, a0h + table(Psi)) # Permutations labels &lt;- sample(1:2, 1, prob = c(0.5, 0.5)) if(labels == 2){ Lambda &lt;- Lambda[per2] MU &lt;- MU[per2] for(i in 1:n){ if(Psi[i] == 1){Psi[i] &lt;- 2 }else{Psi[i] &lt;- 1} } } PostPsi[s, ] &lt;- Psi; PostMu[s,] &lt;- MU PostLambda[s] &lt;- Lambda[1] setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq(burnin, tot, thin) PosteriorMUs &lt;- coda::mcmc(PostMu[keep,]) summary(PosteriorMUs) plot(PosteriorMUs) dfMU &lt;- data.frame(mu1 = PostMu[keep,1], mu2 = PostMu[keep,2]) # Plot require(latex2exp) ggplot(dfMU) + geom_density(aes(x = mu1, color = &quot;mu1&quot;), linewidth = 1) + # First density plot geom_density(aes(x = mu2, color = &quot;mu2&quot;), linewidth = 1) + # Second density plot labs(title = &quot;Density Plot&quot;, x = TeX(&quot;$\\\\mu$&quot;), y = &quot;Density&quot;, color = &quot;Variable&quot;) + theme_minimal() + scale_color_manual(values = c(&quot;mu1&quot; = &quot;blue&quot;, &quot;mu2&quot; = &quot;red&quot;)) # Custom colors PosteriorLAMBDA &lt;- coda::mcmc(PostLambda[keep]) summary(PosteriorLAMBDA) plot(PosteriorLAMBDA) The figure shows the posterior distributions from the random permutation of latent classes in the first simulation setting. We observe that both posterior distributions look similar, as both are targeting a bimodal distribution given by two clusters. In the following setting we simulate a simple regression mixture with two components such that \\(\\psi_{i1}\\sim \\text{Ber}(0.5)\\), consequently, \\(\\psi_{i2}=1-\\psi_{i1}\\), and assume one regressor, \\(x_i\\sim N(0,1)\\), \\(i=1,2,\\dots,1,000\\). Then, \\[p(y_i \\mid \\boldsymbol{x}_i) = 0.5 \\phi(y_i \\mid 2+1.5x_i,1^2)+0.5 \\phi(y_i \\mid -1+0.5x_i,0.8^2).\\] The following code shows how to perform inference in this model, assuming \\(N(0,5)\\) and \\(N(0,2)\\) priors for the intercepts and slopes, respectively. Additionally, we use a \\(Cauchy(0,2)\\) prior truncated at 0 for the standard deviations, and a \\(Dirichlet(1,1)\\) prior for the probabilities. We use the brms package in R, which in turn uses Stan, setting number of MCMC iterations 2,000, a burn-in (warm-up) equal to 1,000, and 4 chains. Remember that Stan software uses Hamiltonian Monte Carlo. ####### Simulation exercise: Gaussian mixture: 2 components ############# rm(list = ls()) set.seed(010101) library(brms) library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 1000 x &lt;- rnorm(n) z &lt;- rbinom(n, 1, 0.5) # Latent class indicator y &lt;- ifelse(z == 0, rnorm(n, 2 + 1.5*x, 1), rnorm(n, -1 + 0.5*x, 0.8)) data &lt;- data.frame(y, x) # Plot ggplot(data, aes(x = y)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + # Density plot with fill color labs(title = &quot;Density Plot&quot;, x = &quot;y&quot;, y = &quot;Density&quot;) + theme_minimal() # Define priors priors &lt;- c( set_prior(&quot;normal(0, 5)&quot;, class = &quot;Intercept&quot;, dpar = &quot;mu1&quot;), # First component intercept set_prior(&quot;normal(0, 5)&quot;, class = &quot;Intercept&quot;, dpar = &quot;mu2&quot;), # Second component intercept set_prior(&quot;normal(0, 2)&quot;, class = &quot;b&quot;, dpar = &quot;mu1&quot;), # First component slope set_prior(&quot;normal(0, 2)&quot;, class = &quot;b&quot;, dpar = &quot;mu2&quot;), # Second component slope set_prior(&quot;cauchy(0, 2)&quot;, class = &quot;sigma1&quot;, lb = 0), # First component sigma set_prior(&quot;cauchy(0, 2)&quot;, class = &quot;sigma2&quot;, lb = 0), # Second component sigma set_prior(&quot;dirichlet(1, 1)&quot;, class = &quot;theta&quot;) # Mixing proportions ) # Fit a 2-component Gaussian mixture regression model fit &lt;- brm( bf(y ~ 1 + x, family = mixture(gaussian, gaussian)), # Two normal distributions data = data, prior = priors, chains = 4, iter = 2000, warmup = 1000, cores = 4 ) prior_summary(fit) # Summary of priors summary(fit) # Summary of posterior draws plot(fit) # Plots of posterior draws The following code performs inference in this simulation from scratch using Gibbs sampling. We do not implement the random permutation of latent classes algorithm for facilitating exposition and comparability with the results from the package brms. We use non-informative priors, setting \\(\\alpha_{h0}=\\delta_{h0}=0.01\\), \\(\\boldsymbol{\\beta}_{h0}=\\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_{h0}=\\boldsymbol{I}_2\\), and \\(\\boldsymbol{\\alpha}_0=[1/2 \\ 1/2]^{\\top}\\). The number of MCMC iterations is 5,000, the burn-in is 1,000, and the thinning parameter is 2. In general, the Gibbs sampler appears to yield good posterior results as all 95% intervals encompass the population parameters. ########### Perform inference from scratch ############### rm(list = ls()); set.seed(010101) library(brms); library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 1000 x &lt;- rnorm(n) z &lt;- rbinom(n, 1, 0.5) # Latent class indicator y &lt;- ifelse(z == 0, rnorm(n, 2 + 1.5*x, 1), rnorm(n, -1 + 0.5*x, 0.8)) # Hyperparameters d0 &lt;- 0.001; a0 &lt;- 0.001 b0 &lt;- rep(0, 2); B0 &lt;- diag(2); B0i &lt;- solve(B0) a01 &lt;- 1/2; a02 &lt;- 1/2 # MCMC parameters mcmc &lt;- 5000; burnin &lt;- 1000 tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions PostSig2 &lt;- function(Betah, Xh, yh){ Nh &lt;- length(yh); an &lt;- a0 + Nh dn &lt;- d0 + t(yh - Xh%*%Betah)%*%(yh - Xh%*%Betah) sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2h, Xh, yh){ Bn &lt;- solve(B0i + sig2h^(-1)*t(Xh)%*%Xh) bn &lt;- Bn%*%(B0i%*%b0 + sig2h^(-1)*t(Xh)%*%yh) Beta &lt;- MASS::mvrnorm(1, bn, Bn) return(Beta) } PostBetas1 &lt;- matrix(0, mcmc+burnin, 2) PostBetas2 &lt;- matrix(0, mcmc+burnin, 2) PostSigma21 &lt;- rep(0, mcmc+burnin) PostSigma22 &lt;- rep(0, mcmc+burnin) PostPsi &lt;- matrix(0, mcmc+burnin, n) PostLambda &lt;- rep(0, mcmc+burnin) Id1 &lt;- which(y&lt;1) # 1 is from inspection of the density plot of y N1 &lt;- length(Id1); Lambda1 &lt;- N1/n Id2 &lt;- which(y&gt;=1) N2 &lt;- length(Id2); Lambda2 &lt;- N2/n Reg1 &lt;- lm(y ~ x, subset = Id1) SumReg1 &lt;- summary(Reg1); Beta1 &lt;- Reg1$coefficients sig21 &lt;- SumReg1$sigma^2 Reg2 &lt;- lm(y ~ x, subset = Id2); SumReg2 &lt;- summary(Reg2) Beta2 &lt;- Reg2$coefficients sig22 &lt;- SumReg2$sigma^2 X &lt;- cbind(1, x); Psi &lt;- rep(NA, n) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:n){ lambdai1 &lt;- Lambda1*dnorm(y[i], X[i,]%*%Beta1, sig21^0.5) lambdai2 &lt;- Lambda2*dnorm(y[i], X[i,]%*%Beta2, sig22^0.5) Psi[i] &lt;- sample(c(1,2), 1, prob = c(lambdai1, lambdai2)) } PostPsi[s, ] &lt;- Psi Id1 &lt;- which(Psi == 1); Id2 &lt;- which(Psi == 2) N1 &lt;- length(Id1); N2 &lt;- length(Id2) sig21 &lt;- PostSig2(Betah = Beta1, Xh = X[Id1, ], yh = y[Id1]) sig22 &lt;- PostSig2(Betah = Beta2, Xh = X[Id2, ], yh = y[Id2]) PostSigma21[s] &lt;- sig21; PostSigma22[s] &lt;- sig22 Beta1 &lt;- PostBeta(sig2h = sig21, Xh = X[Id1, ], yh = y[Id1]) Beta2 &lt;- PostBeta(sig2h = sig22, Xh = X[Id2, ], yh = y[Id2]) PostBetas1[s,] &lt;- Beta1; PostBetas2[s,] &lt;- Beta2 Lambda &lt;- sort(MCMCpack::rdirichlet(1, c(a01 + N1, a02 + N2)), decreasing = TRUE) Lambda1 &lt;- Lambda[1]; Lambda2 &lt;- Lambda[2] PostLambda[s] &lt;- Lambda1 setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq((burnin+1), tot, thin) PosteriorBetas1 &lt;- coda::mcmc(PostBetas1[keep,]) summary(PosteriorBetas1) plot(PosteriorBetas1) PosteriorBetas2 &lt;- coda::mcmc(PostBetas2[keep,]) summary(PosteriorBetas2) plot(PosteriorBetas2) PosteriorSigma21 &lt;- coda::mcmc(PostSigma21[keep]) summary(PosteriorSigma21) plot(PosteriorSigma21) PosteriorSigma22 &lt;- coda::mcmc(PostSigma22[keep]) summary(PosteriorSigma22) plot(PosteriorSigma22) Let’s perform another simulation exercise in which we conduct a semi-parametric analysis where the stochastic error follows a Student’s t-distribution with 3 degrees of freedom. Specifically, \\[\\begin{align*} y_i &amp;= 1 - 0.5x_{i1} + 1.5x_{i2} + \\mu_i, \\ i=1,2,\\dots,500. \\end{align*}\\] The variables \\(x_{i1}\\) and \\(x_{i2}\\) are standard normally distributed. Let’s set \\(H=5\\), and use non-informative priors setting \\(\\alpha_{h0}=\\delta_{h0}=0.01\\), \\(\\boldsymbol{\\beta}_0=\\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_0=\\boldsymbol{I}_2\\), \\(\\mu_{h0}=0\\), \\(\\sigma^2_{\\mu 0}=10\\) and \\(\\boldsymbol{\\alpha}_0=[1/H \\ \\dots \\ 1/H]^{\\top}\\). Use 6,000 MCMC iterations, burn-in equal to 4,000, and thinning parameter equal to 2. In this exercise, there is no need to address the label-switching issue, as we are not specifically interested in the individual components of the posterior distributions of the clusters. Exercise 1 asks how to get the posterior density of the stochastic errors in semi-parametric specifications. We can see from the posterior estimates that three components disappear after the burn-in iterations. The 95% credible intervals encompass the population values of the slope parameters. The 95% credible intervals for the probabilities are (0.70, 0.89) and (0.11, 0.30), and the 95% credible interval for the weighted average of the intercepts encompasses the population parameter. rm(list = ls()); set.seed(010101) library(ggplot2) # Simulate data from a 2-component mixture model n &lt;- 500 x1 &lt;- rnorm(n); x2 &lt;- rnorm(n) X &lt;- cbind(x1,x2); B &lt;- c(-0.5, 1.5) u &lt;- rt(n, 3); y &lt;- 1 + X%*%B + u Reg &lt;- lm(y ~ X) Res &lt;- Reg$residuals data &lt;- data.frame(Res) # Plot ggplot(data, aes(x = Res)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + # Density plot with fill color labs(title = &quot;Density Plot&quot;, x = &quot;Residuals&quot;, y = &quot;Density&quot;) + theme_minimal() # Hyperparameters d0 &lt;- 0.001; a0 &lt;- 0.001; b0 &lt;- rep(0, 2) B0 &lt;- diag(2); B0i &lt;- solve(B0) mu0 &lt;- 0; sig2mu0 &lt;- 10; H &lt;- 5; a0h &lt;- rep(1/H, H) # MCMC parameters mcmc &lt;- 2000; burnin &lt;- 4000 tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions PostSig2 &lt;- function(Beta, muh, Xh, yh){ Nh &lt;- length(yh); an &lt;- a0 + Nh dn &lt;- d0 + t(yh - muh - Xh%*%Beta)%*%(yh - muh - Xh%*%Beta) sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2, mu, X, y, Psi){ XtX &lt;- matrix(0, 2, 2); Xty &lt;- matrix(0, 2, 1) Hs &lt;- length(mu) for(h in 1:Hs){ idh &lt;- which(Psi == h) if(length(idh) == 1){ Xh &lt;- matrix(X[idh,], 1, 2) XtXh &lt;- sig2[h]^(-1)*t(Xh)%*%Xh yh &lt;- y[idh] Xtyh &lt;- sig2[h]^(-1)*t(Xh)%*%(yh - mu[h]) }else{ Xh &lt;- X[idh,] XtXh &lt;- sig2[h]^(-1)*t(Xh)%*%Xh yh &lt;- y[idh] Xtyh &lt;- sig2[h]^(-1)*t(Xh)%*%(yh - mu[h]) } XtX &lt;- XtX + XtXh; Xty &lt;- Xty + Xtyh } Bn &lt;- solve(B0i + XtX); bn &lt;- Bn%*%(B0i%*%b0 + Xty) Beta &lt;- MASS::mvrnorm(1, bn, Bn) return(Beta) } Postmu &lt;- function(sig2h, Beta, Xh, yh){ Nh &lt;- length(yh) sig2mu &lt;- (1/sig2mu0 + Nh/sig2h)^(-1) mun &lt;- sig2mu*(mu0/sig2mu0 + sum((yh - Xh%*%Beta))/sig2h) mu &lt;- rnorm(1, mun, sig2mu^0.5) return(mu) } PostBetas &lt;- matrix(0, mcmc+burnin, 2) PostPsi &lt;- matrix(0, mcmc+burnin, n) PostSigma2 &lt;- list(); PostMu &lt;- list() PostLambda &lt;- list() Resq &lt;- quantile(Res, c(0.2, 0.4, 0.6, 0.8)) Id1 &lt;- which(Res &lt;= Resq[1]) Id2 &lt;- which(Res &gt; Resq[1] &amp; Res &lt;= Resq[2]) Id3 &lt;- which(Res &gt; Resq[2] &amp; Res &lt;= Resq[3]) Id4 &lt;- which(Res &gt; Resq[3] &amp; Res &lt;= Resq[4]) Id5 &lt;- which(Res &gt; Resq[4]) Nh &lt;- rep(n/H, H); Lambda &lt;- rep(1/H, H) MU &lt;- c(mean(Res[Id1]), mean(Res[Id2]), mean(Res[Id3]), mean(Res[Id4]), mean(Res[Id5])) Sig2 &lt;- c(var(Res[Id1]), var(Res[Id2]), var(Res[Id3]), var(Res[Id4]), var(Res[Id5])) Beta &lt;- Reg$coefficients[2:3] Psi &lt;- rep(NA, n); Hs &lt;- length(MU) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:n){ lambdai &lt;- NULL for(h in 1:Hs){ lambdaih &lt;- Lambda[h]*dnorm(y[i] - X[i,]%*%Beta, MU[h], Sig2[h]^0.5) lambdai &lt;- c(lambdai, lambdaih) } Psi[i] &lt;- sample(1:Hs, 1, prob = lambdai) } PostPsi[s, ] &lt;- Psi Hs &lt;- length(table(Psi)) for(h in 1:Hs){ idh &lt;- which(Psi == h) Sig2[h] &lt;- PostSig2(Beta = Beta, muh = MU[h], Xh = X[idh,], yh = y[idh]) MU[h] &lt;- Postmu(sig2h = Sig2[h], Beta = Beta, Xh = X[idh,], yh = y[idh]) } PostSigma2[[s]] &lt;- Sig2 PostMu[[s]] &lt;- MU Beta &lt;- PostBeta(sig2 = Sig2, mu = MU, X = X, y = y, Psi = Psi) PostBetas[s,] &lt;- Beta Lambda &lt;- sort(MCMCpack::rdirichlet(1, a0h[1:Hs] + table(Psi)), decreasing = TRUE) PostLambda[[s]] &lt;- Lambda setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq(burnin, tot, thin) PosteriorBetas &lt;- coda::mcmc(PostBetas[keep,]) summary(PosteriorBetas) plot(PosteriorBetas) PosteriorPsi &lt;- PostPsi[keep,] Clusters &lt;- sapply(1:length(keep), function(i){length(table(PosteriorPsi[i,]))}) NClus &lt;- 2 PosteriorSIGMA &lt;- matrix(NA, length(keep), NClus) PosteriorMU &lt;- matrix(NA, length(keep), NClus) PosteriorLAMBDA &lt;- matrix(NA, length(keep), NClus) l &lt;- 1 for (s in keep){ PosteriorSIGMA[l,] &lt;- PostSigma2[[s]][1:NClus] PosteriorMU[l,] &lt;- PostMu[[s]][1:NClus] PosteriorLAMBDA[l,] &lt;- PostLambda[[s]][1:NClus] l &lt;- l + 1 } summary(coda::mcmc(PosteriorSIGMA)) summary(coda::mcmc(PosteriorMU)) summary(coda::mcmc(PosteriorLAMBDA)) 11.1.2 Direchlet processes A Dirichlet process (DP) is a probability distribution over probability distributions, and a generalization of the Dirichlet distribution. It was introduced by Ferguson (1973), and it is commonly used as a prior for unknown distributions, making it particularly useful in non-parametric settings. Unlike finite Gaussian mixture models, a Dirichlet process does not require pre-specifying the number of components, allowing for greater flexibility in modeling complex data structures. In this sense, DPs can be viewed as the limiting case of finite mixtures when the number of components approaches infinity. A Dirichlet process \\(G\\sim DP(\\alpha G_0)\\) is defined by a precision parameter \\(\\alpha&gt;0\\), and a base probability measure \\(G_0\\) on a probability space \\(\\Omega\\).1 The DP assigns probability \\(G(B)\\) to any (measurable) set \\(B\\) in \\(\\Omega\\) such that for any finite (measurable) partition \\(\\left\\{B_1, B_2, \\dots, B_k\\right\\}\\) of \\(\\Omega\\), \\[ G(B_1), G(B_2), \\dots, G(B_k) \\sim \\text{Dirichlet}(\\alpha G_0(B_1), \\alpha G_0(B_2), \\dots, \\alpha G_0(B_k)). \\] In particular, \\(\\mathbb{E}\\left[G(B)\\right]=G_0(B)\\) and \\(\\mathbb{V}ar\\left[G(B)\\right]=G_0(B)\\left[1-G_0(B)\\right]/(1+\\alpha)\\) (Müller et al. 2015). Observe that as \\(\\alpha \\rightarrow \\infty\\), \\(G\\) concentrates at \\(G_0\\), which is why \\(\\alpha\\) is called the precision parameter. A key property of the DP is its discrete nature, which allows it to be expressed as \\[ G(\\cdot)=\\sum_{h=1}^{\\infty}\\lambda_h\\delta_{\\boldsymbol{\\theta}_h}(\\cdot), \\] where \\(\\lambda_h\\) is the probability mass at \\(\\boldsymbol{\\theta}_h\\), and \\(\\delta_{\\boldsymbol{\\theta}_h}(\\cdot)\\) denotes the Dirac measure that assigns mass one to the atom \\(\\boldsymbol{\\theta}_h\\).2 Given this property, a particularly useful construction of the DP is the stick-breaking representation (Sethuraman 1994), which is given by \\[\\begin{align*} G(\\cdot)&amp;=\\sum_{h=1}^{\\infty}\\lambda_h\\delta_{\\boldsymbol{\\theta}_h}(\\cdot),\\\\ \\lambda_h&amp;=V_h\\prod_{m&lt;h}(1-V_m), \\quad V_h\\sim \\text{Beta}(1,\\alpha),\\\\ \\boldsymbol{\\theta}_h&amp;\\stackrel{iid}{\\sim} G_0. \\end{align*}\\] The intuition behind this representation is straightforward. We begin with a stick of length 1 and break off a random proportion \\(V_1\\) drawn from a \\(\\text{Beta}(1,\\alpha)\\) distribution. This assigns a probability mass of \\(\\lambda_1=V_1\\) to \\(\\boldsymbol{\\theta}_1\\), which is sampled from \\(G_0\\). Next, from the remaining stick of length \\((1-V_1)\\), we break off a fraction proportional to \\(V_2 \\sim \\text{Beta}(1,\\alpha)\\), assigning a probability mass of \\(\\lambda_2=V_2(1-V_1)\\) to a new draw \\(\\boldsymbol{\\theta}_2\\) from \\(G_0\\). This process continues indefinitely. Since \\(\\mathbb{E}(V_h) = \\frac{1}{1+\\alpha}\\), as \\(\\alpha \\rightarrow \\infty\\), we have \\(\\mathbb{E}(V_h) \\rightarrow 0\\). Consequently, the DP places mass on a large number of atoms, leading to convergence to the base distribution \\(G_0\\). Note that DPs give as realizations discrete distributions, this poses challenges when working with continuous distributions. One way to overcome this limitation is to use the DP as a mixing distribution for simple parametric distributions, such as the normal distribution (Escobar and West 1995). This leads to Dirichlet process mixtures (DPM), which are defined as \\[\\begin{align*} f_G(y) &amp;= \\int f_{\\boldsymbol{\\theta}}(y) dG(\\boldsymbol{\\theta}). \\end{align*}\\] Observe that the mixing measure \\(G\\) is discrete when a DP is the prior, with mass concentrated at an infinite number of atoms \\(\\boldsymbol{\\theta}_h\\). Consequently, if \\(f_{\\boldsymbol{\\theta}}(y)\\) follows a Gaussian distribution, the resulting mixture resembles a finite Gaussian mixture. However, unlike finite mixtures, the DP-based approach eliminates the need to pre-specify the number of components, as it provides an automatic mechanism for determining them. Thus, if we assume \\(y\\sim N(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta},\\sigma^2)\\), then the DPM is \\[ p(y_i \\mid \\{\\lambda_h, \\boldsymbol{\\beta}_h, \\sigma_h^2\\}_{h=1}^{\\infty}, \\boldsymbol{x}_i) = \\sum_{h=1}^{\\infty} \\lambda_h \\phi(y_i \\mid \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h, \\sigma_h^2), \\] where \\(\\lambda_h\\) are drawn from the stick-breaking representation of the DP, and \\(\\boldsymbol{\\theta}_h=[\\boldsymbol{\\beta}_h^{\\top} \\ \\sigma^2_h]^{\\top}\\). This mixture model can be expressed in a hierarchical structure: \\[\\begin{align*} y_i\\mid \\boldsymbol{\\theta}_i &amp; \\stackrel{ind}{\\sim}f_{\\boldsymbol{\\theta}_i}\\\\ \\boldsymbol{\\theta}_i \\mid G &amp; \\stackrel{iid}{\\sim} G\\\\ G \\mid \\alpha,G_0 &amp; \\sim DP(\\alpha G_0). \\end{align*}\\] Note that the hierarchical representation induces specific unit parameters, leading to a probabilistic clustering model (Antoniak 1974), similar to a finite Gaussian mixture. However, the DPM is not consistent in estimating the number of clusters, it tends to overestimate the number of clusters (see simulation exercise below); although there is posterior asymptotic concentration in the other model components (Miller and Harrison 2014). The hierarchical representation implies that there are latent assignment variables \\(s_i = h\\), such that when \\(\\boldsymbol{\\theta}_i\\) is equal to the \\(h\\)-th unique \\(\\boldsymbol{\\theta}_h^*\\) — that is, \\(\\boldsymbol{\\theta}_i = \\boldsymbol{\\theta}_h^*\\)— then \\(s_i = h\\). Then, \\[\\begin{align*} s_i&amp;\\sim \\sum_{h=0}^{\\infty}\\lambda_h\\delta_h,\\\\ y_i\\mid s_i, \\boldsymbol{\\theta}_{s_i}&amp;\\sim N(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_{s_i},\\sigma^2_{s_i}), \\end{align*}\\] where \\(\\lambda_h=P(\\boldsymbol{\\theta}_{i}=\\boldsymbol{\\theta}_{h}^*)\\). This latent assignment structure, the Pólya urn representation of the DP (Blackwell and MacQueen 1973), which is obtained when \\(G\\) is marginalized out to avoid infinite atoms, and the use of conjugate priors allow for convenient computational inference in DPM. Specifically, we assume \\(\\boldsymbol{\\beta}_i\\mid \\sigma^2_i\\sim N(\\boldsymbol{\\beta}_0, \\sigma^2_i\\boldsymbol{B}_0)\\) and \\(\\sigma_i^2\\sim IG(\\alpha_0/2,\\delta_0/2)\\). In addition, we can add a layer in the hierarchical representation, \\(\\alpha\\sim G(a,b)\\) such that introducing the latent variable \\(\\xi|\\alpha,N\\sim Be(\\alpha+1,N)\\), allows to easily sample the posterior draws of \\(\\alpha|\\xi,H,\\pi_{\\xi}\\sim\\pi_{\\xi}{G}(a+H,b-log(\\xi))+(1-\\pi_{\\xi}){G}(a+H-1,b-log(\\xi))\\), where \\(\\frac{\\pi_{\\xi}}{1-\\pi_{\\xi}}=\\frac{a+H-1}{N(b-log(\\xi))}\\), \\(H\\) is the number of atoms (mixture components) (Escobar and West 1995). The conditional posterior distribution of \\(\\boldsymbol\\theta_i\\) is \\[\\begin{align*} \\boldsymbol\\theta_i|\\left\\{\\boldsymbol\\theta_{i&#39;},\\boldsymbol s_{i&#39;}:i&#39;\\neq i\\right\\}, y_i, \\alpha &amp; \\sim \\sum_{i&#39;\\neq i}\\frac{N_h^{(i)}}{\\alpha+N-1}f_N(y_i|\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}_h,\\sigma_h^2)\\\\ &amp; +\\frac{\\alpha}{\\alpha+N-1}\\int_{\\mathcal{R}^K}\\int_{0}^{\\infty}f_N(y_i|\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta},\\sigma^2)f_N\\left(\\boldsymbol\\beta\\Big|\\boldsymbol\\beta_0,\\sigma^2\\boldsymbol B_0\\right)f_{IG}(\\sigma^2|\\alpha_0,\\delta_0)d\\sigma^2 d\\boldsymbol\\beta, \\end{align*}\\] where \\(N_h^{(i)}\\) is the number of observations such that \\(s_{i&#39;}=h\\), \\(i&#39;\\neq i\\). Observe that the probability of belonging to a particular cluster has a reinforcement property, as it increases with the cluster size; therefore, a DPM exhibits a self-reinforcing property, the more often a given value has been sampled in the past, the more likely it is to be sampled again. Observe that the integral in the previous equation has exactly the same form as in the marginal likelihood presented in Section 3.3. Thus, \\[\\begin{align*} p(y_i)&amp;=\\int_{\\mathcal{R}^K}\\int_{0}^{\\infty}f_N(y_i|\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta},\\sigma^2)f_N\\left(\\boldsymbol\\beta\\Big|\\boldsymbol\\beta_0,\\sigma^2\\boldsymbol B_0\\right)f_{IG}(\\sigma^2|\\alpha_0,\\delta_0)d\\sigma^2 d\\boldsymbol\\beta\\\\ &amp;=\\frac{1}{\\pi^{1/2}}\\frac{\\delta_0^{\\alpha_0/2}}{\\delta_n^{\\alpha_n/2}}\\frac{|{\\boldsymbol{B}}_n|^{1/2}}{|{\\boldsymbol{B}}_0|^{1/2}}\\frac{\\Gamma(\\alpha_n/2)}{\\Gamma(\\alpha_0/2)}, \\end{align*}\\] where \\(\\alpha_n=1+\\alpha_0\\), \\(\\delta_n=\\delta_0 + y^{\\top}y + \\boldsymbol{\\beta}_0^{\\top}{\\boldsymbol{B}}_0^{-1}\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_n^{\\top}{\\boldsymbol{B}}_n^{-1}\\boldsymbol{\\beta}_n\\), \\(\\boldsymbol{B}_n = (\\boldsymbol{B}_0^{-1} + \\boldsymbol{x}\\boldsymbol{x}^{\\top})^{-1}\\) and \\(\\boldsymbol{\\beta}_n = \\boldsymbol{B}_n(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0 + \\boldsymbol{x}y)\\). Therefore, we sample \\(s_i\\) as follows, \\[\\begin{equation*} s_i|\\left\\{\\boldsymbol\\beta_{i&#39;},\\sigma_{i&#39;}^2,\\boldsymbol s_{i&#39;}:i&#39;\\neq i\\right\\}, y_i, \\alpha\\sim\\begin{Bmatrix}P(s_i=0|\\cdot)=q_0^*\\\\ P(s_i=h|\\cdot)=q_h^*, h=1,2,\\dots,H^{(i)}\\end{Bmatrix}, \\end{equation*}\\] where \\(H^{(i)}\\) is the number of clusters excluding \\(i\\), which may have its own cluster (singleton cluster), \\(q^*_c=\\frac{q_c}{q_0+\\sum_h q_h}\\), \\(q_c=\\left\\{q_0,q_h\\right\\}\\), \\(q_h=\\frac{N_h^{(i)}}{\\alpha+N-1}f_N(y_i|\\boldsymbol{x}_i^{\\top}\\boldsymbol\\beta_h,\\sigma_h^2)\\) and \\(q_0=\\frac{\\alpha}{\\alpha+N-1}p(y_i)\\). If \\(s_i=0\\) is sampled, then \\(s_i=H+1\\), and a new \\(\\sigma_h^2\\) is sampled from \\(IG\\left(\\alpha_n/2,\\delta_n/2\\right)\\), a new \\(\\boldsymbol\\beta_h\\) is sample from \\(N(\\boldsymbol\\beta_n,\\sigma_h^2\\boldsymbol B_n)\\). Discarding \\(\\boldsymbol\\theta_h\\)’s from last step, we use \\(\\boldsymbol s\\) and the total number of components to sample \\(\\sigma_h^2\\) from \\[\\begin{equation*} IG\\left(\\frac{\\alpha_0+N_m}{2},\\frac{\\delta_0+\\boldsymbol y_h^{\\top}\\boldsymbol y_h+\\boldsymbol{\\beta}_0^{\\top}{\\boldsymbol{B}}_0^{-1}\\boldsymbol{\\beta}_0-\\boldsymbol{\\beta}_{hn}^{\\top}{\\boldsymbol{B}}_{hn}^{-1}\\boldsymbol{\\beta}_{hn}}{2}\\right), \\end{equation*}\\] where \\(\\boldsymbol{B}_{hn}=(\\boldsymbol{B}_0^{-1}+\\boldsymbol{X}_h^{\\top}\\boldsymbol{X}_h)^{-1}\\) and \\(\\boldsymbol{\\beta}_{hn}=\\boldsymbol{B}_{hn}(\\boldsymbol{B}_0^{-1}\\boldsymbol{\\beta}_0+\\boldsymbol{X}_h^{\\top}\\boldsymbol{y}_h)\\), \\(\\boldsymbol{X}_h\\) and \\(\\boldsymbol{y}_h\\) have the \\(\\boldsymbol{x}_i\\) and \\(y_i\\) of individuals in component \\(h\\). We sample \\(\\boldsymbol\\beta_h\\) from \\[\\begin{equation*} N\\left({\\boldsymbol\\beta}_{hn},\\sigma_h^2\\boldsymbol B_{hn}\\right), \\end{equation*}\\] \\(h=1,2,\\dots,H\\). We can also use DPMs in a semi-parametric setting, as we did in the finite Gaussian mixtures case. In Exercise 3, we ask to get the posterior sampler in this setting, that is, \\[\\begin{align*} y_i&amp;=\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}+e_i\\\\ e_i\\mid \\mu_i,\\sigma_i^2 &amp;\\stackrel{iid}{\\sim} N(\\mu_i,\\sigma_i^2). \\end{align*}\\] We should not include the intercept in \\(\\boldsymbol{\\beta}\\), such that \\(\\mu_i\\) allows to get flexibility in the distribution of the stochastic errors. Note that mixture models can be easily extended to non-linear models, where posterior inference is based on data augmentation, such as the probit and tobit models in Chapter 6. The basic idea is to incorporate the mixture (finite or infinite) into the specification of the latent variable implicit in the data-generating process. For instance, Basu and Chib (2003) perform inference in the probit model using DPMs. Furthermore, mixtures can also be used in the multivariate models presented in Chapter 7 and the hierarchical models presented in Chapter 9. Ramı́rez–Hassan and López-Vera (2024) perform semi-parametric inference in the exact affine demand system (Lewbel and Pendakur 2009) using DPMs, while Basu and Chib (2003) apply them in hierarchical models. To sum up, a Dirichlet process mixture is a flexible way to model non-parametrically a distribution using an infinite weighted sum of discrete distributions, where each individual weight increases with the number of observations that belongs to it. Example: Simulation exercises Let’s simulate again the simple regression mixture with two components such that \\(\\psi_{i1}\\sim \\text{Ber}(0.5)\\), consequently, \\(\\psi_{i2}=1-\\psi_{i1}\\), and assume one regressor, \\(x_i\\sim N(0,1)\\), \\(i=1,2,\\dots,1,000\\). Then, \\[p(y_i \\mid \\boldsymbol{x}_i) = 0.5 \\phi(y_i \\mid 2+1.5x_i,1^2)+0.5 \\phi(y_i \\mid -1+0.5x_i,0.8^2).\\] We use non-informative priors again, setting \\(\\alpha_{0}=\\delta_{0}=0.01\\), \\(\\boldsymbol{\\beta}_{0}=\\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_{0}=\\boldsymbol{I}_2\\), and \\(a=b=0.1\\). The number of MCMC iterations is 5,000, the burn-in is 1,000, and the thinning parameter is 2. However, we do not have to fix in advance the number of clusters using the DPM. The following code demonstrates how to perform inference using the stated Gibbs sampler for the DPM in the R package. We see from the results that the DPM overestimates the number of clusters. After the burn-in period, there are typically three clusters. Additionally, the posterior plots illustrate the label-switching problem. Eventually, the posterior chains of clusters reach different posterior modes, causing a high variability of the posterior draws. In Exercise 5, we ask to fix this issue using random permutation of latent classes. rm(list = ls()) set.seed(010101) # Simulate data from a 2-component mixture model N &lt;- 1000; x &lt;- rnorm(N); z &lt;- rbinom(N, 1, 0.5) y &lt;- ifelse(z == 0, rnorm(N, 2 + 1.5*x, 1), rnorm(N, -1 + 0.5*x, 0.8)) X &lt;- cbind(1, x) k &lt;- 2 data &lt;- data.frame(y, x); Reg &lt;- lm(y ~ x) SumReg &lt;- summary(Reg) # Hyperparameters a0 &lt;- 0.001; d0 &lt;- 0.001 b0 &lt;- rep(0, k); B0 &lt;- diag(k) B0i &lt;- solve(B0) a &lt;- 0.1; b &lt;- 0.1 # MCMC parameters mcmc &lt;- 5000; burnin &lt;- 1000 tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions PostSig2 &lt;- function(Xh, yh){ Nh &lt;- length(yh) yh &lt;- matrix(yh, Nh, 1) if(Nh == 1){ Xh &lt;- matrix(Xh, k, 1) Bn &lt;- solve(Xh%*%t(Xh) + B0i) bn &lt;- Bn%*%(B0i%*%b0 + Xh%*%yh) }else{ Xh &lt;- matrix(Xh, Nh, k) Bn &lt;- solve(t(Xh)%*%Xh + B0i) bn &lt;- Bn%*%(B0i%*%b0 + t(Xh)%*%yh) } Bni &lt;- solve(Bn) an &lt;- a0 + Nh dn &lt;- d0 + t(yh)%*%yh + t(b0)%*%B0i%*%b0 - t(bn)%*%Bni%*%bn sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2h, Xh, yh){ Nh &lt;- length(yh) yh &lt;- matrix(yh, Nh, 1) if(Nh == 1){ Xh &lt;- matrix(Xh, k, 1) Bn &lt;- solve(Xh%*%t(Xh) + B0i) bn &lt;- Bn%*%(B0i%*%b0 + Xh%*%yh) }else{ Xh &lt;- matrix(Xh, Nh, k) Bn &lt;- solve(t(Xh)%*%Xh + B0i) bn &lt;- Bn%*%(B0i%*%b0 + t(Xh)%*%yh) } Beta &lt;- MASS::mvrnorm(1, bn, sig2h*Bn) return(Beta) } PostAlpha &lt;- function(s, alpha){ H &lt;- length(unique(s)) psi &lt;- rbeta(1, alpha + 1, N) pi.ratio &lt;- (a + H - 1) / (N * (b - log(psi))) pi &lt;- pi.ratio / (1 + pi.ratio) components &lt;- sample(1:2, prob = c(pi, (1 - pi)), size = 1) cs &lt;- c(a + H, a + H - 1) ds &lt;- b - log(psi) alpha &lt;- rgamma(1, cs[components], ds) return(alpha) } LogMarLikLM &lt;- function(xh, yh){ xh &lt;- matrix(xh, k, 1) Bn &lt;- solve(xh%*%t(xh) + B0i) Bni &lt;- solve(Bn) bn &lt;- Bn%*%(B0i%*%b0 + xh%*%yh) an &lt;- a0 + 1 dn &lt;- d0 + yh^2 + t(b0)%*%B0i%*%b0 - t(bn)%*%Bni%*%bn # Log marginal likelihood logpy &lt;- (1/2)*log(1/pi)+(a0/2)*log(d0)-(an/2)*log(dn) + 0.5*log(det(Bn)/det(B0)) + lgamma(an/2)-lgamma(a0/2) return(logpy) } PostS &lt;- function(BETA, SIGMA, Alpha, s, i){ Nl &lt;- table(s[-i]); H &lt;- length(Nl) qh &lt;- sapply(1:H, function(h){(Nl[h]/(N+Alpha-1))*dnorm(y[i], mean = t(X[i,])%*%BETA[,h], sd = SIGMA[h])}) q0 &lt;- (Alpha/(N+Alpha-1))*exp(LogMarLikLM(xh = X[i,], yh = y[i])) qh &lt;- c(q0, qh) Clust &lt;- as.numeric(names(Nl)) si &lt;- sample(c(0, Clust), 1, prob = qh) if(si == 0){ si &lt;- Clust[H] + 1 Sig2New &lt;- PostSig2(Xh = X[i,], yh = y[i]) SIGMA &lt;- c(SIGMA, Sig2New^0.5) BetaNew &lt;- PostBeta(sig2h = Sig2New, Xh = X[i,], yh = y[i]) BETA &lt;- cbind(BETA, BetaNew) }else{si == si } return(list(si = si, BETA = BETA, SIGMA = SIGMA)) } PostBetas &lt;- list(); PostSigma &lt;- list() Posts &lt;- matrix(0, tot, N); PostAlphas &lt;- rep(0, tot) S &lt;- sample(1:3, N, replace = T, prob = c(0.5, 0.3, 0.2)) BETA &lt;- cbind(Reg$coefficients, Reg$coefficients, Reg$coefficients) SIGMA &lt;- rep(SumReg$sigma, 3) Alpha &lt;- rgamma(1, a, b) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:N){ Rests &lt;- PostS(BETA = BETA, SIGMA = SIGMA, Alpha = Alpha, s = S, i = i) S[i] &lt;- Rests$si BETA &lt;- Rests$BETA; SIGMA &lt;- Rests$SIGMA } sFreq &lt;- table(S) lt &lt;- 1 for(li in as.numeric(names(sFreq))){ Index &lt;- which(S == li) if(li == lt){S[Index] &lt;- li } else {S[Index] &lt;- lt } lt &lt;- lt + 1 } Alpha &lt;- PostAlpha(s = S, alpha = Alpha) Nl &lt;- table(S); H &lt;- length(Nl) SIGMA &lt;- rep(NA, H) BETA &lt;- matrix(NA, k, H) l &lt;- 1 for(h in unique(S)){ Idh &lt;- which(S == h) SIGMA[l] &lt;- (PostSig2(Xh = X[Idh, ], yh = y[Idh]))^0.5 BETA[,l] &lt;- PostBeta(sig2h = SIGMA[l]^2, Xh = X[Idh, ], yh = y[Idh]) l &lt;- l + 1 } PostBetas[[s]] &lt;- BETA PostSigma[[s]] &lt;- SIGMA Posts[s, ] &lt;- S PostAlphas[s] &lt;- Alpha setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) keep &lt;- seq((burnin+1), tot, thin) PosteriorS&lt;- Posts[keep,] Clusters &lt;- sapply(1:length(keep), function(i){length(table(PosteriorS[i,]))}) table(Clusters) PosteriorBeta1 &lt;- matrix(NA, length(keep), k); j &lt;- 1 for(s in keep){ PosteriorBeta1[j,] &lt;- PostBetas[[s]][,1]; j &lt;- j + 1 } print(summary(coda::mcmc(PosteriorBeta1))) PosteriorBeta2 &lt;- matrix(NA, length(keep), k);j &lt;- 1 for(s in keep){ PosteriorBeta2[j,] &lt;- PostBetas[[s]][,2]; j &lt;- j + 1 } print(summary(coda::mcmc(PosteriorBeta2))) PosteriorBeta3 &lt;- matrix(NA, length(keep), k); j &lt;- 1 for(s in keep){ PosteriorBeta3[j,] &lt;- PostBetas[[s]][,3]; j &lt;- j + 1 } print(summary(coda::mcmc(PosteriorBeta3))) Example: Consumption of marijuana in Colombia Let’s use the dataset MarijuanaColombia.csv from our GitHub repository to perform inference on the demand for marijuana in Colombia. This dataset contains information on the (log) monthly demand in 2019 from the National Survey of the Consumption of Psychoactive Substances. It includes variables such as the presence of a drug dealer in the neighborhood (Dealer), gender (Female), indicators of good physical and mental health (PhysicalHealthGood and MentalHealthGood), age (Age and Age2), years of schooling (YearsEducation), and (log) prices of marijuana, cocaine, and crack by individual (LogPriceMarijuana, LogPriceCocaine, and LogPriceCrack). The sample size is 1,156. We are interested in the own-price and cross-price elasticities of marijuana demand. However, there is a potential endogeneity issue between price and demand due to strategic provider search (omission of a relevant regressor) or measurement errors in self-reported prices. Endogeneity should be properly addressed for rigorous scientific analysis. This example is purely illustrative. In Exercise 2 we ask to perform inference using a finite Gaussian mixture regression starting with five potential clusters. Here we perform inference using a Dirichlet process mixture. The following code shows how to perform this analysis using non-informative priors, setting \\(\\alpha_{0}=\\delta_{0}=0.01\\), \\(\\boldsymbol{\\beta}_{0}=\\boldsymbol{0}_K\\), \\(\\boldsymbol{B}_{0}=\\boldsymbol{I}_K\\), and \\(a=b=0.1\\), \\(K\\) is the number of regressors, 11 including the intercept. The number of MCMC iterations is 5,000, the burn-in is 1,000, and the thinning parameter is 2. rm(list = ls()); set.seed(010101) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/MarijuanaColombia.csv&quot;) attach(Data) y &lt;- LogMarijuana; X &lt;- as.matrix(cbind(1, Data[,-1])) Reg &lt;- lm(y ~ X - 1); SumReg &lt;- summary(Reg) k &lt;- dim(X)[2] N &lt;- dim(X)[1] library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3 ggplot(Data, aes(x = LogMarijuana)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(title = &quot;Density Plot: Marijuana (log) monthly consumption in Colombia&quot;, x = &quot;y&quot;, y = &quot;Density&quot;) + theme_minimal() a0 &lt;- 0.001; d0 &lt;- 0.001 b0 &lt;- rep(0, k); B0 &lt;- diag(k) B0i &lt;- solve(B0) a &lt;- 0.1; b &lt;- 0.1 # MCMC parameters mcmc &lt;- 5000; burnin &lt;- 1000 tot &lt;- mcmc + burnin; thin &lt;- 2 # Gibbs sampling functions PostSig2 &lt;- function(Xh, yh){ Nh &lt;- length(yh) yh &lt;- matrix(yh, Nh, 1) if(Nh == 1){ Xh &lt;- matrix(Xh, k, 1) Bn &lt;- solve(Xh%*%t(Xh) + B0i) bn &lt;- Bn%*%(B0i%*%b0 + Xh%*%yh) }else{ Xh &lt;- matrix(Xh, Nh, k) Bn &lt;- solve(t(Xh)%*%Xh + B0i) bn &lt;- Bn%*%(B0i%*%b0 + t(Xh)%*%yh) } Bni &lt;- solve(Bn) an &lt;- a0 + Nh dn &lt;- d0 + t(yh)%*%yh + t(b0)%*%B0i%*%b0 - t(bn)%*%Bni%*%bn sig2 &lt;- invgamma::rinvgamma(1, shape = an/2, rate = dn/2) return(sig2) } PostBeta &lt;- function(sig2h, Xh, yh){ Nh &lt;- length(yh) yh &lt;- matrix(yh, Nh, 1) if(Nh == 1){ Xh &lt;- matrix(Xh, k, 1) Bn &lt;- solve(Xh%*%t(Xh) + B0i) bn &lt;- Bn%*%(B0i%*%b0 + Xh%*%yh) }else{ Xh &lt;- matrix(Xh, Nh, k) Bn &lt;- solve(t(Xh)%*%Xh + B0i) bn &lt;- Bn%*%(B0i%*%b0 + t(Xh)%*%yh) } Beta &lt;- MASS::mvrnorm(1, bn, sig2h*Bn) return(Beta) } PostAlpha &lt;- function(s, alpha){ H &lt;- length(unique(s)) psi &lt;- rbeta(1, alpha + 1, N) pi.ratio &lt;- (a + H - 1) / (N * (b - log(psi))) pi &lt;- pi.ratio / (1 + pi.ratio) components &lt;- sample(1:2, prob = c(pi, (1 - pi)), size = 1) cs &lt;- c(a + H, a + H - 1) ds &lt;- b - log(psi) alpha &lt;- rgamma(1, cs[components], ds) return(alpha) } LogMarLikLM &lt;- function(xh, yh){ xh &lt;- matrix(xh, k, 1) Bn &lt;- solve(xh%*%t(xh) + B0i) Bni &lt;- solve(Bn) bn &lt;- Bn%*%(B0i%*%b0 + xh%*%yh) an &lt;- a0 + 1 dn &lt;- d0 + yh^2 + t(b0)%*%B0i%*%b0 - t(bn)%*%Bni%*%bn logpy &lt;- (1/2)*log(1/pi)+(a0/2)*log(d0)-(an/2)*log(dn) + 0.5*log(det(Bn)/det(B0)) + lgamma(an/2)-lgamma(a0/2) return(logpy) } PostS &lt;- function(BETA, SIGMA, Alpha, s, i){ Nl &lt;- table(s[-i]); H &lt;- length(Nl) qh &lt;- sapply(1:H, function(h){(Nl[h]/(N+Alpha-1))*dnorm(y[i], mean = t(X[i,])%*%BETA[,h], sd = SIGMA[h])}) q0 &lt;- (Alpha/(N+Alpha-1))*exp(LogMarLikLM(xh = X[i,], yh = y[i])) qh &lt;- c(q0, qh) Clust &lt;- as.numeric(names(Nl)) si &lt;- sample(c(0, Clust), 1, prob = qh) if(si == 0){ si &lt;- Clust[H] + 1 Sig2New &lt;- PostSig2(Xh = X[i,], yh = y[i]) SIGMA &lt;- c(SIGMA, Sig2New^0.5) BetaNew &lt;- PostBeta(sig2h = Sig2New, Xh = X[i,], yh = y[i]) BETA &lt;- cbind(BETA, BetaNew) }else{si == si } return(list(si = si, BETA = BETA, SIGMA = SIGMA)) } PostBetas &lt;- list(); PostSigma &lt;- list() Posts &lt;- matrix(0, tot, N); PostAlphas &lt;- rep(0, tot) S &lt;- sample(1:3, N, replace = T, prob = c(0.5, 0.3, 0.2)) BETA &lt;- cbind(Reg$coefficients, Reg$coefficients, Reg$coefficients) SIGMA &lt;- rep(SumReg$sigma, 3) Alpha &lt;- rgamma(1, a, b) pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, max = tot, width = 300) for(s in 1:tot){ for(i in 1:N){ Rests &lt;- PostS(BETA = BETA, SIGMA = SIGMA, Alpha = Alpha, s = S, i = i) S[i] &lt;- Rests$si BETA &lt;- Rests$BETA; SIGMA &lt;- Rests$SIGMA } sFreq &lt;- table(S) lt &lt;- 1 for(li in as.numeric(names(sFreq))){ Index &lt;- which(S == li) if(li == lt){S[Index] &lt;- li } else {S[Index] &lt;- lt } lt &lt;- lt + 1 } Alpha &lt;- PostAlpha(s = S, alpha = Alpha) Nl &lt;- table(S); H &lt;- length(Nl) SIGMA &lt;- rep(NA, H) BETA &lt;- matrix(NA, k, H) l &lt;- 1 for(h in unique(S)){ Idh &lt;- which(S == h) SIGMA[l] &lt;- (PostSig2(Xh = X[Idh, ], yh = y[Idh]))^0.5 BETA[,l] &lt;- PostBeta(sig2h = SIGMA[l]^2, Xh = X[Idh, ], yh = y[Idh]) l &lt;- l + 1 } PostBetas[[s]] &lt;- BETA PostSigma[[s]] &lt;- SIGMA Posts[s, ] &lt;- S PostAlphas[s] &lt;- Alpha setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),&quot;% done&quot;)) } close(pb) ## NULL keep &lt;- seq((burnin+1), tot, thin) PosteriorS&lt;- Posts[keep,] Clusters &lt;- sapply(1:length(keep), function(i){length(table(PosteriorS[i,]))}) table(Clusters) ## Clusters ## 3 ## 2500 NClus &lt;- 3 sapply(1:length(keep), function(i){print(table(PosteriorS[i,]))}) ## ## 1 2 3 ## 479 330 347 ## ## 1 2 3 ## 358 390 408 ## ## 1 2 3 ## 368 399 389 ## ## 1 2 3 ## 349 428 379 ## ## 1 2 3 ## 380 370 406 ## ## 1 2 3 ## 350 365 441 ## ## 1 2 3 ## 366 395 395 ## ## 1 2 3 ## 377 380 399 ## ## 1 2 3 ## 359 353 444 ## ## 1 2 3 ## 363 360 433 ## ## 1 2 3 ## 323 358 475 ## ## 1 2 3 ## 354 406 396 ## ## 1 2 3 ## 411 378 367 ## ## 1 2 3 ## 362 459 335 ## ## 1 2 3 ## 383 408 365 ## ## 1 2 3 ## 375 402 379 ## ## 1 2 3 ## 374 354 428 ## ## 1 2 3 ## 394 356 406 ## ## 1 2 3 ## 342 351 463 ## ## 1 2 3 ## 344 407 405 ## ## 1 2 3 ## 360 457 339 ## ## 1 2 3 ## 320 508 328 ## ## 1 2 3 ## 394 367 395 ## ## 1 2 3 ## 382 417 357 ## ## 1 2 3 ## 404 388 364 ## ## 1 2 3 ## 397 378 381 ## ## 1 2 3 ## 381 387 388 ## ## 1 2 3 ## 362 398 396 ## ## 1 2 3 ## 387 360 409 ## ## 1 2 3 ## 447 345 364 ## ## 1 2 3 ## 375 375 406 ## ## 1 2 3 ## 419 346 391 ## ## 1 2 3 ## 400 351 405 ## ## 1 2 3 ## 419 358 379 ## ## 1 2 3 ## 402 432 322 ## ## 1 2 3 ## 396 407 353 ## ## 1 2 3 ## 360 355 441 ## ## 1 2 3 ## 362 362 432 ## ## 1 2 3 ## 365 370 421 ## ## 1 2 3 ## 378 405 373 ## ## 1 2 3 ## 383 375 398 ## ## 1 2 3 ## 322 367 467 ## ## 1 2 3 ## 383 402 371 ## ## 1 2 3 ## 373 393 390 ## ## 1 2 3 ## 394 364 398 ## ## 1 2 3 ## 439 352 365 ## ## 1 2 3 ## 380 404 372 ## ## 1 2 3 ## 397 362 397 ## ## 1 2 3 ## 326 360 470 ## ## 1 2 3 ## 401 375 380 ## ## 1 2 3 ## 373 419 364 ## ## 1 2 3 ## 375 416 365 ## ## 1 2 3 ## 370 422 364 ## ## 1 2 3 ## 376 416 364 ## ## 1 2 3 ## 392 381 383 ## ## 1 2 3 ## 416 371 369 ## ## 1 2 3 ## 398 379 379 ## ## 1 2 3 ## 360 397 399 ## ## 1 2 3 ## 406 360 390 ## ## 1 2 3 ## 347 375 434 ## ## 1 2 3 ## 418 390 348 ## ## 1 2 3 ## 396 358 402 ## ## 1 2 3 ## 385 382 389 ## ## 1 2 3 ## 424 336 396 ## ## 1 2 3 ## 341 439 376 ## ## 1 2 3 ## 370 360 426 ## ## 1 2 3 ## 389 366 401 ## ## 1 2 3 ## 413 354 389 ## ## 1 2 3 ## 389 386 381 ## ## 1 2 3 ## 387 395 374 ## ## 1 2 3 ## 366 351 439 ## ## 1 2 3 ## 357 410 389 ## ## 1 2 3 ## 405 371 380 ## ## 1 2 3 ## 367 423 366 ## ## 1 2 3 ## 399 378 379 ## ## 1 2 3 ## 384 387 385 ## ## 1 2 3 ## 378 396 382 ## ## 1 2 3 ## 388 400 368 ## ## 1 2 3 ## 361 431 364 ## ## 1 2 3 ## 323 436 397 ## ## 1 2 3 ## 422 323 411 ## ## 1 2 3 ## 394 333 429 ## ## 1 2 3 ## 395 413 348 ## ## 1 2 3 ## 426 386 344 ## ## 1 2 3 ## 410 411 335 ## ## 1 2 3 ## 402 394 360 ## ## 1 2 3 ## 440 363 353 ## ## 1 2 3 ## 428 372 356 ## ## 1 2 3 ## 347 384 425 ## ## 1 2 3 ## 371 435 350 ## ## 1 2 3 ## 326 412 418 ## ## 1 2 3 ## 360 373 423 ## ## 1 2 3 ## 413 365 378 ## ## 1 2 3 ## 404 383 369 ## ## 1 2 3 ## 442 325 389 ## ## 1 2 3 ## 353 369 434 ## ## 1 2 3 ## 376 386 394 ## ## 1 2 3 ## 381 392 383 ## ## 1 2 3 ## 375 362 419 ## ## 1 2 3 ## 440 389 327 ## ## 1 2 3 ## 419 382 355 ## ## 1 2 3 ## 392 407 357 ## ## 1 2 3 ## 377 356 423 ## ## 1 2 3 ## 417 390 349 ## ## 1 2 3 ## 402 415 339 ## ## 1 2 3 ## 398 403 355 ## ## 1 2 3 ## 373 420 363 ## ## 1 2 3 ## 426 414 316 ## ## 1 2 3 ## 418 379 359 ## ## 1 2 3 ## 418 363 375 ## ## 1 2 3 ## 388 416 352 ## ## 1 2 3 ## 400 364 392 ## ## 1 2 3 ## 375 386 395 ## ## 1 2 3 ## 342 402 412 ## ## 1 2 3 ## 354 377 425 ## ## 1 2 3 ## 374 453 329 ## ## 1 2 3 ## 368 414 374 ## ## 1 2 3 ## 386 385 385 ## ## 1 2 3 ## 395 332 429 ## ## 1 2 3 ## 387 417 352 ## ## 1 2 3 ## 377 407 372 ## ## 1 2 3 ## 416 363 377 ## ## 1 2 3 ## 412 363 381 ## ## 1 2 3 ## 408 399 349 ## ## 1 2 3 ## 397 432 327 ## ## 1 2 3 ## 397 387 372 ## ## 1 2 3 ## 376 329 451 ## ## 1 2 3 ## 345 361 450 ## ## 1 2 3 ## 312 380 464 ## ## 1 2 3 ## 336 379 441 ## ## 1 2 3 ## 398 404 354 ## ## 1 2 3 ## 381 399 376 ## ## 1 2 3 ## 363 376 417 ## ## 1 2 3 ## 378 391 387 ## ## 1 2 3 ## 386 424 346 ## ## 1 2 3 ## 366 392 398 ## ## 1 2 3 ## 385 332 439 ## ## 1 2 3 ## 404 371 381 ## ## 1 2 3 ## 418 334 404 ## ## 1 2 3 ## 378 429 349 ## ## 1 2 3 ## 398 335 423 ## ## 1 2 3 ## 389 347 420 ## ## 1 2 3 ## 425 348 383 ## ## 1 2 3 ## 372 385 399 ## ## 1 2 3 ## 405 345 406 ## ## 1 2 3 ## 360 372 424 ## ## 1 2 3 ## 349 427 380 ## ## 1 2 3 ## 387 389 380 ## ## 1 2 3 ## 385 392 379 ## ## 1 2 3 ## 370 417 369 ## ## 1 2 3 ## 377 402 377 ## ## 1 2 3 ## 356 423 377 ## ## 1 2 3 ## 328 439 389 ## ## 1 2 3 ## 340 360 456 ## ## 1 2 3 ## 353 361 442 ## ## 1 2 3 ## 331 388 437 ## ## 1 2 3 ## 426 383 347 ## ## 1 2 3 ## 392 372 392 ## ## 1 2 3 ## 372 415 369 ## ## 1 2 3 ## 391 380 385 ## ## 1 2 3 ## 346 386 424 ## ## 1 2 3 ## 336 484 336 ## ## 1 2 3 ## 377 348 431 ## ## 1 2 3 ## 354 374 428 ## ## 1 2 3 ## 410 390 356 ## ## 1 2 3 ## 393 376 387 ## ## 1 2 3 ## 384 362 410 ## ## 1 2 3 ## 383 373 400 ## ## 1 2 3 ## 409 379 368 ## ## 1 2 3 ## 432 370 354 ## ## 1 2 3 ## 490 352 314 ## ## 1 2 3 ## 447 327 382 ## ## 1 2 3 ## 421 374 361 ## ## 1 2 3 ## 381 394 381 ## ## 1 2 3 ## 357 385 414 ## ## 1 2 3 ## 363 410 383 ## ## 1 2 3 ## 375 411 370 ## ## 1 2 3 ## 404 396 356 ## ## 1 2 3 ## 373 389 394 ## ## 1 2 3 ## 386 399 371 ## ## 1 2 3 ## 346 468 342 ## ## 1 2 3 ## 417 360 379 ## ## 1 2 3 ## 433 340 383 ## ## 1 2 3 ## 447 360 349 ## ## 1 2 3 ## 357 384 415 ## ## 1 2 3 ## 389 367 400 ## ## 1 2 3 ## 364 358 434 ## ## 1 2 3 ## 408 337 411 ## ## 1 2 3 ## 361 412 383 ## ## 1 2 3 ## 386 426 344 ## ## 1 2 3 ## 381 386 389 ## ## 1 2 3 ## 345 433 378 ## ## 1 2 3 ## 340 414 402 ## ## 1 2 3 ## 318 430 408 ## ## 1 2 3 ## 376 439 341 ## ## 1 2 3 ## 328 489 339 ## ## 1 2 3 ## 384 371 401 ## ## 1 2 3 ## 424 379 353 ## ## 1 2 3 ## 406 388 362 ## ## 1 2 3 ## 394 392 370 ## ## 1 2 3 ## 435 354 367 ## ## 1 2 3 ## 355 393 408 ## ## 1 2 3 ## 392 360 404 ## ## 1 2 3 ## 346 389 421 ## ## 1 2 3 ## 369 399 388 ## ## 1 2 3 ## 406 398 352 ## ## 1 2 3 ## 399 428 329 ## ## 1 2 3 ## 373 409 374 ## ## 1 2 3 ## 416 392 348 ## ## 1 2 3 ## 362 443 351 ## ## 1 2 3 ## 352 495 309 ## ## 1 2 3 ## 381 386 389 ## ## 1 2 3 ## 403 381 372 ## ## 1 2 3 ## 393 367 396 ## ## 1 2 3 ## 362 386 408 ## ## 1 2 3 ## 427 386 343 ## ## 1 2 3 ## 388 450 318 ## ## 1 2 3 ## 349 479 328 ## ## 1 2 3 ## 385 387 384 ## ## 1 2 3 ## 347 387 422 ## ## 1 2 3 ## 324 413 419 ## ## 1 2 3 ## 342 358 456 ## ## 1 2 3 ## 357 429 370 ## ## 1 2 3 ## 343 467 346 ## ## 1 2 3 ## 357 455 344 ## ## 1 2 3 ## 349 417 390 ## ## 1 2 3 ## 405 357 394 ## ## 1 2 3 ## 340 402 414 ## ## 1 2 3 ## 384 430 342 ## ## 1 2 3 ## 372 451 333 ## ## 1 2 3 ## 418 441 297 ## ## 1 2 3 ## 336 415 405 ## ## 1 2 3 ## 402 394 360 ## ## 1 2 3 ## 397 407 352 ## ## 1 2 3 ## 376 380 400 ## ## 1 2 3 ## 380 377 399 ## ## 1 2 3 ## 377 419 360 ## ## 1 2 3 ## 401 366 389 ## ## 1 2 3 ## 372 397 387 ## ## 1 2 3 ## 403 392 361 ## ## 1 2 3 ## 347 433 376 ## ## 1 2 3 ## 400 379 377 ## ## 1 2 3 ## 390 440 326 ## ## 1 2 3 ## 411 406 339 ## ## 1 2 3 ## 379 360 417 ## ## 1 2 3 ## 365 365 426 ## ## 1 2 3 ## 343 435 378 ## ## 1 2 3 ## 333 384 439 ## ## 1 2 3 ## 354 372 430 ## ## 1 2 3 ## 390 376 390 ## ## 1 2 3 ## 370 431 355 ## ## 1 2 3 ## 422 374 360 ## ## 1 2 3 ## 437 370 349 ## ## 1 2 3 ## 361 377 418 ## ## 1 2 3 ## 324 403 429 ## ## 1 2 3 ## 411 406 339 ## ## 1 2 3 ## 453 345 358 ## ## 1 2 3 ## 375 355 426 ## ## 1 2 3 ## 334 378 444 ## ## 1 2 3 ## 338 428 390 ## ## 1 2 3 ## 382 423 351 ## ## 1 2 3 ## 326 353 477 ## ## 1 2 3 ## 410 372 374 ## ## 1 2 3 ## 401 387 368 ## ## 1 2 3 ## 349 451 356 ## ## 1 2 3 ## 380 405 371 ## ## 1 2 3 ## 424 386 346 ## ## 1 2 3 ## 388 374 394 ## ## 1 2 3 ## 364 424 368 ## ## 1 2 3 ## 439 352 365 ## ## 1 2 3 ## 372 346 438 ## ## 1 2 3 ## 407 385 364 ## ## 1 2 3 ## 378 436 342 ## ## 1 2 3 ## 390 393 373 ## ## 1 2 3 ## 406 359 391 ## ## 1 2 3 ## 424 408 324 ## ## 1 2 3 ## 426 422 308 ## ## 1 2 3 ## 405 397 354 ## ## 1 2 3 ## 353 402 401 ## ## 1 2 3 ## 378 420 358 ## ## 1 2 3 ## 398 392 366 ## ## 1 2 3 ## 346 357 453 ## ## 1 2 3 ## 366 378 412 ## ## 1 2 3 ## 403 385 368 ## ## 1 2 3 ## 390 380 386 ## ## 1 2 3 ## 332 333 491 ## ## 1 2 3 ## 343 343 470 ## ## 1 2 3 ## 331 335 490 ## ## 1 2 3 ## 340 354 462 ## ## 1 2 3 ## 378 352 426 ## ## 1 2 3 ## 361 355 440 ## ## 1 2 3 ## 319 413 424 ## ## 1 2 3 ## 443 382 331 ## ## 1 2 3 ## 493 375 288 ## ## 1 2 3 ## 414 434 308 ## ## 1 2 3 ## 375 381 400 ## ## 1 2 3 ## 416 364 376 ## ## 1 2 3 ## 358 376 422 ## ## 1 2 3 ## 382 374 400 ## ## 1 2 3 ## 419 356 381 ## ## 1 2 3 ## 406 375 375 ## ## 1 2 3 ## 367 383 406 ## ## 1 2 3 ## 388 338 430 ## ## 1 2 3 ## 390 377 389 ## ## 1 2 3 ## 389 367 400 ## ## 1 2 3 ## 357 364 435 ## ## 1 2 3 ## 347 380 429 ## ## 1 2 3 ## 398 374 384 ## ## 1 2 3 ## 398 381 377 ## ## 1 2 3 ## 417 396 343 ## ## 1 2 3 ## 420 390 346 ## ## 1 2 3 ## 404 396 356 ## ## 1 2 3 ## 430 410 316 ## ## 1 2 3 ## 384 344 428 ## ## 1 2 3 ## 344 403 409 ## ## 1 2 3 ## 383 383 390 ## ## 1 2 3 ## 370 382 404 ## ## 1 2 3 ## 379 389 388 ## ## 1 2 3 ## 378 386 392 ## ## 1 2 3 ## 397 382 377 ## ## 1 2 3 ## 395 390 371 ## ## 1 2 3 ## 369 377 410 ## ## 1 2 3 ## 369 351 436 ## ## 1 2 3 ## 384 403 369 ## ## 1 2 3 ## 471 364 321 ## ## 1 2 3 ## 371 397 388 ## ## 1 2 3 ## 390 364 402 ## ## 1 2 3 ## 395 394 367 ## ## 1 2 3 ## 398 419 339 ## ## 1 2 3 ## 385 409 362 ## ## 1 2 3 ## 420 408 328 ## ## 1 2 3 ## 424 382 350 ## ## 1 2 3 ## 412 366 378 ## ## 1 2 3 ## 376 430 350 ## ## 1 2 3 ## 377 397 382 ## ## 1 2 3 ## 386 370 400 ## ## 1 2 3 ## 352 369 435 ## ## 1 2 3 ## 352 348 456 ## ## 1 2 3 ## 408 376 372 ## ## 1 2 3 ## 382 391 383 ## ## 1 2 3 ## 452 338 366 ## ## 1 2 3 ## 372 412 372 ## ## 1 2 3 ## 402 371 383 ## ## 1 2 3 ## 386 322 448 ## ## 1 2 3 ## 325 371 460 ## ## 1 2 3 ## 368 376 412 ## ## 1 2 3 ## 447 320 389 ## ## 1 2 3 ## 356 370 430 ## ## 1 2 3 ## 367 334 455 ## ## 1 2 3 ## 342 420 394 ## ## 1 2 3 ## 308 463 385 ## ## 1 2 3 ## 383 408 365 ## ## 1 2 3 ## 322 384 450 ## ## 1 2 3 ## 338 417 401 ## ## 1 2 3 ## 308 432 416 ## ## 1 2 3 ## 332 369 455 ## ## 1 2 3 ## 367 434 355 ## ## 1 2 3 ## 370 332 454 ## ## 1 2 3 ## 386 407 363 ## ## 1 2 3 ## 363 316 477 ## ## 1 2 3 ## 359 353 444 ## ## 1 2 3 ## 344 381 431 ## ## 1 2 3 ## 374 437 345 ## ## 1 2 3 ## 381 373 402 ## ## 1 2 3 ## 376 393 387 ## ## 1 2 3 ## 358 401 397 ## ## 1 2 3 ## 378 418 360 ## ## 1 2 3 ## 460 355 341 ## ## 1 2 3 ## 348 401 407 ## ## 1 2 3 ## 348 319 489 ## ## 1 2 3 ## 406 367 383 ## ## 1 2 3 ## 419 355 382 ## ## 1 2 3 ## 368 396 392 ## ## 1 2 3 ## 358 351 447 ## ## 1 2 3 ## 354 432 370 ## ## 1 2 3 ## 322 406 428 ## ## 1 2 3 ## 409 358 389 ## ## 1 2 3 ## 356 400 400 ## ## 1 2 3 ## 362 385 409 ## ## 1 2 3 ## 371 373 412 ## ## 1 2 3 ## 321 381 454 ## ## 1 2 3 ## 375 389 392 ## ## 1 2 3 ## 389 387 380 ## ## 1 2 3 ## 363 436 357 ## ## 1 2 3 ## 434 406 316 ## ## 1 2 3 ## 364 429 363 ## ## 1 2 3 ## 470 361 325 ## ## 1 2 3 ## 432 347 377 ## ## 1 2 3 ## 320 428 408 ## ## 1 2 3 ## 344 437 375 ## ## 1 2 3 ## 336 428 392 ## ## 1 2 3 ## 335 370 451 ## ## 1 2 3 ## 397 342 417 ## ## 1 2 3 ## 435 360 361 ## ## 1 2 3 ## 389 383 384 ## ## 1 2 3 ## 458 352 346 ## ## 1 2 3 ## 381 396 379 ## ## 1 2 3 ## 358 388 410 ## ## 1 2 3 ## 373 375 408 ## ## 1 2 3 ## 415 337 404 ## ## 1 2 3 ## 406 413 337 ## ## 1 2 3 ## 394 350 412 ## ## 1 2 3 ## 318 453 385 ## ## 1 2 3 ## 389 406 361 ## ## 1 2 3 ## 402 369 385 ## ## 1 2 3 ## 399 431 326 ## ## 1 2 3 ## 437 409 310 ## ## 1 2 3 ## 430 410 316 ## ## 1 2 3 ## 387 428 341 ## ## 1 2 3 ## 442 360 354 ## ## 1 2 3 ## 496 333 327 ## ## 1 2 3 ## 518 350 288 ## ## 1 2 3 ## 421 349 386 ## ## 1 2 3 ## 372 435 349 ## ## 1 2 3 ## 337 378 441 ## ## 1 2 3 ## 372 370 414 ## ## 1 2 3 ## 399 361 396 ## ## 1 2 3 ## 368 394 394 ## ## 1 2 3 ## 419 333 404 ## ## 1 2 3 ## 496 316 344 ## ## 1 2 3 ## 379 394 383 ## ## 1 2 3 ## 377 392 387 ## ## 1 2 3 ## 415 389 352 ## ## 1 2 3 ## 408 368 380 ## ## 1 2 3 ## 389 391 376 ## ## 1 2 3 ## 412 348 396 ## ## 1 2 3 ## 438 376 342 ## ## 1 2 3 ## 441 384 331 ## ## 1 2 3 ## 453 380 323 ## ## 1 2 3 ## 439 361 356 ## ## 1 2 3 ## 410 336 410 ## ## 1 2 3 ## 359 388 409 ## ## 1 2 3 ## 391 398 367 ## ## 1 2 3 ## 407 377 372 ## ## 1 2 3 ## 356 400 400 ## ## 1 2 3 ## 349 368 439 ## ## 1 2 3 ## 388 311 457 ## ## 1 2 3 ## 420 307 429 ## ## 1 2 3 ## 339 340 477 ## ## 1 2 3 ## 367 362 427 ## ## 1 2 3 ## 379 341 436 ## ## 1 2 3 ## 410 391 355 ## ## 1 2 3 ## 425 358 373 ## ## 1 2 3 ## 425 389 342 ## ## 1 2 3 ## 378 358 420 ## ## 1 2 3 ## 388 388 380 ## ## 1 2 3 ## 386 392 378 ## ## 1 2 3 ## 369 443 344 ## ## 1 2 3 ## 394 360 402 ## ## 1 2 3 ## 348 430 378 ## ## 1 2 3 ## 420 371 365 ## ## 1 2 3 ## 395 372 389 ## ## 1 2 3 ## 405 345 406 ## ## 1 2 3 ## 368 386 402 ## ## 1 2 3 ## 371 396 389 ## ## 1 2 3 ## 413 378 365 ## ## 1 2 3 ## 368 340 448 ## ## 1 2 3 ## 328 355 473 ## ## 1 2 3 ## 417 357 382 ## ## 1 2 3 ## 371 342 443 ## ## 1 2 3 ## 346 440 370 ## ## 1 2 3 ## 372 404 380 ## ## 1 2 3 ## 420 365 371 ## ## 1 2 3 ## 434 339 383 ## ## 1 2 3 ## 396 422 338 ## ## 1 2 3 ## 350 423 383 ## ## 1 2 3 ## 358 345 453 ## ## 1 2 3 ## 372 365 419 ## ## 1 2 3 ## 317 366 473 ## ## 1 2 3 ## 360 356 440 ## ## 1 2 3 ## 381 349 426 ## ## 1 2 3 ## 357 391 408 ## ## 1 2 3 ## 337 323 496 ## ## 1 2 3 ## 385 322 449 ## ## 1 2 3 ## 330 311 515 ## ## 1 2 3 ## 322 309 525 ## ## 1 2 3 ## 353 360 443 ## ## 1 2 3 ## 374 360 422 ## ## 1 2 3 ## 398 357 401 ## ## 1 2 3 ## 367 381 408 ## ## 1 2 3 ## 364 355 437 ## ## 1 2 3 ## 391 357 408 ## ## 1 2 3 ## 368 340 448 ## ## 1 2 3 ## 340 414 402 ## ## 1 2 3 ## 381 377 398 ## ## 1 2 3 ## 391 324 441 ## ## 1 2 3 ## 388 354 414 ## ## 1 2 3 ## 402 371 383 ## ## 1 2 3 ## 393 390 373 ## ## 1 2 3 ## 408 377 371 ## ## 1 2 3 ## 411 405 340 ## ## 1 2 3 ## 395 390 371 ## ## 1 2 3 ## 369 404 383 ## ## 1 2 3 ## 435 364 357 ## ## 1 2 3 ## 400 416 340 ## ## 1 2 3 ## 429 428 299 ## ## 1 2 3 ## 427 365 364 ## ## 1 2 3 ## 409 315 432 ## ## 1 2 3 ## 375 365 416 ## ## 1 2 3 ## 380 424 352 ## ## 1 2 3 ## 409 410 337 ## ## 1 2 3 ## 407 401 348 ## ## 1 2 3 ## 361 404 391 ## ## 1 2 3 ## 354 408 394 ## ## 1 2 3 ## 389 397 370 ## ## 1 2 3 ## 391 381 384 ## ## 1 2 3 ## 418 368 370 ## ## 1 2 3 ## 398 382 376 ## ## 1 2 3 ## 370 435 351 ## ## 1 2 3 ## 317 329 510 ## ## 1 2 3 ## 326 389 441 ## ## 1 2 3 ## 358 401 397 ## ## 1 2 3 ## 375 354 427 ## ## 1 2 3 ## 343 379 434 ## ## 1 2 3 ## 441 263 452 ## ## 1 2 3 ## 502 324 330 ## ## 1 2 3 ## 374 326 456 ## ## 1 2 3 ## 329 333 494 ## ## 1 2 3 ## 413 339 404 ## ## 1 2 3 ## 376 321 459 ## ## 1 2 3 ## 381 363 412 ## ## 1 2 3 ## 381 389 386 ## ## 1 2 3 ## 358 367 431 ## ## 1 2 3 ## 404 382 370 ## ## 1 2 3 ## 415 365 376 ## ## 1 2 3 ## 357 395 404 ## ## 1 2 3 ## 355 395 406 ## ## 1 2 3 ## 390 391 375 ## ## 1 2 3 ## 371 400 385 ## ## 1 2 3 ## 378 354 424 ## ## 1 2 3 ## 367 301 488 ## ## 1 2 3 ## 321 392 443 ## ## 1 2 3 ## 399 429 328 ## ## 1 2 3 ## 397 404 355 ## ## 1 2 3 ## 400 391 365 ## ## 1 2 3 ## 447 370 339 ## ## 1 2 3 ## 435 344 377 ## ## 1 2 3 ## 410 431 315 ## ## 1 2 3 ## 404 370 382 ## ## 1 2 3 ## 408 393 355 ## ## 1 2 3 ## 400 438 318 ## ## 1 2 3 ## 395 377 384 ## ## 1 2 3 ## 388 424 344 ## ## 1 2 3 ## 377 394 385 ## ## 1 2 3 ## 397 397 362 ## ## 1 2 3 ## 377 464 315 ## ## 1 2 3 ## 449 376 331 ## ## 1 2 3 ## 378 442 336 ## ## 1 2 3 ## 387 388 381 ## ## 1 2 3 ## 367 386 403 ## ## 1 2 3 ## 362 390 404 ## ## 1 2 3 ## 400 341 415 ## ## 1 2 3 ## 452 318 386 ## ## 1 2 3 ## 360 327 469 ## ## 1 2 3 ## 428 310 418 ## ## 1 2 3 ## 460 311 385 ## ## 1 2 3 ## 383 348 425 ## ## 1 2 3 ## 341 370 445 ## ## 1 2 3 ## 425 388 343 ## ## 1 2 3 ## 348 394 414 ## ## 1 2 3 ## 354 419 383 ## ## 1 2 3 ## 358 352 446 ## ## 1 2 3 ## 356 342 458 ## ## 1 2 3 ## 305 396 455 ## ## 1 2 3 ## 364 303 489 ## ## 1 2 3 ## 413 294 449 ## ## 1 2 3 ## 460 261 435 ## ## 1 2 3 ## 303 449 404 ## ## 1 2 3 ## 373 329 454 ## ## 1 2 3 ## 352 341 463 ## ## 1 2 3 ## 347 335 474 ## ## 1 2 3 ## 339 415 402 ## ## 1 2 3 ## 373 402 381 ## ## 1 2 3 ## 436 360 360 ## ## 1 2 3 ## 477 360 319 ## ## 1 2 3 ## 312 360 484 ## ## 1 2 3 ## 369 386 401 ## ## 1 2 3 ## 299 390 467 ## ## 1 2 3 ## 364 363 429 ## ## 1 2 3 ## 342 345 469 ## ## 1 2 3 ## 369 389 398 ## ## 1 2 3 ## 388 324 444 ## ## 1 2 3 ## 333 346 477 ## ## 1 2 3 ## 350 354 452 ## ## 1 2 3 ## 402 394 360 ## ## 1 2 3 ## 422 373 361 ## ## 1 2 3 ## 398 387 371 ## ## 1 2 3 ## 342 396 418 ## ## 1 2 3 ## 348 375 433 ## ## 1 2 3 ## 404 373 379 ## ## 1 2 3 ## 391 426 339 ## ## 1 2 3 ## 377 398 381 ## ## 1 2 3 ## 341 378 437 ## ## 1 2 3 ## 366 374 416 ## ## 1 2 3 ## 342 372 442 ## ## 1 2 3 ## 331 328 497 ## ## 1 2 3 ## 411 370 375 ## ## 1 2 3 ## 371 360 425 ## ## 1 2 3 ## 395 371 390 ## ## 1 2 3 ## 403 347 406 ## ## 1 2 3 ## 371 372 413 ## ## 1 2 3 ## 395 392 369 ## ## 1 2 3 ## 356 408 392 ## ## 1 2 3 ## 377 394 385 ## ## 1 2 3 ## 363 398 395 ## ## 1 2 3 ## 378 385 393 ## ## 1 2 3 ## 370 391 395 ## ## 1 2 3 ## 395 378 383 ## ## 1 2 3 ## 404 345 407 ## ## 1 2 3 ## 343 368 445 ## ## 1 2 3 ## 389 377 390 ## ## 1 2 3 ## 346 406 404 ## ## 1 2 3 ## 412 383 361 ## ## 1 2 3 ## 401 377 378 ## ## 1 2 3 ## 407 376 373 ## ## 1 2 3 ## 386 426 344 ## ## 1 2 3 ## 375 436 345 ## ## 1 2 3 ## 400 360 396 ## ## 1 2 3 ## 399 383 374 ## ## 1 2 3 ## 399 405 352 ## ## 1 2 3 ## 387 336 433 ## ## 1 2 3 ## 388 421 347 ## ## 1 2 3 ## 358 400 398 ## ## 1 2 3 ## 420 372 364 ## ## 1 2 3 ## 390 417 349 ## ## 1 2 3 ## 348 414 394 ## ## 1 2 3 ## 392 388 376 ## ## 1 2 3 ## 373 413 370 ## ## 1 2 3 ## 381 378 397 ## ## 1 2 3 ## 419 355 382 ## ## 1 2 3 ## 390 407 359 ## ## 1 2 3 ## 438 372 346 ## ## 1 2 3 ## 396 427 333 ## ## 1 2 3 ## 391 422 343 ## ## 1 2 3 ## 346 388 422 ## ## 1 2 3 ## 404 386 366 ## ## 1 2 3 ## 426 350 380 ## ## 1 2 3 ## 371 404 381 ## ## 1 2 3 ## 368 404 384 ## ## 1 2 3 ## 421 373 362 ## ## 1 2 3 ## 377 384 395 ## ## 1 2 3 ## 468 356 332 ## ## 1 2 3 ## 401 413 342 ## ## 1 2 3 ## 408 359 389 ## ## 1 2 3 ## 365 368 423 ## ## 1 2 3 ## 346 362 448 ## ## 1 2 3 ## 387 349 420 ## ## 1 2 3 ## 440 357 359 ## ## 1 2 3 ## 387 357 412 ## ## 1 2 3 ## 355 357 444 ## ## 1 2 3 ## 409 364 383 ## ## 1 2 3 ## 379 397 380 ## ## 1 2 3 ## 357 421 378 ## ## 1 2 3 ## 369 399 388 ## ## 1 2 3 ## 428 347 381 ## ## 1 2 3 ## 390 437 329 ## ## 1 2 3 ## 369 450 337 ## ## 1 2 3 ## 424 367 365 ## ## 1 2 3 ## 430 391 335 ## ## 1 2 3 ## 417 367 372 ## ## 1 2 3 ## 399 397 360 ## ## 1 2 3 ## 394 379 383 ## ## 1 2 3 ## 358 395 403 ## ## 1 2 3 ## 442 413 301 ## ## 1 2 3 ## 385 440 331 ## ## 1 2 3 ## 378 379 399 ## ## 1 2 3 ## 431 347 378 ## ## 1 2 3 ## 434 341 381 ## ## 1 2 3 ## 461 356 339 ## ## 1 2 3 ## 382 362 412 ## ## 1 2 3 ## 381 383 392 ## ## 1 2 3 ## 324 408 424 ## ## 1 2 3 ## 393 397 366 ## ## 1 2 3 ## 357 406 393 ## ## 1 2 3 ## 307 428 421 ## ## 1 2 3 ## 392 385 379 ## ## 1 2 3 ## 385 370 401 ## ## 1 2 3 ## 371 375 410 ## ## 1 2 3 ## 355 402 399 ## ## 1 2 3 ## 364 373 419 ## ## 1 2 3 ## 384 361 411 ## ## 1 2 3 ## 404 361 391 ## ## 1 2 3 ## 418 381 357 ## ## 1 2 3 ## 412 357 387 ## ## 1 2 3 ## 440 368 348 ## ## 1 2 3 ## 392 398 366 ## ## 1 2 3 ## 399 400 357 ## ## 1 2 3 ## 381 394 381 ## ## 1 2 3 ## 358 397 401 ## ## 1 2 3 ## 373 362 421 ## ## 1 2 3 ## 356 409 391 ## ## 1 2 3 ## 356 422 378 ## ## 1 2 3 ## 374 372 410 ## ## 1 2 3 ## 391 352 413 ## ## 1 2 3 ## 399 370 387 ## ## 1 2 3 ## 408 359 389 ## ## 1 2 3 ## 434 346 376 ## ## 1 2 3 ## 400 388 368 ## ## 1 2 3 ## 411 383 362 ## ## 1 2 3 ## 364 414 378 ## ## 1 2 3 ## 368 434 354 ## ## 1 2 3 ## 391 430 335 ## ## 1 2 3 ## 439 405 312 ## ## 1 2 3 ## 454 392 310 ## ## 1 2 3 ## 449 399 308 ## ## 1 2 3 ## 510 360 286 ## ## 1 2 3 ## 395 342 419 ## ## 1 2 3 ## 475 357 324 ## ## 1 2 3 ## 507 325 324 ## ## 1 2 3 ## 395 389 372 ## ## 1 2 3 ## 375 450 331 ## ## 1 2 3 ## 393 429 334 ## ## 1 2 3 ## 378 397 381 ## ## 1 2 3 ## 458 356 342 ## ## 1 2 3 ## 388 341 427 ## ## 1 2 3 ## 379 355 422 ## ## 1 2 3 ## 442 361 353 ## ## 1 2 3 ## 416 366 374 ## ## 1 2 3 ## 473 330 353 ## ## 1 2 3 ## 387 385 384 ## ## 1 2 3 ## 415 351 390 ## ## 1 2 3 ## 431 398 327 ## ## 1 2 3 ## 454 373 329 ## ## 1 2 3 ## 479 366 311 ## ## 1 2 3 ## 394 325 437 ## ## 1 2 3 ## 390 347 419 ## ## 1 2 3 ## 360 352 444 ## ## 1 2 3 ## 417 385 354 ## ## 1 2 3 ## 511 336 309 ## ## 1 2 3 ## 427 399 330 ## ## 1 2 3 ## 431 357 368 ## ## 1 2 3 ## 404 422 330 ## ## 1 2 3 ## 402 428 326 ## ## 1 2 3 ## 446 348 362 ## ## 1 2 3 ## 425 345 386 ## ## 1 2 3 ## 405 388 363 ## ## 1 2 3 ## 374 433 349 ## ## 1 2 3 ## 411 401 344 ## ## 1 2 3 ## 418 391 347 ## ## 1 2 3 ## 413 401 342 ## ## 1 2 3 ## 455 366 335 ## ## 1 2 3 ## 398 359 399 ## ## 1 2 3 ## 426 356 374 ## ## 1 2 3 ## 459 351 346 ## ## 1 2 3 ## 424 351 381 ## ## 1 2 3 ## 455 354 347 ## ## 1 2 3 ## 509 308 339 ## ## 1 2 3 ## 520 328 308 ## ## 1 2 3 ## 410 360 386 ## ## 1 2 3 ## 423 360 373 ## ## 1 2 3 ## 375 349 432 ## ## 1 2 3 ## 368 363 425 ## ## 1 2 3 ## 379 446 331 ## ## 1 2 3 ## 415 357 384 ## ## 1 2 3 ## 460 347 349 ## ## 1 2 3 ## 402 378 376 ## ## 1 2 3 ## 385 401 370 ## ## 1 2 3 ## 420 355 381 ## ## 1 2 3 ## 389 400 367 ## ## 1 2 3 ## 385 340 431 ## ## 1 2 3 ## 419 437 300 ## ## 1 2 3 ## 451 350 355 ## ## 1 2 3 ## 447 344 365 ## ## 1 2 3 ## 425 422 309 ## ## 1 2 3 ## 415 418 323 ## ## 1 2 3 ## 392 417 347 ## ## 1 2 3 ## 449 386 321 ## ## 1 2 3 ## 415 411 330 ## ## 1 2 3 ## 433 418 305 ## ## 1 2 3 ## 386 406 364 ## ## 1 2 3 ## 469 366 321 ## ## 1 2 3 ## 439 398 319 ## ## 1 2 3 ## 434 392 330 ## ## 1 2 3 ## 474 354 328 ## ## 1 2 3 ## 387 398 371 ## ## 1 2 3 ## 413 385 358 ## ## 1 2 3 ## 371 360 425 ## ## 1 2 3 ## 448 345 363 ## ## 1 2 3 ## 388 330 438 ## ## 1 2 3 ## 374 359 423 ## ## 1 2 3 ## 330 407 419 ## ## 1 2 3 ## 363 385 408 ## ## 1 2 3 ## 369 313 474 ## ## 1 2 3 ## 358 394 404 ## ## 1 2 3 ## 423 373 360 ## ## 1 2 3 ## 395 387 374 ## ## 1 2 3 ## 431 365 360 ## ## 1 2 3 ## 357 371 428 ## ## 1 2 3 ## 404 357 395 ## ## 1 2 3 ## 383 364 409 ## ## 1 2 3 ## 435 336 385 ## ## 1 2 3 ## 445 315 396 ## ## 1 2 3 ## 450 302 404 ## ## 1 2 3 ## 319 394 443 ## ## 1 2 3 ## 346 437 373 ## ## 1 2 3 ## 373 388 395 ## ## 1 2 3 ## 414 360 382 ## ## 1 2 3 ## 391 370 395 ## ## 1 2 3 ## 362 403 391 ## ## 1 2 3 ## 390 402 364 ## ## 1 2 3 ## 358 391 407 ## ## 1 2 3 ## 403 360 393 ## ## 1 2 3 ## 407 358 391 ## ## 1 2 3 ## 354 412 390 ## ## 1 2 3 ## 349 375 432 ## ## 1 2 3 ## 412 390 354 ## ## 1 2 3 ## 397 441 318 ## ## 1 2 3 ## 388 347 421 ## ## 1 2 3 ## 372 362 422 ## ## 1 2 3 ## 384 362 410 ## ## 1 2 3 ## 368 394 394 ## ## 1 2 3 ## 390 385 381 ## ## 1 2 3 ## 374 406 376 ## ## 1 2 3 ## 420 353 383 ## ## 1 2 3 ## 467 329 360 ## ## 1 2 3 ## 310 378 468 ## ## 1 2 3 ## 358 366 432 ## ## 1 2 3 ## 397 330 429 ## ## 1 2 3 ## 409 312 435 ## ## 1 2 3 ## 401 317 438 ## ## 1 2 3 ## 415 299 442 ## ## 1 2 3 ## 318 417 421 ## ## 1 2 3 ## 326 414 416 ## ## 1 2 3 ## 354 333 469 ## ## 1 2 3 ## 332 375 449 ## ## 1 2 3 ## 362 369 425 ## ## 1 2 3 ## 377 390 389 ## ## 1 2 3 ## 371 394 391 ## ## 1 2 3 ## 445 370 341 ## ## 1 2 3 ## 384 386 386 ## ## 1 2 3 ## 400 341 415 ## ## 1 2 3 ## 414 345 397 ## ## 1 2 3 ## 384 412 360 ## ## 1 2 3 ## 395 383 378 ## ## 1 2 3 ## 432 348 376 ## ## 1 2 3 ## 372 394 390 ## ## 1 2 3 ## 411 380 365 ## ## 1 2 3 ## 385 409 362 ## ## 1 2 3 ## 399 386 371 ## ## 1 2 3 ## 456 349 351 ## ## 1 2 3 ## 489 350 317 ## ## 1 2 3 ## 461 352 343 ## ## 1 2 3 ## 382 360 414 ## ## 1 2 3 ## 436 352 368 ## ## 1 2 3 ## 470 317 369 ## ## 1 2 3 ## 406 351 399 ## ## 1 2 3 ## 411 399 346 ## ## 1 2 3 ## 405 354 397 ## ## 1 2 3 ## 377 381 398 ## ## 1 2 3 ## 373 369 414 ## ## 1 2 3 ## 398 343 415 ## ## 1 2 3 ## 463 325 368 ## ## 1 2 3 ## 436 337 383 ## ## 1 2 3 ## 373 438 345 ## ## 1 2 3 ## 444 381 331 ## ## 1 2 3 ## 395 342 419 ## ## 1 2 3 ## 385 318 453 ## ## 1 2 3 ## 404 391 361 ## ## 1 2 3 ## 394 378 384 ## ## 1 2 3 ## 388 342 426 ## ## 1 2 3 ## 428 385 343 ## ## 1 2 3 ## 418 367 371 ## ## 1 2 3 ## 363 398 395 ## ## 1 2 3 ## 365 359 432 ## ## 1 2 3 ## 343 440 373 ## ## 1 2 3 ## 370 363 423 ## ## 1 2 3 ## 382 376 398 ## ## 1 2 3 ## 366 345 445 ## ## 1 2 3 ## 385 419 352 ## ## 1 2 3 ## 402 434 320 ## ## 1 2 3 ## 409 383 364 ## ## 1 2 3 ## 431 342 383 ## ## 1 2 3 ## 418 370 368 ## ## 1 2 3 ## 414 355 387 ## ## 1 2 3 ## 396 408 352 ## ## 1 2 3 ## 435 415 306 ## ## 1 2 3 ## 438 313 405 ## ## 1 2 3 ## 409 384 363 ## ## 1 2 3 ## 408 398 350 ## ## 1 2 3 ## 400 408 348 ## ## 1 2 3 ## 428 407 321 ## ## 1 2 3 ## 405 415 336 ## ## 1 2 3 ## 429 348 379 ## ## 1 2 3 ## 415 389 352 ## ## 1 2 3 ## 360 381 415 ## ## 1 2 3 ## 430 327 399 ## ## 1 2 3 ## 339 372 445 ## ## 1 2 3 ## 312 397 447 ## ## 1 2 3 ## 338 355 463 ## ## 1 2 3 ## 317 436 403 ## ## 1 2 3 ## 309 481 366 ## ## 1 2 3 ## 355 418 383 ## ## 1 2 3 ## 423 372 361 ## ## 1 2 3 ## 452 336 368 ## ## 1 2 3 ## 460 333 363 ## ## 1 2 3 ## 354 380 422 ## ## 1 2 3 ## 403 357 396 ## ## 1 2 3 ## 403 369 384 ## ## 1 2 3 ## 395 378 383 ## ## 1 2 3 ## 383 385 388 ## ## 1 2 3 ## 420 350 386 ## ## 1 2 3 ## 347 398 411 ## ## 1 2 3 ## 391 388 377 ## ## 1 2 3 ## 425 362 369 ## ## 1 2 3 ## 451 355 350 ## ## 1 2 3 ## 402 362 392 ## ## 1 2 3 ## 450 372 334 ## ## 1 2 3 ## 444 382 330 ## ## 1 2 3 ## 407 391 358 ## ## 1 2 3 ## 417 368 371 ## ## 1 2 3 ## 398 421 337 ## ## 1 2 3 ## 372 419 365 ## ## 1 2 3 ## 415 366 375 ## ## 1 2 3 ## 478 393 285 ## ## 1 2 3 ## 520 393 243 ## ## 1 2 3 ## 445 469 242 ## ## 1 2 3 ## 432 408 316 ## ## 1 2 3 ## 437 342 377 ## ## 1 2 3 ## 409 372 375 ## ## 1 2 3 ## 360 408 388 ## ## 1 2 3 ## 371 375 410 ## ## 1 2 3 ## 403 364 389 ## ## 1 2 3 ## 384 384 388 ## ## 1 2 3 ## 353 404 399 ## ## 1 2 3 ## 352 408 396 ## ## 1 2 3 ## 389 339 428 ## ## 1 2 3 ## 421 313 422 ## ## 1 2 3 ## 385 368 403 ## ## 1 2 3 ## 404 390 362 ## ## 1 2 3 ## 402 397 357 ## ## 1 2 3 ## 411 397 348 ## ## 1 2 3 ## 390 399 367 ## ## 1 2 3 ## 433 366 357 ## ## 1 2 3 ## 365 423 368 ## ## 1 2 3 ## 391 369 396 ## ## 1 2 3 ## 332 425 399 ## ## 1 2 3 ## 402 402 352 ## ## 1 2 3 ## 436 366 354 ## ## 1 2 3 ## 367 376 413 ## ## 1 2 3 ## 412 369 375 ## ## 1 2 3 ## 489 376 291 ## ## 1 2 3 ## 437 342 377 ## ## 1 2 3 ## 443 412 301 ## ## 1 2 3 ## 412 444 300 ## ## 1 2 3 ## 409 388 359 ## ## 1 2 3 ## 420 391 345 ## ## 1 2 3 ## 400 345 411 ## ## 1 2 3 ## 472 342 342 ## ## 1 2 3 ## 391 365 400 ## ## 1 2 3 ## 347 373 436 ## ## 1 2 3 ## 343 362 451 ## ## 1 2 3 ## 319 378 459 ## ## 1 2 3 ## 330 411 415 ## ## 1 2 3 ## 348 399 409 ## ## 1 2 3 ## 359 365 432 ## ## 1 2 3 ## 403 330 423 ## ## 1 2 3 ## 355 347 454 ## ## 1 2 3 ## 452 349 355 ## ## 1 2 3 ## 463 353 340 ## ## 1 2 3 ## 530 302 324 ## ## 1 2 3 ## 373 374 409 ## ## 1 2 3 ## 389 372 395 ## ## 1 2 3 ## 382 351 423 ## ## 1 2 3 ## 419 357 380 ## ## 1 2 3 ## 412 336 408 ## ## 1 2 3 ## 395 379 382 ## ## 1 2 3 ## 407 378 371 ## ## 1 2 3 ## 419 381 356 ## ## 1 2 3 ## 338 426 392 ## ## 1 2 3 ## 375 374 407 ## ## 1 2 3 ## 393 375 388 ## ## 1 2 3 ## 393 365 398 ## ## 1 2 3 ## 393 345 418 ## ## 1 2 3 ## 407 323 426 ## ## 1 2 3 ## 378 378 400 ## ## 1 2 3 ## 415 353 388 ## ## 1 2 3 ## 414 380 362 ## ## 1 2 3 ## 371 370 415 ## ## 1 2 3 ## 421 366 369 ## ## 1 2 3 ## 439 345 372 ## ## 1 2 3 ## 422 332 402 ## ## 1 2 3 ## 369 364 423 ## ## 1 2 3 ## 371 350 435 ## ## 1 2 3 ## 335 406 415 ## ## 1 2 3 ## 369 360 427 ## ## 1 2 3 ## 358 363 435 ## ## 1 2 3 ## 376 366 414 ## ## 1 2 3 ## 359 354 443 ## ## 1 2 3 ## 350 372 434 ## ## 1 2 3 ## 330 387 439 ## ## 1 2 3 ## 320 358 478 ## ## 1 2 3 ## 319 381 456 ## ## 1 2 3 ## 364 370 422 ## ## 1 2 3 ## 427 325 404 ## ## 1 2 3 ## 466 319 371 ## ## 1 2 3 ## 470 321 365 ## ## 1 2 3 ## 389 343 424 ## ## 1 2 3 ## 337 355 464 ## ## 1 2 3 ## 447 337 372 ## ## 1 2 3 ## 398 341 417 ## ## 1 2 3 ## 391 352 413 ## ## 1 2 3 ## 389 368 399 ## ## 1 2 3 ## 381 371 404 ## ## 1 2 3 ## 363 360 433 ## ## 1 2 3 ## 387 372 397 ## ## 1 2 3 ## 395 388 373 ## ## 1 2 3 ## 392 377 387 ## ## 1 2 3 ## 398 379 379 ## ## 1 2 3 ## 408 380 368 ## ## 1 2 3 ## 382 397 377 ## ## 1 2 3 ## 416 387 353 ## ## 1 2 3 ## 382 353 421 ## ## 1 2 3 ## 418 399 339 ## ## 1 2 3 ## 395 410 351 ## ## 1 2 3 ## 419 403 334 ## ## 1 2 3 ## 409 355 392 ## ## 1 2 3 ## 401 404 351 ## ## 1 2 3 ## 360 399 397 ## ## 1 2 3 ## 377 381 398 ## ## 1 2 3 ## 412 354 390 ## ## 1 2 3 ## 401 371 384 ## ## 1 2 3 ## 362 408 386 ## ## 1 2 3 ## 375 373 408 ## ## 1 2 3 ## 380 376 400 ## ## 1 2 3 ## 377 362 417 ## ## 1 2 3 ## 377 370 409 ## ## 1 2 3 ## 372 356 428 ## ## 1 2 3 ## 393 389 374 ## ## 1 2 3 ## 418 374 364 ## ## 1 2 3 ## 390 398 368 ## ## 1 2 3 ## 393 373 390 ## ## 1 2 3 ## 350 444 362 ## ## 1 2 3 ## 403 408 345 ## ## 1 2 3 ## 390 387 379 ## ## 1 2 3 ## 391 380 385 ## ## 1 2 3 ## 384 375 397 ## ## 1 2 3 ## 363 403 390 ## ## 1 2 3 ## 330 449 377 ## ## 1 2 3 ## 375 369 412 ## ## 1 2 3 ## 462 346 348 ## ## 1 2 3 ## 530 309 317 ## ## 1 2 3 ## 437 369 350 ## ## 1 2 3 ## 396 361 399 ## ## 1 2 3 ## 387 400 369 ## ## 1 2 3 ## 399 384 373 ## ## 1 2 3 ## 408 374 374 ## ## 1 2 3 ## 450 352 354 ## ## 1 2 3 ## 380 427 349 ## ## 1 2 3 ## 456 352 348 ## ## 1 2 3 ## 491 338 327 ## ## 1 2 3 ## 518 314 324 ## ## 1 2 3 ## 360 454 342 ## ## 1 2 3 ## 407 402 347 ## ## 1 2 3 ## 457 389 310 ## ## 1 2 3 ## 421 357 378 ## ## 1 2 3 ## 468 379 309 ## ## 1 2 3 ## 376 417 363 ## ## 1 2 3 ## 399 403 354 ## ## 1 2 3 ## 406 382 368 ## ## 1 2 3 ## 468 349 339 ## ## 1 2 3 ## 492 355 309 ## ## 1 2 3 ## 422 418 316 ## ## 1 2 3 ## 479 408 269 ## ## 1 2 3 ## 410 399 347 ## ## 1 2 3 ## 362 469 325 ## ## 1 2 3 ## 342 451 363 ## ## 1 2 3 ## 369 361 426 ## ## 1 2 3 ## 382 340 434 ## ## 1 2 3 ## 371 409 376 ## ## 1 2 3 ## 424 354 378 ## ## 1 2 3 ## 403 391 362 ## ## 1 2 3 ## 389 399 368 ## ## 1 2 3 ## 416 374 366 ## ## 1 2 3 ## 396 408 352 ## ## 1 2 3 ## 404 436 316 ## ## 1 2 3 ## 389 362 405 ## ## 1 2 3 ## 374 437 345 ## ## 1 2 3 ## 437 365 354 ## ## 1 2 3 ## 482 361 313 ## ## 1 2 3 ## 415 426 315 ## ## 1 2 3 ## 432 431 293 ## ## 1 2 3 ## 423 380 353 ## ## 1 2 3 ## 388 373 395 ## ## 1 2 3 ## 420 388 348 ## ## 1 2 3 ## 396 390 370 ## ## 1 2 3 ## 398 404 354 ## ## 1 2 3 ## 389 381 386 ## ## 1 2 3 ## 418 374 364 ## ## 1 2 3 ## 447 329 380 ## ## 1 2 3 ## 369 344 443 ## ## 1 2 3 ## 405 335 416 ## ## 1 2 3 ## 450 327 379 ## ## 1 2 3 ## 404 329 423 ## ## 1 2 3 ## 346 338 472 ## ## 1 2 3 ## 357 312 487 ## ## 1 2 3 ## 467 365 324 ## ## 1 2 3 ## 422 354 380 ## ## 1 2 3 ## 355 438 363 ## ## 1 2 3 ## 400 340 416 ## ## 1 2 3 ## 472 303 381 ## ## 1 2 3 ## 362 400 394 ## ## 1 2 3 ## 427 356 373 ## ## 1 2 3 ## 384 358 414 ## ## 1 2 3 ## 351 372 433 ## ## 1 2 3 ## 370 346 440 ## ## 1 2 3 ## 363 370 423 ## ## 1 2 3 ## 358 378 420 ## ## 1 2 3 ## 348 414 394 ## ## 1 2 3 ## 322 487 347 ## ## 1 2 3 ## 388 365 403 ## ## 1 2 3 ## 383 353 420 ## ## 1 2 3 ## 377 339 440 ## ## 1 2 3 ## 396 378 382 ## ## 1 2 3 ## 355 368 433 ## ## 1 2 3 ## 363 376 417 ## ## 1 2 3 ## 344 374 438 ## ## 1 2 3 ## 410 402 344 ## ## 1 2 3 ## 366 363 427 ## ## 1 2 3 ## 373 391 392 ## ## 1 2 3 ## 343 420 393 ## ## 1 2 3 ## 380 385 391 ## ## 1 2 3 ## 372 376 408 ## ## 1 2 3 ## 363 355 438 ## ## 1 2 3 ## 439 338 379 ## ## 1 2 3 ## 425 377 354 ## ## 1 2 3 ## 405 342 409 ## ## 1 2 3 ## 428 328 400 ## ## 1 2 3 ## 489 293 374 ## ## 1 2 3 ## 452 346 358 ## ## 1 2 3 ## 393 335 428 ## ## 1 2 3 ## 417 337 402 ## ## 1 2 3 ## 420 393 343 ## ## 1 2 3 ## 409 424 323 ## ## 1 2 3 ## 359 324 473 ## ## 1 2 3 ## 402 327 427 ## ## 1 2 3 ## 343 444 369 ## ## 1 2 3 ## 355 393 408 ## ## 1 2 3 ## 398 381 377 ## ## 1 2 3 ## 412 358 386 ## ## 1 2 3 ## 442 380 334 ## ## 1 2 3 ## 443 374 339 ## ## 1 2 3 ## 414 387 355 ## ## 1 2 3 ## 419 372 365 ## ## 1 2 3 ## 374 408 374 ## ## 1 2 3 ## 382 406 368 ## ## 1 2 3 ## 399 368 389 ## ## 1 2 3 ## 385 414 357 ## ## 1 2 3 ## 391 351 414 ## ## 1 2 3 ## 394 349 413 ## ## 1 2 3 ## 366 365 425 ## ## 1 2 3 ## 362 363 431 ## ## 1 2 3 ## 374 344 438 ## ## 1 2 3 ## 386 415 355 ## ## 1 2 3 ## 412 351 393 ## ## 1 2 3 ## 442 321 393 ## ## 1 2 3 ## 384 402 370 ## ## 1 2 3 ## 366 393 397 ## ## 1 2 3 ## 360 442 354 ## ## 1 2 3 ## 345 395 416 ## ## 1 2 3 ## 354 425 377 ## ## 1 2 3 ## 377 401 378 ## ## 1 2 3 ## 376 384 396 ## ## 1 2 3 ## 377 385 394 ## ## 1 2 3 ## 376 358 422 ## ## 1 2 3 ## 396 408 352 ## ## 1 2 3 ## 401 382 373 ## ## 1 2 3 ## 341 396 419 ## ## 1 2 3 ## 398 391 367 ## ## 1 2 3 ## 374 369 413 ## ## 1 2 3 ## 374 379 403 ## ## 1 2 3 ## 393 388 375 ## ## 1 2 3 ## 386 387 383 ## ## 1 2 3 ## 385 374 397 ## ## 1 2 3 ## 388 386 382 ## ## 1 2 3 ## 357 434 365 ## ## 1 2 3 ## 403 371 382 ## ## 1 2 3 ## 458 370 328 ## ## 1 2 3 ## 441 348 367 ## ## 1 2 3 ## 390 374 392 ## ## 1 2 3 ## 397 373 386 ## ## 1 2 3 ## 348 391 417 ## ## 1 2 3 ## 359 383 414 ## ## 1 2 3 ## 426 360 370 ## ## 1 2 3 ## 356 409 391 ## ## 1 2 3 ## 340 385 431 ## ## 1 2 3 ## 320 432 404 ## ## 1 2 3 ## 309 507 340 ## ## 1 2 3 ## 365 392 399 ## ## 1 2 3 ## 405 390 361 ## ## 1 2 3 ## 368 364 424 ## ## 1 2 3 ## 344 346 466 ## ## 1 2 3 ## 432 355 369 ## ## 1 2 3 ## 398 368 390 ## ## 1 2 3 ## 373 403 380 ## ## 1 2 3 ## 355 431 370 ## ## 1 2 3 ## 411 411 334 ## ## 1 2 3 ## 409 419 328 ## ## 1 2 3 ## 415 339 402 ## ## 1 2 3 ## 411 388 357 ## ## 1 2 3 ## 443 386 327 ## ## 1 2 3 ## 410 373 373 ## ## 1 2 3 ## 373 382 401 ## ## 1 2 3 ## 441 362 353 ## ## 1 2 3 ## 404 376 376 ## ## 1 2 3 ## 417 410 329 ## ## 1 2 3 ## 394 333 429 ## ## 1 2 3 ## 427 372 357 ## ## 1 2 3 ## 387 399 370 ## ## 1 2 3 ## 364 409 383 ## ## 1 2 3 ## 382 396 378 ## ## 1 2 3 ## 368 364 424 ## ## 1 2 3 ## 392 361 403 ## ## 1 2 3 ## 389 357 410 ## ## 1 2 3 ## 448 332 376 ## ## 1 2 3 ## 445 342 369 ## ## 1 2 3 ## 469 337 350 ## ## 1 2 3 ## 483 331 342 ## ## 1 2 3 ## 374 453 329 ## ## 1 2 3 ## 430 379 347 ## ## 1 2 3 ## 399 397 360 ## ## 1 2 3 ## 430 385 341 ## ## 1 2 3 ## 373 416 367 ## ## 1 2 3 ## 391 355 410 ## ## 1 2 3 ## 397 396 363 ## ## 1 2 3 ## 402 377 377 ## ## 1 2 3 ## 353 402 401 ## ## 1 2 3 ## 358 399 399 ## ## 1 2 3 ## 398 380 378 ## ## 1 2 3 ## 403 381 372 ## ## 1 2 3 ## 390 377 389 ## ## 1 2 3 ## 399 422 335 ## ## 1 2 3 ## 433 391 332 ## ## 1 2 3 ## 502 348 306 ## ## 1 2 3 ## 396 431 329 ## ## 1 2 3 ## 439 406 311 ## ## 1 2 3 ## 448 407 301 ## ## 1 2 3 ## 482 381 293 ## ## 1 2 3 ## 549 307 300 ## ## 1 2 3 ## 368 444 344 ## ## 1 2 3 ## 399 406 351 ## ## 1 2 3 ## 368 380 408 ## ## 1 2 3 ## 367 398 391 ## ## 1 2 3 ## 330 373 453 ## ## 1 2 3 ## 395 369 392 ## ## 1 2 3 ## 423 389 344 ## ## 1 2 3 ## 428 398 330 ## ## 1 2 3 ## 402 416 338 ## ## 1 2 3 ## 422 401 333 ## ## 1 2 3 ## 410 431 315 ## ## 1 2 3 ## 386 449 321 ## ## 1 2 3 ## 463 416 277 ## ## 1 2 3 ## 419 457 280 ## ## 1 2 3 ## 449 424 283 ## ## 1 2 3 ## 433 379 344 ## ## 1 2 3 ## 392 428 336 ## ## 1 2 3 ## 418 403 335 ## ## 1 2 3 ## 414 408 334 ## ## 1 2 3 ## 420 389 347 ## ## 1 2 3 ## 404 331 421 ## ## 1 2 3 ## 411 374 371 ## ## 1 2 3 ## 397 366 393 ## ## 1 2 3 ## 430 341 385 ## ## 1 2 3 ## 343 374 439 ## ## 1 2 3 ## 344 411 401 ## ## 1 2 3 ## 421 321 414 ## ## 1 2 3 ## 404 314 438 ## ## 1 2 3 ## 325 359 472 ## ## 1 2 3 ## 458 376 322 ## ## 1 2 3 ## 421 315 420 ## ## 1 2 3 ## 485 302 369 ## ## 1 2 3 ## 519 280 357 ## ## 1 2 3 ## 552 244 360 ## ## 1 2 3 ## 577 229 350 ## ## 1 2 3 ## 609 320 227 ## ## 1 2 3 ## 610 229 317 ## ## 1 2 3 ## 614 218 324 ## ## 1 2 3 ## 655 209 292 ## ## 1 2 3 ## 692 179 285 ## ## 1 2 3 ## 704 176 276 ## ## 1 2 3 ## 712 172 272 ## ## 1 2 3 ## 720 156 280 ## ## 1 2 3 ## 680 156 320 ## ## 1 2 3 ## 735 141 280 ## ## 1 2 3 ## 695 165 296 ## ## 1 2 3 ## 713 162 281 ## ## 1 2 3 ## 709 164 283 ## ## 1 2 3 ## 706 165 285 ## ## 1 2 3 ## 698 165 293 ## ## 1 2 3 ## 700 165 291 ## ## 1 2 3 ## 693 165 298 ## ## 1 2 3 ## 661 164 331 ## ## 1 2 3 ## 636 165 355 ## ## 1 2 3 ## 635 164 357 ## ## 1 2 3 ## 602 165 389 ## ## 1 2 3 ## 589 165 402 ## ## 1 2 3 ## 566 165 425 ## ## 1 2 3 ## 569 165 422 ## ## 1 2 3 ## 542 165 449 ## ## 1 2 3 ## 511 165 480 ## ## 1 2 3 ## 484 165 507 ## ## 1 2 3 ## 436 165 555 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 402 163 591 ## ## 1 2 3 ## 353 165 638 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 410 165 581 ## ## 1 2 3 ## 439 553 164 ## ## 1 2 3 ## 449 165 542 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 412 164 580 ## ## 1 2 3 ## 388 165 603 ## ## 1 2 3 ## 640 351 165 ## ## 1 2 3 ## 408 164 584 ## ## 1 2 3 ## 629 362 165 ## ## 1 2 3 ## 634 357 165 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 605 386 165 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 629 362 165 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 376 165 615 ## ## 1 2 3 ## 339 165 652 ## ## 1 2 3 ## 404 165 587 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 468 165 523 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 398 164 594 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 633 359 164 ## ## 1 2 3 ## 415 164 577 ## ## 1 2 3 ## 394 164 598 ## ## 1 2 3 ## 684 165 307 ## ## 1 2 3 ## 717 274 165 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 430 164 562 ## ## 1 2 3 ## 422 165 569 ## ## 1 2 3 ## 633 358 165 ## ## 1 2 3 ## 381 163 612 ## ## 1 2 3 ## 584 407 165 ## ## 1 2 3 ## 414 164 578 ## ## 1 2 3 ## 417 574 165 ## ## 1 2 3 ## 431 165 560 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 440 164 552 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 461 165 530 ## ## 1 2 3 ## 480 165 511 ## ## 1 2 3 ## 461 165 530 ## ## 1 2 3 ## 604 387 165 ## ## 1 2 3 ## 429 165 562 ## ## 1 2 3 ## 408 165 583 ## ## 1 2 3 ## 339 163 654 ## ## 1 2 3 ## 395 596 165 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 375 165 616 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 611 380 165 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 637 354 165 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 625 366 165 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 363 165 628 ## ## 1 2 3 ## 583 408 165 ## ## 1 2 3 ## 447 165 544 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 388 165 603 ## ## 1 2 3 ## 356 165 635 ## ## 1 2 3 ## 657 334 165 ## ## 1 2 3 ## 383 164 609 ## ## 1 2 3 ## 353 164 639 ## ## 1 2 3 ## 361 630 165 ## ## 1 2 3 ## 408 164 584 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 454 537 165 ## ## 1 2 3 ## 454 165 537 ## ## 1 2 3 ## 595 396 165 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 355 165 636 ## ## 1 2 3 ## 304 165 687 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 345 165 646 ## ## 1 2 3 ## 346 165 645 ## ## 1 2 3 ## 322 165 669 ## ## 1 2 3 ## 342 164 650 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 628 363 165 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 618 373 165 ## ## 1 2 3 ## 431 165 560 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 361 165 630 ## ## 1 2 3 ## 331 164 661 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 649 342 165 ## ## 1 2 3 ## 415 165 576 ## ## 1 2 3 ## 375 165 616 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 308 165 683 ## ## 1 2 3 ## 668 323 165 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 656 335 165 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 397 594 165 ## ## 1 2 3 ## 579 412 165 ## ## 1 2 3 ## 459 165 532 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 641 350 165 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 455 165 536 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 652 339 165 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 365 164 627 ## ## 1 2 3 ## 313 164 679 ## ## 1 2 3 ## 328 165 663 ## ## 1 2 3 ## 599 392 165 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 470 165 521 ## ## 1 2 3 ## 453 165 538 ## ## 1 2 3 ## 415 165 576 ## ## 1 2 3 ## 621 370 165 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 385 164 607 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 644 347 165 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 393 164 599 ## ## 1 2 3 ## 643 349 164 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 609 382 165 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 422 165 569 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 358 164 634 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 629 362 165 ## ## 1 2 3 ## 436 165 555 ## ## 1 2 3 ## 408 165 583 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 423 569 164 ## ## 1 2 3 ## 668 323 165 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 377 164 615 ## ## 1 2 3 ## 665 327 164 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 347 164 645 ## ## 1 2 3 ## 326 164 666 ## ## 1 2 3 ## 410 165 581 ## ## 1 2 3 ## 448 165 543 ## ## 1 2 3 ## 410 165 581 ## ## 1 2 3 ## 357 165 634 ## ## 1 2 3 ## 374 617 165 ## ## 1 2 3 ## 346 165 645 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 434 557 165 ## ## 1 2 3 ## 451 165 540 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 636 356 164 ## ## 1 2 3 ## 641 350 165 ## ## 1 2 3 ## 424 164 568 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 364 164 628 ## ## 1 2 3 ## 661 330 165 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 445 165 546 ## ## 1 2 3 ## 460 165 531 ## ## 1 2 3 ## 438 164 554 ## ## 1 2 3 ## 393 164 599 ## ## 1 2 3 ## 534 457 165 ## ## 1 2 3 ## 494 165 497 ## ## 1 2 3 ## 466 165 525 ## ## 1 2 3 ## 456 164 536 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 608 383 165 ## ## 1 2 3 ## 470 165 521 ## ## 1 2 3 ## 457 164 535 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 326 165 665 ## ## 1 2 3 ## 359 165 632 ## ## 1 2 3 ## 332 165 659 ## ## 1 2 3 ## 404 165 587 ## ## 1 2 3 ## 362 164 630 ## ## 1 2 3 ## 338 165 653 ## ## 1 2 3 ## 697 295 164 ## ## 1 2 3 ## 366 165 625 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 350 165 641 ## ## 1 2 3 ## 331 165 660 ## ## 1 2 3 ## 306 164 686 ## ## 1 2 3 ## 260 165 731 ## ## 1 2 3 ## 341 165 650 ## ## 1 2 3 ## 321 164 671 ## ## 1 2 3 ## 307 165 684 ## ## 1 2 3 ## 264 164 728 ## ## 1 2 3 ## 359 165 632 ## ## 1 2 3 ## 366 625 165 ## ## 1 2 3 ## 386 165 605 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 649 342 165 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 358 164 634 ## ## 1 2 3 ## 347 165 644 ## ## 1 2 3 ## 679 312 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 454 537 165 ## ## 1 2 3 ## 486 165 505 ## ## 1 2 3 ## 439 165 552 ## ## 1 2 3 ## 429 165 562 ## ## 1 2 3 ## 446 165 545 ## ## 1 2 3 ## 442 165 549 ## ## 1 2 3 ## 598 393 165 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 443 165 548 ## ## 1 2 3 ## 363 164 629 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 448 164 544 ## ## 1 2 3 ## 584 408 164 ## ## 1 2 3 ## 435 165 556 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 352 165 639 ## ## 1 2 3 ## 317 165 674 ## ## 1 2 3 ## 301 165 690 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 670 321 165 ## ## 1 2 3 ## 383 165 608 ## ## 1 2 3 ## 344 165 647 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 636 355 165 ## ## 1 2 3 ## 401 165 590 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 422 569 165 ## ## 1 2 3 ## 445 165 546 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 321 165 670 ## ## 1 2 3 ## 432 164 560 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 628 363 165 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 617 164 375 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 658 333 165 ## ## 1 2 3 ## 671 320 165 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 355 164 637 ## ## 1 2 3 ## 315 164 677 ## ## 1 2 3 ## 361 630 165 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 362 629 165 ## ## 1 2 3 ## 380 165 611 ## ## 1 2 3 ## 349 165 642 ## ## 1 2 3 ## 329 164 663 ## ## 1 2 3 ## 375 165 616 ## ## 1 2 3 ## 685 306 165 ## ## 1 2 3 ## 385 164 607 ## ## 1 2 3 ## 340 165 651 ## ## 1 2 3 ## 297 165 694 ## ## 1 2 3 ## 690 301 165 ## ## 1 2 3 ## 317 165 674 ## ## 1 2 3 ## 669 322 165 ## ## 1 2 3 ## 419 164 573 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 333 165 658 ## ## 1 2 3 ## 390 165 601 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 345 165 646 ## ## 1 2 3 ## 312 165 679 ## ## 1 2 3 ## 383 165 608 ## ## 1 2 3 ## 624 367 165 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 375 165 616 ## ## 1 2 3 ## 356 165 635 ## ## 1 2 3 ## 719 165 272 ## ## 1 2 3 ## 320 165 671 ## ## 1 2 3 ## 316 165 675 ## ## 1 2 3 ## 333 165 658 ## ## 1 2 3 ## 299 165 692 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 337 165 654 ## ## 1 2 3 ## 282 165 709 ## ## 1 2 3 ## 275 165 716 ## ## 1 2 3 ## 288 164 704 ## ## 1 2 3 ## 380 165 611 ## ## 1 2 3 ## 435 556 165 ## ## 1 2 3 ## 438 165 553 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 410 165 581 ## ## 1 2 3 ## 398 165 593 ## ## 1 2 3 ## 573 165 418 ## ## 1 2 3 ## 392 164 600 ## ## 1 2 3 ## 413 578 165 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 623 368 165 ## ## 1 2 3 ## 446 165 545 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 607 384 165 ## ## 1 2 3 ## 439 165 552 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 397 594 165 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 582 409 165 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 580 411 165 ## ## 1 2 3 ## 482 165 509 ## ## 1 2 3 ## 496 164 496 ## ## 1 2 3 ## 466 165 525 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 642 349 165 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 353 165 638 ## ## 1 2 3 ## 347 165 644 ## ## 1 2 3 ## 324 165 667 ## ## 1 2 3 ## 306 165 685 ## ## 1 2 3 ## 404 165 587 ## ## 1 2 3 ## 434 557 165 ## ## 1 2 3 ## 417 165 574 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 365 165 626 ## ## 1 2 3 ## 378 165 613 ## ## 1 2 3 ## 332 164 660 ## ## 1 2 3 ## 370 621 165 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 419 165 572 ## ## 1 2 3 ## 606 385 165 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 589 165 402 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 391 600 165 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 603 388 165 ## ## 1 2 3 ## 437 163 556 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 365 164 627 ## ## 1 2 3 ## 374 617 165 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 440 165 551 ## ## 1 2 3 ## 419 164 573 ## ## 1 2 3 ## 395 164 597 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 386 165 605 ## ## 1 2 3 ## 345 165 646 ## ## 1 2 3 ## 332 165 659 ## ## 1 2 3 ## 293 165 698 ## ## 1 2 3 ## 673 318 165 ## ## 1 2 3 ## 376 164 616 ## ## 1 2 3 ## 434 165 557 ## ## 1 2 3 ## 622 369 165 ## ## 1 2 3 ## 438 164 554 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 616 375 165 ## ## 1 2 3 ## 707 284 165 ## ## 1 2 3 ## 723 165 268 ## ## 1 2 3 ## 335 165 656 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 390 164 602 ## ## 1 2 3 ## 391 165 600 ## ## 1 2 3 ## 628 365 163 ## ## 1 2 3 ## 372 165 619 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 640 351 165 ## ## 1 2 3 ## 411 164 581 ## ## 1 2 3 ## 401 165 590 ## ## 1 2 3 ## 364 164 628 ## ## 1 2 3 ## 436 165 555 ## ## 1 2 3 ## 415 165 576 ## ## 1 2 3 ## 653 338 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 371 165 620 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 689 302 165 ## ## 1 2 3 ## 368 165 623 ## ## 1 2 3 ## 414 577 165 ## ## 1 2 3 ## 460 165 531 ## ## 1 2 3 ## 448 165 543 ## ## 1 2 3 ## 605 386 165 ## ## 1 2 3 ## 422 164 570 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 637 354 165 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 448 165 543 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 411 164 581 ## ## 1 2 3 ## 415 165 576 ## ## 1 2 3 ## 410 165 581 ## ## 1 2 3 ## 594 397 165 ## ## 1 2 3 ## 404 164 588 ## ## 1 2 3 ## 656 336 164 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 364 164 628 ## ## 1 2 3 ## 657 334 165 ## ## 1 2 3 ## 390 164 602 ## ## 1 2 3 ## 361 164 631 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 668 323 165 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 374 165 617 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 343 165 648 ## ## 1 2 3 ## 361 165 630 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 605 386 165 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 308 165 683 ## ## 1 2 3 ## 694 297 165 ## ## 1 2 3 ## 709 282 165 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 316 165 675 ## ## 1 2 3 ## 678 313 165 ## ## 1 2 3 ## 372 165 619 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 678 313 165 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 316 165 675 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 424 164 568 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 633 358 165 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 402 163 591 ## ## 1 2 3 ## 625 366 165 ## ## 1 2 3 ## 412 579 165 ## ## 1 2 3 ## 604 387 165 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 417 165 574 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 327 164 665 ## ## 1 2 3 ## 684 307 165 ## ## 1 2 3 ## 702 289 165 ## ## 1 2 3 ## 337 164 655 ## ## 1 2 3 ## 656 335 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 362 165 629 ## ## 1 2 3 ## 685 306 165 ## ## 1 2 3 ## 366 165 625 ## ## 1 2 3 ## 338 165 653 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 347 165 644 ## ## 1 2 3 ## 619 165 372 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 630 361 165 ## ## 1 2 3 ## 390 165 601 ## ## 1 2 3 ## 351 164 641 ## ## 1 2 3 ## 675 316 165 ## ## 1 2 3 ## 375 616 165 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 627 364 165 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 345 165 646 ## ## 1 2 3 ## 314 165 677 ## ## 1 2 3 ## 303 165 688 ## ## 1 2 3 ## 278 165 713 ## ## 1 2 3 ## 644 164 348 ## ## 1 2 3 ## 645 346 165 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 400 591 165 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 387 165 604 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 683 308 165 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 339 165 652 ## ## 1 2 3 ## 328 165 663 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 421 164 571 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 343 165 648 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 646 345 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 374 165 617 ## ## 1 2 3 ## 641 350 165 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 594 397 165 ## ## 1 2 3 ## 438 165 553 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 409 164 583 ## ## 1 2 3 ## 587 164 405 ## ## 1 2 3 ## 677 314 165 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 383 165 608 ## ## 1 2 3 ## 382 164 610 ## ## 1 2 3 ## 347 165 644 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 637 354 165 ## ## 1 2 3 ## 372 165 619 ## ## 1 2 3 ## 350 165 641 ## ## 1 2 3 ## 316 165 675 ## ## 1 2 3 ## 387 604 165 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 445 165 546 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 396 164 596 ## ## 1 2 3 ## 390 601 165 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 588 403 165 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 398 165 593 ## ## 1 2 3 ## 644 347 165 ## ## 1 2 3 ## 428 164 564 ## ## 1 2 3 ## 395 165 596 ## ## 1 2 3 ## 390 165 601 ## ## 1 2 3 ## 615 376 165 ## ## 1 2 3 ## 401 164 591 ## ## 1 2 3 ## 600 165 391 ## ## 1 2 3 ## 674 317 165 ## ## 1 2 3 ## 358 165 633 ## ## 1 2 3 ## 316 165 675 ## ## 1 2 3 ## 404 165 587 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 321 165 670 ## ## 1 2 3 ## 590 401 165 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 337 164 655 ## ## 1 2 3 ## 326 165 665 ## ## 1 2 3 ## 310 165 681 ## ## 1 2 3 ## 369 623 164 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 412 579 165 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 327 165 664 ## ## 1 2 3 ## 591 400 165 ## ## 1 2 3 ## 456 165 535 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 454 165 537 ## ## 1 2 3 ## 456 164 536 ## ## 1 2 3 ## 545 446 165 ## ## 1 2 3 ## 470 165 521 ## ## 1 2 3 ## 471 165 520 ## ## 1 2 3 ## 432 164 560 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 368 165 623 ## ## 1 2 3 ## 635 356 165 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 594 397 165 ## ## 1 2 3 ## 422 164 570 ## ## 1 2 3 ## 618 373 165 ## ## 1 2 3 ## 475 165 516 ## ## 1 2 3 ## 625 366 165 ## ## 1 2 3 ## 420 165 571 ## ## 1 2 3 ## 636 355 165 ## ## 1 2 3 ## 379 165 612 ## ## 1 2 3 ## 638 353 165 ## ## 1 2 3 ## 369 164 623 ## ## 1 2 3 ## 348 165 643 ## ## 1 2 3 ## 678 313 165 ## ## 1 2 3 ## 376 164 616 ## ## 1 2 3 ## 345 164 647 ## ## 1 2 3 ## 343 165 648 ## ## 1 2 3 ## 358 165 633 ## ## 1 2 3 ## 685 306 165 ## ## 1 2 3 ## 374 165 617 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 348 165 643 ## ## 1 2 3 ## 408 165 583 ## ## 1 2 3 ## 390 165 601 ## ## 1 2 3 ## 353 165 638 ## ## 1 2 3 ## 649 342 165 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 664 327 165 ## ## 1 2 3 ## 700 291 165 ## ## 1 2 3 ## 366 165 625 ## ## 1 2 3 ## 331 165 660 ## ## 1 2 3 ## 329 165 662 ## ## 1 2 3 ## 339 165 652 ## ## 1 2 3 ## 369 622 165 ## ## 1 2 3 ## 378 165 613 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 636 355 165 ## ## 1 2 3 ## 404 164 588 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 666 325 165 ## ## 1 2 3 ## 388 163 605 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 619 372 165 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 361 165 630 ## ## 1 2 3 ## 340 163 653 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 359 165 632 ## ## 1 2 3 ## 340 165 651 ## ## 1 2 3 ## 705 286 165 ## ## 1 2 3 ## 348 164 644 ## ## 1 2 3 ## 654 337 165 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 353 165 638 ## ## 1 2 3 ## 704 165 287 ## ## 1 2 3 ## 743 165 248 ## ## 1 2 3 ## 308 683 165 ## ## 1 2 3 ## 265 165 726 ## ## 1 2 3 ## 269 165 722 ## ## 1 2 3 ## 726 265 165 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 300 165 691 ## ## 1 2 3 ## 733 258 165 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 319 165 672 ## ## 1 2 3 ## 380 165 611 ## ## 1 2 3 ## 598 165 393 ## ## 1 2 3 ## 652 339 165 ## ## 1 2 3 ## 697 294 165 ## ## 1 2 3 ## 401 165 590 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 430 561 165 ## ## 1 2 3 ## 456 164 536 ## ## 1 2 3 ## 461 165 530 ## ## 1 2 3 ## 434 165 557 ## ## 1 2 3 ## 427 164 565 ## ## 1 2 3 ## 604 387 165 ## ## 1 2 3 ## 460 165 531 ## ## 1 2 3 ## 452 165 539 ## ## 1 2 3 ## 465 165 526 ## ## 1 2 3 ## 455 165 536 ## ## 1 2 3 ## 454 165 537 ## ## 1 2 3 ## 430 163 563 ## ## 1 2 3 ## 501 165 490 ## ## 1 2 3 ## 468 165 523 ## ## 1 2 3 ## 458 165 533 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 442 165 549 ## ## 1 2 3 ## 419 165 572 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 463 528 165 ## ## 1 2 3 ## 509 165 482 ## ## 1 2 3 ## 484 164 508 ## ## 1 2 3 ## 434 165 557 ## ## 1 2 3 ## 680 311 165 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 374 165 617 ## ## 1 2 3 ## 655 336 165 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 366 164 626 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 375 165 616 ## ## 1 2 3 ## 666 325 165 ## ## 1 2 3 ## 681 310 165 ## ## 1 2 3 ## 387 163 606 ## ## 1 2 3 ## 672 319 165 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 440 165 551 ## ## 1 2 3 ## 611 380 165 ## ## 1 2 3 ## 478 165 513 ## ## 1 2 3 ## 437 165 554 ## ## 1 2 3 ## 527 464 165 ## ## 1 2 3 ## 478 165 513 ## ## 1 2 3 ## 466 165 525 ## ## 1 2 3 ## 580 411 165 ## ## 1 2 3 ## 443 165 548 ## ## 1 2 3 ## 470 521 165 ## ## 1 2 3 ## 484 165 507 ## ## 1 2 3 ## 472 164 520 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 380 164 612 ## ## 1 2 3 ## 355 165 636 ## ## 1 2 3 ## 332 165 659 ## ## 1 2 3 ## 328 165 663 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 388 165 603 ## ## 1 2 3 ## 636 355 165 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 478 165 513 ## ## 1 2 3 ## 455 165 536 ## ## 1 2 3 ## 591 400 165 ## ## 1 2 3 ## 438 164 554 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 451 540 165 ## ## 1 2 3 ## 462 165 529 ## ## 1 2 3 ## 435 556 165 ## ## 1 2 3 ## 476 165 515 ## ## 1 2 3 ## 469 164 523 ## ## 1 2 3 ## 474 163 519 ## ## 1 2 3 ## 478 164 514 ## ## 1 2 3 ## 493 165 498 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 604 387 165 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 658 333 165 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 401 165 590 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 386 165 605 ## ## 1 2 3 ## 400 591 165 ## ## 1 2 3 ## 423 569 164 ## ## 1 2 3 ## 465 165 526 ## ## 1 2 3 ## 422 165 569 ## ## 1 2 3 ## 391 165 600 ## ## 1 2 3 ## 364 163 629 ## ## 1 2 3 ## 334 164 658 ## ## 1 2 3 ## 374 617 165 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 395 165 596 ## ## 1 2 3 ## 644 347 165 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 372 163 621 ## ## 1 2 3 ## 317 165 674 ## ## 1 2 3 ## 663 328 165 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 427 164 565 ## ## 1 2 3 ## 419 165 572 ## ## 1 2 3 ## 363 165 628 ## ## 1 2 3 ## 322 165 669 ## ## 1 2 3 ## 636 165 355 ## ## 1 2 3 ## 349 165 642 ## ## 1 2 3 ## 325 165 666 ## ## 1 2 3 ## 680 311 165 ## ## 1 2 3 ## 363 165 628 ## ## 1 2 3 ## 681 310 165 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 656 335 165 ## ## 1 2 3 ## 661 330 165 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 677 314 165 ## ## 1 2 3 ## 387 605 164 ## ## 1 2 3 ## 440 551 165 ## ## 1 2 3 ## 422 165 569 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 446 545 165 ## ## 1 2 3 ## 459 165 532 ## ## 1 2 3 ## 437 164 555 ## ## 1 2 3 ## 399 164 593 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 331 164 661 ## ## 1 2 3 ## 325 165 666 ## ## 1 2 3 ## 311 164 681 ## ## 1 2 3 ## 733 165 258 ## ## 1 2 3 ## 336 165 655 ## ## 1 2 3 ## 686 305 165 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 331 165 660 ## ## 1 2 3 ## 319 165 672 ## ## 1 2 3 ## 701 290 165 ## ## 1 2 3 ## 363 165 628 ## ## 1 2 3 ## 293 164 699 ## ## 1 2 3 ## 704 287 165 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 352 164 640 ## ## 1 2 3 ## 595 165 396 ## ## 1 2 3 ## 388 165 603 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 352 165 639 ## ## 1 2 3 ## 328 164 664 ## ## 1 2 3 ## 327 165 664 ## ## 1 2 3 ## 324 165 667 ## ## 1 2 3 ## 337 165 654 ## ## 1 2 3 ## 650 341 165 ## ## 1 2 3 ## 379 165 612 ## ## 1 2 3 ## 326 165 665 ## ## 1 2 3 ## 317 165 674 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 390 165 601 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 432 164 560 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 472 165 519 ## ## 1 2 3 ## 449 165 542 ## ## 1 2 3 ## 453 165 538 ## ## 1 2 3 ## 431 165 560 ## ## 1 2 3 ## 397 164 595 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 653 338 165 ## ## 1 2 3 ## 437 165 554 ## ## 1 2 3 ## 381 164 611 ## ## 1 2 3 ## 353 164 639 ## ## 1 2 3 ## 401 164 591 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 302 164 690 ## ## 1 2 3 ## 278 165 713 ## ## 1 2 3 ## 701 290 165 ## ## 1 2 3 ## 782 209 165 ## ## 1 2 3 ## 366 625 165 ## ## 1 2 3 ## 368 164 624 ## ## 1 2 3 ## 408 165 583 ## ## 1 2 3 ## 386 164 606 ## ## 1 2 3 ## 378 165 613 ## ## 1 2 3 ## 667 324 165 ## ## 1 2 3 ## 388 165 603 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 333 165 658 ## ## 1 2 3 ## 309 165 682 ## ## 1 2 3 ## 685 307 164 ## ## 1 2 3 ## 415 164 577 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 383 165 608 ## ## 1 2 3 ## 660 332 164 ## ## 1 2 3 ## 366 165 625 ## ## 1 2 3 ## 376 165 615 ## ## 1 2 3 ## 647 344 165 ## ## 1 2 3 ## 371 165 620 ## ## 1 2 3 ## 352 165 639 ## ## 1 2 3 ## 314 165 677 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 417 574 165 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 427 164 565 ## ## 1 2 3 ## 404 164 588 ## ## 1 2 3 ## 401 164 591 ## ## 1 2 3 ## 412 164 580 ## ## 1 2 3 ## 358 164 634 ## ## 1 2 3 ## 651 340 165 ## ## 1 2 3 ## 376 165 615 ## ## 1 2 3 ## 344 165 647 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 349 165 642 ## ## 1 2 3 ## 399 165 592 ## ## 1 2 3 ## 392 165 599 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 358 164 634 ## ## 1 2 3 ## 375 164 617 ## ## 1 2 3 ## 316 164 676 ## ## 1 2 3 ## 679 312 165 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 368 165 623 ## ## 1 2 3 ## 349 165 642 ## ## 1 2 3 ## 324 165 667 ## ## 1 2 3 ## 333 658 165 ## ## 1 2 3 ## 373 164 619 ## ## 1 2 3 ## 343 165 648 ## ## 1 2 3 ## 419 165 572 ## ## 1 2 3 ## 370 164 622 ## ## 1 2 3 ## 656 335 165 ## ## 1 2 3 ## 440 165 551 ## ## 1 2 3 ## 403 165 588 ## ## 1 2 3 ## 420 164 572 ## ## 1 2 3 ## 417 165 574 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 661 330 165 ## ## 1 2 3 ## 606 385 165 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 370 164 622 ## ## 1 2 3 ## 343 165 648 ## ## 1 2 3 ## 298 165 693 ## ## 1 2 3 ## 686 305 165 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 657 334 165 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 321 165 670 ## ## 1 2 3 ## 393 165 598 ## ## 1 2 3 ## 455 165 536 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 643 349 164 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 593 165 398 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 355 165 636 ## ## 1 2 3 ## 439 165 552 ## ## 1 2 3 ## 419 165 572 ## ## 1 2 3 ## 417 164 575 ## ## 1 2 3 ## 619 372 165 ## ## 1 2 3 ## 619 373 164 ## ## 1 2 3 ## 457 164 535 ## ## 1 2 3 ## 464 165 527 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 408 165 583 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 321 165 670 ## ## 1 2 3 ## 435 165 556 ## ## 1 2 3 ## 613 378 165 ## ## 1 2 3 ## 445 165 546 ## ## 1 2 3 ## 435 165 556 ## ## 1 2 3 ## 424 165 567 ## ## 1 2 3 ## 633 358 165 ## ## 1 2 3 ## 415 165 576 ## ## 1 2 3 ## 620 371 165 ## ## 1 2 3 ## 423 568 165 ## ## 1 2 3 ## 455 537 164 ## ## 1 2 3 ## 479 165 512 ## ## 1 2 3 ## 457 164 535 ## ## 1 2 3 ## 500 491 165 ## ## 1 2 3 ## 547 165 444 ## ## 1 2 3 ## 531 460 165 ## ## 1 2 3 ## 622 370 164 ## ## 1 2 3 ## 456 165 535 ## ## 1 2 3 ## 485 165 506 ## ## 1 2 3 ## 439 165 552 ## ## 1 2 3 ## 420 164 572 ## ## 1 2 3 ## 387 164 605 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 638 353 165 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 361 165 630 ## ## 1 2 3 ## 420 571 165 ## ## 1 2 3 ## 442 165 549 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 447 165 544 ## ## 1 2 3 ## 398 164 594 ## ## 1 2 3 ## 621 370 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 372 165 619 ## ## 1 2 3 ## 365 626 165 ## ## 1 2 3 ## 390 164 602 ## ## 1 2 3 ## 668 323 165 ## ## 1 2 3 ## 382 609 165 ## ## 1 2 3 ## 445 164 547 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 416 164 576 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 452 539 165 ## ## 1 2 3 ## 451 165 540 ## ## 1 2 3 ## 454 163 539 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 624 367 165 ## ## 1 2 3 ## 499 492 165 ## ## 1 2 3 ## 547 165 444 ## ## 1 2 3 ## 521 165 470 ## ## 1 2 3 ## 514 165 477 ## ## 1 2 3 ## 504 165 487 ## ## 1 2 3 ## 520 165 471 ## ## 1 2 3 ## 505 164 487 ## ## 1 2 3 ## 481 165 510 ## ## 1 2 3 ## 467 165 524 ## ## 1 2 3 ## 612 379 165 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 417 165 574 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 618 373 165 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 372 164 620 ## ## 1 2 3 ## 379 612 165 ## ## 1 2 3 ## 379 165 612 ## ## 1 2 3 ## 364 165 627 ## ## 1 2 3 ## 369 623 164 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 684 307 165 ## ## 1 2 3 ## 341 165 650 ## ## 1 2 3 ## 319 164 673 ## ## 1 2 3 ## 684 307 165 ## ## 1 2 3 ## 349 165 642 ## ## 1 2 3 ## 340 165 651 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 401 590 165 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 438 165 553 ## ## 1 2 3 ## 404 163 589 ## ## 1 2 3 ## 614 377 165 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 390 164 602 ## ## 1 2 3 ## 663 328 165 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 330 165 661 ## ## 1 2 3 ## 305 165 686 ## ## 1 2 3 ## 331 165 660 ## ## 1 2 3 ## 399 592 165 ## ## 1 2 3 ## 618 373 165 ## ## 1 2 3 ## 423 164 569 ## ## 1 2 3 ## 379 165 612 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 671 320 165 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 327 165 664 ## ## 1 2 3 ## 603 165 388 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 648 343 165 ## ## 1 2 3 ## 395 165 596 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 416 575 165 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 654 337 165 ## ## 1 2 3 ## 385 165 606 ## ## 1 2 3 ## 391 601 164 ## ## 1 2 3 ## 431 165 560 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 376 165 615 ## ## 1 2 3 ## 653 338 165 ## ## 1 2 3 ## 402 164 590 ## ## 1 2 3 ## 426 165 565 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 368 164 624 ## ## 1 2 3 ## 414 165 577 ## ## 1 2 3 ## 380 165 611 ## ## 1 2 3 ## 360 165 631 ## ## 1 2 3 ## 453 165 538 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 372 165 619 ## ## 1 2 3 ## 524 467 165 ## ## 1 2 3 ## 472 165 519 ## ## 1 2 3 ## 442 165 549 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 358 165 633 ## ## 1 2 3 ## 313 165 678 ## ## 1 2 3 ## 375 616 165 ## ## 1 2 3 ## 383 165 608 ## ## 1 2 3 ## 384 165 607 ## ## 1 2 3 ## 328 165 663 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 370 165 621 ## ## 1 2 3 ## 328 165 663 ## ## 1 2 3 ## 394 165 597 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 667 324 165 ## ## 1 2 3 ## 395 164 597 ## ## 1 2 3 ## 392 599 165 ## ## 1 2 3 ## 413 165 578 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 418 165 573 ## ## 1 2 3 ## 412 165 579 ## ## 1 2 3 ## 461 165 530 ## ## 1 2 3 ## 425 165 566 ## ## 1 2 3 ## 488 165 503 ## ## 1 2 3 ## 460 165 531 ## ## 1 2 3 ## 427 165 564 ## ## 1 2 3 ## 607 384 165 ## ## 1 2 3 ## 472 165 519 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 436 165 555 ## ## 1 2 3 ## 405 164 587 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 377 165 614 ## ## 1 2 3 ## 433 165 558 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 450 165 541 ## ## 1 2 3 ## 585 406 165 ## ## 1 2 3 ## 446 165 545 ## ## 1 2 3 ## 405 165 586 ## ## 1 2 3 ## 373 165 618 ## ## 1 2 3 ## 650 341 165 ## ## 1 2 3 ## 446 165 545 ## ## 1 2 3 ## 441 165 550 ## ## 1 2 3 ## 410 163 583 ## ## 1 2 3 ## 660 332 164 ## ## 1 2 3 ## 395 165 596 ## ## 1 2 3 ## 428 165 563 ## ## 1 2 3 ## 430 165 561 ## ## 1 2 3 ## 396 165 595 ## ## 1 2 3 ## 348 165 643 ## ## 1 2 3 ## 313 165 678 ## ## 1 2 3 ## 678 313 165 ## ## 1 2 3 ## 395 165 596 ## ## 1 2 3 ## 359 165 632 ## ## 1 2 3 ## 423 165 568 ## ## 1 2 3 ## 406 165 585 ## ## 1 2 3 ## 436 165 555 ## ## 1 2 3 ## 367 165 624 ## ## 1 2 3 ## 658 333 165 ## ## 1 2 3 ## 658 334 164 ## ## 1 2 3 ## 688 304 164 ## ## 1 2 3 ## 348 165 643 ## ## 1 2 3 ## 682 309 165 ## ## 1 2 3 ## 389 165 602 ## ## 1 2 3 ## 382 165 609 ## ## 1 2 3 ## 661 332 163 ## ## 1 2 3 ## 365 165 626 ## ## 1 2 3 ## 355 165 636 ## ## 1 2 3 ## 378 613 165 ## ## 1 2 3 ## 400 165 591 ## ## 1 2 3 ## 369 165 622 ## ## 1 2 3 ## 417 574 165 ## ## 1 2 3 ## 463 165 528 ## ## 1 2 3 ## 437 165 554 ## ## 1 2 3 ## 438 165 553 ## ## 1 2 3 ## 426 164 566 ## ## 1 2 3 ## 496 165 495 ## ## 1 2 3 ## 493 165 498 ## ## 1 2 3 ## 486 164 506 ## ## 1 2 3 ## 465 165 526 ## ## 1 2 3 ## 432 165 559 ## ## 1 2 3 ## 409 165 582 ## ## 1 2 3 ## 397 165 594 ## ## 1 2 3 ## 348 165 643 ## ## 1 2 3 ## 379 612 165 ## ## 1 2 3 ## 408 583 165 ## ## 1 2 3 ## 402 165 589 ## ## 1 2 3 ## 381 165 610 ## ## 1 2 3 ## 626 365 165 ## ## 1 2 3 ## 639 352 165 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 661 330 165 ## ## 1 2 3 ## 417 165 574 ## ## 1 2 3 ## 411 165 580 ## ## 1 2 3 ## 374 165 617 ## ## 1 2 3 ## 378 165 613 ## ## 1 2 3 ## 351 165 640 ## ## 1 2 3 ## 342 165 649 ## ## 1 2 3 ## 416 165 575 ## ## 1 2 3 ## 444 165 547 ## ## 1 2 3 ## 421 165 570 ## ## 1 2 3 ## 370 164 622 ## ## 1 2 3 ## 463 165 528 ## ## 1 2 3 ## 446 165 545 ## ## 1 2 3 ## 442 164 550 ## ## 1 2 3 ## 407 165 584 ## ## 1 2 3 ## 357 165 634 ## ## 1 2 3 ## 368 164 624 ## ## 1 2 3 ## 352 165 639 ## ## 1 2 3 ## 638 353 165 ## ## 1 2 3 ## 380 165 611 ## ## 1 2 3 ## 365 165 626 ## ## 1 2 3 ## 314 164 678 ## ## 1 2 3 ## 352 639 165 ## ## 1 2 3 ## 379 165 612 ## ## 1 2 3 ## 375 616 165 ## ## 1 2 3 ## 437 554 165 ## ## 1 2 3 ## 451 165 540 ## ## 1 2 3 ## 437 165 554 ## ## 1 2 3 ## 650 341 165 ## ## 1 2 3 ## 649 342 165 ## ## 1 2 3 ## 730 261 165 ## ## 1 2 3 ## 335 164 657 ## ## 1 2 3 ## 276 165 715 ## ## 1 2 3 ## 398 165 593 ## ## 1 2 3 ## 354 165 637 ## ## 1 2 3 ## 327 165 664 ## ## 1 2 3 ## 306 165 685 ## ## 1 2 3 ## 675 316 165 ## ## 1 2 3 ## 745 246 165 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## 1 479 358 368 349 380 350 366 377 359 363 323 354 411 362 ## 2 330 390 399 428 370 365 395 380 353 360 358 406 378 459 ## 3 347 408 389 379 406 441 395 399 444 433 475 396 367 335 ## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] ## 1 383 375 374 394 342 344 360 320 394 382 404 397 381 ## 2 408 402 354 356 351 407 457 508 367 417 388 378 387 ## 3 365 379 428 406 463 405 339 328 395 357 364 381 388 ## [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39] [,40] ## 1 362 387 447 375 419 400 419 402 396 360 362 365 378 ## 2 398 360 345 375 346 351 358 432 407 355 362 370 405 ## 3 396 409 364 406 391 405 379 322 353 441 432 421 373 ## [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50] [,51] [,52] [,53] ## 1 383 322 383 373 394 439 380 397 326 401 373 375 370 ## 2 375 367 402 393 364 352 404 362 360 375 419 416 422 ## 3 398 467 371 390 398 365 372 397 470 380 364 365 364 ## [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] ## 1 376 392 416 398 360 406 347 418 396 385 424 341 370 ## 2 416 381 371 379 397 360 375 390 358 382 336 439 360 ## 3 364 383 369 379 399 390 434 348 402 389 396 376 426 ## [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] ## 1 389 413 389 387 366 357 405 367 399 384 378 388 361 ## 2 366 354 386 395 351 410 371 423 378 387 396 400 431 ## 3 401 389 381 374 439 389 380 366 379 385 382 368 364 ## [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] [,92] ## 1 323 422 394 395 426 410 402 440 428 347 371 326 360 ## 2 436 323 333 413 386 411 394 363 372 384 435 412 373 ## 3 397 411 429 348 344 335 360 353 356 425 350 418 423 ## [,93] [,94] [,95] [,96] [,97] [,98] [,99] [,100] [,101] [,102] [,103] [,104] ## 1 413 404 442 353 376 381 375 440 419 392 377 417 ## 2 365 383 325 369 386 392 362 389 382 407 356 390 ## 3 378 369 389 434 394 383 419 327 355 357 423 349 ## [,105] [,106] [,107] [,108] [,109] [,110] [,111] [,112] [,113] [,114] [,115] ## 1 402 398 373 426 418 418 388 400 375 342 354 ## 2 415 403 420 414 379 363 416 364 386 402 377 ## 3 339 355 363 316 359 375 352 392 395 412 425 ## [,116] [,117] [,118] [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] ## 1 374 368 386 395 387 377 416 412 408 397 397 ## 2 453 414 385 332 417 407 363 363 399 432 387 ## 3 329 374 385 429 352 372 377 381 349 327 372 ## [,127] [,128] [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] ## 1 376 345 312 336 398 381 363 378 386 366 385 ## 2 329 361 380 379 404 399 376 391 424 392 332 ## 3 451 450 464 441 354 376 417 387 346 398 439 ## [,138] [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148] ## 1 404 418 378 398 389 425 372 405 360 349 387 ## 2 371 334 429 335 347 348 385 345 372 427 389 ## 3 381 404 349 423 420 383 399 406 424 380 380 ## [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158] [,159] ## 1 385 370 377 356 328 340 353 331 426 392 372 ## 2 392 417 402 423 439 360 361 388 383 372 415 ## 3 379 369 377 377 389 456 442 437 347 392 369 ## [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168] [,169] [,170] ## 1 391 346 336 377 354 410 393 384 383 409 432 ## 2 380 386 484 348 374 390 376 362 373 379 370 ## 3 385 424 336 431 428 356 387 410 400 368 354 ## [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178] [,179] [,180] [,181] ## 1 490 447 421 381 357 363 375 404 373 386 346 ## 2 352 327 374 394 385 410 411 396 389 399 468 ## 3 314 382 361 381 414 383 370 356 394 371 342 ## [,182] [,183] [,184] [,185] [,186] [,187] [,188] [,189] [,190] [,191] [,192] ## 1 417 433 447 357 389 364 408 361 386 381 345 ## 2 360 340 360 384 367 358 337 412 426 386 433 ## 3 379 383 349 415 400 434 411 383 344 389 378 ## [,193] [,194] [,195] [,196] [,197] [,198] [,199] [,200] [,201] [,202] [,203] ## 1 340 318 376 328 384 424 406 394 435 355 392 ## 2 414 430 439 489 371 379 388 392 354 393 360 ## 3 402 408 341 339 401 353 362 370 367 408 404 ## [,204] [,205] [,206] [,207] [,208] [,209] [,210] [,211] [,212] [,213] [,214] ## 1 346 369 406 399 373 416 362 352 381 403 393 ## 2 389 399 398 428 409 392 443 495 386 381 367 ## 3 421 388 352 329 374 348 351 309 389 372 396 ## [,215] [,216] [,217] [,218] [,219] [,220] [,221] [,222] [,223] [,224] [,225] ## 1 362 427 388 349 385 347 324 342 357 343 357 ## 2 386 386 450 479 387 387 413 358 429 467 455 ## 3 408 343 318 328 384 422 419 456 370 346 344 ## [,226] [,227] [,228] [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] ## 1 349 405 340 384 372 418 336 402 397 376 380 ## 2 417 357 402 430 451 441 415 394 407 380 377 ## 3 390 394 414 342 333 297 405 360 352 400 399 ## [,237] [,238] [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] ## 1 377 401 372 403 347 400 390 411 379 365 343 ## 2 419 366 397 392 433 379 440 406 360 365 435 ## 3 360 389 387 361 376 377 326 339 417 426 378 ## [,248] [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258] ## 1 333 354 390 370 422 437 361 324 411 453 375 ## 2 384 372 376 431 374 370 377 403 406 345 355 ## 3 439 430 390 355 360 349 418 429 339 358 426 ## [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268] [,269] ## 1 334 338 382 326 410 401 349 380 424 388 364 ## 2 378 428 423 353 372 387 451 405 386 374 424 ## 3 444 390 351 477 374 368 356 371 346 394 368 ## [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278] [,279] [,280] ## 1 439 372 407 378 390 406 424 426 405 353 378 ## 2 352 346 385 436 393 359 408 422 397 402 420 ## 3 365 438 364 342 373 391 324 308 354 401 358 ## [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288] [,289] [,290] [,291] ## 1 398 346 366 403 390 332 343 331 340 378 361 ## 2 392 357 378 385 380 333 343 335 354 352 355 ## 3 366 453 412 368 386 491 470 490 462 426 440 ## [,292] [,293] [,294] [,295] [,296] [,297] [,298] [,299] [,300] [,301] [,302] ## 1 319 443 493 414 375 416 358 382 419 406 367 ## 2 413 382 375 434 381 364 376 374 356 375 383 ## 3 424 331 288 308 400 376 422 400 381 375 406 ## [,303] [,304] [,305] [,306] [,307] [,308] [,309] [,310] [,311] [,312] [,313] ## 1 388 390 389 357 347 398 398 417 420 404 430 ## 2 338 377 367 364 380 374 381 396 390 396 410 ## 3 430 389 400 435 429 384 377 343 346 356 316 ## [,314] [,315] [,316] [,317] [,318] [,319] [,320] [,321] [,322] [,323] [,324] ## 1 384 344 383 370 379 378 397 395 369 369 384 ## 2 344 403 383 382 389 386 382 390 377 351 403 ## 3 428 409 390 404 388 392 377 371 410 436 369 ## [,325] [,326] [,327] [,328] [,329] [,330] [,331] [,332] [,333] [,334] [,335] ## 1 471 371 390 395 398 385 420 424 412 376 377 ## 2 364 397 364 394 419 409 408 382 366 430 397 ## 3 321 388 402 367 339 362 328 350 378 350 382 ## [,336] [,337] [,338] [,339] [,340] [,341] [,342] [,343] [,344] [,345] [,346] ## 1 386 352 352 408 382 452 372 402 386 325 368 ## 2 370 369 348 376 391 338 412 371 322 371 376 ## 3 400 435 456 372 383 366 372 383 448 460 412 ## [,347] [,348] [,349] [,350] [,351] [,352] [,353] [,354] [,355] [,356] [,357] ## 1 447 356 367 342 308 383 322 338 308 332 367 ## 2 320 370 334 420 463 408 384 417 432 369 434 ## 3 389 430 455 394 385 365 450 401 416 455 355 ## [,358] [,359] [,360] [,361] [,362] [,363] [,364] [,365] [,366] [,367] [,368] ## 1 370 386 363 359 344 374 381 376 358 378 460 ## 2 332 407 316 353 381 437 373 393 401 418 355 ## 3 454 363 477 444 431 345 402 387 397 360 341 ## [,369] [,370] [,371] [,372] [,373] [,374] [,375] [,376] [,377] [,378] [,379] ## 1 348 348 406 419 368 358 354 322 409 356 362 ## 2 401 319 367 355 396 351 432 406 358 400 385 ## 3 407 489 383 382 392 447 370 428 389 400 409 ## [,380] [,381] [,382] [,383] [,384] [,385] [,386] [,387] [,388] [,389] [,390] ## 1 371 321 375 389 363 434 364 470 432 320 344 ## 2 373 381 389 387 436 406 429 361 347 428 437 ## 3 412 454 392 380 357 316 363 325 377 408 375 ## [,391] [,392] [,393] [,394] [,395] [,396] [,397] [,398] [,399] [,400] [,401] ## 1 336 335 397 435 389 458 381 358 373 415 406 ## 2 428 370 342 360 383 352 396 388 375 337 413 ## 3 392 451 417 361 384 346 379 410 408 404 337 ## [,402] [,403] [,404] [,405] [,406] [,407] [,408] [,409] [,410] [,411] [,412] ## 1 394 318 389 402 399 437 430 387 442 496 518 ## 2 350 453 406 369 431 409 410 428 360 333 350 ## 3 412 385 361 385 326 310 316 341 354 327 288 ## [,413] [,414] [,415] [,416] [,417] [,418] [,419] [,420] [,421] [,422] [,423] ## 1 421 372 337 372 399 368 419 496 379 377 415 ## 2 349 435 378 370 361 394 333 316 394 392 389 ## 3 386 349 441 414 396 394 404 344 383 387 352 ## [,424] [,425] [,426] [,427] [,428] [,429] [,430] [,431] [,432] [,433] [,434] ## 1 408 389 412 438 441 453 439 410 359 391 407 ## 2 368 391 348 376 384 380 361 336 388 398 377 ## 3 380 376 396 342 331 323 356 410 409 367 372 ## [,435] [,436] [,437] [,438] [,439] [,440] [,441] [,442] [,443] [,444] [,445] ## 1 356 349 388 420 339 367 379 410 425 425 378 ## 2 400 368 311 307 340 362 341 391 358 389 358 ## 3 400 439 457 429 477 427 436 355 373 342 420 ## [,446] [,447] [,448] [,449] [,450] [,451] [,452] [,453] [,454] [,455] [,456] ## 1 388 386 369 394 348 420 395 405 368 371 413 ## 2 388 392 443 360 430 371 372 345 386 396 378 ## 3 380 378 344 402 378 365 389 406 402 389 365 ## [,457] [,458] [,459] [,460] [,461] [,462] [,463] [,464] [,465] [,466] [,467] ## 1 368 328 417 371 346 372 420 434 396 350 358 ## 2 340 355 357 342 440 404 365 339 422 423 345 ## 3 448 473 382 443 370 380 371 383 338 383 453 ## [,468] [,469] [,470] [,471] [,472] [,473] [,474] [,475] [,476] [,477] [,478] ## 1 372 317 360 381 357 337 385 330 322 353 374 ## 2 365 366 356 349 391 323 322 311 309 360 360 ## 3 419 473 440 426 408 496 449 515 525 443 422 ## [,479] [,480] [,481] [,482] [,483] [,484] [,485] [,486] [,487] [,488] [,489] ## 1 398 367 364 391 368 340 381 391 388 402 393 ## 2 357 381 355 357 340 414 377 324 354 371 390 ## 3 401 408 437 408 448 402 398 441 414 383 373 ## [,490] [,491] [,492] [,493] [,494] [,495] [,496] [,497] [,498] [,499] [,500] ## 1 408 411 395 369 435 400 429 427 409 375 380 ## 2 377 405 390 404 364 416 428 365 315 365 424 ## 3 371 340 371 383 357 340 299 364 432 416 352 ## [,501] [,502] [,503] [,504] [,505] [,506] [,507] [,508] [,509] [,510] [,511] ## 1 409 407 361 354 389 391 418 398 370 317 326 ## 2 410 401 404 408 397 381 368 382 435 329 389 ## 3 337 348 391 394 370 384 370 376 351 510 441 ## [,512] [,513] [,514] [,515] [,516] [,517] [,518] [,519] [,520] [,521] [,522] ## 1 358 375 343 441 502 374 329 413 376 381 381 ## 2 401 354 379 263 324 326 333 339 321 363 389 ## 3 397 427 434 452 330 456 494 404 459 412 386 ## [,523] [,524] [,525] [,526] [,527] [,528] [,529] [,530] [,531] [,532] [,533] ## 1 358 404 415 357 355 390 371 378 367 321 399 ## 2 367 382 365 395 395 391 400 354 301 392 429 ## 3 431 370 376 404 406 375 385 424 488 443 328 ## [,534] [,535] [,536] [,537] [,538] [,539] [,540] [,541] [,542] [,543] [,544] ## 1 397 400 447 435 410 404 408 400 395 388 377 ## 2 404 391 370 344 431 370 393 438 377 424 394 ## 3 355 365 339 377 315 382 355 318 384 344 385 ## [,545] [,546] [,547] [,548] [,549] [,550] [,551] [,552] [,553] [,554] [,555] ## 1 397 377 449 378 387 367 362 400 452 360 428 ## 2 397 464 376 442 388 386 390 341 318 327 310 ## 3 362 315 331 336 381 403 404 415 386 469 418 ## [,556] [,557] [,558] [,559] [,560] [,561] [,562] [,563] [,564] [,565] [,566] ## 1 460 383 341 425 348 354 358 356 305 364 413 ## 2 311 348 370 388 394 419 352 342 396 303 294 ## 3 385 425 445 343 414 383 446 458 455 489 449 ## [,567] [,568] [,569] [,570] [,571] [,572] [,573] [,574] [,575] [,576] [,577] ## 1 460 303 373 352 347 339 373 436 477 312 369 ## 2 261 449 329 341 335 415 402 360 360 360 386 ## 3 435 404 454 463 474 402 381 360 319 484 401 ## [,578] [,579] [,580] [,581] [,582] [,583] [,584] [,585] [,586] [,587] [,588] ## 1 299 364 342 369 388 333 350 402 422 398 342 ## 2 390 363 345 389 324 346 354 394 373 387 396 ## 3 467 429 469 398 444 477 452 360 361 371 418 ## [,589] [,590] [,591] [,592] [,593] [,594] [,595] [,596] [,597] [,598] [,599] ## 1 348 404 391 377 341 366 342 331 411 371 395 ## 2 375 373 426 398 378 374 372 328 370 360 371 ## 3 433 379 339 381 437 416 442 497 375 425 390 ## [,600] [,601] [,602] [,603] [,604] [,605] [,606] [,607] [,608] [,609] [,610] ## 1 403 371 395 356 377 363 378 370 395 404 343 ## 2 347 372 392 408 394 398 385 391 378 345 368 ## 3 406 413 369 392 385 395 393 395 383 407 445 ## [,611] [,612] [,613] [,614] [,615] [,616] [,617] [,618] [,619] [,620] [,621] ## 1 389 346 412 401 407 386 375 400 399 399 387 ## 2 377 406 383 377 376 426 436 360 383 405 336 ## 3 390 404 361 378 373 344 345 396 374 352 433 ## [,622] [,623] [,624] [,625] [,626] [,627] [,628] [,629] [,630] [,631] [,632] ## 1 388 358 420 390 348 392 373 381 419 390 438 ## 2 421 400 372 417 414 388 413 378 355 407 372 ## 3 347 398 364 349 394 376 370 397 382 359 346 ## [,633] [,634] [,635] [,636] [,637] [,638] [,639] [,640] [,641] [,642] [,643] ## 1 396 391 346 404 426 371 368 421 377 468 401 ## 2 427 422 388 386 350 404 404 373 384 356 413 ## 3 333 343 422 366 380 381 384 362 395 332 342 ## [,644] [,645] [,646] [,647] [,648] [,649] [,650] [,651] [,652] [,653] [,654] ## 1 408 365 346 387 440 387 355 409 379 357 369 ## 2 359 368 362 349 357 357 357 364 397 421 399 ## 3 389 423 448 420 359 412 444 383 380 378 388 ## [,655] [,656] [,657] [,658] [,659] [,660] [,661] [,662] [,663] [,664] [,665] ## 1 428 390 369 424 430 417 399 394 358 442 385 ## 2 347 437 450 367 391 367 397 379 395 413 440 ## 3 381 329 337 365 335 372 360 383 403 301 331 ## [,666] [,667] [,668] [,669] [,670] [,671] [,672] [,673] [,674] [,675] [,676] ## 1 378 431 434 461 382 381 324 393 357 307 392 ## 2 379 347 341 356 362 383 408 397 406 428 385 ## 3 399 378 381 339 412 392 424 366 393 421 379 ## [,677] [,678] [,679] [,680] [,681] [,682] [,683] [,684] [,685] [,686] [,687] ## 1 385 371 355 364 384 404 418 412 440 392 399 ## 2 370 375 402 373 361 361 381 357 368 398 400 ## 3 401 410 399 419 411 391 357 387 348 366 357 ## [,688] [,689] [,690] [,691] [,692] [,693] [,694] [,695] [,696] [,697] [,698] ## 1 381 358 373 356 356 374 391 399 408 434 400 ## 2 394 397 362 409 422 372 352 370 359 346 388 ## 3 381 401 421 391 378 410 413 387 389 376 368 ## [,699] [,700] [,701] [,702] [,703] [,704] [,705] [,706] [,707] [,708] [,709] ## 1 411 364 368 391 439 454 449 510 395 475 507 ## 2 383 414 434 430 405 392 399 360 342 357 325 ## 3 362 378 354 335 312 310 308 286 419 324 324 ## [,710] [,711] [,712] [,713] [,714] [,715] [,716] [,717] [,718] [,719] [,720] ## 1 395 375 393 378 458 388 379 442 416 473 387 ## 2 389 450 429 397 356 341 355 361 366 330 385 ## 3 372 331 334 381 342 427 422 353 374 353 384 ## [,721] [,722] [,723] [,724] [,725] [,726] [,727] [,728] [,729] [,730] [,731] ## 1 415 431 454 479 394 390 360 417 511 427 431 ## 2 351 398 373 366 325 347 352 385 336 399 357 ## 3 390 327 329 311 437 419 444 354 309 330 368 ## [,732] [,733] [,734] [,735] [,736] [,737] [,738] [,739] [,740] [,741] [,742] ## 1 404 402 446 425 405 374 411 418 413 455 398 ## 2 422 428 348 345 388 433 401 391 401 366 359 ## 3 330 326 362 386 363 349 344 347 342 335 399 ## [,743] [,744] [,745] [,746] [,747] [,748] [,749] [,750] [,751] [,752] [,753] ## 1 426 459 424 455 509 520 410 423 375 368 379 ## 2 356 351 351 354 308 328 360 360 349 363 446 ## 3 374 346 381 347 339 308 386 373 432 425 331 ## [,754] [,755] [,756] [,757] [,758] [,759] [,760] [,761] [,762] [,763] [,764] ## 1 415 460 402 385 420 389 385 419 451 447 425 ## 2 357 347 378 401 355 400 340 437 350 344 422 ## 3 384 349 376 370 381 367 431 300 355 365 309 ## [,765] [,766] [,767] [,768] [,769] [,770] [,771] [,772] [,773] [,774] [,775] ## 1 415 392 449 415 433 386 469 439 434 474 387 ## 2 418 417 386 411 418 406 366 398 392 354 398 ## 3 323 347 321 330 305 364 321 319 330 328 371 ## [,776] [,777] [,778] [,779] [,780] [,781] [,782] [,783] [,784] [,785] [,786] ## 1 413 371 448 388 374 330 363 369 358 423 395 ## 2 385 360 345 330 359 407 385 313 394 373 387 ## 3 358 425 363 438 423 419 408 474 404 360 374 ## [,787] [,788] [,789] [,790] [,791] [,792] [,793] [,794] [,795] [,796] [,797] ## 1 431 357 404 383 435 445 450 319 346 373 414 ## 2 365 371 357 364 336 315 302 394 437 388 360 ## 3 360 428 395 409 385 396 404 443 373 395 382 ## [,798] [,799] [,800] [,801] [,802] [,803] [,804] [,805] [,806] [,807] [,808] ## 1 391 362 390 358 403 407 354 349 412 397 388 ## 2 370 403 402 391 360 358 412 375 390 441 347 ## 3 395 391 364 407 393 391 390 432 354 318 421 ## [,809] [,810] [,811] [,812] [,813] [,814] [,815] [,816] [,817] [,818] [,819] ## 1 372 384 368 390 374 420 467 310 358 397 409 ## 2 362 362 394 385 406 353 329 378 366 330 312 ## 3 422 410 394 381 376 383 360 468 432 429 435 ## [,820] [,821] [,822] [,823] [,824] [,825] [,826] [,827] [,828] [,829] [,830] ## 1 401 415 318 326 354 332 362 377 371 445 384 ## 2 317 299 417 414 333 375 369 390 394 370 386 ## 3 438 442 421 416 469 449 425 389 391 341 386 ## [,831] [,832] [,833] [,834] [,835] [,836] [,837] [,838] [,839] [,840] [,841] ## 1 400 414 384 395 432 372 411 385 399 456 489 ## 2 341 345 412 383 348 394 380 409 386 349 350 ## 3 415 397 360 378 376 390 365 362 371 351 317 ## [,842] [,843] [,844] [,845] [,846] [,847] [,848] [,849] [,850] [,851] [,852] ## 1 461 382 436 470 406 411 405 377 373 398 463 ## 2 352 360 352 317 351 399 354 381 369 343 325 ## 3 343 414 368 369 399 346 397 398 414 415 368 ## [,853] [,854] [,855] [,856] [,857] [,858] [,859] [,860] [,861] [,862] [,863] ## 1 436 373 444 395 385 404 394 388 428 418 363 ## 2 337 438 381 342 318 391 378 342 385 367 398 ## 3 383 345 331 419 453 361 384 426 343 371 395 ## [,864] [,865] [,866] [,867] [,868] [,869] [,870] [,871] [,872] [,873] [,874] ## 1 365 343 370 382 366 385 402 409 431 418 414 ## 2 359 440 363 376 345 419 434 383 342 370 355 ## 3 432 373 423 398 445 352 320 364 383 368 387 ## [,875] [,876] [,877] [,878] [,879] [,880] [,881] [,882] [,883] [,884] [,885] ## 1 396 435 438 409 408 400 428 405 429 415 360 ## 2 408 415 313 384 398 408 407 415 348 389 381 ## 3 352 306 405 363 350 348 321 336 379 352 415 ## [,886] [,887] [,888] [,889] [,890] [,891] [,892] [,893] [,894] [,895] [,896] ## 1 430 339 312 338 317 309 355 423 452 460 354 ## 2 327 372 397 355 436 481 418 372 336 333 380 ## 3 399 445 447 463 403 366 383 361 368 363 422 ## [,897] [,898] [,899] [,900] [,901] [,902] [,903] [,904] [,905] [,906] [,907] ## 1 403 403 395 383 420 347 391 425 451 402 450 ## 2 357 369 378 385 350 398 388 362 355 362 372 ## 3 396 384 383 388 386 411 377 369 350 392 334 ## [,908] [,909] [,910] [,911] [,912] [,913] [,914] [,915] [,916] [,917] [,918] ## 1 444 407 417 398 372 415 478 520 445 432 437 ## 2 382 391 368 421 419 366 393 393 469 408 342 ## 3 330 358 371 337 365 375 285 243 242 316 377 ## [,919] [,920] [,921] [,922] [,923] [,924] [,925] [,926] [,927] [,928] [,929] ## 1 409 360 371 403 384 353 352 389 421 385 404 ## 2 372 408 375 364 384 404 408 339 313 368 390 ## 3 375 388 410 389 388 399 396 428 422 403 362 ## [,930] [,931] [,932] [,933] [,934] [,935] [,936] [,937] [,938] [,939] [,940] ## 1 402 411 390 433 365 391 332 402 436 367 412 ## 2 397 397 399 366 423 369 425 402 366 376 369 ## 3 357 348 367 357 368 396 399 352 354 413 375 ## [,941] [,942] [,943] [,944] [,945] [,946] [,947] [,948] [,949] [,950] [,951] ## 1 489 437 443 412 409 420 400 472 391 347 343 ## 2 376 342 412 444 388 391 345 342 365 373 362 ## 3 291 377 301 300 359 345 411 342 400 436 451 ## [,952] [,953] [,954] [,955] [,956] [,957] [,958] [,959] [,960] [,961] [,962] ## 1 319 330 348 359 403 355 452 463 530 373 389 ## 2 378 411 399 365 330 347 349 353 302 374 372 ## 3 459 415 409 432 423 454 355 340 324 409 395 ## [,963] [,964] [,965] [,966] [,967] [,968] [,969] [,970] [,971] [,972] [,973] ## 1 382 419 412 395 407 419 338 375 393 393 393 ## 2 351 357 336 379 378 381 426 374 375 365 345 ## 3 423 380 408 382 371 356 392 407 388 398 418 ## [,974] [,975] [,976] [,977] [,978] [,979] [,980] [,981] [,982] [,983] [,984] ## 1 407 378 415 414 371 421 439 422 369 371 335 ## 2 323 378 353 380 370 366 345 332 364 350 406 ## 3 426 400 388 362 415 369 372 402 423 435 415 ## [,985] [,986] [,987] [,988] [,989] [,990] [,991] [,992] [,993] [,994] [,995] ## 1 369 358 376 359 350 330 320 319 364 427 466 ## 2 360 363 366 354 372 387 358 381 370 325 319 ## 3 427 435 414 443 434 439 478 456 422 404 371 ## [,996] [,997] [,998] [,999] [,1000] [,1001] [,1002] [,1003] [,1004] [,1005] ## 1 470 389 337 447 398 391 389 381 363 387 ## 2 321 343 355 337 341 352 368 371 360 372 ## 3 365 424 464 372 417 413 399 404 433 397 ## [,1006] [,1007] [,1008] [,1009] [,1010] [,1011] [,1012] [,1013] [,1014] ## 1 395 392 398 408 382 416 382 418 395 ## 2 388 377 379 380 397 387 353 399 410 ## 3 373 387 379 368 377 353 421 339 351 ## [,1015] [,1016] [,1017] [,1018] [,1019] [,1020] [,1021] [,1022] [,1023] ## 1 419 409 401 360 377 412 401 362 375 ## 2 403 355 404 399 381 354 371 408 373 ## 3 334 392 351 397 398 390 384 386 408 ## [,1024] [,1025] [,1026] [,1027] [,1028] [,1029] [,1030] [,1031] [,1032] ## 1 380 377 377 372 393 418 390 393 350 ## 2 376 362 370 356 389 374 398 373 444 ## 3 400 417 409 428 374 364 368 390 362 ## [,1033] [,1034] [,1035] [,1036] [,1037] [,1038] [,1039] [,1040] [,1041] ## 1 403 390 391 384 363 330 375 462 530 ## 2 408 387 380 375 403 449 369 346 309 ## 3 345 379 385 397 390 377 412 348 317 ## [,1042] [,1043] [,1044] [,1045] [,1046] [,1047] [,1048] [,1049] [,1050] ## 1 437 396 387 399 408 450 380 456 491 ## 2 369 361 400 384 374 352 427 352 338 ## 3 350 399 369 373 374 354 349 348 327 ## [,1051] [,1052] [,1053] [,1054] [,1055] [,1056] [,1057] [,1058] [,1059] ## 1 518 360 407 457 421 468 376 399 406 ## 2 314 454 402 389 357 379 417 403 382 ## 3 324 342 347 310 378 309 363 354 368 ## [,1060] [,1061] [,1062] [,1063] [,1064] [,1065] [,1066] [,1067] [,1068] ## 1 468 492 422 479 410 362 342 369 382 ## 2 349 355 418 408 399 469 451 361 340 ## 3 339 309 316 269 347 325 363 426 434 ## [,1069] [,1070] [,1071] [,1072] [,1073] [,1074] [,1075] [,1076] [,1077] ## 1 371 424 403 389 416 396 404 389 374 ## 2 409 354 391 399 374 408 436 362 437 ## 3 376 378 362 368 366 352 316 405 345 ## [,1078] [,1079] [,1080] [,1081] [,1082] [,1083] [,1084] [,1085] [,1086] ## 1 437 482 415 432 423 388 420 396 398 ## 2 365 361 426 431 380 373 388 390 404 ## 3 354 313 315 293 353 395 348 370 354 ## [,1087] [,1088] [,1089] [,1090] [,1091] [,1092] [,1093] [,1094] [,1095] ## 1 389 418 447 369 405 450 404 346 357 ## 2 381 374 329 344 335 327 329 338 312 ## 3 386 364 380 443 416 379 423 472 487 ## [,1096] [,1097] [,1098] [,1099] [,1100] [,1101] [,1102] [,1103] [,1104] ## 1 467 422 355 400 472 362 427 384 351 ## 2 365 354 438 340 303 400 356 358 372 ## 3 324 380 363 416 381 394 373 414 433 ## [,1105] [,1106] [,1107] [,1108] [,1109] [,1110] [,1111] [,1112] [,1113] ## 1 370 363 358 348 322 388 383 377 396 ## 2 346 370 378 414 487 365 353 339 378 ## 3 440 423 420 394 347 403 420 440 382 ## [,1114] [,1115] [,1116] [,1117] [,1118] [,1119] [,1120] [,1121] [,1122] ## 1 355 363 344 410 366 373 343 380 372 ## 2 368 376 374 402 363 391 420 385 376 ## 3 433 417 438 344 427 392 393 391 408 ## [,1123] [,1124] [,1125] [,1126] [,1127] [,1128] [,1129] [,1130] [,1131] ## 1 363 439 425 405 428 489 452 393 417 ## 2 355 338 377 342 328 293 346 335 337 ## 3 438 379 354 409 400 374 358 428 402 ## [,1132] [,1133] [,1134] [,1135] [,1136] [,1137] [,1138] [,1139] [,1140] ## 1 420 409 359 402 343 355 398 412 442 ## 2 393 424 324 327 444 393 381 358 380 ## 3 343 323 473 427 369 408 377 386 334 ## [,1141] [,1142] [,1143] [,1144] [,1145] [,1146] [,1147] [,1148] [,1149] ## 1 443 414 419 374 382 399 385 391 394 ## 2 374 387 372 408 406 368 414 351 349 ## 3 339 355 365 374 368 389 357 414 413 ## [,1150] [,1151] [,1152] [,1153] [,1154] [,1155] [,1156] [,1157] [,1158] ## 1 366 362 374 386 412 442 384 366 360 ## 2 365 363 344 415 351 321 402 393 442 ## 3 425 431 438 355 393 393 370 397 354 ## [,1159] [,1160] [,1161] [,1162] [,1163] [,1164] [,1165] [,1166] [,1167] ## 1 345 354 377 376 377 376 396 401 341 ## 2 395 425 401 384 385 358 408 382 396 ## 3 416 377 378 396 394 422 352 373 419 ## [,1168] [,1169] [,1170] [,1171] [,1172] [,1173] [,1174] [,1175] [,1176] ## 1 398 374 374 393 386 385 388 357 403 ## 2 391 369 379 388 387 374 386 434 371 ## 3 367 413 403 375 383 397 382 365 382 ## [,1177] [,1178] [,1179] [,1180] [,1181] [,1182] [,1183] [,1184] [,1185] ## 1 458 441 390 397 348 359 426 356 340 ## 2 370 348 374 373 391 383 360 409 385 ## 3 328 367 392 386 417 414 370 391 431 ## [,1186] [,1187] [,1188] [,1189] [,1190] [,1191] [,1192] [,1193] [,1194] ## 1 320 309 365 405 368 344 432 398 373 ## 2 432 507 392 390 364 346 355 368 403 ## 3 404 340 399 361 424 466 369 390 380 ## [,1195] [,1196] [,1197] [,1198] [,1199] [,1200] [,1201] [,1202] [,1203] ## 1 355 411 409 415 411 443 410 373 441 ## 2 431 411 419 339 388 386 373 382 362 ## 3 370 334 328 402 357 327 373 401 353 ## [,1204] [,1205] [,1206] [,1207] [,1208] [,1209] [,1210] [,1211] [,1212] ## 1 404 417 394 427 387 364 382 368 392 ## 2 376 410 333 372 399 409 396 364 361 ## 3 376 329 429 357 370 383 378 424 403 ## [,1213] [,1214] [,1215] [,1216] [,1217] [,1218] [,1219] [,1220] [,1221] ## 1 389 448 445 469 483 374 430 399 430 ## 2 357 332 342 337 331 453 379 397 385 ## 3 410 376 369 350 342 329 347 360 341 ## [,1222] [,1223] [,1224] [,1225] [,1226] [,1227] [,1228] [,1229] [,1230] ## 1 373 391 397 402 353 358 398 403 390 ## 2 416 355 396 377 402 399 380 381 377 ## 3 367 410 363 377 401 399 378 372 389 ## [,1231] [,1232] [,1233] [,1234] [,1235] [,1236] [,1237] [,1238] [,1239] ## 1 399 433 502 396 439 448 482 549 368 ## 2 422 391 348 431 406 407 381 307 444 ## 3 335 332 306 329 311 301 293 300 344 ## [,1240] [,1241] [,1242] [,1243] [,1244] [,1245] [,1246] [,1247] [,1248] ## 1 399 368 367 330 395 423 428 402 422 ## 2 406 380 398 373 369 389 398 416 401 ## 3 351 408 391 453 392 344 330 338 333 ## [,1249] [,1250] [,1251] [,1252] [,1253] [,1254] [,1255] [,1256] [,1257] ## 1 410 386 463 419 449 433 392 418 414 ## 2 431 449 416 457 424 379 428 403 408 ## 3 315 321 277 280 283 344 336 335 334 ## [,1258] [,1259] [,1260] [,1261] [,1262] [,1263] [,1264] [,1265] [,1266] ## 1 420 404 411 397 430 343 344 421 404 ## 2 389 331 374 366 341 374 411 321 314 ## 3 347 421 371 393 385 439 401 414 438 ## [,1267] [,1268] [,1269] [,1270] [,1271] [,1272] [,1273] [,1274] [,1275] ## 1 325 458 421 485 519 552 577 609 610 ## 2 359 376 315 302 280 244 229 320 229 ## 3 472 322 420 369 357 360 350 227 317 ## [,1276] [,1277] [,1278] [,1279] [,1280] [,1281] [,1282] [,1283] [,1284] ## 1 614 655 692 704 712 720 680 735 695 ## 2 218 209 179 176 172 156 156 141 165 ## 3 324 292 285 276 272 280 320 280 296 ## [,1285] [,1286] [,1287] [,1288] [,1289] [,1290] [,1291] [,1292] [,1293] ## 1 713 709 706 698 700 693 661 636 635 ## 2 162 164 165 165 165 165 164 165 164 ## 3 281 283 285 293 291 298 331 355 357 ## [,1294] [,1295] [,1296] [,1297] [,1298] [,1299] [,1300] [,1301] [,1302] ## 1 602 589 566 569 542 511 484 436 432 ## 2 165 165 165 165 165 165 165 165 165 ## 3 389 402 425 422 449 480 507 555 559 ## [,1303] [,1304] [,1305] [,1306] [,1307] [,1308] [,1309] [,1310] [,1311] ## 1 409 402 353 409 392 410 439 449 450 ## 2 165 163 165 165 165 165 553 165 165 ## 3 582 591 638 582 599 581 164 542 541 ## [,1312] [,1313] [,1314] [,1315] [,1316] [,1317] [,1318] [,1319] [,1320] ## 1 412 388 640 408 629 634 399 605 433 ## 2 164 165 351 164 362 357 165 386 165 ## 3 580 603 165 584 165 165 592 165 558 ## [,1321] [,1322] [,1323] [,1324] [,1325] [,1326] [,1327] [,1328] [,1329] ## 1 629 420 403 376 339 404 385 413 407 ## 2 362 165 165 165 165 165 165 165 165 ## 3 165 571 588 615 652 587 606 578 584 ## [,1330] [,1331] [,1332] [,1333] [,1334] [,1335] [,1336] [,1337] [,1338] ## 1 468 433 425 398 409 633 415 394 684 ## 2 165 165 165 164 165 359 164 164 165 ## 3 523 558 566 594 582 164 577 598 307 ## [,1339] [,1340] [,1341] [,1342] [,1343] [,1344] [,1345] [,1346] [,1347] ## 1 717 425 430 422 633 381 584 414 417 ## 2 274 165 164 165 358 163 407 164 574 ## 3 165 566 562 569 165 612 165 578 165 ## [,1348] [,1349] [,1350] [,1351] [,1352] [,1353] [,1354] [,1355] [,1356] ## 1 431 382 440 382 461 480 461 604 429 ## 2 165 165 164 165 165 165 165 387 165 ## 3 560 609 552 609 530 511 530 165 562 ## [,1357] [,1358] [,1359] [,1360] [,1361] [,1362] [,1363] [,1364] [,1365] ## 1 408 339 395 413 375 420 611 432 637 ## 2 165 163 596 165 165 165 380 165 354 ## 3 583 654 165 578 616 571 165 559 165 ## [,1366] [,1367] [,1368] [,1369] [,1370] [,1371] [,1372] [,1373] [,1374] ## 1 393 625 425 387 363 583 447 389 414 ## 2 165 366 165 165 165 408 165 165 165 ## 3 598 165 566 604 628 165 544 602 577 ## [,1375] [,1376] [,1377] [,1378] [,1379] [,1380] [,1381] [,1382] [,1383] ## 1 393 444 388 356 657 383 353 361 408 ## 2 165 165 165 165 334 164 164 630 164 ## 3 598 547 603 635 165 609 639 165 584 ## [,1384] [,1385] [,1386] [,1387] [,1388] [,1389] [,1390] [,1391] [,1392] ## 1 382 454 454 595 420 399 381 355 304 ## 2 165 537 165 396 165 165 165 165 165 ## 3 609 165 537 165 571 592 610 636 687 ## [,1393] [,1394] [,1395] [,1396] [,1397] [,1398] [,1399] [,1400] [,1401] ## 1 648 389 345 346 322 342 423 628 421 ## 2 343 165 165 165 165 164 165 363 165 ## 3 165 602 646 645 669 650 568 165 570 ## [,1402] [,1403] [,1404] [,1405] [,1406] [,1407] [,1408] [,1409] [,1410] ## 1 618 431 406 361 331 407 649 415 375 ## 2 373 165 165 165 164 165 342 165 165 ## 3 165 560 585 630 661 584 165 576 616 ## [,1411] [,1412] [,1413] [,1414] [,1415] [,1416] [,1417] [,1418] [,1419] ## 1 354 308 668 351 656 385 397 579 459 ## 2 165 165 323 165 335 165 594 412 165 ## 3 637 683 165 640 165 606 165 165 532 ## [,1420] [,1421] [,1422] [,1423] [,1424] [,1425] [,1426] [,1427] [,1428] ## 1 450 402 641 424 389 455 396 381 652 ## 2 165 165 350 165 165 165 165 165 339 ## 3 541 589 165 567 602 536 595 610 165 ## [,1429] [,1430] [,1431] [,1432] [,1433] [,1434] [,1435] [,1436] [,1437] ## 1 411 365 313 328 599 413 470 453 415 ## 2 165 164 164 165 392 165 165 165 165 ## 3 580 627 679 663 165 578 521 538 576 ## [,1438] [,1439] [,1440] [,1441] [,1442] [,1443] [,1444] [,1445] [,1446] ## 1 621 426 385 392 644 418 393 643 405 ## 2 370 165 164 165 347 165 164 349 165 ## 3 165 565 607 599 165 573 599 164 586 ## [,1447] [,1448] [,1449] [,1450] [,1451] [,1452] [,1453] [,1454] [,1455] ## 1 609 405 430 422 407 358 342 424 629 ## 2 382 165 165 165 165 164 165 165 362 ## 3 165 586 561 569 584 634 649 567 165 ## [,1456] [,1457] [,1458] [,1459] [,1460] [,1461] [,1462] [,1463] [,1464] ## 1 436 408 384 423 668 444 403 402 377 ## 2 165 165 165 569 323 165 165 165 164 ## 3 555 583 607 164 165 547 588 589 615 ## [,1465] [,1466] [,1467] [,1468] [,1469] [,1470] [,1471] [,1472] [,1473] ## 1 665 392 347 326 410 448 410 357 374 ## 2 327 165 164 164 165 165 165 165 617 ## 3 164 599 645 666 581 543 581 634 165 ## [,1474] [,1475] [,1476] [,1477] [,1478] [,1479] [,1480] [,1481] [,1482] ## 1 346 403 434 451 411 394 636 641 424 ## 2 165 165 557 165 165 165 356 350 164 ## 3 645 588 165 540 580 597 164 165 568 ## [,1483] [,1484] [,1485] [,1486] [,1487] [,1488] [,1489] [,1490] [,1491] ## 1 394 364 661 387 445 460 438 393 534 ## 2 165 164 330 165 165 165 164 164 457 ## 3 597 628 165 604 546 531 554 599 165 ## [,1492] [,1493] [,1494] [,1495] [,1496] [,1497] [,1498] [,1499] [,1500] ## 1 494 466 456 433 608 470 457 424 406 ## 2 165 165 164 165 383 165 164 165 165 ## 3 497 525 536 558 165 521 535 567 585 ## [,1501] [,1502] [,1503] [,1504] [,1505] [,1506] [,1507] [,1508] [,1509] ## 1 381 326 359 332 404 362 338 697 366 ## 2 165 165 165 165 165 164 165 295 165 ## 3 610 665 632 659 587 630 653 164 625 ## [,1510] [,1511] [,1512] [,1513] [,1514] [,1515] [,1516] [,1517] [,1518] ## 1 369 350 331 306 260 341 321 307 264 ## 2 165 165 165 164 165 165 164 165 164 ## 3 622 641 660 686 731 650 671 684 728 ## [,1519] [,1520] [,1521] [,1522] [,1523] [,1524] [,1525] [,1526] [,1527] ## 1 359 366 386 421 405 444 411 649 394 ## 2 165 625 165 165 165 165 165 342 165 ## 3 632 165 605 570 586 547 580 165 597 ## [,1528] [,1529] [,1530] [,1531] [,1532] [,1533] [,1534] [,1535] [,1536] ## 1 358 347 679 400 405 454 486 439 429 ## 2 164 165 312 165 165 537 165 165 165 ## 3 634 644 165 591 586 165 505 552 562 ## [,1537] [,1538] [,1539] [,1540] [,1541] [,1542] [,1543] [,1544] [,1545] ## 1 446 442 598 423 443 363 427 448 584 ## 2 165 165 393 165 165 164 165 164 408 ## 3 545 549 165 568 548 629 564 544 164 ## [,1546] [,1547] [,1548] [,1549] [,1550] [,1551] [,1552] [,1553] [,1554] ## 1 435 409 392 352 317 301 373 670 383 ## 2 165 165 165 165 165 165 165 321 165 ## 3 556 582 599 639 674 690 618 165 608 ## [,1555] [,1556] [,1557] [,1558] [,1559] [,1560] [,1561] [,1562] [,1563] ## 1 344 387 636 401 385 422 445 400 387 ## 2 165 165 355 165 165 569 165 165 165 ## 3 647 604 165 590 606 165 546 591 604 ## [,1564] [,1565] [,1566] [,1567] [,1568] [,1569] [,1570] [,1571] [,1572] ## 1 321 432 397 628 420 369 617 394 420 ## 2 165 164 165 363 165 165 164 165 165 ## 3 670 560 594 165 571 622 375 597 571 ## [,1573] [,1574] [,1575] [,1576] [,1577] [,1578] [,1579] [,1580] [,1581] ## 1 658 671 424 428 387 355 315 361 377 ## 2 333 320 165 165 165 164 164 630 165 ## 3 165 165 567 563 604 637 677 165 614 ## [,1582] [,1583] [,1584] [,1585] [,1586] [,1587] [,1588] [,1589] [,1590] ## 1 362 380 349 329 375 685 385 340 297 ## 2 629 165 165 164 165 306 164 165 165 ## 3 165 611 642 663 616 165 607 651 694 ## [,1591] [,1592] [,1593] [,1594] [,1595] [,1596] [,1597] [,1598] [,1599] ## 1 690 317 669 419 397 370 333 390 364 ## 2 301 165 322 164 165 165 165 165 165 ## 3 165 674 165 573 594 621 658 601 627 ## [,1600] [,1601] [,1602] [,1603] [,1604] [,1605] [,1606] [,1607] [,1608] ## 1 345 312 383 624 392 375 356 719 320 ## 2 165 165 165 367 165 165 165 165 165 ## 3 646 679 608 165 599 616 635 272 671 ## [,1609] [,1610] [,1611] [,1612] [,1613] [,1614] [,1615] [,1616] [,1617] ## 1 316 333 299 393 337 282 275 288 380 ## 2 165 165 165 165 165 165 165 164 165 ## 3 675 658 692 598 654 709 716 704 611 ## [,1618] [,1619] [,1620] [,1621] [,1622] [,1623] [,1624] [,1625] [,1626] ## 1 435 438 413 410 398 573 392 413 396 ## 2 556 165 165 165 165 165 164 578 165 ## 3 165 553 578 581 593 418 600 165 595 ## [,1627] [,1628] [,1629] [,1630] [,1631] [,1632] [,1633] [,1634] [,1635] ## 1 623 446 412 607 439 412 403 397 426 ## 2 368 165 165 384 165 165 165 594 165 ## 3 165 545 579 165 552 579 588 165 565 ## [,1636] [,1637] [,1638] [,1639] [,1640] [,1641] [,1642] [,1643] [,1644] ## 1 444 582 450 580 482 496 466 428 412 ## 2 165 409 165 411 165 164 165 165 165 ## 3 547 165 541 165 509 496 525 563 579 ## [,1645] [,1646] [,1647] [,1648] [,1649] [,1650] [,1651] [,1652] [,1653] ## 1 384 642 420 353 347 324 306 404 434 ## 2 165 349 165 165 165 165 165 165 557 ## 3 607 165 571 638 644 667 685 587 165 ## [,1654] [,1655] [,1656] [,1657] [,1658] [,1659] [,1660] [,1661] [,1662] ## 1 417 397 365 378 332 370 394 377 419 ## 2 165 165 165 165 164 621 165 165 165 ## 3 574 594 626 613 660 165 597 614 572 ## [,1663] [,1664] [,1665] [,1666] [,1667] [,1668] [,1669] [,1670] [,1671] ## 1 606 409 589 407 391 416 603 437 377 ## 2 385 165 165 165 600 165 388 163 165 ## 3 165 582 402 584 165 575 165 556 614 ## [,1672] [,1673] [,1674] [,1675] [,1676] [,1677] [,1678] [,1679] [,1680] ## 1 365 374 444 440 419 395 373 386 345 ## 2 164 617 165 165 164 164 165 165 165 ## 3 627 165 547 551 573 597 618 605 646 ## [,1681] [,1682] [,1683] [,1684] [,1685] [,1686] [,1687] [,1688] [,1689] ## 1 332 293 673 376 434 622 438 427 616 ## 2 165 165 318 164 165 369 164 165 375 ## 3 659 698 165 616 557 165 554 564 165 ## [,1690] [,1691] [,1692] [,1693] [,1694] [,1695] [,1696] [,1697] [,1698] ## 1 707 723 335 423 390 391 628 372 413 ## 2 284 165 165 165 164 165 365 165 165 ## 3 165 268 656 568 602 600 163 619 578 ## [,1699] [,1700] [,1701] [,1702] [,1703] [,1704] [,1705] [,1706] [,1707] ## 1 640 411 401 364 436 415 653 400 371 ## 2 351 164 165 164 165 165 338 165 165 ## 3 165 581 590 628 555 576 165 591 620 ## [,1708] [,1709] [,1710] [,1711] [,1712] [,1713] [,1714] [,1715] [,1716] ## 1 373 689 368 414 460 448 605 422 394 ## 2 165 302 165 577 165 165 386 164 165 ## 3 618 165 623 165 531 543 165 570 597 ## [,1717] [,1718] [,1719] [,1720] [,1721] [,1722] [,1723] [,1724] [,1725] ## 1 421 377 637 441 448 430 450 411 415 ## 2 165 165 354 165 165 165 165 164 165 ## 3 570 614 165 550 543 561 541 581 576 ## [,1726] [,1727] [,1728] [,1729] [,1730] [,1731] [,1732] [,1733] [,1734] ## 1 410 594 404 656 377 364 657 390 361 ## 2 165 397 164 336 165 164 334 164 164 ## 3 581 165 588 164 614 628 165 602 631 ## [,1735] [,1736] [,1737] [,1738] [,1739] [,1740] [,1741] [,1742] [,1743] ## 1 412 668 423 374 370 367 354 343 361 ## 2 165 323 165 165 165 165 165 165 165 ## 3 579 165 568 617 621 624 637 648 630 ## [,1744] [,1745] [,1746] [,1747] [,1748] [,1749] [,1750] [,1751] [,1752] ## 1 424 605 396 351 308 694 709 381 316 ## 2 165 386 165 165 165 297 282 165 165 ## 3 567 165 595 640 683 165 165 610 675 ## [,1753] [,1754] [,1755] [,1756] [,1757] [,1758] [,1759] [,1760] [,1761] ## 1 678 372 351 678 389 354 316 399 382 ## 2 313 165 165 313 165 165 165 165 165 ## 3 165 619 640 165 602 637 675 592 609 ## [,1762] [,1763] [,1764] [,1765] [,1766] [,1767] [,1768] [,1769] [,1770] ## 1 424 402 403 633 648 402 625 412 604 ## 2 164 165 165 358 343 163 366 579 387 ## 3 568 589 588 165 165 591 165 165 165 ## [,1771] [,1772] [,1773] [,1774] [,1775] [,1776] [,1777] [,1778] [,1779] ## 1 411 417 393 373 327 684 702 337 656 ## 2 165 165 165 165 164 307 289 164 335 ## 3 580 574 598 618 665 165 165 655 165 ## [,1780] [,1781] [,1782] [,1783] [,1784] [,1785] [,1786] [,1787] [,1788] ## 1 400 362 685 366 338 399 347 619 369 ## 2 165 165 306 165 165 165 165 165 165 ## 3 591 629 165 625 653 592 644 372 622 ## [,1789] [,1790] [,1791] [,1792] [,1793] [,1794] [,1795] [,1796] [,1797] ## 1 630 390 351 675 375 407 627 412 364 ## 2 361 165 164 316 616 165 364 165 165 ## 3 165 601 641 165 165 584 165 579 627 ## [,1798] [,1799] [,1800] [,1801] [,1802] [,1803] [,1804] [,1805] [,1806] ## 1 345 314 303 278 644 645 360 400 426 ## 2 165 165 165 165 164 346 165 591 165 ## 3 646 677 688 713 348 165 631 165 565 ## [,1807] [,1808] [,1809] [,1810] [,1811] [,1812] [,1813] [,1814] [,1815] ## 1 387 385 683 411 373 339 328 406 403 ## 2 165 165 308 165 165 165 165 165 165 ## 3 604 606 165 580 618 652 663 585 588 ## [,1816] [,1817] [,1818] [,1819] [,1820] [,1821] [,1822] [,1823] [,1824] ## 1 405 367 421 396 343 396 646 400 374 ## 2 165 165 164 165 165 165 345 165 165 ## 3 586 624 571 595 648 595 165 591 617 ## [,1825] [,1826] [,1827] [,1828] [,1829] [,1830] [,1831] [,1832] [,1833] ## 1 641 421 594 438 414 409 587 677 385 ## 2 350 165 397 165 165 164 164 314 165 ## 3 165 570 165 553 577 583 405 165 606 ## [,1834] [,1835] [,1836] [,1837] [,1838] [,1839] [,1840] [,1841] [,1842] ## 1 377 648 383 382 347 418 637 372 350 ## 2 165 343 165 164 165 165 354 165 165 ## 3 614 165 608 610 644 573 165 619 641 ## [,1843] [,1844] [,1845] [,1846] [,1847] [,1848] [,1849] [,1850] [,1851] ## 1 316 387 367 445 405 396 390 426 588 ## 2 165 604 165 165 165 164 601 165 403 ## 3 675 165 624 546 586 596 165 565 165 ## [,1852] [,1853] [,1854] [,1855] [,1856] [,1857] [,1858] [,1859] [,1860] ## 1 418 398 644 428 395 390 615 401 600 ## 2 165 165 347 164 165 165 376 164 165 ## 3 573 593 165 564 596 601 165 591 391 ## [,1861] [,1862] [,1863] [,1864] [,1865] [,1866] [,1867] [,1868] [,1869] ## 1 674 358 316 404 360 321 590 405 369 ## 2 317 165 165 165 165 165 401 165 165 ## 3 165 633 675 587 631 670 165 586 622 ## [,1870] [,1871] [,1872] [,1873] [,1874] [,1875] [,1876] [,1877] [,1878] ## 1 337 326 310 369 370 406 412 427 381 ## 2 164 165 165 623 165 165 579 165 165 ## 3 655 665 681 164 621 585 165 564 610 ## [,1879] [,1880] [,1881] [,1882] [,1883] [,1884] [,1885] [,1886] [,1887] ## 1 327 591 456 441 411 454 456 545 470 ## 2 165 400 165 165 165 165 164 446 165 ## 3 664 165 535 550 580 537 536 165 521 ## [,1888] [,1889] [,1890] [,1891] [,1892] [,1893] [,1894] [,1895] [,1896] ## 1 471 432 432 394 368 635 441 594 422 ## 2 165 164 165 165 165 356 165 397 164 ## 3 520 560 559 597 623 165 550 165 570 ## [,1897] [,1898] [,1899] [,1900] [,1901] [,1902] [,1903] [,1904] [,1905] ## 1 618 475 625 420 636 379 638 369 348 ## 2 373 165 366 165 355 165 353 164 165 ## 3 165 516 165 571 165 612 165 623 643 ## [,1906] [,1907] [,1908] [,1909] [,1910] [,1911] [,1912] [,1913] [,1914] ## 1 678 376 345 343 358 685 374 360 348 ## 2 313 164 164 165 165 306 165 165 165 ## 3 165 616 647 648 633 165 617 631 643 ## [,1915] [,1916] [,1917] [,1918] [,1919] [,1920] [,1921] [,1922] [,1923] ## 1 408 390 353 649 394 369 664 700 366 ## 2 165 165 165 342 165 165 327 291 165 ## 3 583 601 638 165 597 622 165 165 625 ## [,1924] [,1925] [,1926] [,1927] [,1928] [,1929] [,1930] [,1931] [,1932] ## 1 331 329 339 369 378 367 648 414 636 ## 2 165 165 165 622 165 165 343 165 355 ## 3 660 662 652 165 613 624 165 577 165 ## [,1933] [,1934] [,1935] [,1936] [,1937] [,1938] [,1939] [,1940] [,1941] ## 1 404 405 385 666 388 381 619 411 361 ## 2 164 165 165 325 163 165 372 165 165 ## 3 588 586 606 165 605 610 165 580 630 ## [,1942] [,1943] [,1944] [,1945] [,1946] [,1947] [,1948] [,1949] [,1950] ## 1 340 424 364 373 359 340 705 348 654 ## 2 163 165 165 165 165 165 286 164 337 ## 3 653 567 627 618 632 651 165 644 165 ## [,1951] [,1952] [,1953] [,1954] [,1955] [,1956] [,1957] [,1958] [,1959] ## 1 425 416 405 353 704 743 308 265 269 ## 2 165 165 165 165 165 165 683 165 165 ## 3 566 575 586 638 287 248 165 726 722 ## [,1960] [,1961] [,1962] [,1963] [,1964] [,1965] [,1966] [,1967] [,1968] ## 1 726 342 300 733 351 319 380 598 652 ## 2 265 165 165 258 165 165 165 165 339 ## 3 165 649 691 165 640 672 611 393 165 ## [,1969] [,1970] [,1971] [,1972] [,1973] [,1974] [,1975] [,1976] [,1977] ## 1 697 401 385 430 456 461 434 427 604 ## 2 294 165 165 561 164 165 165 164 387 ## 3 165 590 606 165 536 530 557 565 165 ## [,1978] [,1979] [,1980] [,1981] [,1982] [,1983] [,1984] [,1985] [,1986] ## 1 460 452 465 455 454 430 501 468 458 ## 2 165 165 165 165 165 163 165 165 165 ## 3 531 539 526 536 537 563 490 523 533 ## [,1987] [,1988] [,1989] [,1990] [,1991] [,1992] [,1993] [,1994] [,1995] ## 1 426 442 419 396 463 509 484 434 680 ## 2 165 165 165 165 528 165 164 165 311 ## 3 565 549 572 595 165 482 508 557 165 ## [,1996] [,1997] [,1998] [,1999] [,2000] [,2001] [,2002] [,2003] [,2004] ## 1 397 377 374 655 418 397 366 412 375 ## 2 165 165 165 336 165 165 164 165 165 ## 3 594 614 617 165 573 594 626 579 616 ## [,2005] [,2006] [,2007] [,2008] [,2009] [,2010] [,2011] [,2012] [,2013] ## 1 666 681 387 672 392 406 377 440 611 ## 2 325 310 163 319 165 165 165 165 380 ## 3 165 165 606 165 599 585 614 551 165 ## [,2014] [,2015] [,2016] [,2017] [,2018] [,2019] [,2020] [,2021] [,2022] ## 1 478 437 527 478 466 580 443 470 484 ## 2 165 165 464 165 165 411 165 521 165 ## 3 513 554 165 513 525 165 548 165 507 ## [,2023] [,2024] [,2025] [,2026] [,2027] [,2028] [,2029] [,2030] [,2031] ## 1 472 425 416 380 355 332 328 405 388 ## 2 164 165 165 164 165 165 165 165 165 ## 3 520 566 575 612 636 659 663 586 603 ## [,2032] [,2033] [,2034] [,2035] [,2036] [,2037] [,2038] [,2039] [,2040] ## 1 636 405 478 455 591 438 427 451 462 ## 2 355 165 165 165 400 164 165 540 165 ## 3 165 586 513 536 165 554 564 165 529 ## [,2041] [,2042] [,2043] [,2044] [,2045] [,2046] [,2047] [,2048] [,2049] ## 1 435 476 469 474 478 493 433 427 604 ## 2 556 165 164 163 164 165 165 165 387 ## 3 165 515 523 519 514 498 558 564 165 ## [,2050] [,2051] [,2052] [,2053] [,2054] [,2055] [,2056] [,2057] [,2058] ## 1 432 658 425 401 413 386 400 423 465 ## 2 165 333 165 165 165 165 591 569 165 ## 3 559 165 566 590 578 605 165 164 526 ## [,2059] [,2060] [,2061] [,2062] [,2063] [,2064] [,2065] [,2066] [,2067] ## 1 422 391 364 334 374 402 395 644 405 ## 2 165 165 163 164 617 165 165 347 165 ## 3 569 600 629 658 165 589 596 165 586 ## [,2068] [,2069] [,2070] [,2071] [,2072] [,2073] [,2074] [,2075] [,2076] ## 1 372 317 663 416 427 419 363 322 636 ## 2 163 165 328 165 164 165 165 165 165 ## 3 621 674 165 575 565 572 628 669 355 ## [,2077] [,2078] [,2079] [,2080] [,2081] [,2082] [,2083] [,2084] [,2085] ## 1 349 325 680 363 681 370 656 661 393 ## 2 165 165 311 165 310 165 335 330 165 ## 3 642 666 165 628 165 621 165 165 598 ## [,2086] [,2087] [,2088] [,2089] [,2090] [,2091] [,2092] [,2093] [,2094] ## 1 677 387 440 422 411 446 459 437 399 ## 2 314 605 551 165 165 545 165 164 164 ## 3 165 164 165 569 580 165 532 555 593 ## [,2095] [,2096] [,2097] [,2098] [,2099] [,2100] [,2101] [,2102] [,2103] ## 1 354 331 325 311 733 336 686 351 331 ## 2 165 164 165 164 165 165 305 165 165 ## 3 637 661 666 681 258 655 165 640 660 ## [,2104] [,2105] [,2106] [,2107] [,2108] [,2109] [,2110] [,2111] [,2112] ## 1 319 701 363 293 704 402 352 595 388 ## 2 165 290 165 164 287 165 164 165 165 ## 3 672 165 628 699 165 589 640 396 603 ## [,2113] [,2114] [,2115] [,2116] [,2117] [,2118] [,2119] [,2120] [,2121] ## 1 364 352 328 327 324 337 650 379 326 ## 2 165 165 164 165 165 165 341 165 165 ## 3 627 639 664 664 667 654 165 612 665 ## [,2122] [,2123] [,2124] [,2125] [,2126] [,2127] [,2128] [,2129] [,2130] ## 1 317 411 390 351 432 397 472 449 453 ## 2 165 165 165 165 164 165 165 165 165 ## 3 674 580 601 640 560 594 519 542 538 ## [,2131] [,2132] [,2133] [,2134] [,2135] [,2136] [,2137] [,2138] [,2139] ## 1 431 397 450 414 653 437 381 353 401 ## 2 165 164 165 165 338 165 164 164 164 ## 3 560 595 541 577 165 554 611 639 591 ## [,2140] [,2141] [,2142] [,2143] [,2144] [,2145] [,2146] [,2147] [,2148] ## 1 351 302 278 701 782 366 368 408 386 ## 2 165 164 165 290 209 625 164 165 164 ## 3 640 690 713 165 165 165 624 583 606 ## [,2149] [,2150] [,2151] [,2152] [,2153] [,2154] [,2155] [,2156] [,2157] ## 1 378 667 388 399 333 309 685 415 370 ## 2 165 324 165 165 165 165 307 164 165 ## 3 613 165 603 592 658 682 164 577 621 ## [,2158] [,2159] [,2160] [,2161] [,2162] [,2163] [,2164] [,2165] [,2166] ## 1 383 660 366 376 647 371 352 314 394 ## 2 165 332 165 165 344 165 165 165 165 ## 3 608 164 625 615 165 620 639 677 597 ## [,2167] [,2168] [,2169] [,2170] [,2171] [,2172] [,2173] [,2174] [,2175] ## 1 417 433 416 427 404 401 412 358 651 ## 2 574 165 165 164 164 164 164 164 340 ## 3 165 558 575 565 588 591 580 634 165 ## [,2176] [,2177] [,2178] [,2179] [,2180] [,2181] [,2182] [,2183] [,2184] ## 1 376 344 399 349 399 392 389 358 375 ## 2 165 165 165 165 165 165 165 164 164 ## 3 615 647 592 642 592 599 602 634 617 ## [,2185] [,2186] [,2187] [,2188] [,2189] [,2190] [,2191] [,2192] [,2193] ## 1 316 679 402 368 349 324 333 373 343 ## 2 164 312 165 165 165 165 658 164 165 ## 3 676 165 589 623 642 667 165 619 648 ## [,2194] [,2195] [,2196] [,2197] [,2198] [,2199] [,2200] [,2201] [,2202] ## 1 419 370 656 440 403 420 417 389 661 ## 2 165 164 335 165 165 164 165 165 330 ## 3 572 622 165 551 588 572 574 602 165 ## [,2203] [,2204] [,2205] [,2206] [,2207] [,2208] [,2209] [,2210] [,2211] ## 1 606 406 370 343 298 686 370 657 351 ## 2 385 165 164 165 165 305 165 334 165 ## 3 165 585 622 648 693 165 621 165 640 ## [,2212] [,2213] [,2214] [,2215] [,2216] [,2217] [,2218] [,2219] [,2220] ## 1 321 393 455 428 643 411 406 384 593 ## 2 165 165 165 165 349 165 165 165 165 ## 3 670 598 536 563 164 580 585 607 398 ## [,2221] [,2222] [,2223] [,2224] [,2225] [,2226] [,2227] [,2228] [,2229] ## 1 423 413 416 409 355 439 419 417 619 ## 2 165 165 165 165 165 165 165 164 372 ## 3 568 578 575 582 636 552 572 575 165 ## [,2230] [,2231] [,2232] [,2233] [,2234] [,2235] [,2236] [,2237] [,2238] ## 1 619 457 464 433 406 418 408 360 321 ## 2 373 164 165 165 165 165 165 165 165 ## 3 164 535 527 558 585 573 583 631 670 ## [,2239] [,2240] [,2241] [,2242] [,2243] [,2244] [,2245] [,2246] [,2247] ## 1 435 613 445 435 424 633 415 620 423 ## 2 165 378 165 165 165 358 165 371 568 ## 3 556 165 546 556 567 165 576 165 165 ## [,2248] [,2249] [,2250] [,2251] [,2252] [,2253] [,2254] [,2255] [,2256] ## 1 455 479 457 500 547 531 622 456 485 ## 2 537 165 164 491 165 460 370 165 165 ## 3 164 512 535 165 444 165 164 535 506 ## [,2257] [,2258] [,2259] [,2260] [,2261] [,2262] [,2263] [,2264] [,2265] ## 1 439 420 387 360 428 360 648 400 638 ## 2 165 164 164 165 165 165 343 165 353 ## 3 552 572 605 631 563 631 165 591 165 ## [,2266] [,2267] [,2268] [,2269] [,2270] [,2271] [,2272] [,2273] [,2274] ## 1 411 389 361 420 442 402 447 398 621 ## 2 165 165 165 571 165 165 165 164 370 ## 3 580 602 630 165 549 589 544 594 165 ## [,2275] [,2276] [,2277] [,2278] [,2279] [,2280] [,2281] [,2282] [,2283] ## 1 400 372 365 390 668 382 445 409 364 ## 2 165 165 626 164 323 609 164 165 165 ## 3 591 619 165 602 165 165 547 582 627 ## [,2284] [,2285] [,2286] [,2287] [,2288] [,2289] [,2290] [,2291] [,2292] ## 1 416 373 452 451 454 441 624 499 547 ## 2 164 165 539 165 163 165 367 492 165 ## 3 576 618 165 540 539 550 165 165 444 ## [,2293] [,2294] [,2295] [,2296] [,2297] [,2298] [,2299] [,2300] [,2301] ## 1 521 514 504 520 505 481 467 612 432 ## 2 165 165 165 165 164 165 165 379 165 ## 3 470 477 487 471 487 510 524 165 559 ## [,2302] [,2303] [,2304] [,2305] [,2306] [,2307] [,2308] [,2309] [,2310] ## 1 423 417 416 618 423 372 379 379 364 ## 2 165 165 165 373 165 164 612 165 165 ## 3 568 574 575 165 568 620 165 612 627 ## [,2311] [,2312] [,2313] [,2314] [,2315] [,2316] [,2317] [,2318] [,2319] ## 1 369 342 684 341 319 684 349 340 381 ## 2 623 165 307 165 164 307 165 165 165 ## 3 164 649 165 650 673 165 642 651 610 ## [,2320] [,2321] [,2322] [,2323] [,2324] [,2325] [,2326] [,2327] [,2328] ## 1 401 450 438 404 614 414 390 663 370 ## 2 590 165 165 163 377 165 164 328 165 ## 3 165 541 553 589 165 577 602 165 621 ## [,2329] [,2330] [,2331] [,2332] [,2333] [,2334] [,2335] [,2336] [,2337] ## 1 330 305 331 399 618 423 379 367 671 ## 2 165 165 165 592 373 164 165 165 320 ## 3 661 686 660 165 165 569 612 624 165 ## [,2338] [,2339] [,2340] [,2341] [,2342] [,2343] [,2344] [,2345] [,2346] ## 1 354 327 603 342 648 395 428 369 416 ## 2 165 165 165 165 343 165 165 165 575 ## 3 637 664 388 649 165 596 563 622 165 ## [,2347] [,2348] [,2349] [,2350] [,2351] [,2352] [,2353] [,2354] [,2355] ## 1 382 654 385 391 431 406 430 384 376 ## 2 165 337 165 601 165 165 165 165 165 ## 3 609 165 606 164 560 585 561 607 615 ## [,2356] [,2357] [,2358] [,2359] [,2360] [,2361] [,2362] [,2363] [,2364] ## 1 653 402 426 396 368 414 380 360 453 ## 2 338 164 165 165 164 165 165 165 165 ## 3 165 590 565 595 624 577 611 631 538 ## [,2365] [,2366] [,2367] [,2368] [,2369] [,2370] [,2371] [,2372] [,2373] ## 1 418 372 524 472 442 421 384 358 313 ## 2 165 165 467 165 165 165 165 165 165 ## 3 573 619 165 519 549 570 607 633 678 ## [,2374] [,2375] [,2376] [,2377] [,2378] [,2379] [,2380] [,2381] [,2382] ## 1 375 383 384 328 397 370 328 394 354 ## 2 616 165 165 165 165 165 165 165 165 ## 3 165 608 607 663 594 621 663 597 637 ## [,2383] [,2384] [,2385] [,2386] [,2387] [,2388] [,2389] [,2390] [,2391] ## 1 667 395 392 413 430 418 412 461 425 ## 2 324 164 599 165 165 165 165 165 165 ## 3 165 597 165 578 561 573 579 530 566 ## [,2392] [,2393] [,2394] [,2395] [,2396] [,2397] [,2398] [,2399] [,2400] ## 1 488 460 427 607 472 430 397 342 436 ## 2 165 165 165 384 165 165 165 165 165 ## 3 503 531 564 165 519 561 594 649 555 ## [,2401] [,2402] [,2403] [,2404] [,2405] [,2406] [,2407] [,2408] [,2409] ## 1 405 441 377 433 405 450 585 446 405 ## 2 164 165 165 165 165 165 406 165 165 ## 3 587 550 614 558 586 541 165 545 586 ## [,2410] [,2411] [,2412] [,2413] [,2414] [,2415] [,2416] [,2417] [,2418] ## 1 373 650 446 441 410 660 395 428 430 ## 2 165 341 165 165 163 332 165 165 165 ## 3 618 165 545 550 583 164 596 563 561 ## [,2419] [,2420] [,2421] [,2422] [,2423] [,2424] [,2425] [,2426] [,2427] ## 1 396 348 313 678 395 359 423 406 436 ## 2 165 165 165 313 165 165 165 165 165 ## 3 595 643 678 165 596 632 568 585 555 ## [,2428] [,2429] [,2430] [,2431] [,2432] [,2433] [,2434] [,2435] [,2436] ## 1 367 658 658 688 348 682 389 382 661 ## 2 165 333 334 304 165 309 165 165 332 ## 3 624 165 164 164 643 165 602 609 163 ## [,2437] [,2438] [,2439] [,2440] [,2441] [,2442] [,2443] [,2444] [,2445] ## 1 365 355 378 400 369 417 463 437 438 ## 2 165 165 613 165 165 574 165 165 165 ## 3 626 636 165 591 622 165 528 554 553 ## [,2446] [,2447] [,2448] [,2449] [,2450] [,2451] [,2452] [,2453] [,2454] ## 1 426 496 493 486 465 432 409 397 348 ## 2 164 165 165 164 165 165 165 165 165 ## 3 566 495 498 506 526 559 582 594 643 ## [,2455] [,2456] [,2457] [,2458] [,2459] [,2460] [,2461] [,2462] [,2463] ## 1 379 408 402 381 626 639 421 661 417 ## 2 612 583 165 165 365 352 165 330 165 ## 3 165 165 589 610 165 165 570 165 574 ## [,2464] [,2465] [,2466] [,2467] [,2468] [,2469] [,2470] [,2471] [,2472] ## 1 411 374 378 351 342 416 444 421 370 ## 2 165 165 165 165 165 165 165 165 164 ## 3 580 617 613 640 649 575 547 570 622 ## [,2473] [,2474] [,2475] [,2476] [,2477] [,2478] [,2479] [,2480] [,2481] ## 1 463 446 442 407 357 368 352 638 380 ## 2 165 165 164 165 165 164 165 353 165 ## 3 528 545 550 584 634 624 639 165 611 ## [,2482] [,2483] [,2484] [,2485] [,2486] [,2487] [,2488] [,2489] [,2490] ## 1 365 314 352 379 375 437 451 437 650 ## 2 165 164 639 165 616 554 165 165 341 ## 3 626 678 165 612 165 165 540 554 165 ## [,2491] [,2492] [,2493] [,2494] [,2495] [,2496] [,2497] [,2498] [,2499] ## 1 649 730 335 276 398 354 327 306 675 ## 2 342 261 164 165 165 165 165 165 316 ## 3 165 165 657 715 593 637 664 685 165 ## [,2500] ## 1 745 ## 2 246 ## 3 165 PosteriorSIGMA &lt;- matrix(NA, length(keep), NClus) l &lt;- 1 for (s in keep){ PosteriorSIGMA[l,] &lt;- PostSigma[[s]][1:NClus] l &lt;- l + 1 } summary(coda::mcmc(PosteriorSIGMA)) ## ## Iterations = 1:2500 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2500 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.7836 0.2280 0.004559 0.03482 ## [2,] 0.5113 0.4391 0.008783 0.21018 ## [3,] 0.9277 0.3460 0.006920 0.02735 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.494983 0.60264 0.6959 1.0058 1.219 ## var2 0.002274 0.00252 0.6344 0.8454 1.227 ## var3 0.002377 0.91677 1.0471 1.1205 1.261 plot(coda::mcmc(PosteriorSIGMA)) l &lt;- 1 for (s in keep){ PosteriorSIGMA[l,] &lt;- PostSigma[[s]][1:NClus] l &lt;- l + 1 } summary(coda::mcmc(PosteriorSIGMA)) ## ## Iterations = 1:2500 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2500 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## [1,] 0.7836 0.2280 0.004559 0.03482 ## [2,] 0.5113 0.4391 0.008783 0.21018 ## [3,] 0.9277 0.3460 0.006920 0.02735 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## var1 0.494983 0.60264 0.6959 1.0058 1.219 ## var2 0.002274 0.00252 0.6344 0.8454 1.227 ## var3 0.002377 0.91677 1.0471 1.1205 1.261 plot(coda::mcmc(PosteriorSIGMA)) PosteriorBeta1 &lt;- matrix(NA, length(keep), k) j &lt;- 1 for(s in keep){ PosteriorBeta1[j,] &lt;- PostBetas[[s]][,1] j &lt;- j + 1 } colnames(PosteriorBeta1) &lt;- c(&quot;Ct&quot;, names(Data[,-1])) HDI &lt;- HDInterval::hdi(PosteriorBeta1, credMass = 0.95) print(summary(coda::mcmc(PosteriorBeta1))) ## ## Iterations = 1:2500 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2500 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Ct 0.405683 1.1074976 2.215e-02 0.0221500 ## LogPriceMarijuana -0.171252 0.3121297 6.243e-03 0.0658276 ## LogPriceCocaine 0.039528 0.1641623 3.283e-03 0.0153124 ## LogPriceCrack 0.288845 0.1537906 3.076e-03 0.0066270 ## YearsEducation -0.072525 0.0290163 5.803e-04 0.0037453 ## Age 0.077693 0.0699418 1.399e-03 0.0149658 ## Age2 -0.001084 0.0009667 1.933e-05 0.0001371 ## Dealer 0.145768 0.2862052 5.724e-03 0.0461187 ## Female -0.246570 0.3913882 7.828e-03 0.0325345 ## PhysicalHealthGood -0.039570 0.2757394 5.515e-03 0.0075938 ## MentalHealthGood 0.037232 0.2571613 5.143e-03 0.0207817 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Ct -1.345787 -0.345119 0.212901 0.9629155 3.034e+00 ## LogPriceMarijuana -0.870160 -0.421030 -0.033462 0.0826315 1.975e-01 ## LogPriceCocaine -0.190178 -0.080900 -0.001156 0.1418116 4.445e-01 ## LogPriceCrack 0.050908 0.202496 0.262900 0.3396974 6.985e-01 ## YearsEducation -0.135629 -0.087848 -0.067903 -0.0552094 -2.314e-02 ## Age -0.009011 0.025112 0.055832 0.1167922 2.484e-01 ## Age2 -0.003469 -0.001651 -0.000756 -0.0003767 9.087e-05 ## Dealer -0.369594 -0.004654 0.101844 0.2816789 8.191e-01 ## Female -1.251837 -0.464579 -0.150631 -0.0302261 4.238e-01 ## PhysicalHealthGood -0.693888 -0.185913 -0.047068 0.1255614 4.891e-01 ## MentalHealthGood -0.439235 -0.096817 -0.008959 0.1144303 7.317e-01 plot(coda::mcmc(PosteriorBeta1)) PosteriorBeta2 &lt;- matrix(NA, length(keep), k) j &lt;- 1 for(s in keep){ PosteriorBeta2[j,] &lt;- PostBetas[[s]][,2] j &lt;- j + 1 } colnames(PosteriorBeta2) &lt;- c(&quot;Ct&quot;, names(Data[,-1])) HDI &lt;- HDInterval::hdi(PosteriorBeta2, credMass = 0.95) print(summary(coda::mcmc(PosteriorBeta2))) ## ## Iterations = 1:2500 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2500 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Ct 0.1903787 0.9095578 1.819e-02 0.0227965 ## LogPriceMarijuana -0.1195767 0.2463217 4.926e-03 0.0445382 ## LogPriceCocaine 0.0396846 0.1435888 2.872e-03 0.0120810 ## LogPriceCrack 0.1613809 0.1833913 3.668e-03 0.0442256 ## YearsEducation -0.0471415 0.0445695 8.914e-04 0.0200420 ## Age 0.0549649 0.0677576 1.355e-03 0.0223704 ## Age2 -0.0007621 0.0009622 1.924e-05 0.0002594 ## Dealer 0.1363190 0.2786775 5.574e-03 0.0402224 ## Female -0.1752409 0.4314105 8.628e-03 0.0441923 ## PhysicalHealthGood 0.0199015 0.2492203 4.984e-03 0.0083112 ## MentalHealthGood 0.0286076 0.2274205 4.548e-03 0.0105278 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Ct -1.3578660 -1.527e-01 5.038e-04 3.823e-01 2.549e+00 ## LogPriceMarijuana -0.7842976 -1.857e-01 -3.040e-04 3.135e-04 1.534e-01 ## LogPriceCocaine -0.1701218 -1.394e-02 6.614e-05 6.630e-02 4.163e-01 ## LogPriceCrack -0.0007425 7.722e-05 1.386e-01 2.704e-01 6.033e-01 ## YearsEducation -0.1326593 -8.036e-02 -5.286e-02 -2.420e-05 1.189e-04 ## Age -0.0002134 3.198e-05 3.487e-02 8.557e-02 2.318e-01 ## Age2 -0.0033290 -1.151e-03 -4.554e-04 -3.901e-07 3.133e-06 ## Dealer -0.2290362 -1.668e-04 9.359e-04 2.549e-01 9.312e-01 ## Female -1.4336419 -1.841e-01 -4.225e-04 3.486e-04 2.337e-01 ## PhysicalHealthGood -0.6426674 -1.396e-03 1.934e-04 1.590e-01 4.763e-01 ## MentalHealthGood -0.3545483 -4.158e-02 -4.153e-05 3.424e-02 7.264e-01 plot(coda::mcmc(PosteriorBeta2)) PosteriorBeta3 &lt;- matrix(NA, length(keep), k) j &lt;- 1 for(s in keep){ PosteriorBeta3[j,] &lt;- PostBetas[[s]][,3] j &lt;- j + 1 } colnames(PosteriorBeta3) &lt;- c(&quot;Ct&quot;, names(Data[,-1])) HDI &lt;- HDInterval::hdi(PosteriorBeta3, credMass = 0.95) print(summary(coda::mcmc(PosteriorBeta3))) ## ## Iterations = 1:2500 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 2500 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Ct 1.370371 1.1773368 2.355e-02 5.772e-02 ## LogPriceMarijuana -0.319820 0.2179452 4.359e-03 1.588e-02 ## LogPriceCocaine 0.159830 0.1469832 2.940e-03 9.052e-03 ## LogPriceCrack 0.289903 0.1873586 3.747e-03 1.140e-02 ## YearsEducation -0.060389 0.0385408 7.708e-04 3.714e-03 ## Age 0.093644 0.0604804 1.210e-03 6.929e-03 ## Age2 -0.001405 0.0008818 1.764e-05 9.683e-05 ## Dealer 0.039276 0.3164144 6.328e-03 1.139e-02 ## Female -0.525653 0.5067244 1.013e-02 5.661e-02 ## PhysicalHealthGood -0.074274 0.2561935 5.124e-03 6.584e-03 ## MentalHealthGood 0.022416 0.2761121 5.522e-03 4.765e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Ct -0.8358587 0.350746 1.4344783 2.2421805 3.521e+00 ## LogPriceMarijuana -0.8508963 -0.423312 -0.3270284 -0.2133991 4.863e-02 ## LogPriceCocaine -0.1026361 0.048694 0.1583187 0.2503828 4.614e-01 ## LogPriceCrack -0.0082786 0.187685 0.2997469 0.3936415 6.953e-01 ## YearsEducation -0.1481121 -0.079640 -0.0598300 -0.0405261 9.547e-05 ## Age -0.0001286 0.057777 0.0919203 0.1252678 2.340e-01 ## Age2 -0.0033705 -0.001867 -0.0014015 -0.0008891 1.732e-06 ## Dealer -0.6343830 -0.083594 0.0003901 0.1370651 7.892e-01 ## Female -1.9906335 -0.668074 -0.4900828 -0.2600554 2.073e-01 ## PhysicalHealthGood -0.6205275 -0.208951 -0.0639384 0.0345855 4.939e-01 ## MentalHealthGood -0.5380165 -0.092068 0.0002416 0.1090661 7.452e-01 plot(coda::mcmc(PosteriorBeta3)) DataElast &lt;- data.frame(Marijuana = PosteriorBeta1[,2], Cocaine = PosteriorBeta1[,3], Crack = PosteriorBeta1[,4]) dens1 &lt;- ggplot(DataElast, aes(x = Marijuana)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(x = &quot;Marijuana&quot;, y = &quot;Density&quot;) + theme_minimal() dens2 &lt;- ggplot(DataElast, aes(x = Cocaine)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(x = &quot;Cocaine&quot;, y = &quot;Density&quot;) + theme_minimal() dens3 &lt;- ggplot(DataElast, aes(x = Crack)) + geom_density(fill = &quot;blue&quot;, alpha = 0.3) + labs(x = &quot;Crack&quot;, y = &quot;Density&quot;) + theme_minimal() library(ggpubr) ggarrange(dens1, dens2, dens3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), ncol = 3, nrow = 1, legend = &quot;bottom&quot;, common.legend = TRUE) In this application, there are three clusters after the burn-in period, with individuals in the sample relatively evenly allocated between them. The posterior density plots of the elasticities associated with the three clusters appear very similar, which implies that the posterior draws associated with each cluster are targeting the same posterior distribution. Consequently, the figure presents the posterior density estimates of the demand price elasticities of marijuana in Colombia for the first cluster. Panel A of the figure shows that the own-price elasticity of marijuana demand is bimodal. The first mode suggests that some individuals have an elasticity near -0.5, meaning that a 10% increase in the price of marijuana leads to a 5% decrease in their demand. The second mode is near 0, indicating that these individuals do not adjust their marijuana consumption in response to price changes. Panel B presents the cross-price elasticity of marijuana demand with respect to the price of cocaine. This posterior distribution is also bimodal. The first mode suggests that some individuals do not change their marijuana demand in response to cocaine price variations. The second mode indicates that marijuana and cocaine are substitutes: a 10% increase in the price of cocaine leads to an approximately 2.5% increase in marijuana demand. Panel C shows that marijuana and crack are also substitutes. This posterior distribution is unimodal, indicating that most individuals increase their marijuana consumption when the price of crack rises. On average, a 10% increase in the price of crack results in an approximately 3% increase in marijuana demand. These results seem plausible. However, they should be interpreted with caution, as potential endogeneity issues may affect the estimates. References "],["sec11_2.html", "11.2 Splines", " 11.2 Splines Another common approach to performing non-parametric or semi-parametric inference is using splines. These are piecewise polynomial functions that allow for approximating complex relationships in data. To clarify ideas, let’s begin with a simple univariate case. The starting point is to assume that \\[ y_i = f(x_{i}) + \\mu_i, \\] where \\(\\mu_i \\sim N(0, \\sigma^2)\\) is independent of the regressor \\(x_{i}\\), and \\(f(x_{i})\\) is a smooth function such that nearby points in the support of \\(x_{i}\\) should have similar values. Observe that this means we must always sort the data in increasing order according to \\(x_i\\). This has no effect, as we assume a random sample. The index \\(i = 1,2,\\dots, N\\) represents the observations. The spline approach defines \\[\\begin{align} f(x_{i}) = \\beta_{1}b_1(x_{i}) + \\beta_{2}b_2(x_{i}) + \\dots + \\beta_{H}b_H(x_{i}) = \\boldsymbol{b}(x_{i})^{\\top}\\boldsymbol{\\beta}, \\tag{11.1} \\end{align}\\] where \\(\\boldsymbol{\\beta} = [\\beta_{1} \\ \\beta_{2} \\ \\dots \\ \\beta_{H}]^{\\top}\\) and \\(\\boldsymbol{b}(x_{i}) = [b_1(x_{i}) \\ b_2(x_{i}) \\ \\dots \\ b_H(x_{i})]^{\\top}\\) are the vector of parameters and the vector of spline basis (B-splines) function values at \\(x_{i}\\), respectively. Note that in this setting, there is only one regressor; however, there are \\(H\\) location parameters. Given \\(N\\) observations, the design matrix is: \\[\\begin{align*} \\boldsymbol{W}=\\begin{bmatrix} b_1(x_{1}) &amp; b_2(x_{1}) &amp; \\dots &amp; b_H(x_{1})\\\\ b_1(x_{2}) &amp; b_2(x_{2}) &amp; \\dots &amp; b_H(x_{2})\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_1(x_{N}) &amp; b_2(x_{N}) &amp; \\dots &amp; b_H(x_{N})\\\\ \\end{bmatrix}. \\end{align*}\\] Thus, we have a standard linear model where we can proceed as in Chapter 3. Actually, Equation (11.1) represents the basis function approach, where \\(\\boldsymbol{b}(x_{i})\\) is a specified set of basis functions, such as B-splines. However, other functions derived from the Fourier transform, Gaussian radial basis functions, wavelets, etc., can also be used. Here, we focus on the most popular case: cubic B-splines. However, the same ideas apply to other basis functions. In particular, cubic B-splines provide the lowest degree necessary to generate sufficiently smooth curves; the first and second derivatives are nonzero, which is not the case with constant and linear splines (Martin, Kumar, and Lao 2021). Thus, from Equation (11.1), we see that a spline is a linear combination of \\(H\\) B-splines. The latter are polynomials of a certain degree, meaning that splines are piecewise polynomial functions. Any spline function of order \\(n\\) can be expressed as a linear combination of B-splines of order \\(n\\), and B-splines serve as the basis functions for the spline function space. The cubic B-spline is \\[\\begin{align*} b_{h,3}(x_i)=\\begin{Bmatrix} \\frac{u^3}{6} &amp; x_h \\leq x_i &lt; x_{h+1}, &amp; u=(x_i-x_h)/\\delta\\\\ \\frac{1+3u+3u^2-3u^3}{6} &amp; x_{h+1} \\leq x_i &lt; x_{h+2}, &amp; u=(x_i-x_{h+1})/\\delta\\\\ \\frac{4-6u^2+3u^3}{6} &amp; x_{h+2} \\leq x_i &lt; x_{h+3}, &amp; u=(x_i-x_{h+2})/\\delta\\\\ \\frac{1-3u+3u^2-3u^3}{6} &amp; x_{h+3} \\leq x_i &lt; x_{h+4}, &amp; u=(x_i-x_{h+3})/\\delta\\\\ 0 &amp; \\text{otherwise} \\end{Bmatrix}, \\end{align*}\\] where \\(x_{h+k}\\) are called the knots, for \\(k=0,1,\\dots,4\\). These are the points in the domain of the function where the pieces of the spline join, satisfying \\(x_{h+k} \\leq x_{h+k+1}\\). The knots control the shape and flexibility of the spline curve; more knots imply greater flexibility but less smoothness. Note that \\(\\delta\\) determines the distance between the knots, and that this expression clearly shows that the cubic B-spline is a piecewise polynomial function. The following code implements this function in R using a delta of 1.5 and 5 knots \\(\\left\\{2, 3.5, 5, 6.5, 8\\right\\}\\). The knots \\(\\left\\{2, 8\\right\\}\\) are called boundary knots, which can be given by the range of \\(x\\), and the knots \\(\\left\\{3.5, 5, 6.5\\right\\}\\) are called internal knots. B-splines with evenly distributed knots are called uniform B-splines. However, this is not always the case. The figure compares the results of our own function (black circles) with those obtained using the bs function from the splines package (red line). SplineOwn &lt;- function(x, knots, delta){ if(knots[1] &lt;= x &amp; x &lt; knots[2]){ u &lt;- (x - knots[1])/delta b &lt;- u^3/6 }else{ if(knots[2] &lt;= x &amp; x &lt; knots[3]){ u &lt;- (x - knots[2])/delta b &lt;- (1/6)*(1 + 3*u + 3*u^2 - 3*u^3) }else{ if(knots[3] &lt;= x &amp; x &lt; knots[4]){ u &lt;- (x - knots[3])/delta b &lt;- (1/6)*(4 - 6*u^2 + 3*u^3) }else{ if(knots[4] &lt;= x &amp; x &lt; knots[5]){ u &lt;- (x - knots[4])/delta b &lt;- (1/6)*(1 - 3*u + 3*u^2 - u^3) }else{ b &lt;- 0 } } } } return(b) } delta &lt;- 1.5 knotsA &lt;- seq(2, 8, delta) xA &lt;- seq(2, 8, 0.1) Ens &lt;- sapply(xA, function(xi) {SplineOwn(xi, knots = knotsA, delta = delta)}) plot(xA, Ens, xlab = &quot;x&quot;, ylab = &quot;B-spline&quot;, main = &quot;Cubic B-spline comparison: own function vs bs&quot;) require(splines) ## Loading required package: splines BSfunc &lt;- bs(xA, knots = knotsA, degree = 3) lines(xA, BSfunc[,4], col = &quot;red&quot;) The figure shows a particular cubic B-spline, but \\(f(x_i)\\) in Equation (11.1) involves \\(H\\) splines over the range of potential values of \\(x\\), as we need to evaluate the splines centered at different knots. Here, we should clarify that to avoid issues with the use of splines at boundary regions, it is essential to artificially increase the number of boundary knots by repeating these knots as many times as the degree of the polynomial. For instance, in our example, the final set of knots is \\(\\left\\{2, 2, 2, 2, 3.5, 5, 6.5, 8, 8, 8, 8\\right\\}\\). The set of B-splines can be constructed using the Cox–de Boor recursion formula. The starting point is the B-spline of degree 0, \\[\\begin{align*} b_{h,0}(x_i)=\\begin{Bmatrix} 1 &amp; x_h \\leq x_i &lt; x_{h+1}, &amp; u=(x_i-x_h)/\\delta\\\\ 0 &amp; \\text{otherwise} \\end{Bmatrix}, \\end{align*}\\] and then, the other B-splines are defined by the recursion \\[\\begin{align} b_{h,p}(x_i)=\\frac{x_i-x_{h}}{x_{h+p}-x_h}b_{h,p-1}(x_i)+\\frac{x_{h+p+1}-x_i}{x_{h+p+1}-x_{h+1}}b_{h+1,p-1}(x_i). \\end{align}\\] Note that B-splines are defined by their degree and their knots. The following code demonstrates how to perform this recursion using the same settings as in the previous code and compares the results with the bs function from the splines package. As shown in the figure, we obtain the same results. We will continue using the bs function from the splines package for convenience in the process, but it seems that we have gained some understanding of the underlying process. cubic_bspline &lt;- function(x, knots, degree = 3) { extended_knots &lt;- c(rep(knots[1], degree), knots, rep(knots[length(knots)], degree)) num_basis &lt;- length(knots) + degree - 1 basis_matrix &lt;- matrix(0, nrow = length(x), ncol = num_basis) # Function to compute B-spline basis recursively b_spline_basis &lt;- function(x, degree, i, knots) { if (degree == 0) { return(ifelse(x &gt;= knots[i] &amp; x &lt; knots[i + 1], 1, 0)) } else { left_num &lt;- (x - knots[i]) left_den &lt;- (knots[i + degree] - knots[i]) left &lt;- ifelse(left_den != 0, (left_num / left_den) * b_spline_basis(x, degree - 1, i, knots), 0) right_num &lt;- (knots[i + degree + 1] - x) right_den &lt;- (knots[i + degree + 1] - knots[i + 1]) right &lt;- ifelse(right_den != 0, (right_num / right_den) * b_spline_basis(x, degree - 1, i + 1, knots), 0) return(left + right) } } for (i in 1:num_basis) { basis_matrix[, i] &lt;- sapply(x, function(xi) b_spline_basis(xi, degree, i, extended_knots)) } if(x[length(x)] == knots[length(knots)]){ basis_matrix[length(x), num_basis] &lt;- 1 } return(basis_matrix) } delta &lt;- 1.5 knotsA &lt;- seq(2, 8, delta) xA &lt;- seq(2, 8, 0.1) basis_matrix &lt;- cubic_bspline(xA, knots = knotsA, degree = 3) library(splines) bs_matrix &lt;- bs(xA, knots = knotsA[-c(1, length(knotsA))], degree = 3, intercept = TRUE, Boundary.knots = range(knotsA)) bs_matrix_matrix &lt;- as.matrix(bs_matrix) par(mfrow = c(1,2)) matplot(xA, basis_matrix, type = &quot;l&quot;, lty = 1, col = rainbow(ncol(basis_matrix)), ylab = &quot;B-spline Basis&quot;, xlab = &quot;x&quot;, main = &quot;Own function&quot;) matplot(xA, bs_matrix_matrix, type = &quot;l&quot;, lty = 1, col = rainbow(ncol(basis_matrix)), ylab = &quot;B-spline Basis&quot;, xlab = &quot;x&quot;, main = &quot;bs function&quot;) The idea in splines is to calculate the linear combination of the \\(H\\) B-splines, as those shown in the figure, that bets fit the dependent variable. Let’s perform a simulation exercise to fix ideas. Splines can also be extended to non-linear models based on data augmentation, such as the probit and tobit models. Again, the basic idea is to incorporate splines into the implicit latent variables. Koop (2003) presents these extensions. Simulation exercise: Splines Let’s assume that the data generating process is \\[\\begin{align*} y_i &amp; = 0.4 + 0.25\\sin(8x_i - 5) + 0.4\\exp(-16(4x_i - 2.5)^2) + \\mu_i, \\end{align*}\\] where \\(\\mu_i \\sim N(0,0.15^2)\\), and \\(x_i\\) is a sequence in \\((0,1)\\), with 100 random draws of the pairs \\((x_i, y_i)\\). In addition, we choose the knots \\(\\left\\{0, 0.25, 0.5, 0.75, 1\\right\\}\\). We then calculate the cubic B-splines; however, we should take into account that the last two B-spline columns generate multicollinearity issues. Therefore, we exclude them when generating random realizations of the splines by drawing the coefficients from \\(N(0, 0.35^2)\\). The intercept is fixed to maintain the scale in the figure, where we observe the data points, \\(f(x)\\), and the different splines associated with various random realizations of the coefficients. The following code demonstrates how to perform this simulation. ########### Simulation: B-splines ########### rm(list = ls()) library(ggplot2); library(splines) set.seed(010101) x &lt;- seq(0, 1, 0.001) ysignal &lt;- 0.4 + 0.25*sin(8*x - 5) + 0.4*exp(-16*(4*x - 2.5)^2) sig &lt;- 0.15 e &lt;- rnorm(length(ysignal), 0, sd = sig) y &lt;- ysignal + e N &lt;- 100 ids &lt;- sort(sample(1:length(ysignal), N)) xobs &lt;- x[ids] yobs &lt;- y[ids] knots &lt;- seq(0, 1, 0.25) BS &lt;- bs(xobs, knots = knots, degree = 3, Boundary.knots = range(x), intercept = FALSE) # Splines Spline1 &lt;- 0.56 + BS[,-c(7:8)] %*% rnorm(6, 0, 0.35) Spline2 &lt;- 0.56 + BS[,-c(7:8)] %*% rnorm(6, 0, 0.35) Spline3 &lt;- 0.56 + BS[,-c(7:8)] %*% rnorm(6, 0, 0.35) Spline4 &lt;- 0.56 + BS[,-c(7:8)] %*% rnorm(6, 0, 0.35) # Create data frames for the true signal, observed data, and Splines data_true_signal &lt;- data.frame(x = x, y = ysignal, Type = &quot;True Signal&quot;) data_obs &lt;- data.frame(x = xobs, y = yobs, Type = &quot;Observed Data&quot;) # Create separate data frames for each Spline data_Spline1 &lt;- data.frame(x = xobs, y = Spline1, Type = &quot;Spline 1&quot;) data_Spline2 &lt;- data.frame(x = xobs, y = Spline2, Type = &quot;Spline 2&quot;) data_Spline3 &lt;- data.frame(x = xobs, y = Spline3, Type = &quot;Spline 3&quot;) data_Spline4 &lt;- data.frame(x = xobs, y = Spline4, Type = &quot;Spline 4&quot;) data &lt;- rbind(data_true_signal, data_obs, data_Spline1, data_Spline2, data_Spline3, data_Spline4) ggplot(data, aes(x = x, y = y)) + geom_line(data = subset(data, Type == &quot;True Signal&quot;), aes(color = &quot;True Signal&quot;), linewidth = 1) + geom_point(data = subset(data, Type == &quot;Observed Data&quot;), aes(color = &quot;Observed Data&quot;), shape = 16) + geom_line(data = subset(data, Type == &quot;Spline 1&quot;), aes(color = &quot;Splines&quot;), linewidth = 1, linetype = &quot;solid&quot;) + geom_line(data = subset(data, Type == &quot;Spline 2&quot;), aes(color = &quot;Splines&quot;), linewidth = 1, linetype = &quot;solid&quot;) + geom_line(data = subset(data, Type == &quot;Spline 3&quot;), aes(color = &quot;Splines&quot;), linewidth = 1, linetype = &quot;solid&quot;) + geom_line(data = subset(data, Type == &quot;Spline 4&quot;), aes(color = &quot;Splines&quot;), linewidth = 1, linetype = &quot;solid&quot;) + scale_color_manual(values = c(&quot;True Signal&quot; = &quot;black&quot;, &quot;Observed Data&quot; = &quot;red&quot;, &quot;Splines&quot; = &quot;blue&quot;)) + labs(y = &quot;y&quot;, color = &quot;Legend&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) We can also use this simulation to examine the effect of changing the knots. The following code performs the fit using least squares, which is equivalent to using a non-informative prior in a Bayesian linear regression framework. The figure illustrates the fit of different splines based on varying numbers of knots. We observe that increasing the number of knots enhances the flexibility of the spline, providing better local control. However, too many knots can result in a wiggly, overly complex fit that captures noise rather than the true underlying trend and may also increase multicollinearity. Therefore, selecting an appropriate number of knots is a crucial aspect of spline modeling. To address this issue, we can apply the strategies discussed in Chapter 10, and the strategies that will be discussed in the section of regularization in Chapter 12. rm(list = ls()) library(ggplot2); library(splines) # Data generation set.seed(10101) x &lt;- seq(0, 1, 0.001) ysignal &lt;- 0.4 + 0.25*sin(8*x - 5) + 0.4*exp(-16*(4*x - 2.5)^2) sig &lt;- 0.15; e &lt;- rnorm(length(ysignal), 0, sd = sig) y &lt;- ysignal + e; N &lt;- 100 ids &lt;- sort(sample(1:length(ysignal), N)) xobs &lt;- x[ids] yobs &lt;- y[ids] # Generate Fits with different knot placements knots_list &lt;- list(seq(0, 1, 0.33), seq(0, 1, 0.25), seq(0, 1, 0.2), seq(0, 1, 0.1)) Fits &lt;- list() for (i in 1:4) { BS &lt;- bs(xobs, knots = knots_list[[i]], degree = 3, Boundary.knots = range(x), intercept = FALSE) fm &lt;- lm(yobs ~ BS) Fits[[i]] &lt;- predict(fm) } # Create data frames data_true_signal &lt;- data.frame(x = x, y = ysignal, Type = &quot;True Signal&quot;) data_obs &lt;- data.frame(x = xobs, y = yobs, Type = &quot;Observed Data&quot;) data_preds &lt;- data.frame( x = rep(xobs, 4), y = c(Fits[[1]], Fits[[2]], Fits[[3]], Fits[[4]]), Type = rep(c(&quot;Fit 1&quot;, &quot;Fit 2&quot;, &quot;Fit 3&quot;, &quot;Fit 4&quot;), each = length(xobs)) ) data &lt;- rbind(data_true_signal, data_obs, data_preds) # Create ggplot ggplot(data, aes(x = x, y = y, color = Type)) + geom_line(data = subset(data, Type == &quot;True Signal&quot;), linewidth = 1) + geom_point(data = subset(data, Type == &quot;Observed Data&quot;), shape = 16, size = 2) + geom_line(data = subset(data, grepl(&quot;Fit&quot;, Type)), linewidth = 1, linetype = &quot;solid&quot;) + scale_color_manual(values = c(&quot;True Signal&quot; = &quot;black&quot;, &quot;Observed Data&quot; = &quot;red&quot;, &quot;Fit 1&quot; = &quot;blue&quot;, &quot;Fit 2&quot; = &quot;green&quot;, &quot;Fit 3&quot; = &quot;orange&quot;, &quot;Fit 4&quot; = &quot;purple&quot;)) + labs(y = &quot;y&quot;, color = &quot;Legend&quot;) + theme_minimal() + theme(legend.position = &quot;top&quot;) Note that splines use local information to perform the fit. This implies the curse of dimensionality issue, where increasing the number of variables leads to more dispersed regions in the regressor space. In other words, the observations become increasingly further apart as we have more regressors. As a result, splines become less reliable in high-dimensional spaces. A strategy to overcome the curse of dimensionality is to specify the partial linear model: \\[\\begin{align} y_i &amp; = \\boldsymbol{z}_i^{\\top}\\boldsymbol{\\gamma} + f(x_i) + \\mu_i. \\tag{11.2} \\end{align}\\] In this specification, some regressors enter in a linear way, while potentially the most relevant regressor — the one under primary investigation — enters in a non-parametric way. Note that we only need to increase the dimension of the matrix \\(\\boldsymbol{W}\\) by adding the columns from \\(\\boldsymbol{z}\\), and proceed as we did previously. Example: Consumption of marijuana in Colombia continues Let’s continue using the dataset MarijuanaColombia.csv to perform a partial linear model as in Equation (11.2), where \\(y_i\\) is the (log) marijuana monthly consumption, \\(\\boldsymbol{z}_i\\) represents the presence of a drug dealer in the neighborhood (Dealer), gender (Female), indicators of good physical and mental health (PhysicalHealthGood and MentalHealthGood), years of education (YearsEducation), and the (log) prices of marijuana, cocaine, and crack by individual. We set the knots as the percentiles \\(\\left\\{0,0.05,\\dots,0.95,1\\right\\}\\) of age, use cubic B-splines, non-informative conjugate priors in the linear regression model, 5,000 MCMC iterations, and 5,000 burn-in iterations. rm(list = ls()); set.seed(010101) library(splines); library(ggplot2) Data &lt;- read.csv(&quot;https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/MarijuanaColombia.csv&quot;) attach(Data) ## The following objects are masked from Data (pos = 6): ## ## Age, Age2, Dealer, Female, LogMarijuana, LogPriceCocaine, ## LogPriceCrack, LogPriceMarijuana, MentalHealthGood, ## PhysicalHealthGood, YearsEducation IdOrd &lt;- order(Age) y &lt;- LogMarijuana[IdOrd] Z &lt;- as.matrix(cbind(Data[IdOrd,-c(1, 6, 7)])) x &lt;- Age[IdOrd] knots &lt;- quantile(x, seq(0, 1, 0.05)) BS &lt;- bs(x, knots = knots, degree = 3, Boundary.knots = range(x), intercept = FALSE) matplot(x, BS, type = &quot;l&quot;, lty = 1, col = rainbow(ncol(BS))) X &lt;- cbind(1, BS[,1:22], Z) # Get B-splines without multicollinearity issues k &lt;- dim(X)[2]; N &lt;- dim(X)[1] # Hyperparameters d0 &lt;- 0.001; a0 &lt;- 0.001 b0 &lt;- rep(0, k); c0 &lt;- 1000; B0 &lt;- c0*diag(k); B0i &lt;- solve(B0) # MCMC parameters mcmc &lt;- 5000; burnin &lt;- 5000 tot &lt;- mcmc + burnin; thin &lt;- 1 posterior &lt;- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin) summary(coda::mcmc(posterior)) ## ## Iterations = 1:5000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 5000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 4.762070 1.59464 0.0225517 0.0225517 ## X1 0.414460 1.14161 0.0161448 0.0161448 ## X2 -0.434707 1.32159 0.0186902 0.0182422 ## X3 1.200738 1.07888 0.0152577 0.0152577 ## X4 1.373621 0.85336 0.0120684 0.0120684 ## X5 1.676279 0.88480 0.0125129 0.0125129 ## X6 1.220783 0.85900 0.0121481 0.0121481 ## X7 1.241079 0.87359 0.0123544 0.0120724 ## X8 1.816973 0.86389 0.0122173 0.0122173 ## X9 1.773895 0.84875 0.0120031 0.0120031 ## X10 1.262535 0.85882 0.0121456 0.0118736 ## X11 1.458600 0.86801 0.0122755 0.0122755 ## X12 1.694007 0.89295 0.0126283 0.0122990 ## X13 0.969907 0.87151 0.0123250 0.0123250 ## X14 2.020513 0.88300 0.0124875 0.0124875 ## X15 2.096505 0.85861 0.0121426 0.0121426 ## X16 1.075789 0.88201 0.0124735 0.0121409 ## X17 2.271827 0.87125 0.0123214 0.0120323 ## X18 1.613481 0.90826 0.0128448 0.0128448 ## X19 1.649940 0.86303 0.0122050 0.0119253 ## X20 1.795846 0.99558 0.0140796 0.0140796 ## X21 0.551177 0.95249 0.0134702 0.0134702 ## X22 1.578060 1.52457 0.0215607 0.0215607 ## XLogPriceMarijuana -0.548236 0.07428 0.0010505 0.0010505 ## XLogPriceCocaine 0.004347 0.09533 0.0013481 0.0013481 ## XLogPriceCrack 0.230243 0.09680 0.0013689 0.0013689 ## XYearsEducation -0.108868 0.01382 0.0001955 0.0001955 ## XDealer 0.283355 0.09489 0.0013419 0.0012233 ## XFemale -0.524882 0.10884 0.0015392 0.0015392 ## XPhysicalHealthGood -0.104649 0.12127 0.0017151 0.0017444 ## XMentalHealthGood 0.068732 0.10384 0.0014685 0.0014685 ## sigma2 2.428064 0.10092 0.0014272 0.0014272 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X 1.55699 3.7175175 4.739009 5.80172 7.92279 ## X1 -1.79003 -0.3583041 0.398603 1.19511 2.64498 ## X2 -2.99804 -1.3469332 -0.448332 0.46987 2.14830 ## X3 -0.92834 0.4836180 1.196875 1.92378 3.34203 ## X4 -0.28717 0.7970619 1.376339 1.93079 3.08974 ## X5 -0.05263 1.0887515 1.686176 2.25558 3.41183 ## X6 -0.42825 0.6494850 1.217303 1.79922 2.95536 ## X7 -0.46407 0.6662542 1.239668 1.82647 2.96068 ## X8 0.13876 1.2494689 1.812351 2.40049 3.53973 ## X9 0.14501 1.1917504 1.776206 2.33589 3.43701 ## X10 -0.40892 0.6947791 1.272198 1.82679 2.96285 ## X11 -0.23940 0.8657272 1.456057 2.05675 3.20787 ## X12 -0.06415 1.1114082 1.714477 2.27762 3.45773 ## X13 -0.72303 0.3695078 0.978630 1.55659 2.69103 ## X14 0.27912 1.4376210 2.018344 2.60509 3.77226 ## X15 0.41493 1.5289959 2.098777 2.66159 3.80006 ## X16 -0.59638 0.4805844 1.064190 1.67853 2.82543 ## X17 0.58853 1.6874131 2.267223 2.85066 3.98668 ## X18 -0.17849 0.9960272 1.618077 2.20946 3.45835 ## X19 -0.01256 1.0733493 1.640711 2.21205 3.34662 ## X20 -0.15662 1.1339269 1.789473 2.45008 3.75248 ## X21 -1.28909 -0.1001288 0.544697 1.20134 2.43123 ## X22 -1.51384 0.5779855 1.564216 2.59474 4.62015 ## XLogPriceMarijuana -0.69875 -0.5969960 -0.547761 -0.49908 -0.40337 ## XLogPriceCocaine -0.18313 -0.0562270 0.002893 0.06634 0.19409 ## XLogPriceCrack 0.03801 0.1655022 0.230836 0.29429 0.42126 ## XYearsEducation -0.13605 -0.1180723 -0.108845 -0.09958 -0.08225 ## XDealer 0.09831 0.2186432 0.284203 0.34647 0.47046 ## XFemale -0.73478 -0.5975294 -0.523892 -0.45189 -0.30925 ## XPhysicalHealthGood -0.33999 -0.1863400 -0.105786 -0.02289 0.13607 ## XMentalHealthGood -0.13261 -0.0005379 0.066461 0.13917 0.27269 ## sigma2 2.23758 2.3591542 2.424957 2.49384 2.63860 # Predict values with 95% credible intervals xfit &lt;- seq(min(x), max(x), 0.2) H &lt;- length(xfit) i &lt;- sample(1:N, 1) idfit &lt;- sample(1:N, H) BSfit &lt;- bs(xfit, knots = knots, degree = 3, Boundary.knots = range(x), intercept = FALSE) Xfit &lt;- cbind(1, BSfit[,1:22], Z[rep(i, H),]) # Relevant regressors, PIP &gt; 0.5 Fit &lt;- matrix(NA, mcmc, H) # posterior[posterior &gt; 0] &lt;- 0 for(s in 1:mcmc){ Fit[s,] &lt;- Xfit%*%posterior[s,1:31] } # Create a data frame for ggplot plot_data &lt;- data.frame(x = xfit, fit = colMeans(Fit), liminf = apply(Fit, 2, quantile, 0.025), limsup = apply(Fit, 2, quantile, 0.975)) ggplot() + geom_line(data = plot_data, aes(x, fit), color = &quot;blue&quot;, linewidth = 1) + geom_ribbon(data = plot_data, aes(x, ymin = liminf, ymax = limsup), fill = &quot;blue&quot;, alpha = 0.2) + labs(title = &quot;B-Spline Regression with 95% Confidence Interval&quot;, x = &quot;Age&quot;, y = &quot;Log Marijuana&quot;) + theme_minimal() The previous code illustrates the procedure. The posterior 95% credible intervals indicate that marijuana is an inelastic good, there is substitution between marijuana and crack, more educated female individuals consume less, and the presence of a drug dealer in the neighborhood increases consumption. The figure shows the fitted spline, specifically the mean (blue line) and the 95% credible intervals (shaded blue). Notice that there is a lot of wiggliness due to the use of many knots, suggesting that this relationship could be expressed in a more parsimonious way. In Exercise 6, we ask to use variable selection methods, such as the BIC approximation presented in Chapter 10, to perform regressor selection, particularly for the splines. In general, the selection of \\(H\\) is a problem of variable selection (regressor uncertainty) and can be handled using other approaches, such as those discussed in the regularization section of the following chapter. References "],["sec11_3.html", "11.3 Summary", " 11.3 Summary In this chapter, we introduce the most common Bayesian methods for inference in non-parametric and semi-parametric models. Finite Gaussian and Dirichlet process mixtures, as well as splines, are highly flexible methods; however, they also have limitations, such as the label-switching issue in mixtures and potential multicollinearity in splines. Additionally, it is wise to elicit informative priors and conduct sensitivity analyses to assess the robustness of the results in real-world applications. "],["sec11_4.html", "11.4 Exercises", " 11.4 Exercises Simulate a semi-parametric regression where \\[\\begin{align*} y_i &amp;= 0.5x_{i1} - 1.2x_{i2} + \\mu_i, \\\\ p(\\mu_i) &amp;= 0.3 \\phi(\\mu_i \\mid -0.5,0.5^2) + 0.7 \\phi(\\mu_i \\mid 1,0.8^2). \\end{align*}\\] Assume that \\(x_{i1}\\) and \\(x_{i2}\\) follow a standard normal distribution and that the sample size is 1,000. Perform inference in this model assuming that the number of components is unknown. Start with \\(H=5\\) and use non-informative priors, setting \\(\\alpha_{h0}=\\delta_{h0}=0.01\\), \\(\\boldsymbol{\\beta}_0=\\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_0=\\boldsymbol{I}_2\\), \\(\\mu_{h0}=0\\), \\(\\sigma^2_{\\mu 0}=10\\), and \\(\\boldsymbol{\\alpha}_0=[1/H \\ \\dots \\ 1/H]^{\\top}\\). Use 6,000 MCMC iterations, a burn-in period of 4,000, and a thinning parameter of 2. Compare the population parameters with the posterior estimates and plot the population density along with the posterior density estimate of \\(\\boldsymbol{\\mu}\\) (the mean, and the 95% credible interval). Example: Consumption of marijuana in Colombia continues I Use the dataset MarijuanaColombia.csv from our GitHub repository to perform inference on the demand for marijuana in Colombia. This dataset contains information on the (log) monthly demand in 2019 from the National Survey of the Consumption of Psychoactive Substances. It includes variables such as the presence of a drug dealer in the neighborhood (Dealer), gender (Female), indicators of good physical and mental health (PhysicalHealthGood and MentalHealthGood), age (Age and Age2), years of schooling (YearsEducation), and (log) prices of marijuana, cocaine, and crack by individual (LogPriceMarijuana, LogPriceCocaine, and LogPriceCrack). The sample size is 1,156. Estimate a finite Gaussian mixture regression using non-informative priors, that is, \\(\\alpha_{0}=\\delta_{0}=0.01\\), \\(\\boldsymbol{\\beta}_{0}=\\boldsymbol{0}_K\\), \\(\\boldsymbol{B}_{0}=\\boldsymbol{I}_K\\), and and \\(\\boldsymbol{\\alpha}_0=[1/H \\ \\dots \\ 1/H]^{\\top}\\), \\(K\\) is the number of regressors, 11 including the intercept. The number of MCMC iterations is 5,000, the burn-in is 1,000, and the thinning parameter is 2. Start with five potential clusters. Obtain the posterior distribution of the own-price elasticity of marijuana and the cross-price elasticities of marijuana demand with respect to the prices of cocaine and crack. Get the posterior sampler in the semi-parametric setting using a Dirichlet process mixture: \\[\\begin{align*} y_i&amp;=\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}+e_i\\\\ e_i\\mid \\mu_i,\\sigma_i^2 &amp;\\stackrel{iid}{\\sim} N(\\mu_i,\\sigma_i^2), \\end{align*}\\] Do not include the intercept in \\(\\boldsymbol{\\beta}\\) to get flexibility in the distribution of the stochastic errors. Let’s assume \\(\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\beta}_0,\\boldsymbol{B}_0)\\), \\(\\sigma_i^2\\sim IG(\\alpha_0/2,\\delta_0/2)\\), \\(\\mu_i\\sim N(\\mu_0,\\sigma_i^2/\\beta_0)\\), \\(\\alpha\\sim G(a,b)\\) such that introducing the latent variable \\(\\xi|\\alpha,N\\sim Be(\\alpha+1,N)\\), allows to easily sample the posterior draws of \\(\\alpha|\\xi,H,\\pi_{\\xi}\\sim\\pi_{\\xi}{G}(a+H,b-log(\\xi))+(1-\\pi_{\\xi}){G}(a+H-1,b-log(\\xi))\\), where \\(\\frac{\\pi_{\\xi}}{1-\\pi_{\\xi}}=\\frac{a+H-1}{N(b-log(\\xi))}\\), \\(H\\) is the number of atoms (mixture components). Example: Exercise 1 and 3 continue Perform inference in the simulation of the semi-parametric model of Exercise 1 using the sampler of Exercise 3. Use non-informative priors, setting \\(\\alpha_{0}=\\delta_{0}=0.01\\), \\(\\boldsymbol{\\beta}_{0}=\\boldsymbol{0}_2\\), \\(\\boldsymbol{B}_{0}=\\boldsymbol{I}_2\\), and \\(a=b=0.1\\). The number of MCMC iterations is 5,000, the burn-in is 1,000, and the thinning parameter is 2. Example: Simulation exercise continues Fix the label-switching problem of the simulation exercise of the DPM using random permutation of latent classes. Example: Consumption of marijuana in Colombia continues II Perform the application of marijuana consumption with the following specification: \\[\\begin{align*} y_i &amp; = \\boldsymbol{z}_i^{\\top} \\boldsymbol{\\gamma} + f(Age_{i}) + \\mu_i, \\end{align*}\\] where \\(y_i\\) is the (log) marijuana monthly consumption, \\(\\boldsymbol{z}_i\\) represents the presence of a drug dealer in the neighborhood (Dealer), gender (Female), indicators of good physical and mental health (PhysicalHealthGood and MentalHealthGood), years of education (YearsEducation), and the (log) prices of marijuana, cocaine, and crack by individual. Initially, set the knots as the percentiles \\(\\left\\{0,0.05,\\dots,0.95,1\\right\\}\\) of age and use cubic B-splines. Then, apply the BIC approximation to perform variable selection in this model with non-informative conjugate priors, 5,000 MCMC iterations, and 5,000 burn-in iterations. Do you think that using a linear regression with a second-degree polynomial in age provides a good approximation to the relationship found using splines in this application? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
