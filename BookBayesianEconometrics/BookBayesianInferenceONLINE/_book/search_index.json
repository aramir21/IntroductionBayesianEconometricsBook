[["Chap2.html", "Chapter 2 Conceptual differences between the Bayesian and Frequentist approaches", " Chapter 2 Conceptual differences between the Bayesian and Frequentist approaches We outline some of the conceptual differences between the Bayesian and Frequentist inferential approaches. We emphasize Bayesian concepts, as most readers may already be familiar with the Frequentist statistical framework. We illustrate the differences between these two inferential approaches using a simple example. In addition, we provide some potential explanations for why the Bayesian inferential framework is not well known at the introductory level among practitioners and applied researchers. "],["sec21.html", "2.1 The concept of probability", " 2.1 The concept of probability Let’s begin with the following thought experiment: Assume that you are watching the international game show “Who Wants to Be a Millionaire?”. The contestant is asked to answer a very simple question: What is the last name of the brothers who are credited with inventing the world’s first successful motor-operated airplane? What is the probability that the contestant answers this question correctly? Unless you have: watched this particular contestant participate in this show many times, seen her asked this same question each time, and computed the relative frequency with which she gives the correct answer, you need to answer this question as a Bayesian! Uncertainty about the event answering this question needs to be expressed as a “degree of belief,” informed by both data on the skill of the particular participant and how much she knows about inventors, as well as possibly prior knowledge of her performance in other game shows. Of course, your prior knowledge of the contestant may be minimal, or it may be well-informed. Either way, your final answer remains a degree of belief about an uncertain, and inherently unrepeatable, state of nature. The point of this hypothetical, light-hearted scenario is simply to highlight that a key distinction between the Frequentist and Bayesian approaches to inference is not the use (or nature) of prior information, but the manner in which probability is used. To the Bayesian, probability is the mathematical construct used to quantify uncertainty about an unknown state of nature, conditional on observed data and prior knowledge about the context in which that state occurs. To the Frequentist, probability is intrinsically linked to the concept of a repeated experiment, and the relative frequency with which a particular outcome occurs, conditional on that unknown state. This distinction remains key whether the Bayesian chooses to be informative or subjective in the specification of prior information, or chooses to be non-informative or objective. Frequentists consider probability to be a physical phenomenon, like mass or wavelength, whereas Bayesians stipulate that probability exists in the mind of scientists, as any scientific construct (Parmigiani and Inoue 2008). It seems that the understanding of the concept of probability for the common human being is more associated with “degrees of belief” rather than relative frequency. Peter Diggle, President of The Royal Statistical Society between 2014 and 2016, was asked in an interview, “A different trend which has surged upwards in statistics during Peter’s career is the popularity of Bayesian statistics. Does Peter consider himself a Bayesian?” He replied, “… you can’t not believe in Bayes’ theorem because it’s true. But that doesn’t make you a Bayesian in the philosophical sense. When people are making personal decisions – even if they don’t formally process Bayes’ theorem in their mind – they are adapting what they think they should believe in response to new evidence as it comes in. Bayes’ theorem is just the formal mathematical machinery for doing that.” However, we should mention that psychological experiments suggest that human beings suffer from anchoring, a cognitive bias that causes us to rely too heavily on previous information (the prior), so that the updating process (posterior) due to new information (likelihood) is not as strong as Bayes’ rule would suggest (Kahneman 2011). References Kahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan. Parmigiani, G., and L. Inoue. 2008. Decision Theory Principles and Approaches. John Wiley &amp; Sons. "],["sec22.html", "2.2 Subjectivity is not the key", " 2.2 Subjectivity is not the key The concepts of subjectivity and objectivity indeed characterize both statistical paradigms in differing ways. Among Bayesians, there are those who are immersed in subjective rationality (Ramsey 1926; De Finetti 1937; Savage 1954; D. V. Lindley 2000), but others who adopt objective prior distributions such as Jeffreys’, reference, empirical, or robust priors (T. Bayes 1763; Laplace 1812; Jeffreys 1961; Berger 2006) to operationalize Bayes’ rule and thereby weight quantitative (data-based) evidence. Among Frequentists, there are choices made about significance levels which, if not explicitly subjective, are typically not grounded in any objective and documented assessment of the relative losses of Type I and Type II errors.1 In addition, both Frequentist and Bayesian Econometricians/Statisticians make decisions about the form of the data generating process, or “model”, which – if not subject to rigorous diagnostic assessment – retains a subjective element that potentially influences the final inferential outcome. Although we all know that by definition, a model is a schematic and simplified approximation to reality, “Since all models are wrong, the scientist cannot obtain a correct one by excessive elaboration. On the contrary, following William of Occam, he should seek an economical description of natural phenomena.” (G. E. P. Box 1976). We also know that “All models are wrong, but some are useful” (G. E. Box 1979), which is why model diagnostics are important. This task can be performed in both approaches. Particularly, the Bayesian framework can use predictive p-values for absolute testing (A. Gelman and Meng 1996; M. Bayarri and Berger 2000) or posterior odds ratios for relative statements (Jeffreys 1935; R. E. Kass and Raftery 1995). This is because the marginal likelihood, conditional on data, is interpreted as the evidence for the prior distribution (Berger 1993). In addition, what does objectivity mean in a Frequentist approach? For example, why should we use a 5% or 1% significance level rather than any other value? As someone said, the apparent objectivity is really a consensus (D. V. Lindley 2000). In fact, “Student” (William Gosset) saw statistical significance at any level as being “nearly valueless” in itself (Ziliak 2008). But, this is not just a situation in the Frequentist approach. The cut-offs used to “establish” scientific evidence against a null hypothesis, in terms of \\(log_{10}\\) scale (Jeffreys 1961) or \\(log_{e}\\) scale (R. E. Kass and Raftery 1995) as shown in Table 1.1, are also ad hoc. Although the true state of nature in Bayesian inference is expressed in “degrees of belief”, the distinction between the two paradigms does not reside in one being more, or less, subjective than the other. Rather, the differences are philosophical, pedagogical, and methodological. References Bayarri, M., and J. Berger. 2000. “P–Values for Composite Null Models.” Journal of American Statistical Association 95: 1127–42. Bayes, T. 1763. “An Essay Towards Solving a Problem in the Doctrine of Chances.” Philosophical Transactions of the Royal Society of London 53: 370–416. Berger, J. 1993. Statistical Decision Theory and Bayesian Analysis. Third Edition. Springer. ———. 2006. “The Case for Objective Bayesian Analysis.” Bayesian Analysis 1 (3): 385–402. Box, G. E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71: 791–99. Box, George EP. 1979. “Robustness in the Strategy of Scientific Model Building.” In Robustness in Statistics, 201–36. Elsevier. De Finetti, Bruno. 1937. “Foresight: Its Logical Laws, Its Subjective Sources.” In Studies in Subjective Probability, edited by H. E. Kyburg and H. E. Smokler. New York: Krieger. Gelman, A., and X. Meng. 1996. “Model Checking and Model Improvement.” In In Markov Chain Monte Carlo in Practice, edited by Gilks, Richardson, and Speigelhalter. Springer US. Jeffreys, H. 1935. “Some Test of Significance, Treated by the Theory of Probability.” Proccedings of the Cambridge Philosophy Society 31: 203–22. ———. 1961. Theory of Probability. London: Oxford University Press. Kass, R E, and A E Raftery. 1995. “Bayes factors.” Journal of the American Statistical Association 90 (430): 773–95. Laplace, P. 1812. Théorie Analytique Des Probabilités. Courcier. Lindley, D. V. 2000. “The Philosophy of Statistics.” The Statistician 49 (3): 293–337. Ramsey, F. 1926. “Truth and Probability.” In The Foundations of Mathematics and Other Logical Essays, edited by Routledge and Kegan Paul. London: New York: Harcourt, Brace; Company. Savage, L. J. 1954. The Foundations of Statistics. New York: John Wiley &amp; Sons, Inc. Ziliak, S. 2008. “Guinnessometrics; the Economic Foundation of Student’s t.” Journal of Economic Perspectives 22 (4): 199–216. Type I error is rejecting the null hypothesis when it is true, and Type II error is not rejecting the null hypothesis when it is false.↩︎ "],["sec23.html", "2.3 Estimation, hypothesis testing and prediction", " 2.3 Estimation, hypothesis testing and prediction All that is required to perform estimation, hypothesis testing (model selection), and prediction in the Bayesian approach is to apply Bayes’ rule. This ensures coherence under a probabilistic view. However, there is no free lunch: coherence reduces flexibility. On the other hand, the Frequentist approach may not be coherent from a probabilistic point of view, but it is highly flexible. This approach can be seen as a toolkit that offers inferential solutions under the umbrella of understanding probability as relative frequency. For instance, a point estimator in a Frequentist approach is found such that it satisfies good sampling properties like unbiasedness, efficiency, or a large sample property such as consistency. A notable difference is that optimal Bayesian decisions are calculated by minimizing the expected value of the loss function with respect to the posterior distribution, i.e., conditional on observed data. In contrast, Frequentist “optimal” actions are based on the expected values over the distribution of the estimator (a function of data), conditional on the unknown parameters. This involves considering sampling variability. The Bayesian approach allows for the derivation of the posterior distribution of any unknown object, such as parameters, latent variables, future or unobserved variables, or models. A major advantage is that predictions can account for estimation error, and predictive distributions (probabilistic forecasts) can be easily derived. Hypothesis testing (model selection) in the Bayesian framework is based on inductive logic reasoning (inverse probability). Based on observed data, we evaluate which hypothesis is most tenable, performing this evaluation using posterior odds. These odds are in turn based on Bayes factors, which assess the evidence in favor of a null hypothesis while explicitly considering the alternative (R. E. Kass and Raftery 1995), following the rules of probability (D. V. Lindley 2000). This approach compares how well hypotheses predict data (Goodman 1999), minimizes the weighted sum of type I and type II error probabilities (DeGroot 1975; Pericchi and Pereira 2015), and takes into account the implicit balance of losses (Jeffreys 1961; Bernardo and Smith 1994). Posterior odds allow for the use of the same framework to analyze nested and non-nested models and perform model averaging. However, Bayes factors cannot be based on improper or vague priors (Koop 2003), the practical interplay between model selection and posterior distributions is not as straightforward as it may be in the Frequentist approach, and the computational burden can be more demanding due to the need to solve potentially difficult integrals. On the other hand, the Frequentist approach establishes most of its estimators as the solution to a system of equations. Observe that optimization problems often reduce to solving systems. We can potentially obtain the distribution of these estimators, but most of the time, asymptotic arguments or resampling techniques are required. Hypothesis testing relies on pivotal quantities and/or resampling, and prediction is typically based on a plug-in approach, which means that estimation error is not taken into account.2 Comparing models depends on their structure. For instance, there are different Frequentist statistical approaches to compare nested and non-nested models. A nice feature in some situations is that there is a practical interplay between hypothesis testing and confidence intervals. For example, in the normal population mean hypothesis framework, you cannot reject a null hypothesis \\(H_0: \\mu = \\mu^0\\) at the \\(\\alpha\\) significance level (Type I error) if \\(\\mu^0\\) is in the \\(1-\\alpha\\) confidence interval. Specifically, \\[ P\\left( \\mu \\in \\left[\\hat{\\mu} - |t_{N-1}^{\\alpha/2}| \\times \\hat{\\sigma}_{\\hat{\\mu}}, \\hat{\\mu} + |t_{N-1}^{\\alpha/2}| \\times \\hat{\\sigma}_{\\hat{\\mu}}\\right] \\right) = 1 - \\alpha, \\] where \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}_{\\hat{\\mu}}\\) are the maximum likelihood estimators of the mean and standard error, \\(t_{N-1}^{\\alpha/2}\\) is the quantile value of the Student’s \\(t\\)-distribution at the \\(\\alpha/2\\) probability level with \\(N-1\\) degrees of freedom, and \\(N\\) is the sample size. A remarkable difference between the Bayesian and Frequentist inferential frameworks is the interpretation of credible/confidence intervals. Observe that once we have estimates, such that, for example, the previous interval is \\([0.2, 0.4]\\) given a 95% confidence level, we cannot say that \\(P(\\mu \\in [0.2, 0.4]) = 0.95\\) in the Frequentist framework. In fact, this probability is either 0 or 1 in this approach, as \\(\\mu\\) is either in the interval or it is not. The problem is that we will never know for certain in applied settings. This is because \\[ P(\\mu \\in [\\hat{\\mu} - |t_{N-1}^{0.025}| \\times \\hat{\\sigma}_{\\hat{\\mu}}, \\hat{\\mu} + |t_{N-1}^{0.025}| \\times \\hat{\\sigma}_{\\hat{\\mu}}]) = 0.95 \\] is interpreted in the context of repeated sampling. On the other hand, once we have the posterior distribution in the Bayesian framework, we can say that \\(P(\\mu \\in [0.2, 0.4]) = 0.95\\). Following common practice, most researchers and practitioners conduct hypothesis testing based on the p-value in the Frequentist framework. But what is a p-value? Most users do not know the answer, as statistical inference is often not performed by statisticians (Berger 2006).3 A p-value is the probability of obtaining a statistical summary of the data equal to or more extreme than what was actually observed, assuming that the null hypothesis is true. Therefore, p-value calculations involve not just the observed data, but also more extreme hypothetical observations. Thus, “What the use of p implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.” (Jeffreys 1961) Some researchers and practitioners using Frequentist inference often intertwines two distinct logical frameworks: Fisher’s p-value approach (Fisher 1958) and the Neyman–Pearson significance testing framework (Neyman and Pearson 1933). The p-value serves as an informal, data-dependent measure of evidence against the null hypothesis. It is rooted in reduction to absurdity reasoning, where the extremeness of the observed data is assessed under the assumption that the null hypothesis is true. However, the p-value is frequently misinterpreted as the probability that the null hypothesis is false—a misconception known as the p-value fallacy (Goodman 1999). In contrast, the Neyman–Pearson framework adopts a deductive, long-run perspective: it defines decision rules that control the frequency of Type I errors over repeated sampling, irrespective of the evidence in any particular case. Conflating these two frameworks leads to interpretational inconsistencies, especially when the p-value is used both as a measure of evidence and as a decision-making threshold. A clearer separation of these paradigms is essential for coherent statistical reasoning. The American Statistical Association has several concerns regarding the use of the p-value as a cornerstone for hypothesis testing in science. This concern motivates the ASA’s statement on p-values (Wasserstein and Lazar 2016), which can be summarized in the following principles: “P-values can indicate how incompatible the data are with a specified statistical model.” “P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.” “Scientific conclusions and business or policy decisions should not be based solely on whether a p-value passes a specific threshold.” “Proper inference requires full reporting and transparency.” “A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.” “By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.” To sum up, Fisher proposed the p-value as a witness rather than a judge. So, a p-value lower than the significance level means more inspection of the null hypothesis, but it is not a final conclusion about it. Another key distinction between frequentist and Bayesian approaches lies in how scientific hypotheses are evaluated. Users of the Frequentist approach rely on the p-value, which quantifies the probability of observing data as extreme as—or more extreme than—the sample under the assumption that the null hypothesis is true. Bayesians, in contrast, use the Bayes factor, which compares the performance of two competing hypotheses by evaluating the ratio of their marginal likelihoods. While the p-value reflects \\(P(\\text{data} \\mid \\text{hypothesis})\\), the Bayes factor is more aligned with \\(P(\\text{hypothesis} \\mid \\text{data})\\), though not equivalent. Notably, there exists an approximate relationship between the \\(t\\)-statistic and the Bayes factor in the context of regression coefficients (Raftery 1995), which offers some practical interpretability across paradigms. In particular, \\[ |t|&gt;(\\log(N)+6)^{1/2} \\] corresponds to strong evidence in favor of rejecting the null hypothesis of no relevance of a control in a regression. Observe that, in this setting, the threshold of the \\(t\\) statistic, and as a consequence the significance level, depends on the sample size. This setting agrees with the idea in experimental designs of selecting the sample size such that we control Type I and Type II errors. In observational studies, we cannot control the sample size, but we can select the significance level. See also Sellke, Bayarri, and Berger (2001) and Benjamin et al. (2018) for exercises that reveal potential flaws of the p-value (\\(p\\)) due to \\(p \\sim U[0,1]\\) under the null hypothesis,4 and calibrations of the p-value to interpret it as the odds ratio and the error probability. In particular, \\[ B(p)=-e \\times p \\times \\log(p) \\quad \\text{when} \\quad p &lt; e^{-1} \\] and interpret this as the Bayes factor of \\(H_0\\) to \\(H_1\\), where \\(H_1\\) denotes the unspecified alternative to \\(H_0\\), and \\[ \\alpha(p) = \\left(1 + \\left[-e \\times p \\times \\log(p)\\right]^{-1}\\right)^{-1} \\] as the error probability \\(\\alpha\\) in rejecting \\(H_0\\). Take into account that \\(B(p)\\) and \\(\\alpha(p)\\) are lower bounds. The logic of argumentation in the Frequentist approach is based on deductive logic, which means that it starts from a statement about the true state of nature (null hypothesis) and predicts what should be observed if this statement were true. On the other hand, the Bayesian approach is based on inductive logic, which means that it defines which hypothesis is more consistent with what is observed. The former inferential approach establishes that the truth of the premises implies the truth of the conclusion, which is why we reject or fail to reject hypotheses. The latter establishes that the premises supply some evidence, but not full assurance, of the truth of the conclusion, which is why we get probabilistic statements. Here, there is a distinction between the effects of causes (forward causal inference) and the causes of effects (reverse causal inference) (Andrew Gelman and Imbens 2013; Dawid, Musio, and Fienberg 2016). To illustrate this point, imagine that a firm increases the price of a specific good. Economic theory would suggest that, as a result, demand for the good decreases. In this case, the premise (null hypothesis) is the price increase, and the consequence is the decrease in the firm’s demand. Alternatively, one could observe a reduction in a firm’s demand and attempt to identify the cause behind it. For example, a reduction in quantity could be due to a negative supply shock. The Frequentist approach typically follows the first view (effects of causes), while Bayesian reasoning focuses on determining the probability of potential causes (causes of effects). References Benjamin, Daniel J, James O Berger, Magnus Johannesson, Brian A Nosek, E-J Wagenmakers, Richard Berk, Kenneth A Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. ———. 2006. “The Case for Objective Bayesian Analysis.” Bayesian Analysis 1 (3): 385–402. Bernardo, J., and A. Smith. 1994. Bayesian Theory. Chichester: Wiley. Dawid, A. P., M. Musio, and S. E. Fienberg. 2016. “From Statistical Evidence to Evidence of Causality.” Bayesian Analysis 11 (3): 725–52. DeGroot, M. H. 1975. Probability and Statistics. London: Addison-Wesley Publishing Co. Fisher, R. 1958. Statistical Methods for Research Workers. 13th ed. New York: Hafner. Gelman, Andrew, and Guido Imbens. 2013. “Why Ask Why? Forward Causal Inference and Reverse Causal Questions.” National Bureau of Economic Research. Goodman, S. N. 1999. “Toward Evidence-Based Medical Statistics. 1: The P Value Fallacy.” Annals of Internal Medicine 130 (12): 995–1004. ———. 1961. Theory of Probability. London: Oxford University Press. Kass, R E, and A E Raftery. 1995. “Bayes factors.” Journal of the American Statistical Association 90 (430): 773–95. Koop, Gary M. 2003. Bayesian Econometrics. John Wiley &amp; Sons Inc. Lindley, D. V. 2000. “The Philosophy of Statistics.” The Statistician 49 (3): 293–337. Neyman, J., and E. Pearson. 1933. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society, Series A 231: 289–337. Pericchi, Luis, and Carlos Pereira. 2015. “Adaptative significance levels using optimal decision rules: Balancing by weighting the error probabilities.” Brazilian Journal of Probability and Statistics. Raftery, A. 1995. “Bayesian Model Selection in Social Research.” Sociological Methodology 25: 111–63. Sellke, Thomas, MJ Bayarri, and James O Berger. 2001. “Calibration of p Values for Testing Precise Null Hypotheses.” The American Statistician 55 (1): 62–71. Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p–Values: Context, Process and Purpose.” The American Statistician. A pivot quantity is a function of unobserved parameters and observations whose probability distribution does not depend on the unknown parameters.↩︎ See also: https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/↩︎ See: https://joyeuserrance.wordpress.com/2011/04/22/proof-that-p-values-under-the-null-are-uniformly-distributed/ for a simple proof.↩︎ "],["sec24.html", "2.4 The likelihood principle", " 2.4 The likelihood principle The likelihood principle states that in making inferences or decisions about the state of nature, all the relevant experimental information is given by the likelihood function. The Bayesian framework follows this statement, i.e., it is conditional on observed data. We follow Berger (1993), who in turn followed D. V. Lindley and Phillips (1976), to illustrate the likelihood principle. We are given a coin and are interested in the probability, \\(\\theta\\), of it landing heads when flipped. We wish to test \\(H_0: \\theta = 1/2\\) versus \\(H_1: \\theta &gt; 1/2\\). An experiment is conducted by flipping the coin (independently) in a series of trials, with the result being the observation of 9 heads and 3 tails. This is not yet enough information to specify \\(p(y|\\theta)\\), since the series of trials has not been explained. Two possibilities arise: The experiment consisted of a predetermined 12 flips, so that \\(Y = [ \\text{Heads} ]\\) follows a \\(B(12, \\theta)\\) distribution. In this case, \\[ p_1(y|\\theta) = \\binom{12}{y} \\theta^y (1 - \\theta)^{12 - y} = 220 \\times \\theta^9 (1 - \\theta)^3. \\] The experiment consisted of flipping the coin until 3 tails were observed (\\(r = 3\\)). In this case, \\(Y\\), the number of heads (failures) before obtaining 3 tails, follows a \\(NB(3, 1 - \\theta)\\) distribution. Here, \\[ p_2(y|\\theta) = \\binom{y + r - 1}{r - 1} (1 - (1 - \\theta)^y)(1 - \\theta)^r = 55 \\times \\theta^9 (1 - \\theta)^3. \\] Using a Frequentist approach, the significance level of \\(y=9\\) using the Binomial model against \\(\\theta=1/2\\) would be: \\[ \\alpha_1=P_{1/2}(Y\\geq 9)=p_1(9|1/2)+p_1(10|1/2)+p_1(11|1/2)+p_1(12|1/2)=0.073. \\] # Binomial test: one-sided significance level for observing 9 or more successes in 12 trials # Parameters successes &lt;- 9 # Number of observed successes n_trials &lt;- 12 # Total number of trials p_null &lt;- 0.5 # Null hypothesis success probability # Calculate one-sided significance level significance_level &lt;- sum(dbinom(successes:n_trials, size = n_trials, prob = p_null)) # Output result with context message(sprintf(&quot;One-sided significance level (P(X ≥ %d | H0: p = %.1f)): %.4f&quot;, successes, p_null, significance_level)) ## One-sided significance level (P(X ≥ 9 | H0: p = 0.5)): 0.0730 For the Negative Binomial model, the significance level would be: \\[ \\alpha_2=P_{1/2}(Y\\geq 9)=p_2(9|1/2)+p_2(10|1/2)+\\ldots=0.0327. \\] # Negative binomial test: Probability of observing at least 3 tails before 9 heads (failures) # Parameters target_successes &lt;- 3 # Number of target successes (e.g., tails) num_failures &lt;- 9 # Number of failures (e.g., heads) p_success &lt;- 0.5 # Probability of success (e.g., tails) # Compute the one-sided significance level: P(X ≥ 3 successes before 9 failures) significance_level &lt;- 1 - pnbinom(q = num_failures - 1, size = target_successes, prob = p_success) # Print result message(sprintf(&quot;P(at least %d tails before %d heads): %.4f&quot;, target_successes, num_failures, significance_level)) ## P(at least 3 tails before 9 heads): 0.0327 We arrive at different conclusions using a significance level of 5%, whereas we obtain the same outcomes using a Bayesian approach because the kernels of both distributions are identical (\\(\\theta^9 \\times (1 - \\theta)^3\\)). References Berger, J. 1993. Statistical Decision Theory and Bayesian Analysis. Third Edition. Springer. Lindley, D. V., and L. D. Phillips. 1976. “Inference for a Bernoulli Process (a Bayesian View).” American Statistician 30: 112–19. "],["sec25.html", "2.5 Why is not the Bayesian approach that popular?", " 2.5 Why is not the Bayesian approach that popular? At this stage, one might wonder why the Bayesian statistical framework is not the dominant inferential approach, despite its historical origin in 1763 (Thomas Bayes 1763), whereas the Frequentist statistical framework was largely developed in the early 20th century. The scientific debate over the Bayesian inferential approach lasted for 150 years, and this may be explained by some of the following factors. One issue is the apparent subjectivity of the Bayesian approach, which runs counter to the strong conviction that science demands objectivity. Bayesian probability is considered a measure of degrees of belief, where the initial prior may be just a guess. This was not accepted as objective and rigorous science. Initial critics argued that Bayes was quantifying ignorance by assigning equal probabilities to all potential outcomes. As a consequence, prior distributions were dismissed (McGrayne 2011). Bayes himself seemed not to have believed in his idea. Although it seems that Bayes made his breakthrough in the late 1740s, he did not submit it for publication to the Royal Society. It was his friend, Richard Price, another Presbyterian minister, who rediscovered Bayes’ idea, polished it, and published it. However, it was Laplace who independently generalized Bayes’ theorem in 1781. Initially, he applied it to gambling problems and soon thereafter to astronomy, combining various sources of information to advance research in situations where data was scarce. He later sought to apply his discovery to finding the probability of causes, which he thought required large datasets, thus turning to demography. In this field, he had to perform large-scale calculations, leading to the development of Laplace’s approximation and the central limit theorem (Laplace 1812). Unfortunately, this came at the cost of abandoning his research on Bayesian inference. Once Laplace passed away in 1827, Bayes’ rule disappeared from the scientific discourse for almost a century. In part, personal attacks against Laplace led to the rule being forgotten. Moreover, there was a prevailing belief that statistics should not address causation, and that the prior was too subjective to be compatible with science. Nonetheless, practitioners continued to use Bayes’ rule to solve problems in astronomy, communication, medicine, military affairs, and social issues with remarkable results. Thus, the concept of degrees of belief to operationalize probability was abandoned in favor of scientific objectivity. Probability was then defined as the frequency with which an event occurs in many repeatable trials, which became the accepted norm. Critics of Laplace argued that these two concepts were diametrically opposed, although Laplace considered them to be basically equivalent when large sample sizes are involved (McGrayne 2011). The era of Frequentists, or sampling theorists, began, led by Karl Pearson and his nemesis, Ronald Fisher. Both were brilliant and persuasive characters, opposing the inverse probability (Bayesian) approach and making it nearly impossible to argue against their ideas. Pearson’s legacy was carried on by his son, Egon, and Egon’s friend, Jerzy Neyman. Both inherited the anti-Bayesian and anti-Fisher sentiments. Despite the anti-Bayesian campaign among statisticians, some independent thinkers continued to develop Bayesian ideas, including Borel, Ramsey, and de Finetti, who were isolated in different countries: France, England, and Italy. However, the anti-Bayesian trio of Fisher, Neyman, and Egon Pearson dominated the spotlight in the 1920s and 1930s. Only a geophysicist, Harold Jeffreys, kept Bayesian inference alive during the 1930s and 1940s. Jeffreys was a quiet, reserved gentleman working in the astronomy department at Cambridge. He was Fisher’s friend due to their shared character, although they were intellectual opposites when it came to Bayesian inference, leading to intense intellectual battles. Unfortunately for the Bayesian approach, Jeffreys lost. His work was highly technical, using confusing high-level mathematics. He focused on inference from scientific evidence, rather than guiding future actions based on decision theory, which was crucial in that era for mathematical statistics, especially during the Second World War. In contrast, Fisher was a dominant figure, persuasive in public and a master of practical applications, with his techniques written in a popular style with minimal mathematics. Nevertheless, Bayes’ rule achieved remarkable results in applied settings such as at AT&amp;T and the U.S. Social Security system. Bayesian inference also played a significant role during the Second World War and the Cold War. Alan Turing used inverse probability at Bletchley Park to crack German messages encoded using the Enigma machine, which was employed by U-boats. Andrei Kolmogorov used Bayesian methods to improve firing tables for Russian artillery. Bernard Koopman applied it for searching targets at sea, and the RAND Corporation used it during the Cold War. Unfortunately, these Bayesian developments remained top secret for almost 40 years, keeping the contribution of inverse probability hidden from modern history. In the 1950s and 1960s, three mathematicians led the resurgence of the Bayesian approach: Good, Savage, and Lindley. However, it seems that they were reluctant to apply their theories to real-world problems. Despite the fact that the Bayesian approach proved its worth in various areas such as business decisions, naval searches, and lung cancer detection, it was largely applied to simple models due to its mathematical complexity and requirement for large computations. However, some breakthroughs changed this. First, hierarchical models were introduced by Lindley and Smith, where a complex model is decomposed into many smaller, easier-to-solve models. Second, Markov chain Monte Carlo (MCMC) methods were developed by Hastings in the 1970s (Hastings 1970) and the Geman brothers in the 1980s (Geman and Geman 1984). These methods were incorporated into the Bayesian inferential framework in the 1990s by Gelfand and Smith (Gelfand and Smith 1990), and Tierney (Tierney 1994), when desktop computers gained sufficient computational power to solve complex models. Since then, the Bayesian inferential framework has gained increasing popularity among both practitioners and scientists. References Bayes, Thomas. 1763. “LII. An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, FRS Communicated by Mr. Price, in a Letter to John Canton, AMFR S.” Philosophical Transactions of the Royal Society of London, no. 53: 370–418. Gelfand, A. E., and A. F. M. Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” Journal of the American Statistical Association 85: 398–409. Geman, S, and D. Geman. 1984. “Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence 6: 721–41. Hastings, W. 1970. “Monte Carlo Sampling Methods Using Markov Chains and Their Application.” Biometrika 57: 97–109. Laplace, P. 1812. Théorie Analytique Des Probabilités. Courcier. McGrayne, Sharon Bertsch. 2011. The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted down Russian Submarines, &amp; Emerged Triumphant from Two Centuries of c. Yale University Press. Tierney, Luke. 1994. “Markov Chains for Exploring Posterior Distributions.” The Annals of Statistics, 1701–28. "],["sec26.html", "2.6 A simple working example", " 2.6 A simple working example We will illustrate some conceptual differences between the Bayesian and Frequentist statistical approaches by performing inference on a random sample \\(\\mathbf{Y} = [Y_1, Y_2, \\dots, Y_N]\\), where \\(Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) for \\(i = 1, 2, \\dots, N\\). In particular, we set \\(\\pi(\\mu, \\sigma) = \\pi(\\mu) \\pi(\\sigma) \\propto \\frac{1}{\\sigma}\\). This is a standard non-informative improper prior (Jeffreys prior, see Chapter 3). That is, this prior is perfectly compatible with the sample information. Thus, the posterior distribution is \\[ \\begin{aligned} \\pi(\\mu,\\sigma|\\mathbf{y}) &amp;\\propto \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2\\right\\} \\\\ &amp;= \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N \\left((y_i-\\bar{y}) - (\\mu-\\bar{y})\\right)^2\\right\\} \\\\ &amp;= \\frac{1}{\\sigma}\\exp\\left\\{-\\frac{N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}\\times (\\sigma)^{-N}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\bar{y})^2\\right\\} \\\\ &amp;= \\frac{1}{\\sigma}\\exp\\left\\{-\\frac{N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}\\times (\\sigma)^{-(\\alpha_n+1)}\\exp\\left\\{-\\frac{\\alpha_n\\hat{\\sigma}^2}{2\\sigma^2}\\right\\}, \\end{aligned} \\] where \\(\\bar{y} = \\frac{\\sum_{i=1}^N y_i}{N}\\), \\(\\alpha_n = N-1\\), and \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N (y_i-\\bar{y})^2}{N-1}\\). The first term in the last expression is the kernel of a normal density, \\(\\mu|\\sigma,\\mathbf{y} \\sim N(\\bar{y}, \\sigma^2 / N)\\). The second term is the kernel of an inverted gamma density (Zellner 1996, 371), \\(\\sigma|\\mathbf{y} \\sim IG(\\alpha_n, \\hat{\\sigma}^2)\\). Therefore, \\[ \\pi(\\mu|\\sigma,\\mathbf{y}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2 / N}} \\exp\\left\\{-\\frac{N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}, \\] and \\[ \\pi(\\sigma|\\mathbf{y}) = \\frac{2}{\\Gamma(\\alpha_n / 2)} \\left(\\frac{\\alpha_n \\hat{\\sigma}^2}{2}\\right)^{\\alpha_n / 2} \\frac{1}{\\sigma^{\\alpha_n+1}} \\exp\\left\\{-\\frac{\\alpha_n \\hat{\\sigma}^2}{2 \\sigma^2}\\right\\}. \\] Observe that \\(\\mathbb{E}[\\mu | \\sigma, \\mathbf{y}] = \\bar{y}\\); this is also the maximum likelihood (Frequentist) point estimate of \\(\\mu\\) in this setting. In addition, the Frequentist \\((1-\\alpha)\\%\\) confidence interval and the Bayesian \\((1-\\alpha)\\%\\) credible interval have exactly the same form, \\(\\bar{y} \\pm |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{N}}\\), where \\(z_{\\alpha/2}\\) is the \\(\\alpha/2\\) percentile of a standard normal distribution. However, the interpretations are entirely different. The confidence interval has a probabilistic interpretation under sampling variability of \\(\\bar{Y}\\): in repeated sampling, \\((1-\\alpha)\\%\\) of the intervals \\(\\bar{Y} \\pm |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{N}}\\) would include \\(\\mu\\). However, given an observed realization of \\(\\bar{Y}\\), say \\(\\bar{y}\\), the probability of \\(\\bar{y} \\pm |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{N}}\\) including \\(\\mu\\) is either 1 or 0. This is why we refer to it as a \\((1-\\alpha)\\%\\) confidence interval. On the other hand, \\(\\bar{y} \\pm |z_{\\alpha/2}| \\frac{\\sigma}{\\sqrt{N}}\\) has a straightforward probabilistic interpretation in the Bayesian framework: there is a \\((1-\\alpha)\\%\\) probability that \\(\\mu\\) lies within this interval. If we want to get the marginal posterior density of \\(\\mu\\), \\[\\begin{align*} \\pi(\\mu|\\mathbf{y})&amp;=\\int_{0}^{\\infty} \\pi(\\mu,\\sigma|\\mathbf{y}) d\\sigma\\\\ &amp;\\propto \\int_{0}^{\\infty} \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2\\right\\} d\\sigma\\\\ &amp;= \\int_{0}^{\\infty} \\left(\\frac{1}{\\sigma}\\right)^{N+1} \\exp\\left\\{-\\frac{N}{2\\sigma^2}\\frac{\\sum_{i=1}^N (y_i-\\mu)^2}{N}\\right\\} d\\sigma\\\\ &amp;=\\left[\\frac{2}{\\Gamma(N/2)}\\left(\\frac{N\\sum_{i=1}^N (y_i-\\mu)^2}{2N}\\right)^{N/2}\\right]^{-1}\\\\ &amp;\\propto \\left[\\sum_{i=1}^N (y_i-\\mu)^2\\right]^{-N/2}\\\\ &amp;=\\left[\\sum_{i=1}^N ((y_i-\\bar{y})-(\\mu-\\bar{y}))^2\\right]^{-N/2}\\\\ &amp;=[\\alpha_n\\hat{\\sigma}^2+N(\\mu-\\bar{y})^2]^{-N/2}\\\\ &amp;\\propto \\left[1+\\frac{1}{\\alpha_n}\\left(\\frac{\\mu-\\bar{y}}{\\hat{\\sigma}/\\sqrt{N}}\\right)^2\\right]^{-(\\alpha_n+1)/2}. \\end{align*}\\] The fourth line arises from the kernel of an inverted gamma density with \\(N\\) degrees of freedom in the integral (Zellner 1996). The last expression represents the kernel of a Student’s \\(t\\)-distribution with \\(\\alpha_n = N - 1\\) degrees of freedom, expected value equal to \\(\\bar{y}\\), and variance \\(\\frac{\\hat{\\sigma}^2}{N} \\left( \\frac{\\alpha_n}{\\alpha_n - 2} \\right)\\). Therefore, \\(\\mu | \\mathbf{y} \\sim t \\left( \\bar{y}, \\frac{\\hat{\\sigma}^2}{N} \\left( \\frac{\\alpha_n}{\\alpha_n - 2} \\right), \\alpha_n \\right)\\). Observe that a \\((1-\\alpha)\\%\\) confidence interval and a \\((1-\\alpha)\\%\\) credible interval have the same form, \\(\\bar{y} \\pm |t_{\\alpha/2}^{\\alpha_n}| \\frac{\\hat{\\sigma}}{\\sqrt{N}}\\), where \\(t_{\\alpha/2}^{\\alpha_n}\\) is the \\(\\alpha/2\\) percentile of a Student’s \\(t\\)-distribution. However, the interpretations are entirely different. The mathematical similarity between the Frequentist and Bayesian expressions in this example arises from the use of an improper prior. Example: Math test You have a random sample of math scores of size \\(N = 50\\) from a normal distribution, \\(Y_i \\sim N(\\mu, \\sigma^2)\\). The sample mean and variance are equal to 102 and 10, respectively. Assuming an improper prior equal to \\(\\frac{1}{\\sigma}\\), we proceed with the following tasks: Compute the 95% confidence and credible intervals for \\(\\mu\\). Determine the posterior probability that \\(\\mu &gt; 103\\). Using the fact that \\(\\mu | \\mathbf{y} \\sim t\\left(\\bar{y}, \\frac{\\hat{\\sigma}^2}{N} \\left( \\frac{\\alpha_n}{\\alpha_n - 2} \\right), \\alpha_n \\right)\\), which implies that the confidence and credible intervals for \\(\\mu\\) are given by: \\[ \\begin{aligned} \\bar{y} \\pm |t_{\\alpha/2}^{\\alpha_n}| \\frac{\\hat{\\sigma}}{\\sqrt{N}}, \\end{aligned} \\] where \\(\\bar{y} = 102\\), \\(\\hat{\\sigma}^2 = 10\\), and \\(\\alpha_n = 49\\). Thus, the 95% confidence and credible intervals for \\(\\mu\\) are the same, namely \\((101.1, 102.9)\\), and the posterior probability that \\(\\mu &gt; 103\\) is 1.49% given the sample information. # Load required package library(metRology) ## ## Adjuntando el paquete: &#39;metRology&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind # Input values n &lt;- 50 # Sample size sample_mean &lt;- 102 # Sample mean sample_var &lt;- 10 # Sample variance alpha &lt;- n - 1 # Degrees of freedom se &lt;- sqrt(sample_var / n) # Standard error # Confidence interval for the mean (95%) lower_bound &lt;- sample_mean - abs(qt(0.025, df = alpha)) * se upper_bound &lt;- sample_mean + abs(qt(0.025, df = alpha)) * se cat(sprintf(&quot;95%% CI: [%.3f, %.3f]\\n&quot;, lower_bound, upper_bound)) ## 95% CI: [101.101, 102.899] # Probability that population mean is greater than y_cut y_cut &lt;- 103 prob_mu_gt_ycut &lt;- 1 - pt.scaled(y_cut, df = alpha, mean = sample_mean, sd = se) cat(sprintf(&quot;P(mu &gt; %.1f) = %.4f\\n&quot;, y_cut, prob_mu_gt_ycut)) ## P(mu &gt; 103.0) = 0.0150 References Zellner, Arnold. 1996. “Introduction to Bayesian Inference in Econometrics.” "],["sec27.html", "2.7 Summary", " 2.7 Summary The differences between the Bayesian and Frequentist inferential approaches are philosophical, particularly with regard to the role of probability; pedagogical, especially in relation to the use of inference for decision-making; and methodological, due to differences in their mathematical and computational frameworks. Although, at the methodological level, the debate has become considerably muted —except for certain aspects of inference— there is widespread recognition that each approach has much to contribute to econometrics/statistical practice (Good (1992), M. J. Bayarri and Berger (2004), R. Kass (2011)). As Bradley Efron stated, “Computer-age statistical inference at its most successful combines elements of the two philosophies” (Efron and Hastie (2016)). References Bayarri, M. J., and J. Berger. 2004. “The Interplay of Bayesian and Frequentist Analysis.” Statistical Science 19 (1): 58–80. Efron, Bradley, and Trevor Hastie. 2016. Computer Age Statistical Inference. Vol. 5. Cambridge University Press. Good, I. J. 1992. “The Bayes/Non Bayes Compromise: A Brief Review.” Journal of the American Statistical Association 87 (419): 597–606. Kass, R. 2011. “Statistical Inference: The Big Picture.” Statistical Science 26 (1): 1–9. "],["sec28.html", "2.8 Exercises", " 2.8 Exercises Jeffreys-Lindley’s Paradox The Jeffreys-Lindley’s paradox (Jeffreys 1961; Dennis V. Lindley 1957) represents an apparent disagreement between the Bayesian and Frequentist frameworks in a hypothesis testing scenario. In particular, assume that in a city, 49,581 boys and 48,870 girls have been born over 20 years. Assume that the male births follow a Binomial distribution with probability \\(\\theta\\). We wish to test the null hypothesis \\(H_0: \\ \\theta = 0.5\\) versus the alternative hypothesis \\(H_1: \\ \\theta \\neq 0.5\\). Show that the posterior model probability for the null model is approximately 0.95. Assume \\(\\pi(H_0) = \\pi(H_1) = 0.5\\), and that \\(\\pi(\\theta)\\) follows a uniform distribution, i.e., \\({U}(0,1)\\), under \\(H_1\\). Show that the p-value for this hypothesis test is 0.0235 using the normal approximation, \\(Y \\sim N(N \\times \\theta, N \\times \\theta \\times (1 - \\theta))\\). We want to test \\(H_0: \\ \\mu = \\mu_0\\) versus \\(H_1: \\ \\mu \\neq \\mu_0\\) given \\(Y_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\). Assume \\(\\pi(H_0) = \\pi(H_1) = 0.5\\), and that \\(\\pi(\\mu, \\sigma) \\propto 1/\\sigma\\) under the alternative hypothesis. Show that \\[ p(\\mathbf{y}|\\mathcal{M}_1) = \\frac{\\pi^{-N/2}}{2} \\Gamma(N/2) \\left( \\frac{1}{\\alpha_n \\hat{\\sigma}^2} \\right)^{N/2} \\left( \\frac{N}{\\alpha_n \\hat{\\sigma}^2} \\right)^{-1/2} \\frac{\\Gamma(1/2) \\Gamma(\\alpha_n/2)}{\\Gamma((\\alpha_n+1)/2)} \\] and \\[ p(\\mathbf{y}|\\mathcal{M}_0) = (2\\pi)^{-N/2} \\left[ \\frac{2}{\\Gamma(N/2)} \\left( \\frac{N}{2} \\frac{\\sum_{i=1}^N (y_i - \\mu_0)^2}{N} \\right)^{N/2} \\right]^{-1}. \\] Then, the posterior odds ratio is: \\[ PO_{01} = \\frac{p(\\mathbf{y}|\\mathcal{M}_0)}{p(\\mathbf{y}|\\mathcal{M}_1)} = \\frac{\\Gamma((\\alpha_n+1)/2)}{\\Gamma(1/2)\\Gamma(\\alpha_n/2)} (\\alpha_n \\hat{\\sigma}^2 / N)^{-1/2} \\left[ 1 + \\frac{(\\mu_0 - \\bar{y})^2}{\\alpha_n \\hat{\\sigma}^2 / N} \\right]^{-\\left(\\frac{\\alpha_n + 1}{2}\\right)}, \\] where \\(\\alpha_n = N - 1\\) and \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N (y_i - \\bar{y})^2}{N-1}\\). Find the relationship between the posterior odds ratio and the classical test statistic for the null hypothesis. Math Test Continues Using the setting of the Example: Math Test in Subsection 2.6, test \\(H_0: \\ \\mu = \\mu_0\\) versus \\(H_1: \\ \\mu \\neq \\mu_0\\) where \\(\\mu_0 = \\{ 100, 100.5, 101, 101.5, 102 \\}\\). What is the p-value for these hypothesis tests? Find the posterior model probability of the null model for each \\(\\mu_0\\). References ———. 1961. Theory of Probability. London: Oxford University Press. Lindley, Dennis V. 1957. “A Statistical Paradox.” Biometrika 44 (1/2): 187–92. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
