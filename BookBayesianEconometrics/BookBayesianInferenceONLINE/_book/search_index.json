[["Chap12.html", "Chapter 12 Bayesian machine learning", " Chapter 12 Bayesian machine learning In this chapter, we focus on Bayesian approaches to supervised Machine learning (ML) problems, where the outcome variable is observed and used to guide prediction or inference. In contrast, unsupervised ML refers to settings in which the outcome variable is not observed, such as in clustering or dimensionality reduction. Machine learning methods are often characterized by high-dimensional parameter spaces, particularly in the context of nonparametric inference. It is important to note that nonparametric inference does not imply the absence of parameters, but rather models with potentially infinitely many parameters. This setting is often referred to as the wide problem, where the number of input variables, and consequently parameters, can exceed the sample size. Another common challenge in ML is the tall problem, which occurs when the sample size is extremely large, necessitating scalable algorithms. Specifically, we introduce Bayesian ML tools for regression, including regularization techniques, regression trees, and Gaussian processes. Extensions of these methods for binary classification are explored in some of the exercises. The section begins with a discussion on the relationship between cross-validation and Bayes factors and concludes with Bayesian approaches for addressing large-scale data challenges. "],["sec12_1.html", "12.1 Cross-validation and Bayes factors", " 12.1 Cross-validation and Bayes factors Prediction is central to machine learning, particularly in supervised learning. The starting point is a set of raw regressors or features, denoted by \\(\\mathbf{x}\\), which are used to construct a set of input variables fed into the model: \\(\\mathbf{w} = T(\\mathbf{x})\\), where \\(T(\\cdot)\\) represents a dictionary of transformations such as polynomials, interactions between variables, or the application of functions like exponentials, and so on. These inputs are then used to predict \\(y\\) through a model \\(f(\\mathbf{w})\\): \\[ y_i = f(\\mathbf{w}_i) + \\mu_i, \\] where \\(\\mu_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)\\). We predict \\(y\\) using \\(\\hat{f}(\\mathbf{w})\\), a model trained on the data. The expected squared error (ESE) at a fixed input \\(\\mathbf{w}_i\\) is given by: \\[\\begin{align*} \\text{ESE} &amp;= \\mathbb{E}_{\\mathcal{D},y} \\left[ (y_i - \\hat{f}(\\mathbf{w}_i))^2 \\right] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D},\\boldsymbol{\\mu}} \\left[ \\left(f(\\mathbf{w}_i) + \\mu_i - \\hat{f}(\\mathbf{w}_i)\\right)^2 \\right] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D},\\boldsymbol{\\mu}} \\left[ (f(\\mathbf{w}_i) - \\hat{f}(\\mathbf{w}_i))^2 + 2\\mu_i(f(\\mathbf{w}_i) - \\hat{f}(\\mathbf{w}_i)) + \\mu_i^2 \\right] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left[ (f(\\mathbf{w}_i) - \\hat{f}(\\mathbf{w}_i))^2 \\right] + \\mathbb{E}_{\\boldsymbol{\\mu}} \\left[ \\mu_i^2 \\right] \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} \\left[ \\left((f(\\mathbf{w}_i) - \\bar{f}(\\mathbf{w}_i)) - (\\hat{f}(\\mathbf{w}_i) - \\bar{f}(\\mathbf{w}_i)) \\right)^2 \\right] + \\sigma^2 \\\\ &amp;= \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ (f(\\mathbf{w}_i) - \\bar{f}(\\mathbf{w}_i))^2 \\right]}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ (\\hat{f}(\\mathbf{w}_i) - \\bar{f}(\\mathbf{w}_i))^2 \\right]}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}. \\end{align*}\\] Here, \\(\\mathcal{D}\\) denotes the distribution over datasets defined by the feature space. Independence between the noise \\(\\mu_i \\sim \\mathcal{N}(0, \\sigma^2)\\) and the estimator ensures that \\(\\mathbb{E}_{\\mathcal{D},\\boldsymbol{\\mu}} \\left[\\mu_i(f(\\mathbf{w}_i) - \\hat{f}(\\mathbf{w}_i))\\right] = 0\\). We also define \\(\\bar{f}(\\mathbf{w}_i) = \\mathbb{E}_{\\mathcal{D}}[\\hat{f}(\\mathbf{w}_i)]\\) as the expected predictor across datasets. Thus, the ESE is composed of the squared bias, the variance of the prediction (which is a random variable due to data variation \\(\\mathcal{D}\\)), and the irreducible error. The key insight is that increasing model complexity, such as by including more inputs, typically reduces bias but increases variance. This trade-off can lead to overfitting, where models perform well on the training data but poorly on new, unseen data. There is, therefore, an optimal point at which the predictive error is minimized (see the next Figure).1 To avoid overfitting in machine learning, an important step called cross-validation is often employed. This involves splitting the dataset into multiple parts (called folds) and systematically training and testing models on these parts (Hastie, Tibshirani, and Friedman 2009; Efron and Hastie 2021). The main goal is to evaluate how well models generalize to “unseen data”. There is a compelling justification for cross-validation within Bayesian inference proposed by Bernardo and Smith (1994) in their section 6.1.6. The point of departure is to assume an \\(\\mathcal{M}-open\\) view of nature, in which there exists a set of models \\[ \\mathcal{M} = \\{M_j : j \\in J\\} \\] under comparison, but none of them represents the true data-generating process, which is assumed to be unknown. Nevertheless, we can compare the models in \\(\\mathcal{M}\\) based on their posterior risk functions (see Chapter 1) without requiring the specification of a true model. In this framework, we select the model that minimizes the posterior expected loss. Unfortunately, we cannot explicitly compute this posterior expected loss due to the lack of knowledge of the true posterior distribution under the \\(\\mathcal{M}-open\\) assumption.2 Given the expected loss conditional on model \\(j\\) for a predictive problem, where the action \\(a\\) is chosen to minimize the posterior expected loss: \\[ \\mathbb{E}_{y_0}[L(Y_0,a) \\mid M_j, \\mathbf{y}] = \\int_{\\mathcal{Y}_0} L(y_0, a \\mid M_j, \\mathbf{y}) \\, \\pi(y_0 \\mid \\mathbf{y}) \\, dy_0, \\] we note that there are \\(N\\) possible partitions of the dataset \\[ \\mathbf{y} = \\{y_1, y_2, \\dots, y_N\\} \\] following a leave-one-out strategy, i.e., \\[ \\mathbf{y} = \\{\\mathbf{y}_{-k}, y_k\\}, \\quad k = 1, \\dots, N, \\] where \\(\\mathbf{y}_{-k}\\) denotes the dataset excluding observation \\(y_k\\). Assuming that \\(\\mathbf{y}\\) is exchangeable (i.e., its joint distribution is invariant to reordering) and that \\(N\\) is large, then \\(\\mathbf{y}_{-k}\\) and \\(y_k\\) are good proxies for \\(\\mathbf{y}\\) and \\(y_0\\), respectively. Consequently, \\[ \\frac{1}{K} \\sum_{k=1}^K L(y_k, a \\mid M_j, \\mathbf{y}_{-k}) \\stackrel{p}{\\rightarrow} \\int_{\\mathcal{Y}_0} L(y_0, a \\mid M_j, \\mathbf{y}) \\, \\pi(y_0 \\mid \\mathbf{y}) \\, dy_0, \\] by the law of large numbers, as \\(N \\to \\infty\\) and \\(K \\to \\infty\\). Thus, we can select a model by minimizing the expected squared error based on its expected predictions \\(\\mathbb{E}[y_k \\mid M_j, \\mathbf{y}_{-k}]\\); that is, we select the model with the lowest value of \\[ \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbb{E}[y_k \\mid M_j, \\mathbf{y}_{-k}] - y_k \\right)^2. \\] Note that if we want to compare two models based on their relative predictive accuracy using the log-score function (Martin et al. 2022), we select model \\(j\\) if \\[ \\int_{\\mathcal{Y}_0} \\log\\frac{p(y_0 \\mid M_j, \\mathbf{y})}{p(y_0 \\mid M_l, \\mathbf{y})} \\, \\pi(y_0 \\mid \\mathbf{y}) \\, dy_0 &gt; 0. \\] However, we know that \\[ \\frac{1}{K}\\sum_{k=1}^K\\log\\frac{p(y_k \\mid M_j, \\mathbf{y}_{-k})}{p(y_k \\mid M_l, \\mathbf{y}_{-k})} \\stackrel{p}{\\rightarrow} \\int_{\\mathcal{Y}_0} \\log\\frac{p(y_0 \\mid M_j, \\mathbf{y})}{p(y_0 \\mid M_l, \\mathbf{y})} \\, \\pi(y_0 \\mid \\mathbf{y}) \\, dy_0, \\] by the law of large numbers as \\(N \\to \\infty\\) and \\(K \\to \\infty\\). This implies that we select model \\(j\\) over model \\(l\\) if \\[ \\prod_{k=1}^K \\left( \\frac{p(y_k \\mid M_j, \\mathbf{y}_{-k})}{p(y_k \\mid M_l, \\mathbf{y}_{-k})} \\right)^{1/K} = \\prod_{k=1}^K \\left( B_{jl}(y_k, \\mathbf{y}_{-k}) \\right)^{1/K} &gt; 1, \\] where \\(B_{jl}(y_k, \\mathbf{y}_{-k})\\) is the Bayes factor comparing model \\(j\\) to model \\(l\\), based on the posterior \\(\\pi(\\boldsymbol{\\theta}_m \\mid M_m, \\mathbf{y}_{-k})\\) and the predictive \\(\\pi(y_k \\mid \\boldsymbol{\\theta}_m)\\), for \\(m \\in \\{j, l\\}\\). Thus, under the log-score function, cross-validation reduces to the geometric average of Bayes factors that evaluate predictive performance based on the leave-one-out samples \\(\\mathbf{y}_{-k}\\). References Bartlett, Peter L, Philip M Long, Gábor Lugosi, and Alexander Tsigler. 2020. “Benign Overfitting in Linear Regression.” Proceedings of the National Academy of Sciences 117 (48): 30063–70. Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. “Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.” Proceedings of the National Academy of Sciences 116 (32): 15849–54. Bernardo, J., and A. Smith. 1994. Bayesian Theory. Chichester: Wiley. Efron, Bradley, and Trevor Hastie. 2021. Computer Age Statistical Inference, Student Edition: Algorithms, Evidence, and Data Science. Vol. 6. Cambridge University Press. Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. 2022. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” Annals of Statistics 50 (2): 949. Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York: Springer. https://link.springer.com/book/10.1007/978-0-387-84858-7. Martin, Gael M, Rubén Loaiza-Maya, Worapree Maneesoonthorn, David T Frazier, and Andrés Ramı́rez-Hassan. 2022. “Optimal Probabilistic Forecasts: When Do They Work?” International Journal of Forecasting 38 (1): 384–406. However, recent developments show that powerful modern machine learning methods, such as deep neural networks, often overfit and yet generalize remarkably well on unseen data. This phenomenon is known as double descent or benign overfitting (Belkin et al. 2019; Bartlett et al. 2020; Hastie et al. 2022).↩︎ This is not the case under an \\(\\mathcal{M}-closed\\) view of nature, where one of the candidate models is assumed to be the true data-generating process. In that setting, the posterior distribution becomes a mixture distribution with mixing probabilities given by the posterior model probabilities (see Chapter 10).↩︎ "],["sec12_2.html", "12.2 Regularization", " 12.2 Regularization In this century, the amount of available data continues to grow. This means we have access to more covariates for prediction, and we can also generate additional inputs to enhance the predictive power of our models. As a result, we often encounter wide datasets, where the number of inputs may exceed the number of observations. Even in modest settings, we might have hundreds of inputs, and we can use them to identify which ones contribute to accurate predictions. However, we generally avoid using all inputs at once due to the risk of overfitting. Thus, we require a class of input selection or regularization. In the standard linear regression setting, \\[ \\mathbf{y} = \\mathbf{i}_N \\beta_0 + \\mathbf{W}\\boldsymbol{\\beta} + \\boldsymbol{\\mu}, \\] where \\(\\mathbf{i}_N\\) is an \\(N\\)-dimensional vector of ones, \\(\\mathbf{W}\\) is the \\(N \\times K\\) design matrix of inputs, and \\(\\boldsymbol{\\mu} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_N)\\), there has been extensive development of techniques aimed at regularization within the Frequentist inferential framework. These include methods such as Ridge regression (Hoerl and Kennard 1970); discrete subset selection techniques like best subset selection (Furnival and Wilson 1974), forward selection, and backward stepwise selection (Hastie, Tibshirani, and Friedman 2009); as well as continuous subset selection approaches such as the LASSO (Tibshirani 1996), the elastic net (Zou and Hastie 2005), and OSCAR (Bondell and Reich 2008). It is important to note, however, that Ridge regression does not perform variable selection; rather, it shrinks coefficient estimates toward zero without setting them exactly to zero. Ridge regression and the LASSO can be viewed as special cases of a more general class of estimators known as Bridge regression (Fu 1998), which also admits a Bayesian interpretation. Consider the following penalized least squares criterion in the linear regression setting: \\[ \\hat{\\boldsymbol{\\beta}}^{\\text{Bridge}} = \\arg\\min_{\\boldsymbol{\\beta}} \\left\\{ \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\tilde{w}_{ik} \\beta_k \\right)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|^q \\right\\}, \\quad q \\geq 0, \\] where \\(\\tilde{w}_{ik}\\) denotes the standardized inputs. Standardizing inputs is important in variable selection problems to avoid issues caused by differences in scale; otherwise, variables with larger magnitudes will be penalized less and disproportionately influence the regularization path. Interpreting \\(|\\beta_k|^q\\) as proportional to the negative log-prior density of \\(\\beta_k\\), the penalty shapes the contours of the prior distribution on the parameters. Specifically: \\(q = 0\\) corresponds to best subset selection, where the penalty counts the number of nonzero coefficients. \\(q = 1\\) yields the LASSO, which corresponds to a Laplace (double-exponential) prior. \\(q = 2\\) yields ridge regression, which corresponds to a Gaussian prior. In this light, best subset selection, the LASSO, and ridge regression can be viewed as maximum a posteriori (MAP) estimators under different priors centered at zero (Park and Casella 2008). However, they are not Bayes estimators in the strict sense, since Bayes estimators are typically defined as the posterior mean. While ridge regression coincides with the posterior mean under a Gaussian prior (Ishwaran and Rao 2005), the LASSO and best subset selection yield posterior modes. This distinction is important because the Bayesian framework naturally incorporates regularization through the use of proper priors, which helps mitigate overfitting. Specifically, when proper shrinkage priors are used, the posterior balances data likelihood and prior information, thus controlling model complexity. Furthermore, empirical Bayes methods, where the marginal likelihood is optimized, or cross-validation can be used to estimate the scale parameter of the prior covariance matrix for the regression coefficients. This scale parameter, in turn, determines the strength of regularization in ridge regression. Note that regularization introduces bias into the parameter estimates because it constrains the model, shrinking the location parameters toward zero. However, it substantially reduces variance, as the estimates are prevented from varying excessively across samples. As a result, the mean squared error (MSE) of the estimates, which equals the sum of the squared bias and the variance, is often lower for regularization methods compared to ordinary least squares (OLS), which remains unbiased under the classical assumptions. This trade-off is particularly important when the goal is to identify causal effects, where unbiasedness may be preferred over predictive accuracy (see Chapter 13). 12.2.1 Bayesian LASSO Given the popularity of the LASSO as a variable selection technique, we present its Bayesian formulation in this subsection (Park and Casella 2008). The Gibbs sampler for the Bayesian LASSO exploits the representation of the Laplace distribution as a scale mixture of normals. This leads to the following hierarchical representation of the model: \\[ \\begin{aligned} \\mathbf{y} \\mid \\beta_0, \\boldsymbol{\\beta}, \\sigma^2, \\mathbf{W} &amp;\\sim \\mathcal{N}(\\mathbf{i}_N \\beta_0 + \\mathbf{W} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_N), \\\\ \\boldsymbol{\\beta} \\mid \\sigma^2, \\tau_1^2, \\dots, \\tau_K^2 &amp;\\sim \\mathcal{N}(\\mathbf{0}_K, \\sigma^2 \\mathbf{D}_{\\tau}), \\\\ \\tau_1^2, \\dots, \\tau_K^2 &amp;\\sim \\prod_{k=1}^K \\frac{\\lambda^2}{2} \\exp\\left\\{ -\\frac{\\lambda^2}{2} \\tau_k^2 \\right\\}, \\\\ \\sigma^2 &amp;\\sim \\frac{1}{\\sigma^2}, \\\\ \\beta_0 &amp;\\sim c, \\end{aligned} \\] where \\(\\mathbf{D}_{\\tau} = \\operatorname{diag}(\\tau_1^2, \\dots, \\tau_K^2)\\) and \\(c\\) is a constant. After integrating out \\(\\tau_1^2, \\dots, \\tau_K^2\\), the conditional prior of \\(\\boldsymbol{\\beta} \\mid \\sigma^2\\) is: \\[ \\pi(\\boldsymbol{\\beta} \\mid \\sigma^2) = \\prod_{k=1}^K \\frac{\\lambda}{2 \\sqrt{\\sigma^2}} \\exp\\left\\{ -\\frac{\\lambda}{\\sqrt{\\sigma^2}} |\\beta_k| \\right\\}, \\] which implies that the log-prior is proportional to \\(\\lambda \\sum_{k=1}^K |\\beta_k|\\), matching the penalty term in the LASSO optimization problem. The conditional posterior distributions for the Gibbs sampler are (Park and Casella 2008): \\[ \\begin{aligned} \\boldsymbol{\\beta} \\mid \\sigma^2, \\tau_1^2, \\dots, \\tau_K^2, \\tilde{\\mathbf{W}}, \\tilde{\\mathbf{y}} &amp;\\sim \\mathcal{N}(\\boldsymbol{\\beta}_n, \\sigma^2 \\mathbf{B}_n), \\\\ \\sigma^2 \\mid \\boldsymbol{\\beta}, \\tau_1^2, \\dots, \\tau_K^2, \\tilde{\\mathbf{W}}, \\tilde{\\mathbf{y}} &amp;\\sim \\text{Inverse-Gamma}(\\alpha_n/2, \\delta_n/2), \\\\ 1/\\tau_k^2 \\mid \\boldsymbol{\\beta}, \\sigma^2 &amp;\\sim \\text{Inverse-Gaussian}(\\mu_{kn}, \\lambda_n), \\\\ \\beta_0 \\mid \\sigma^2, \\tilde{\\mathbf{W}}, \\tilde{\\mathbf{y}} &amp;\\sim \\mathcal{N}(\\bar{y}, \\sigma^2 / N), \\end{aligned} \\] where: \\[ \\begin{aligned} \\boldsymbol{\\beta}_n &amp;= \\mathbf{B}_n \\tilde{\\mathbf{W}}^{\\top} \\tilde{\\mathbf{y}}, \\\\ \\mathbf{B}_n &amp;= \\left( \\tilde{\\mathbf{W}}^{\\top} \\tilde{\\mathbf{W}} + \\mathbf{D}_{\\tau}^{-1} \\right)^{-1}, \\\\ \\alpha_n &amp;= (N - 1) + K, \\\\ \\delta_n &amp;= (\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{W}} \\boldsymbol{\\beta})^{\\top} (\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{W}} \\boldsymbol{\\beta}) + \\boldsymbol{\\beta}^{\\top} \\mathbf{D}_{\\tau}^{-1} \\boldsymbol{\\beta}, \\\\ \\mu_{kn} &amp;= \\sqrt{ \\frac{ \\lambda^2 \\sigma^2 }{ \\beta_k^2 } }, \\\\ \\lambda_n &amp;= \\lambda^2, \\end{aligned} \\] and \\(\\tilde{\\mathbf{W}}\\) is the matrix of standardized inputs, and \\(\\tilde{\\mathbf{y}}\\) is the centered response vector. Note that the posterior distribution of \\(\\boldsymbol{\\tau}\\) depends on the data through \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\), which is a typical feature of hierarchical models. In this formulation, we can interpret \\(\\tau_k\\) as local shrinkage parameters, while \\(\\lambda\\) acts as a global shrinkage parameter. Higher values of \\(\\tau_k\\) and \\(\\lambda\\) imply stronger shrinkage toward zero. Park and Casella (2008) propose two approaches for specifying the global shrinkage parameter: empirical Bayes estimation or a fully Bayesian hierarchical specification, where \\(\\lambda^2\\) is assigned a Gamma prior. We should acknowledge that the Bayesian LASSO is more computationally expensive than the Frequentist LASSO. However, it provides credible intervals for the parameters automatically. In contrast, obtaining standard errors in the Frequentist LASSO is more challenging, particularly for parameters estimated to be exactly zero (Kyung et al. 2010). Example: Simulation exercise to study the Bayesian LASSO performance We simulate the process \\[\\begin{equation*} y_i = \\beta_0 + \\sum_{k=1}^{10} \\beta_k w_{ik} + \\mu_i, \\end{equation*}\\] where \\(\\beta_k \\sim \\mathcal{U}(-3, 3)\\), \\(\\mu_i \\sim \\mathcal{N}(0, 1)\\), and \\(w_{ik} \\sim \\mathcal{N}(0, 1)\\), for \\(i = 1, 2, \\dots, 500\\). Additionally, we generate 90 extra potential inputs from a standard normal distribution, which are included in the model specification. Our goal is to assess whether the Bayesian LASSO can successfully identify the truly relevant inputs. We use the bayesreg package in R to perform the Bayesian LASSO, using 5,000 MCMC draws and 1,000 burn-in iterations. The following code illustrates the simulation exercise and compares the posterior means with the true population values. The summary of the fit and the plot comparing the population parameters with the posterior means show that the Bayesian LASSO is able to identify the variables that are relevant in the data generating process. In Exercise 1, we propose programming the Gibbs sampler from scratch, assuming a hierarchical structure for the global shrinkage parameter, and comparing the results with those obtained using the monomvn package. ####### Bayesian LASSSO ####### rm(list = ls()); set.seed(10101) library(bayesreg) ## Loading required package: pgdraw ## Loading required package: doParallel ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel # Parameters n &lt;- 500 # sample size p &lt;- 100 # number of predictors s &lt;- 10 # number of non-zero coefficients # Generate design matrix X &lt;- matrix(rnorm(n * p), nrow = n, ncol = p) # True beta: first s coefficients are non-zero, rest are zero beta_true &lt;- c(runif(s, -3, 3), rep(0, p - s)) # Generate response with some noise sigma &lt;- 1 y &lt;- X %*% beta_true + rnorm(n, sd = sigma) df &lt;- data.frame(X,y) ### Using bayesreg ### # Fit the model fit &lt;- bayesreg::bayesreg(y ~ X, data = df, model = &quot;gaussian&quot;, prior = &quot;lasso&quot;, n.samples = 5000, burnin = 1000) # Check summary summary(fit) ## ========================================================================================== ## | Bayesian Penalised Regression Estimation ver. 1.3 | ## | (c) Enes Makalic, Daniel F Schmidt. 2016-2024 | ## ========================================================================================== ## Bayesian Gaussian lasso regression Number of obs = 500 ## Number of vars = 100 ## MCMC Samples = 5000 std(Error) = 1.0288 ## MCMC Burnin = 1000 R-squared = 0.9759 ## MCMC Thinning = 5 WAIC = 766.95 ## ## -------------+----------------------------------------------------------------------------- ## Parameter | mean(Coef) std(Coef) [95% Cred. Interval] tStat Rank ESS ## -------------+----------------------------------------------------------------------------- ## X1 | -0.06846 0.04787 -0.16553 0.02235 -1.430 19 * 5000 ## X2 | -2.20889 0.05214 -2.31015 -2.10684 -42.367 3 ** 5000 ## X3 | -2.63620 0.05064 -2.73225 -2.53394 -52.061 1 ** 4902 ## X4 | -0.73971 0.04945 -0.83421 -0.64341 -14.959 9 ** 4874 ## X5 | 1.81048 0.05031 1.70944 1.90729 35.990 6 ** 4909 ## X6 | -2.11694 0.05024 -2.21527 -2.01832 -42.135 3 ** 4875 ## X7 | -2.09674 0.05091 -2.19591 -1.99645 -41.188 5 ** 4883 ## X8 | 0.88083 0.04749 0.78826 0.97690 18.546 8 ** 5000 ## X9 | 1.26806 0.05265 1.16364 1.36835 24.083 7 ** 4782 ## X10 | -2.55504 0.05074 -2.65378 -2.45360 -50.357 2 ** 5000 ## X11 | -0.02984 0.04505 -0.12174 0.05499 -0.662 51 5000 ## X12 | -0.02383 0.04923 -0.12258 0.07081 -0.484 64 5000 ## X13 | -0.06461 0.05090 -0.16903 0.03217 -1.269 22 * 4935 ## X14 | -0.02827 0.04544 -0.11954 0.05899 -0.622 51 4720 ## X15 | -0.04619 0.04855 -0.14425 0.04514 -0.951 35 5000 ## X16 | -0.08119 0.04886 -0.17889 0.00997 -1.662 14 * 5000 ## X17 | -0.02091 0.04785 -0.12014 0.07259 -0.437 64 4930 ## X18 | -0.01181 0.04622 -0.10366 0.07920 -0.255 64 5000 ## X19 | 0.06294 0.04849 -0.02904 0.16060 1.298 23 * 5000 ## X20 | 0.01938 0.04746 -0.07228 0.11705 0.408 59 5000 ## X21 | 0.03023 0.04717 -0.06063 0.12744 0.641 51 5000 ## X22 | -0.02100 0.04783 -0.11441 0.07403 -0.439 64 5000 ## X23 | 0.03310 0.04831 -0.05959 0.13041 0.685 46 5000 ## X24 | -0.00293 0.04287 -0.08775 0.08277 -0.068 83 4604 ## X25 | -0.00359 0.04305 -0.09036 0.08023 -0.083 94 5000 ## X26 | -0.01312 0.04641 -0.10476 0.07753 -0.283 64 5000 ## X27 | -0.05874 0.04766 -0.15534 0.03207 -1.232 23 * 5000 ## X28 | 0.02100 0.04708 -0.06942 0.11605 0.446 59 5000 ## X29 | 0.08368 0.05153 -0.01476 0.18622 1.624 17 * 5000 ## X30 | 0.04349 0.04802 -0.04534 0.14131 0.906 37 4878 ## X31 | 0.02810 0.04488 -0.05767 0.11858 0.626 51 5000 ## X32 | -0.08010 0.04764 -0.17565 0.00993 -1.681 15 * 4601 ## X33 | -0.02008 0.04646 -0.11440 0.07407 -0.432 64 4923 ## X34 | -0.01960 0.04745 -0.11445 0.07097 -0.413 64 5000 ## X35 | -0.01971 0.04646 -0.11370 0.07044 -0.424 64 5000 ## X36 | 0.00137 0.04606 -0.09109 0.09042 0.030 94 5000 ## X37 | -0.04898 0.04633 -0.14278 0.04081 -1.057 30 4860 ## X38 | -0.00762 0.04814 -0.10455 0.08705 -0.158 83 5000 ## X39 | 0.01465 0.04858 -0.08168 0.11057 0.302 64 4921 ## X40 | -0.02052 0.04789 -0.11806 0.07202 -0.429 64 5000 ## X41 | -0.00984 0.04475 -0.10075 0.07534 -0.220 83 5000 ## X42 | 0.03611 0.04572 -0.05046 0.12763 0.790 41 4993 ## X43 | -0.05567 0.04624 -0.14978 0.03144 -1.204 26 * 5000 ## X44 | -0.01799 0.04614 -0.11113 0.07238 -0.390 83 5000 ## X45 | 0.07281 0.04817 -0.01763 0.16846 1.512 18 * 4974 ## X46 | -0.03812 0.04603 -0.13067 0.04984 -0.828 41 4772 ## X47 | -0.05021 0.05050 -0.15135 0.04692 -0.994 33 5000 ## X48 | 0.02324 0.04788 -0.07006 0.12100 0.485 64 4890 ## X49 | -0.01159 0.04908 -0.11191 0.08513 -0.236 83 4487 ## X50 | -0.02054 0.04440 -0.10933 0.06671 -0.463 64 5000 ## X51 | -0.05342 0.04534 -0.14696 0.02961 -1.178 27 * 4864 ## X52 | -0.00232 0.04802 -0.09979 0.09112 -0.048 94 5000 ## X53 | -0.04888 0.04983 -0.14710 0.04458 -0.981 34 4843 ## X54 | -0.04495 0.04985 -0.14137 0.05127 -0.902 37 5000 ## X55 | 0.04064 0.04794 -0.05000 0.13733 0.848 35 5000 ## X56 | 0.03062 0.04573 -0.05816 0.12243 0.669 46 5000 ## X57 | 0.01757 0.04627 -0.07281 0.11120 0.380 59 5000 ## X58 | 0.00791 0.04478 -0.08072 0.09645 0.177 94 5000 ## X59 | -0.05204 0.04614 -0.14660 0.03497 -1.128 30 5000 ## X60 | -0.00137 0.04525 -0.09169 0.08659 -0.030 94 5000 ## X61 | 0.15134 0.04751 0.06003 0.24554 3.185 10 ** 4766 ## X62 | 0.03062 0.04718 -0.06161 0.12551 0.649 51 4748 ## X63 | -0.08321 0.05087 -0.18511 0.01556 -1.636 16 * 4923 ## X64 | -0.00506 0.04859 -0.10080 0.09523 -0.104 83 4871 ## X65 | -0.03445 0.04583 -0.12700 0.05569 -0.752 45 4834 ## X66 | -0.02377 0.04641 -0.11660 0.06637 -0.512 59 4948 ## X67 | 0.01857 0.04584 -0.06913 0.11214 0.405 82 4966 ## X68 | -0.03071 0.04850 -0.12906 0.06060 -0.633 51 4812 ## X69 | -0.05059 0.04590 -0.14304 0.03433 -1.102 27 5000 ## X70 | -0.06572 0.04679 -0.15715 0.02238 -1.404 21 * 5000 ## X71 | -0.04130 0.04662 -0.13542 0.04777 -0.886 37 4923 ## X72 | -0.00592 0.04640 -0.10103 0.08507 -0.128 94 5000 ## X73 | -0.03940 0.04850 -0.13918 0.05487 -0.812 44 5000 ## X74 | 0.02703 0.04527 -0.05906 0.11829 0.597 59 4863 ## X75 | 0.06576 0.04664 -0.02126 0.15939 1.410 20 * 5000 ## X76 | -0.01297 0.04721 -0.10485 0.08043 -0.275 83 4962 ## X77 | 0.04205 0.04812 -0.05121 0.13592 0.874 37 5000 ## X78 | -0.02925 0.04861 -0.12989 0.06276 -0.602 51 5000 ## X79 | -0.01863 0.04579 -0.11045 0.07033 -0.407 83 5000 ## X80 | -0.05865 0.04764 -0.15234 0.03341 -1.231 25 * 5000 ## X81 | 0.03343 0.04691 -0.05330 0.13119 0.713 46 5000 ## X82 | -0.02566 0.04479 -0.11608 0.05957 -0.573 64 4007 ## X83 | -0.04784 0.04870 -0.14747 0.04214 -0.982 32 4422 ## X84 | -0.09470 0.05132 -0.19578 0.00349 -1.845 12 * 5000 ## X85 | -0.04008 0.04675 -0.13326 0.05164 -0.857 41 5000 ## X86 | -0.05453 0.04663 -0.14806 0.03520 -1.169 27 * 4998 ## X87 | -0.00647 0.04631 -0.09761 0.08573 -0.140 83 4853 ## X88 | 0.02031 0.04739 -0.07393 0.11454 0.429 64 5000 ## X89 | -0.08392 0.04824 -0.17978 0.00652 -1.740 13 * 4796 ## X90 | 0.01793 0.04702 -0.07622 0.11131 0.381 64 5000 ## X91 | -0.00289 0.04563 -0.09412 0.08779 -0.063 100 5000 ## X92 | 0.02671 0.04723 -0.06359 0.12439 0.566 51 5000 ## X93 | -0.09987 0.05326 -0.20431 0.00284 -1.875 11 * 4768 ## X94 | -0.02222 0.04350 -0.11087 0.06412 -0.511 64 4820 ## X95 | 0.00413 0.04754 -0.09094 0.09844 0.087 83 5000 ## X96 | 0.01958 0.04755 -0.07093 0.11457 0.412 64 5000 ## X97 | -0.03615 0.04896 -0.13597 0.05546 -0.738 46 5000 ## X98 | 0.02667 0.04584 -0.06241 0.11810 0.582 64 5000 ## X99 | -0.03480 0.04707 -0.12850 0.05742 -0.739 46 4833 ## X100 | -0.01154 0.04404 -0.10090 0.07055 -0.262 83 5000 ## (Intercept) | 0.06165 0.05040 -0.03503 0.16116 . . . ## -------------+----------------------------------------------------------------------------- # Extract posterior means of beta beta_post_mean &lt;- rowMeans(fit$beta) # Compare true vs estimated plot(beta_true, beta_post_mean, pch = 19, col = &quot;steelblue&quot;, xlab = &quot;True beta&quot;, ylab = &quot;Posterior mean of beta&quot;, main = &quot;Bayesian LASSO Shrinkage&quot;) abline(0, 1, col = &quot;red&quot;, lty = 2) 12.2.2 Stochastic search variable selection Another well-known Bayesian strategy for variable selection in the presence of a large set of regressors (inputs) is stochastic search variable selection (SSVS) (E. I. George and McCulloch 1993; E. George and McCulloch 1997). SSVS is a particular case of the broader class of spike-and-slab priors, in which the prior distribution for the location parameters is specified as a hierarchical mixture that captures the uncertainty inherent in variable selection problems (Ishwaran and Rao 2005). The hierarchical structure of the model is given by: \\[ \\begin{aligned} \\mathbf{y} \\mid \\beta_0, \\boldsymbol{\\beta}, \\sigma^2, \\mathbf{W} &amp;\\sim \\mathcal{N}(\\mathbf{i}_N \\beta_0 + \\mathbf{W} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_N), \\\\ \\boldsymbol{\\beta} \\mid \\sigma^2, \\boldsymbol{\\gamma} &amp;\\sim \\mathcal{N}(\\mathbf{0}_K, \\sigma^2 \\mathbf{D}_\\gamma \\mathbf{R} \\mathbf{D}_\\gamma), \\\\ \\sigma^2 &amp;\\sim \\text{Inverse-Gamma}\\left(\\frac{v}{2}, \\frac{v \\lambda_\\gamma}{2}\\right), \\\\ \\gamma_k &amp;\\sim \\text{Bernoulli}(p_k), \\end{aligned} \\] where \\(p_k\\) is the prior inclusion probability of regressor \\(w_k\\), that is, \\(P(\\gamma_k = 1) = 1 - P(\\gamma_k = 0) = p_k\\), \\(\\mathbf{R}\\) is a correlation matrix, and \\(\\mathbf{D}_\\gamma\\) is a diagonal matrix whose \\((k,k)\\)-th element is defined as: \\[ (\\mathbf{D}_\\gamma)_{kk} = \\begin{Bmatrix} v_{0k}, &amp; \\text{if } \\gamma_k = 0, \\\\ v_{1k}, &amp; \\text{if } \\gamma_k = 1 \\end{Bmatrix}. \\] This formulation implies that: \\[ \\beta_k \\sim (1 - \\gamma_k) \\, \\mathcal{N}(0, v_{0k}) + \\gamma_k \\, \\mathcal{N}(0, v_{1k}), \\] where \\(v_{0k}\\) and \\(v_{1k}\\) are chosen such that \\(v_{0k}\\) is small and \\(v_{1k}\\) is large. Therefore, when the data favors \\(\\gamma_k = 0\\), the corresponding \\(\\beta_k\\) is shrunk toward zero, effectively excluding input \\(k\\) from the model. In this sense, \\(\\mathcal{N}(0, v_{0k})\\) is a spike prior concentrated at zero, while \\(\\mathcal{N}(0, v_{1k})\\) is a diffuse slab prior. A critical aspect of SSVS is the choice of the hyperparameters \\(v_{0k}\\) and \\(v_{1k}\\), as they determine the amount of shrinkage applied to the regression coefficients (see E. I. George and McCulloch (1993) and E. George and McCulloch (1997) for details). The assumption \\(\\gamma_k \\sim \\text{Bernoulli}(p_k)\\) implies that the prior on the inclusion indicators is given by: \\[ \\pi(\\boldsymbol{\\gamma}) = \\prod_{k=1}^K p_k^{\\gamma_k} (1 - p_k)^{1 - \\gamma_k}. \\] This means that the inclusion of input \\(k\\) is independent of the inclusion of any other input \\(j \\neq k\\). A common choice is the uniform prior \\(\\pi(\\boldsymbol{\\gamma}) = 2^{-K}\\), which corresponds to setting \\(p_k = 1/2\\), giving each regressor an equal chance of being included (Ishwaran and Rao 2005). A practical choice for the correlation matrix is to set \\(\\mathbf{R} \\propto (\\tilde{\\mathbf{W}}^{\\top} \\tilde{\\mathbf{W}})^{-1}\\) (E. I. George and McCulloch 1993). Regarding the hyperparameters \\(v\\) and \\(\\lambda_\\gamma\\), it is helpful to interpret \\(\\lambda_\\gamma\\) as a prior estimate of \\(\\sigma^2\\), and \\(v\\) as the prior sample size associated with this estimate. In the absence of prior information, E. I. George and McCulloch (1997) recommend setting \\(\\lambda_\\gamma\\) equal to the least squares estimate of the variance from the saturated model, that is, the model including all regressors, and \\(v\\) to a small number, for instance, 0.01. The conditional posterior distributions for the Gibbs sampler are (E. I. George and McCulloch 1993): \\[ \\begin{aligned} \\boldsymbol{\\beta} \\mid \\sigma^2, \\gamma_1, \\dots, \\gamma_K, \\tilde{\\mathbf{W}}, \\tilde{\\mathbf{y}} &amp;\\sim N(\\boldsymbol{\\beta}_n, \\mathbf{B}_n), \\\\ \\sigma^2 \\mid \\boldsymbol{\\beta}, \\gamma_1, \\dots, \\gamma_K, \\tilde{\\mathbf{W}}, \\tilde{\\mathbf{y}} &amp;\\sim \\text{Inverse-Gamma}(\\alpha_n/2, \\delta_n/2), \\\\ \\gamma_k \\mid \\boldsymbol{\\beta}, \\sigma^2 &amp;\\sim \\text{Bernoulli}(p_{kn}), \\end{aligned} \\] where: \\[ \\begin{aligned} \\boldsymbol{\\beta}_n &amp;= \\sigma^{-2} \\mathbf{B}_n \\tilde{\\mathbf{W}}^{\\top} \\tilde{\\mathbf{y}}, \\\\ \\mathbf{B}_n &amp;= \\left(\\sigma^{-2} \\tilde{\\mathbf{W}}^{\\top} \\tilde{\\mathbf{W}} + \\mathbf{D}_{\\gamma}^{-1}\\mathbf{R}^{-1}\\mathbf{D}_{\\gamma}^{-1} \\right)^{-1}, \\\\ \\alpha_n &amp;= N + v, \\\\ \\delta_n &amp;= (\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{W}} \\boldsymbol{\\beta})^{\\top} (\\tilde{\\mathbf{y}} - \\tilde{\\mathbf{W}} \\boldsymbol{\\beta}) + v\\lambda_{\\gamma}, \\\\ p_{kn} &amp;= \\frac{\\pi(\\boldsymbol{\\beta}\\mid \\boldsymbol{\\gamma}_{-k},\\gamma_k=1)\\times p_k}{\\pi(\\boldsymbol{\\beta}\\mid \\boldsymbol{\\gamma}_{-k},\\gamma_k=1)\\times p_k+\\pi(\\boldsymbol{\\beta}\\mid \\boldsymbol{\\gamma}_{-k},\\gamma_k=0)\\times (1-p_k)}, \\end{aligned} \\] where \\(\\tilde{\\mathbf{W}}\\) is the matrix of standardized inputs, \\(\\tilde{\\mathbf{y}}\\) is the centered response vector, \\(\\boldsymbol{\\gamma}_{-k}\\) denotes the vector composed of \\(\\gamma_1, \\dots, \\gamma_K\\) excluding \\(\\gamma_k\\), and \\(\\pi(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\gamma}_{-k}, \\gamma_k = \\delta)\\) is the posterior density of \\(\\boldsymbol{\\beta}\\) evaluated at \\(\\boldsymbol{\\gamma}_{-k}\\) and \\(\\gamma_k = \\delta\\), where \\(\\delta \\in \\{0,1\\}\\). In general, it is wise to consider the inclusion of regressors jointly due to potential correlations among them; that is, the marginal frequency of \\(\\gamma_k = 1\\) should be interpreted with caution. SSVS is more effective at identifying a good set of potential models rather than selecting a single best model. Example: Simulation exercise to study SSVS performance Let’s use the simulation setting from the previous example to evaluate the performance of SSVS in uncovering the data-generating process. In particular, we use the BoomSpikeSlab package to implement this example. The analysis is performed using 5,000 posterior draws and the default prior. However, the package allows the user to modify the default prior via the SpikeSlabPrior function. The results show that the posterior inclusion probabilities for regressors 2 through 10 are 100%, and the model with the highest posterior probability (94%) includes all of these nine variables. However, the true data-generating process, which also includes regressor 1, receives a posterior model probability of 0%. This is because the population coefficient of this regressor is essentially zero. The plot comparing the posterior means with the true population parameters indicates good performance of SSVS. In general, Bayesian methods for variable selection perform well, and the choice of the most suitable method largely depends on the prior specification (O’Hara and Sillanpää 2009). ####### Stochastic search variable selection ####### rm(list = ls()); set.seed(10101) library(BoomSpikeSlab) ## Loading required package: Boom ## ## Attaching package: &#39;Boom&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## rWishart ## ## Attaching package: &#39;BoomSpikeSlab&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## knots library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tibble) # Parameters n &lt;- 500 # sample size k &lt;- 100 # number of predictors s &lt;- 10 # number of non-zero coefficients # Generate design matrix X &lt;- matrix(rnorm(n * k), nrow = n, ncol = k) # True beta: first s coefficients are non-zero, rest are zero beta_true &lt;- c(runif(s, -3, 3), rep(0, k - s)) # Generate response with some noise sigma &lt;- 1 y &lt;- X %*% beta_true + rnorm(n, sd = sigma) df &lt;- data.frame(X,y) ### Using BoomSpikeSlab ### #Scale regressors W &lt;- scale(X); yh &lt;- y - mean(y) prior &lt;- SpikeSlabPrior(W, yh, expected.model.size = ncol(W)/2, # expect 50 nonzero predictors prior.df = .01, # weaker prior than the default prior.information.weight = .01, diagonal.shrinkage = 0) # shrink to zero niter &lt;- 5000 ######Estimate model######## SSBoomNew &lt;- lm.spike(yh ~ W - 1, niter = niter, prior = prior) ## =-=-=-=-= Iteration 0 Tue Nov 18 18:15:38 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 500 Tue Nov 18 18:15:39 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 1000 Tue Nov 18 18:15:39 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 1500 Tue Nov 18 18:15:40 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 2000 Tue Nov 18 18:15:40 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 2500 Tue Nov 18 18:15:41 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 3000 Tue Nov 18 18:15:41 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 3500 Tue Nov 18 18:15:42 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 4000 Tue Nov 18 18:15:42 2025 ## =-=-=-=-= ## =-=-=-=-= Iteration 4500 Tue Nov 18 18:15:43 2025 ## =-=-=-=-= Models &lt;- SSBoomNew$beta != 0 PIP &lt;- colMeans(SSBoomNew$beta != 0) # Convert the logical matrix to a data frame and then to a tibble df &lt;- as.data.frame(Models); df_tbl &lt;- as_tibble(df) # Count identical rows row_counts &lt;- df_tbl %&gt;% count(across(everything()), name = &quot;frequency&quot;) %&gt;% arrange(desc(frequency)) sum(row_counts[1:100,101]) ## [1] 3948 # Ensure your vector and matrix are logical trueModel &lt;- c(rep(1, 10), rep(0, 90)) == 1 # convert to logical if needed # Assume your matrix is named &#39;mat&#39; matching_rows &lt;- apply(row_counts[,-101], 1, function(row) all(row == trueModel)) # Get indices (row numbers) where the match is TRUE row_counts[which(matching_rows), 101] ## # A tibble: 1 × 1 ## frequency ## &lt;int&gt; ## 1 3 # Coefficients SummarySS &lt;- summary(coda::mcmc(SSBoomNew$beta)) # Extract posterior means of beta beta_post_mean &lt;- SummarySS$statistics[, 1] # Compare true vs estimated plot(beta_true, beta_post_mean, pch = 19, col = &quot;steelblue&quot;, xlab = &quot;True beta&quot;, ylab = &quot;Posterior mean of beta&quot;, main = &quot;SSVS Shrinkage&quot;) abline(0, 1, col = &quot;red&quot;, lty = 2) The examples and exercises presented thus far have considered scenarios in which the number of inputs is smaller than the number of observations (\\(K &lt; N\\)). In Exercise 4, we challenge the Bayesian LASSO and SSVS in a setting where the number of inputs exceeds the sample size (\\(K &gt; N\\)). As you will observe in that experiment, both the Bayesian LASSO and SSVS perform well. However, the Bayesian LASSO requires more time to produce results compared to SSVS in this exercise. Ročková and George (2018) propose a connection between the LASSO and spike-and-slab priors for variable selection in linear models, offering oracle properties and optimal posterior concentration even in high-dimensional settings where \\(K &gt; N\\). In addition, there are other Bayesian methods for regularization, such as the spike-and-slab approach proposed by Ishwaran and Rao (2005) and non-local priors introduced by Johnson and Rossell (2012), which can be implemented using the R packages spikeslab and mombf, respectively. References Bondell, Howard D., and Brian J. Reich. 2008. “Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR.” Biometrics 64 (1): 115–23. https://doi.org/10.1111/j.1541-0420.2007.00843.x. Fu, Wenjiang J. 1998. “Penalized Regression: The Bridge Versus the Lasso.” Journal of Computational and Graphical Statistics 7 (3): 397–416. https://doi.org/10.1080/10618600.1998.10474784. Furnival, George M., and R. W. Wilson. 1974. “Regressions by Leaps and Bounds.” Technometrics 16 (4): 499–511. https://doi.org/10.1080/00401706.1974.10489152. George, Edward I, and Robert E McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89. ———. 1997. “Approaches for Bayesian Variable Selection.” Statistica Sinica 7: 339–73. George, E., and R. McCulloch. 1997. “Approaches for Bayesian Variable Selection.” Statistica Sinica 7: 339–73. Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York: Springer. https://link.springer.com/book/10.1007/978-0-387-84858-7. Hoerl, Arthur E., and Robert W. Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67. https://doi.org/10.1080/00401706.1970.10488634. Ishwaran, H., and J. S. Rao. 2005. “Spike and Slab Variable Selection: Frequentist and Bayesian Strategies.” The Annals of Statistics 33 (2): 730–73. Johnson, Valen E, and David Rossell. 2012. “Bayesian Model Selection in High-Dimensional Settings.” Journal of the American Statistical Association 107 (498): 649–60. Kyung, Minjung, Jeff Gill, Malay Ghosh, and George Casella. 2010. “Penalized Regression, Standard Errors, and Bayesian Lassos.” Bayesian Analysis 5 (2): 369–411. https://doi.org/10.1214/10-BA607. O’Hara, Robert B., and Mikko J. Sillanpää. 2009. “A Review of Bayesian Variable Selection Methods: What, How and Which.” Bayesian Analysis 4 (1): 85–118. https://doi.org/10.1214/09-BA403. Park, T., and G. Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. Ročková, Veronika, and Edward I. George. 2018. “The Spike-and-Slab LASSO.” Journal of the American Statistical Association 113 (521): 431–44. https://doi.org/10.1080/01621459.2016.1260469. Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://www.jstor.org/stable/2346178. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. https://doi.org/10.1111/j.1467-9868.2005.00503.x. "],["sec12_3.html", "12.3 Bayesian additive regression trees", " 12.3 Bayesian additive regression trees A Classification and Regression Tree (CART) is a method used to predict outcomes based on inputs without assuming a parametric model. It is referred to as a classification tree when the outcome variable is qualitative, and as a regression tree when the outcome variable is quantitative. The method works by recursively partitioning the dataset into smaller, more homogeneous subsets using decision rules based on the input variables. For regression tasks, CART selects splits that minimize prediction error, typically measured by the sum of squared residuals. For classification problems, it aims to create the purest possible groups, using criteria such as Gini impurity or entropy. The result is a tree-like structure in which each internal node represents a decision based on one variable, and each leaf node corresponds to a prediction. CART was popularized in the statistical community by Breiman et al. (1984). The following figure displays a binary regression tree with two variables: one continuous (\\(x_1\\)) and one categorical (\\(x_2 \\in \\{A, B, C\\}\\)). The tree has seven nodes in total, four of which are terminal nodes (\\(B = 4\\)), dividing the input space \\(\\mathbf{x} = (x_1, x_2)\\) into four non-overlapping regions. Each internal node indicates the splitting variable and rule, while each terminal node shows the value \\(\\theta_b\\), representing the conditional mean of \\(y\\) given \\(\\mathbf{x}\\). The first split is based on the rule \\(x_1 \\leq 5\\) (left) versus \\(x_1 &gt; 5\\) (right). The leftmost terminal node corresponds to \\(x_2 \\in \\{A\\}\\) with \\(\\mu_1 = 2\\). The second terminal node, with \\(\\mu_2 = 3\\), is defined by \\(x_1 \\leq 3\\) and \\(x_2 \\in \\{B, C\\}\\). The third node assigns \\(\\mu_3 = 5\\) for \\(3 &lt; x_1 \\leq 5\\) and \\(x_2 \\in \\{B, C\\}\\). Finally, the rightmost terminal node assigns \\(\\mu_4 = 8\\) for all observations with \\(x_1 &gt; 5\\). We can view a CART model as specifying the conditional distribution of \\(y\\) given the vector of features \\(\\mathbf{x} = [x_1, x_2, \\dots, x_K]^{\\top}\\). There are two main components: (i) the binary tree structure \\(T\\), which consists of a sequence of binary decision rules of the form \\(x_k \\in A\\) versus \\(x_k \\notin A\\), where \\(A\\) is a subset of the range of \\(x_k\\), and \\(B\\) terminal nodes that define a non-overlapping and exhaustive partition of the input space; and (ii) the parameter vector \\(\\boldsymbol{\\theta} = [\\mu_1, \\mu_2, \\dots, \\mu_B]^{\\top}\\) and \\(\\sigma^2\\), where each \\(\\mu_b\\) corresponds to the parameter associated with terminal node \\(b\\), and \\(\\sigma^2\\) is the variance. Consequently, the response \\(y \\mid \\mathbf{x} \\sim p(y \\mid \\mu_b,\\sigma^2)\\) if \\(\\mathbf{x}\\) belongs to the region associated with terminal node \\(b\\), where \\(p\\) denotes a parametric distribution indexed by \\(\\mu_b\\) and \\(\\sigma^2\\). Assuming that \\(y_{bi}\\) is independently and identically distributed within each terminal node and independently across nodes, for \\(b = 1, 2, \\dots, B\\) and \\(i = 1, 2, \\dots, n_b\\), we have: \\[ p(\\mathbf{y} \\mid \\mathbf{x}, T, \\boldsymbol{\\theta}, \\sigma^2) = \\prod_{b=1}^B p(\\mathbf{y}_b \\mid \\mu_b,\\sigma^2) = \\prod_{b=1}^B \\prod_{i=1}^{n_b} p(y_{bi} \\mid \\mu_b,\\sigma^2), \\] where \\(\\mathbf{y}_b = [y_{b1}, y_{b2}, \\dots, y_{bn_b}]^{\\top}\\) is the set of observations in terminal node \\(b\\). Chipman, George, and McCulloch (1998) introduced a Bayesian formulation of CART models, in which inference is carried out through a combination of prior specification on the binary tree structure \\(T\\) and the parameters \\(\\boldsymbol{\\theta}, \\sigma^2 \\mid T\\), along with a stochastic search strategy based on a Metropolis-Hastings algorithm. The transition kernels used in the algorithm include operations such as growing, pruning, changing, and swapping tree branches, and candidate trees are evaluated based on their marginal likelihood. This approach enables exploration of a richer set of potential tree models and offers a more flexible and effective alternative to the greedy algorithms commonly used in classical CART. While CART is a simple yet powerful tool, it is prone to overfitting. To mitigate this, ensemble methods such as boosting, bagging, and random forests are often used. Boosting combines multiple weak trees sequentially, each correcting the errors of its predecessor, to create a strong predictive model (Freund and Schapire 1997). Bagging builds multiple models on bootstrapped datasets and averages their predictions to reduce variance (Breiman 1996), and random forests extend bagging by using decision trees and adding random input selection at each split to increase model diversity (Breiman 2001). Although a single tree might overfit and generalize poorly, aggregating many randomized trees typically yields more accurate and stable predictions. Chipman, George, and McCulloch (2010) introduced Bayesian Additive Regression Trees (BART). The starting point is the model: \\[ y_i = f(\\mathbf{x}_i) + \\mu_i, \\] where \\(\\mu_i \\sim N(0, \\sigma^2)\\). Thus, the conditional expectation \\(\\mathbb{E}[y_i \\mid \\mathbf{x}_i] = f(\\mathbf{x}_i)\\) is approximated as \\[\\begin{equation} \\label{eq:BART} f(\\mathbf{x}_i) \\approx h(\\mathbf{x}_i) = \\sum_{j=1}^J g_j(\\mathbf{x}_i \\mid T_j, \\boldsymbol{\\theta}_j), \\end{equation}\\] that is, \\(f(\\mathbf{x}_i)\\) is approximated by the sum of \\(J\\) regression trees. Each tree is defined by a structure \\(T_j\\) and a corresponding set of terminal node parameters \\(\\boldsymbol{\\theta}_j\\), where \\(g_j(\\mathbf{x}_i \\mid T_j, \\boldsymbol{\\theta}_j)\\) denotes the function that assigns the value \\(\\mu_{bj} \\in \\boldsymbol{\\theta}_j\\) to \\(\\mathbf{x}_i\\) according to the partition defined by \\(T_j\\). The main idea is to construct this sum-of-trees model by imposing a prior that regularizes the fit, ensuring that the individual contribution of each tree remains small. Thus, each tree \\(g_j\\) explains a small and distinct portion of \\(f\\). This is achieved through Bayesian backfitting MCMC (Hastie and Tibshirani 2000), where successive fits to the residuals are added. In this sense, BART can be viewed as a Bayesian version of boosting. Chipman, George, and McCulloch (2010) use the following prior structure: \\[\\begin{align*} \\pi\\left(\\{(T_1,\\boldsymbol{\\theta}_1), \\dots, (T_J,\\boldsymbol{\\theta}_J), \\sigma^2\\}\\right) &amp; = \\left[\\prod_{j=1}^J \\pi(T_j,\\boldsymbol{\\theta}_j)\\right]\\pi(\\sigma)\\\\ &amp; = \\left[\\prod_{j=1}^J \\pi(\\boldsymbol{\\theta}_j \\mid T_j) \\pi(T_j)\\right] \\pi(\\sigma)\\\\ &amp; = \\left[\\prod_{j=1}^J \\prod_{b=1}^B \\pi(\\mu_{bj} \\mid T_j) \\pi(T_j)\\right] \\pi(\\sigma). \\end{align*}\\] The prior for the binary tree structure has three components: (i) the probability that a node at depth \\(d = 0, 1, \\dots\\) is nonterminal, given by \\(\\alpha(1 + d)^{-\\beta}\\), where \\(\\alpha \\in (0,1)\\) and \\(\\beta \\in [0, \\infty)\\), the default values are \\(\\alpha = 0.95\\) and \\(\\beta = 2\\); (ii) a uniform prior over the set of available regressors to define the distribution of splitting variable assignments at each interior node; (iii) a uniform prior over the discrete set of available splitting values to define the splitting rule assignment, conditional on the chosen splitting variable. The prior for the terminal node parameters \\(\\pi(\\mu_{bj} \\mid T_j)\\) is specified as \\(N(0, 0.5 / (k \\sqrt{J}))\\). The observed values of \\(y\\) are scaled and shifted to lie within the range \\(y_{\\text{min}} = -0.5\\) to \\(y_{\\text{max}} = 0.5\\), and the default value \\(k = 2\\) ensures that \\[ P_{\\pi(\\mu_{bj} \\mid T_j)}\\left(\\mathbb{E}[y \\mid \\mathbf{x}] \\in (-0.5, 0.5)\\right) = 0.95. \\] Note that this prior shrinks the effect of individual trees toward zero, ensuring that each tree contributes only a small amount to the overall prediction. Moreover, although the dependent variable is transformed, there is no need to transform the input variables, since the tree-splitting rules are invariant to monotonic transformations of the regressors. The prior for \\(\\sigma^2\\) is specified as \\(\\pi(\\sigma^2) \\sim v\\lambda / \\chi^2_v\\). Chipman, George, and McCulloch (2010) recommend setting \\(v = 3\\), and choosing \\(\\lambda\\) such that \\(P(\\sigma &lt; \\hat{\\sigma}) = q, q = 0.9\\), where \\(\\hat{\\sigma}\\) is an estimate of the residual standard deviation from a saturated linear model, i.e., a model including all available regressors when \\(K &lt; N\\), or the standard deviation of \\(y\\) when \\(K \\geq N\\). Finally, regarding the number of trees \\(J\\), the default value is 200. As \\(J\\) increases from 1, BART’s predictive performance typically improves substantially until it reaches a plateau, after which performance may degrade very slowly. However, if the primary goal is variable selection, using a smaller \\(J\\) is preferable, as it facilitates identifying the most important regressors by enhancing the internal competition among variables within a limited number of trees. To sum up, the default hyperparameters are \\((\\alpha, \\beta, k, J, v, q) = (0.95, 2, 2, 200, 3, 0.9)\\); however, cross-validation can be used to tune these hyperparameters if desired. BART’s predictive performance appears to be relatively robust to the choice of hyperparameters, provided they are set to sensible values, except in cases where \\(K &gt; N\\), in which cross-validated tuning tends to yield better performance, albeit at the cost of increased computational time. Given this specification, we can use a Gibbs sampler that cycles through the \\(J\\) trees. At each iteration, we sample from the conditional posterior distribution: \\[ \\pi(T_j, \\boldsymbol{\\theta}_j \\mid R_j, \\sigma) = \\pi(T_j \\mid R_j, \\sigma) \\times \\pi(\\boldsymbol{\\theta}_j \\mid T_j, R_j, \\sigma), \\] where \\(R_j = y - \\sum_{k \\neq j} g_k(\\mathbf{x} \\mid T_k, \\boldsymbol{\\theta}_k)\\) represents the residuals excluding the contribution of the \\(j\\)-th tree. The posterior distribution \\(\\pi(T_j \\mid R_j, \\sigma)\\) is explored using a Metropolis-Hastings algorithm, where the candidate tree is generated by one of the following moves (Chipman, George, and McCulloch 1998): (i) growing a terminal node with probability 0.25; (ii) pruning a pair of terminal nodes with probability 0.25; (iii) changing a nonterminal splitting rule with probability 0.4; or (iv) swapping a rule between a parent and child node with probability 0.1. The posterior distribution of \\(\\boldsymbol{\\theta}_j\\) is the product of the posterior distributions \\(\\pi(\\mu_{jb} \\mid T_j, R_j, \\sigma)\\), which are Gaussian. The posterior distribution of \\(\\sigma\\) is inverse-gamma. The posterior draws of \\(\\mu_{bj}\\) are then used to update the residuals \\(R_{j+1}\\), allowing the sampler to proceed to the next tree in the cycle. The number of iterations in the Gibbs sampler does not need to be very large; for instance, 200 burn-in iterations and 1,000 post-burn-in iterations typically work well. As we obtain posterior draws from the sum-of-trees model, we can compute point estimates at each \\(\\mathbf{x}_i\\) using the posterior mean, \\(\\mathbb{E}[y \\mid \\mathbf{x}_i] = f(\\mathbf{x}_i)\\). Additionally, pointwise measures of uncertainty can be derived from the quantiles of the posterior draws. We can also identify the most relevant predictors by tracking the relative frequency with which each regressor appears in the sum-of-trees model across iterations. Moreover, we can perform inference on functionals of \\(f\\), such as the partial dependence functions (Friedman 2001), which quantify the marginal effects of the regressors. Specifically, if we are interested in the effect of \\(\\mathbf{x}_s\\) on \\(y\\), while marginalizing over the remaining variables \\(\\mathbf{x}_c\\), such that \\(\\mathbf{x} = [\\mathbf{x}_s^{\\top}, \\mathbf{x}_c^{\\top}]^{\\top}\\), the partial dependence function is defined as: \\[ f(\\mathbf{x}_s) = \\frac{1}{N} \\sum_{i=1}^N f(\\mathbf{x}_s, \\mathbf{x}_{ic}), \\] where \\(\\mathbf{x}_{ic}\\) denotes the \\(i\\)-th observed value of \\(\\mathbf{x}_c\\) in the dataset. Note that the calculation of the partial dependence function assumes that a subset of the variables is held fixed while averaging over the remainder. This assumption may be questionable when strong dependence exists among input variables, as fixing some variables while varying others may result in unrealistic or implausible combinations. Therefore, caution is warranted when interpreting the results. Linero (2018) extended BART models to high-dimensional settings for prediction and variable selection, while Hill (2011) and Hahn, Murray, and Carvalho (2020) applied them to causal inference. The asymptotic properties of BART models have been studied by Ročková and Saha (2019), Ročková and Pas (2020), and Ročková (2020). Example: Simulation exercise to study BART performance We use the BART package (Sparapani, Spanbauer, and McCulloch 2021) in the R software environment to perform estimation, prediction, inference, and marginal analysis using Bayesian Additive Regression Trees. In addition to modeling continuous outcomes, this package also supports dichotomous, categorical, and time-to-event outcomes. We adopt the simulation setting proposed by Friedman (1991), which is also used by Chipman, George, and McCulloch (2010): \\[ y_i = 10\\sin(\\pi x_{i1}x_{i2}) + 20(x_{i3}-0.5)^2 + 10 x_{i4} + 5 x_{i5} + \\mu_i, \\] where \\(\\mu_i \\sim N(0,1)\\), for \\(i = 1, 2, \\dots, 500\\). We set the hyperparameters to \\((\\alpha, \\beta, k, J, v, q) = (0.95, 2, 2, 200, 3, 0.9)\\), with a burn-in of 100 iterations, a thinning parameter of 1, and 1,000 post-burn-in MCMC iterations. We analyze the trace plot of the posterior draws of \\(\\sigma\\) to assess convergence, compare the true values of \\(y\\) with the posterior mean and 95% predictive intervals for both the training and test sets (using 80% of the data for training and 20% for testing), and visualize the relative importance of the regressors across different values of \\(J = 10, 20, 50, 100, 200\\). ####### Bayesian Additive Regression Trees ####### rm(list = ls()); set.seed(10101) library(BART); library(tidyr) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## Loading required package: survival library(ggplot2); library(dplyr) N &lt;- 500; K &lt;- 10 # Simulate the dataset MeanFunct &lt;- function(x){ f &lt;- 10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5] return(f) } sig2 &lt;- 1 e &lt;- rnorm(N, 0, sig2^0.5) X &lt;- matrix(runif(N*K),N,K) y &lt;- MeanFunct(X[,1:5]) + e # Train and test c &lt;- 0.8 Ntrain &lt;- floor(c * N) Ntest &lt;- N - Ntrain X.train &lt;- X[1:Ntrain, ] y.train &lt;- y[1:Ntrain] X.test &lt;- X[(Ntrain+1):N, ] y.test &lt;- y[(Ntrain+1):N] # Hyperparameters alpha &lt;- 0.95; beta &lt;- 2; k &lt;- 2 v &lt;- 3; q &lt;- 0.9; J &lt;- 200 # MCMC parameters MCMCiter &lt;- 1000; burnin &lt;- 100; thinning &lt;- 1 # Estimate BART BARTfit &lt;- wbart(x.train = X.train, y.train = y.train, x.test = X.test, base = alpha, power = beta, k = k, sigdf = v, sigquant = q, ntree = J, ndpost = MCMCiter, nskip = burnin, keepevery = thinning) ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 200 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.481183,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 6s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 # Trace plot sigma keep &lt;- seq(burnin + 1, MCMCiter + burnin, thinning) df_sigma &lt;- data.frame(iteration = 1:length(keep), sigma = BARTfit$sigma[keep]) ggplot(df_sigma, aes(x = iteration, y = sigma)) + geom_line(color = &quot;steelblue&quot;) + labs(title = &quot;Trace Plot of Sigma&quot;, x = &quot;Iteration&quot;, y = expression(sigma)) + theme_minimal() # Prediction plot training train_preds &lt;- data.frame( y_true = y.train, mean = apply(BARTfit$yhat.train, 2, mean), lower = apply(BARTfit$yhat.train, 2, quantile, 0.025), upper = apply(BARTfit$yhat.train, 2, quantile, 0.975)) %&gt;% arrange(y_true) %&gt;% mutate(index = row_number()) ggplot(train_preds, aes(x = index)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = &quot;95% Interval&quot;), alpha = 0.4, show.legend = TRUE) + geom_line(aes(y = mean, color = &quot;Predicted Mean&quot;)) + geom_line(aes(y = y_true, color = &quot;True y&quot;)) + scale_color_manual(name = &quot;Line&quot;, values = c(&quot;Predicted Mean&quot; = &quot;blue&quot;, &quot;True y&quot; = &quot;black&quot;)) + scale_fill_manual(name = &quot;Interval&quot;, values = c(&quot;95% Interval&quot; = &quot;lightblue&quot;)) + labs(title = &quot;Training Data: Ordered Predictions with 95% Intervals&quot;, x = &quot;Ordered Index&quot;, y = &quot;y&quot;) + theme_minimal() # Prediction plot test test_preds &lt;- data.frame( y_true = y.test, mean = apply(BARTfit$yhat.test, 2, mean), lower = apply(BARTfit$yhat.test, 2, quantile, 0.025), upper = apply(BARTfit$yhat.test, 2, quantile, 0.975)) %&gt;% arrange(y_true) %&gt;% mutate(index = row_number()) ggplot(test_preds, aes(x = index)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = &quot;95% Interval&quot;), alpha = 0.4, show.legend = TRUE) + geom_line(aes(y = mean, color = &quot;Predicted Mean&quot;)) + geom_line(aes(y = y_true, color = &quot;True y&quot;)) + scale_color_manual(name = &quot;Line&quot;, values = c(&quot;Predicted Mean&quot; = &quot;blue&quot;, &quot;True y&quot; = &quot;black&quot;)) + scale_fill_manual(name = &quot;Interval&quot;, values = c(&quot;95% Interval&quot; = &quot;lightblue&quot;)) + labs(title = &quot;Test Data: Ordered Predictions with 95% Intervals&quot;, x = &quot;Ordered Index&quot;, y = &quot;y&quot;) + theme_minimal() # Relevant regressors Js &lt;- c(10, 20, 50, 100, 200) VarImportance &lt;- matrix(0, length(Js), K) l &lt;- 1 for (j in Js){ BARTfit &lt;- wbart(x.train = X.train, y.train = y.train, x.test = X.test, base = alpha, power = beta, k = k, sigdf = v, sigquant = q, ntree = j, ndpost = MCMCiter, nskip = burnin, keepevery = thinning) VarImportance[l, ] &lt;- BARTfit[[&quot;varcount.mean&quot;]]/j l &lt;- l + 1 } ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 10 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,2.151914,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 1s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 20 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,1.521633,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 0s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 50 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.962365,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 2s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 100 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.680495,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 3s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 ## *****Into main of wbart ## *****Data: ## data:n,p,np: 400, 10, 100 ## y1,yn: -4.287055, -6.380098 ## x1,x[n*p]: 0.031887, 0.455171 ## xp1,xp[np*p]: 0.373776, 0.051661 ## *****Number of Trees: 200 ## *****Number of Cut Points: 100 ... 100 ## *****burn and ndpost: 100, 1000 ## *****Prior:beta,alpha,tau,nu,lambda: 2.000000,0.950000,0.481183,3.000000,1.534927 ## *****sigma: 2.807107 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,10,0 ## *****nkeeptrain,nkeeptest,nkeeptestme,nkeeptreedraws: 1000,1000,1000,1000 ## *****printevery: 100 ## *****skiptr,skipte,skipteme,skiptreedraws: 1,1,1,1 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 6s ## check counts ## trcnt,tecnt,temecnt,treedrawscnt: 1000,1000,1000,1000 rownames(VarImportance) &lt;- c(&quot;10&quot;, &quot;20&quot;, &quot;50&quot;, &quot;100&quot;, &quot;200&quot;) colnames(VarImportance) &lt;- as.character(1:10) importance_df &lt;- as.data.frame(VarImportance) %&gt;% mutate(trees = rownames(.)) %&gt;% pivot_longer(cols = -trees, names_to = &quot;variable&quot;, values_to = &quot;percent_used&quot;) importance_df$variable &lt;- as.numeric(importance_df$variable) importance_df$trees &lt;- factor(importance_df$trees, levels = c(&quot;10&quot;, &quot;20&quot;, &quot;50&quot;, &quot;100&quot;, &quot;200&quot;)) ggplot(importance_df, aes(x = variable, y = percent_used, color = trees, linetype = trees)) + geom_line() + geom_point() + scale_color_manual(values = c(&quot;10&quot; = &quot;red&quot;, &quot;20&quot; = &quot;green&quot;, &quot;50&quot; = &quot;blue&quot;, &quot;100&quot; = &quot;cyan&quot;, &quot;200&quot; = &quot;magenta&quot;)) + scale_x_continuous(breaks = 1:10) + labs(x = &quot;variable&quot;, y = &quot;percent used&quot;, color = &quot;#trees&quot;, linetype = &quot;#trees&quot;) + theme_minimal() The first figure displays the trace plot of \\(\\sigma\\), which appears to have reached a stationary distribution. However, the posterior draws are slightly lower than the true value (1). The second and third figures compare the true values of \\(y\\) with the posterior mean and 95% predictive intervals. The BART model performs well in both sets, and the intervals in the test set are notably wider than those in the training set. The last figure shows the relative frequency with which each variable is used in the trees, a measure of variable relevance. When the number of trees is small, the algorithm more clearly identifies the most relevant predictors (variables 1–5). As the number of trees increases, this discrimination gradually disappears. References Breiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. Classification and Regression Trees. Belmont, CA: Wadsworth International Group. Chipman, Hugh A, Edward I George, and Robert E McCulloch. 1998. “Bayesian CART Model Search.” Journal of the American Statistical Association 93 (443): 935–48. https://doi.org/10.1080/01621459.1998.10473750. ———. 2010. “BART: Bayesian Additive Regression Trees.” The Annals of Applied Statistics 4 (1): 266–98. Freund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” Journal of Computer and System Sciences 55 (1): 119–39. Friedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” Annals of Statistics 19 (1): 1–67. https://doi.org/10.1214/aos/1176347963. ———. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics 29 (5): 1189–1232. Hahn, P Richard, Jared S Murray, and Carlos M Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056. Hastie, Trevor, and Robert Tibshirani. 2000. “Bayesian Backfitting (with Discussion).” Journal of the American Statistical Association 95 (452): 1228–40. https://doi.org/10.1080/01621459.2000.10474339. Hill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for Causal Inference.” Journal of Computational and Graphical Statistics 20 (1): 217–40. Linero, Antonio R. 2018. “Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection.” Journal of the American Statistical Association 113 (522): 626–36. Ročková, Veronika. 2020. “On Semi-Parametric Inference for BART.” In Proceedings of the 37th International Conference on Machine Learning, edited by Hal Daumé III and Aarti Singh, 119:8137–46. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v119/rockova20a.html. Ročková, Veronika, and Stephanie van der Pas. 2020. “Posterior Concentration for Bayesian Regression Trees and Forests.” The Annals of Statistics 48 (4): 2103–30. https://doi.org/10.1214/19-AOS1879. Ročková, Veronika, and Enakshi Saha. 2019. “On Theory for BART.” In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, edited by Kamalika Chaudhuri and Masashi Sugiyama, 89:2839–48. Proceedings of Machine Learning Research. PMLR. Sparapani, Rodney A., Charles Spanbauer, and Robert McCulloch. 2021. “Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART r Package.” Journal of Statistical Software 97 (1): 1–66. https://doi.org/10.18637/jss.v097.i01. "],["sec12_4.html", "12.4 Gaussian processes", " 12.4 Gaussian processes A Gaussian Process (GP) is an infinite collection of random variables, any finite subset of which follows a joint Gaussian distribution. A GP is fully specified by its mean function and covariance function, that is, \\[ f(\\mathbf{x}) \\sim \\text{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}&#39;)), \\] where \\(m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})]\\) and \\(k(\\mathbf{x}, \\mathbf{x}&#39;) = \\mathbb{E}[(f(\\mathbf{x}) - m(\\mathbf{x}))(f(\\mathbf{x}&#39;) - m(\\mathbf{x}&#39;))]\\). It is common to assume \\(m(\\mathbf{x}) = 0\\) to simplify calculations, although this is not required. Perhaps the most commonly used covariance function in Gaussian Processes is the squared exponential kernel (or radial basis function) (Jacobi et al. 2024), defined as \\[ k(\\mathbf{x}, \\mathbf{x}&#39;) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2} \\|\\mathbf{x} - \\mathbf{x}&#39;\\|^2\\right), \\] where \\(\\sigma_f^2\\) is the signal variance, which controls the vertical variation (amplitude) of the function, \\(l\\) is the length-scale parameter, which determines how quickly the function varies with features distance, and \\(\\|\\mathbf{x} - \\mathbf{x}&#39;\\|^2\\) is the squared Euclidean distance between the feature vectors \\(\\mathbf{x}\\) and \\(\\mathbf{x}&#39;\\). The squared exponential kernel implies that the function is infinitely differentiable, leading to very smooth function draws. While this smoothness may be desirable in some applications, it can be too restrictive in others. Alternative kernels like the Matérn class allow for more flexibility by controlling the degree of differentiability (Rasmussen and Williams 2006). A GP can be interpreted as a prior distribution over a space of functions. The starting point in working with GPs is the specification of this prior before any data are observed. The following code illustrates five sample paths drawn from a GP with a squared exponential kernel, assuming a signal variance \\(\\sigma_f^2 = 1\\) and a length-scale \\(l = 0.2\\), evaluated over a grid of input values \\(x \\in [0,1]\\). A small jitter term is added to the covariance matrix to ensure numerical stability during simulation. The following figure displays the five realizations drawn from the Gaussian Process. ####### Gaussian Process ####### rm(list = ls()) set.seed(10101) library(ggplot2); library(dplyr) library(tidyr); library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select # Simulation setup n &lt;- 100 x &lt;- seq(0, 1, length.out = n) sigma_f &lt;- 1 l &lt;- 0.2 sigma_n &lt;- 1e-8 # Squared Exponential Kernel function SE_kernel &lt;- function(x1, x2, sigma_f, l) { outer(x1, x2, function(a, b) sigma_f^2 * exp(-0.5 * (a - b)^2 / l^2)) } K &lt;- SE_kernel(x, x, sigma_f, l) + diag(sigma_n, n) samples &lt;- mvrnorm(n = 5, mu = rep(0, n), Sigma = K) # Transpose and rename columns to f1, f2, ..., f5 samples_t &lt;- t(samples) colnames(samples_t) &lt;- paste0(&quot;f&quot;, 1:5) # Convert to tidy data frame df &lt;- data.frame(x = x, samples_t) |&gt; pivot_longer(cols = -x, names_to = &quot;draw&quot;, values_to = &quot;value&quot;) # Plot ggplot(df, aes(x = x, y = value, color = draw)) + geom_line(linewidth = 1) + labs( title = &quot;Simulated Gaussian Process Draws&quot;, x = &quot;x&quot;, y = &quot;f(x)&quot;, color = &quot;Function&quot; ) + theme_minimal(base_size = 14) + theme(legend.position = &quot;top&quot;) Thus, for any finite set of feature points \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N\\), the corresponding function values follow a multivariate Gaussian distribution: \\[ \\mathbf{f} = \\begin{bmatrix} f(\\mathbf{x}_1) \\\\ f(\\mathbf{x}_2) \\\\ \\vdots \\\\ f(\\mathbf{x}_N) \\end{bmatrix} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}(\\mathbf{X}, \\mathbf{X})), \\] where the \\((i,j)\\)-th entry of the covariance matrix \\(\\mathbf{K}(\\mathbf{X}, \\mathbf{X})\\) is given by \\(\\mathbf{K}_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\). If we are interested in the properties of a function evaluated at a finite set of input points \\(\\{(f_i, x_i)\\}_{i=1}^N\\), inference can be performed using only those points, effectively disregarding the uncountably infinite values the function may take elsewhere. The following code illustrates how to perform inference for a GP given four observed points \\(\\{(f_i, x_i)\\}_{i=1}^4\\), assuming that the true underlying process is \\[ f_i = \\sin(2\\pi x_i). \\] The inference is based on the properties of the conditional Gaussian distribution (see below). The figure shows that the posterior mean (solid blue line) interpolates the observed points (red dots). Moreover, the level of uncertainty (light blue shaded area) increases in regions that are farther from the observed inputs, where the posterior mean tends to deviate more from the true underlying function (dashed green line). In situations where the input locations can be selected, such as in experimental designs, active learning strategies can be employed to choose the points that minimize predictive uncertainty. This is typically achieved by optimizing an acquisition function that quantifies the expected informativeness of candidate locations (Settles 2012). Consequently, GPs play a central role in Bayesian optimization, a stochastic method for finding the maximum of expensive or unknown objective functions. In this approach, a prior is placed over the objective function, which is then updated using observed data to form a posterior distribution over possible functions. This posterior guides the selection of new input points by balancing exploration and exploitation through the acquisition function (Brochu, Cora, and Freitas 2010). ####### Gaussian Process ####### rm(list = ls()); set.seed(10101) library(ggplot2); library(MASS) # Define the squared exponential kernel SE_kernel &lt;- function(x1, x2, sigma_f, l) { outer(x1, x2, function(a, b) sigma_f^2 * exp(-0.5 * (a - b)^2 / l^2)) } # Define the input space and observed points x_star &lt;- seq(0, 1, length.out = 200) x0 &lt;- c(0.1, 0.2, 0.5, 0.9) y0 &lt;- sin(2 * pi * x0) # Hyperparameters sigma_f &lt;- 1 l &lt;- 0.2 sigma_n &lt;- 1e-8 # Jitter term for stability # Compute covariance matrices K_x0x0 &lt;- SE_kernel(x0, x0, sigma_f, l) + diag(sigma_n, length(x0)) K_xstarx0 &lt;- SE_kernel(x_star, x0, sigma_f, l) K_xstarxstar &lt;- SE_kernel(x_star, x_star, sigma_f, l) + diag(sigma_n, length(x_star)) # Compute posterior mean and covariance K_inv &lt;- solve(K_x0x0) posterior_mean &lt;- K_xstarx0 %*% K_inv %*% y0 posterior_cov &lt;- K_xstarxstar - K_xstarx0 %*% K_inv %*% t(K_xstarx0) # Sample from the posterior sample_draw &lt;- sin(2 * pi * x_star) # Compute 95% intervals posterior_sd &lt;- sqrt(diag(posterior_cov)) lower &lt;- posterior_mean - 1.96 * posterior_sd upper &lt;- posterior_mean + 1.96 * posterior_sd # Data frame for plotting df &lt;- data.frame( x = x_star, mean = posterior_mean, lower = lower, upper = upper, sample = sample_draw ) obs &lt;- data.frame(x = x0, y = y0) # Plot ggplot(df, aes(x = x)) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;lightblue&quot;, alpha = 0.4) + geom_line(aes(y = mean), color = &quot;blue&quot;, linewidth = 1.2) + geom_line(aes(y = sample), color = &quot;darkgreen&quot;, linewidth = 1, linetype = &quot;dashed&quot;) + geom_point(data = obs, aes(x = x, y = y), color = &quot;red&quot;, size = 3) + labs( title = &quot;Gaussian Process with Conditioning Points&quot;, x = &quot;x&quot;, y = &quot;f(x)&quot;, caption = &quot;Blue: Posterior mean | Light blue: 95% interval | Dashed green: Population | Red: Observed points&quot; ) + theme_minimal(base_size = 14) In practice, we have an observed dataset \\(\\{(y_i, \\mathbf{x}_i)\\}_{i=1}^N\\) such that \\[ y_i = f(\\mathbf{x}_i) + \\mu_i, \\] where \\(\\mu_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)\\). This means that \\(y_i\\) is a noisy observation of \\(f(\\mathbf{x}_i)\\). Thus, the marginal distribution of the observed outputs is \\[ \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}_N), \\] where \\(\\mathbf{K}(\\mathbf{X}, \\mathbf{X})\\) is the covariance matrix generated by the GP kernel evaluated at the training inputs. Note that this implies the log marginal likelihood is given by \\[ \\log p(\\mathbf{y} \\mid \\mathbf{X}) = -\\frac{1}{2} \\mathbf{y}^{\\top} (\\mathbf{K} + \\sigma^2 \\mathbf{I}_N)^{-1} \\mathbf{y} - \\frac{1}{2} \\log \\left| \\mathbf{K} + \\sigma^2 \\mathbf{I}_N \\right| - \\frac{N}{2} \\log 2\\pi. \\] We can adopt an empirical Bayes approach to estimate the hyperparameters of the GP prior by maximizing the log marginal likelihood with respect to the kernel parameters (e.g., \\(\\sigma_f^2\\), \\(l\\)) and the noise variance \\(\\sigma^2\\). To make predictions at a new set of features \\(\\mathbf{X}_*\\), we consider the joint distribution: \\[ \\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{bmatrix} \\sim \\mathcal{N}\\left( \\mathbf{0}, \\begin{bmatrix} \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}_N &amp; \\mathbf{K}(\\mathbf{X}, \\mathbf{X}_*) \\\\ \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) &amp; \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}_*) \\end{bmatrix} \\right). \\] Using the conditional distribution of a multivariate Gaussian, the posterior predictive distribution (Rasmussen and Williams 2006) is: \\[ \\mathbf{f}_* \\mid \\mathbf{y} \\sim \\mathcal{N}(\\bar{\\mathbf{f}}_*, \\operatorname{cov}(\\mathbf{f}_*)), \\] where \\[ \\begin{aligned} \\bar{\\mathbf{f}}_* &amp;= \\mathbb{E}[\\mathbf{f}_* \\mid \\mathbf{y}, \\mathbf{X}, \\mathbf{X}_*] = \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) [\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}_N]^{-1} \\mathbf{y}, \\\\ \\operatorname{cov}(\\mathbf{f}_*) &amp;= \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}_*) - \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) [\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma^2 \\mathbf{I}_N]^{-1} \\mathbf{K}(\\mathbf{X}, \\mathbf{X}_*). \\end{aligned} \\] Therefore, Gaussian Process (GP) regression provides a flexible and efficient nonparametric framework for predicting unobserved responses, with accuracy that improves as more data become available. GPs are widely used due to their favorable computational properties, including the availability of closed-form expressions, and posterior consistency under mild conditions (Choi and Schervish 2007; Stuart and Teckentrup 2018). Moreover, predictive performance can be further enhanced by incorporating derivative information, as the derivative of a GP is itself a GP (Solak et al. 2003; Jacobi et al. 2024). However, a major limitation of GPs is the need to invert an \\(N \\times N\\) covariance matrix, which requires \\(O(N^3)\\) computational operations, making them computationally expensive for large datasets. To address this, several scalable methods have been proposed that reduce the computational burden. For instance, Wilson and Nickisch (2015), Gardner et al. (2018) and Pleiss et al. (2018) develop algorithms that reduce complexity to \\(O(N)\\). Example: Simulation exercise to study GP performance We simulate the process \\[ f_i = \\sin(2\\pi x_{i1}) + \\cos(2\\pi x_{i2}) + \\sin(x_{i1} x_{i2}), \\] where \\(x_{i1}\\) and \\(x_{i2}\\) are independently drawn from a uniform distribution on the interval \\([0, 1]\\), for \\(i = 1, 2, \\dots, 100\\). We use the DiceKriging package in R to estimate and make predictions using a Gaussian Process. This package applies maximum likelihood estimation to infer the length-scale parameters (\\(l_k\\)) and the signal variance (\\(\\sigma_f^2\\)). Note that there are two separate length-scale parameters, one for each input variable. The following code illustrates how to carry out this example, and the following figure displays a 3D plot with the observed points and the posterior mean surface. The package also provides pointwise credible intervals for the predictions. ####### Gaussian Process ####### # Load required packages library(DiceKriging); library(rgl) # Simulate training data set.seed(10101); n_train &lt;- 100 x1 &lt;- runif(n_train); x2 &lt;- runif(n_train) X_train &lt;- data.frame(x1 = x1, x2 = x2) # True function without noise f_train &lt;- sin(2 * pi * X_train$x1) + cos(2 * pi * X_train$x2) + sin(X_train$x1 * X_train$x2) # Fit Gaussian Process fit_km &lt;- km(design = X_train, response = f_train, covtype = &quot;gauss&quot;, nugget = 1e-10) ## ## optimisation start ## ------------------ ## * estimation method : MLE ## * optimisation method : BFGS ## * analytical gradient : used ## * trend model : ~1 ## * covariance model : ## - type : gauss ## - nugget : 1e-10 ## - parameters lower bounds : 1e-10 1e-10 ## - parameters upper bounds : 1.958963 1.986954 ## - variance bounds : 0.08882249 11.58394 ## - best initial criterion value(s) : 245.853 ## ## N = 3, M = 5 machine precision = 2.22045e-16 ## At X0, 0 variables are exactly at the bounds ## At iterate 0 f= -245.85 |proj g|= 1.8198 ## At iterate 1 f = -507.42 |proj g|= 10.555 ## At iterate 2 f = -554.97 |proj g|= 10.565 ## At iterate 3 f = -559.13 |proj g|= 10.573 ## At iterate 4 f = -559.52 |proj g|= 10.566 ## At iterate 5 f = -559.71 |proj g|= 10.562 ## At iterate 6 f = -561.14 |proj g|= 10.514 ## At iterate 7 f = -563.7 |proj g|= 10.4 ## At iterate 8 f = -568.91 |proj g|= 10.123 ## At iterate 9 f = -576.93 |proj g|= 9.622 ## At iterate 10 f = -578.62 |proj g|= 6.2251 ## At iterate 11 f = -588.79 |proj g|= 6.5073 ## At iterate 12 f = -593.65 |proj g|= 6.2906 ## At iterate 13 f = -596.53 |proj g|= 4.235 ## At iterate 14 f = -600.42 |proj g|= 2.3681 ## At iterate 15 f = -601.07 |proj g|= 1.616 ## At iterate 16 f = -603.31 |proj g|= 1.4519 ## At iterate 17 f = -605.28 |proj g|= 0.52334 ## At iterate 18 f = -605.49 |proj g|= 1.4752 ## At iterate 19 f = -605.57 |proj g|= 1.4721 ## At iterate 20 f = -605.57 |proj g|= 0.1007 ## At iterate 21 f = -605.57 |proj g|= 0.013566 ## At iterate 22 f = -605.57 |proj g|= 0.0006761 ## At iterate 23 f = -605.57 |proj g|= 0.0012742 ## Bad direction in the line search; ## refresh the lbfgs memory and restart the iteration. ## At iterate 24 f = -605.57 |proj g|= 0.00016589 ## At iterate 25 f = -605.57 |proj g|= 0.00016589 ## ## iterations 25 ## function evaluations 62 ## segments explored during Cauchy searches 28 ## BFGS updates skipped 0 ## active bounds at final generalized Cauchy point 1 ## norm of the final projected gradient 0.000165894 ## final function value -605.575 ## ## F = -605.575 ## final value -605.574558 ## converged # Prediction grid grid_points &lt;- 30 x1_seq &lt;- seq(0, 1, length.out = grid_points) x2_seq &lt;- seq(0, 1, length.out = grid_points) grid &lt;- expand.grid(x1 = x1_seq, x2 = x2_seq) # Predict GP surface pred &lt;- predict(fit_km, newdata = grid, type = &quot;UK&quot;) z_pred &lt;- matrix(pred$mean, nrow = grid_points, ncol = grid_points) # Plot persp3d(x = x1_seq, y = x2_seq, z = z_pred, col = &quot;lightblue&quot;, alpha = 0.7, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;, zlab = &quot;GP Mean&quot;) points3d(x = X_train$x1, y = X_train$x2, z = f_train, col = &quot;red&quot;, size = 8) fit_km@covariance@range.val # length-scale ## [1] 0.5196972 0.5150810 fit_km@covariance@sd2 # Signal variance ## [1] 11.58394 A limitation of the DiceKriging package is that it is designed for deterministic simulations and, consequently, does not estimate the noise variance. Therefore, in Exercise 7, we ask to simulate the process \\[ f_i = \\sin(2\\pi x_{i1}) + \\cos(2\\pi x_{i2}) + \\sin(x_{i1} x_{i2}) + \\mu_i, \\] where \\(\\mu_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 0.1^2)\\), and to use an empirical Bayes approach to estimate the hyperparameters. These estimated hyperparameters should then be used to perform GP prediction. References Brochu, Eric, Vlad M. Cora, and Nando de Freitas. 2010. “A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning.” arXiv Preprint arXiv:1012.2599. Choi, Taeryon, and Mark J. Schervish. 2007. “On Posterior Consistency in Nonparametric Regression Problems.” Journal of Multivariate Analysis 98 (10): 1969–87. https://doi.org/10.1016/j.jmva.2007.01.004. Gardner, Jacob R., Geoff Pleiss, David Bindel Wu, Kilian Q. Weinberger, and Andrew Gordon Wilson. 2018. “Product Kernel Interpolation for Scalable Gaussian Processes.” In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS), 84:1407–16. PMLR. Jacobi, Liana, Chun Fung Kwok, Andrés Ramı́rez-Hassan, and Nhung Nghiem. 2024. “Posterior Manifolds over Prior Parameter Regions: Beyond Pointwise Sensitivity Assessments for Posterior Statistics from MCMC Inference.” Studies in Nonlinear Dynamics &amp; Econometrics 28 (2): 403–34. Pleiss, Geoff, Jacob R. Gardner, Kilian Q. Weinberger, and Andrew Gordon Wilson. 2018. “Constant-Time Predictive Distributions for Gaussian Processes.” In Proceedings of the 35th International Conference on Machine Learning (ICML), 80:4111–20. PMLR. Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. Cambridge, MA: MIT Press. Settles, Burr. 2012. Active Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers. https://doi.org/10.2200/S00429ED1V01Y201207AIM018. Solak, Ercan, Roderick Murray-Smith, W. E. Leithead, D. J. Leith, and C. E. Rasmussen. 2003. “Derivative Observations in Gaussian Process Models of Dynamic Systems.” In Advances in Neural Information Processing Systems, 1033–40. MIT Press. Stuart, Andrew M., and Aretha L. Teckentrup. 2018. “Posterior Consistency for Gaussian Process Approximations of Bayesian Posterior Distributions.” Mathematics of Computation 87 (310): 721–53. https://doi.org/10.1090/mcom/3244. Wilson, Andrew G., and Hannes Nickisch. 2015. “Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP).” In Proceedings of the 32nd International Conference on Machine Learning (ICML), 37:1775–84. PMLR. "],["sec12_5.html", "12.5 Tall data problems", " 12.5 Tall data problems In this section, we explore several methods developed to perform Bayesian inference when the sample size is large, particularly when there is a large number of observational units, commonly referred to as tall datasets. Bayesian inference in such settings is computationally demanding because each iteration of an MCMC algorithm requires evaluating the likelihood function over all \\(N\\) observations. For large \\(N\\), this renders standard MCMC methods prohibitively expensive. Recent efforts have focused on developing scalable Monte Carlo algorithms that significantly reduce the computational cost compared to standard approaches. One alternative is to use Variational Bayes (see Chapter 14); however, it can be challenging to implement and may exhibit limitations in uncertainty quantification, particularly for the joint posterior distribution. Another alternative is the Integrated Nested Laplace Approximation (INLA, see Chapter 14); however, its computational cost grows exponentially with the dimension of the parameter space. In scenarios where observations are assumed to be independent, two main frameworks have been proposed to scale MCMC algorithms: divide-and-conquer approaches and subsampling-based algorithms. Divide-and-conquer methods partition the dataset into disjoint subsets, run MCMC independently on each batch to obtain subposteriors, and then combine them to approximate the full posterior distribution. Subsampling-based algorithms, on the other hand, aim to reduce the number of data points used to evaluate the likelihood at each iteration, often relying on pseudo-marginal MCMC methods (Andrieu and Roberts 2009). The key idea of pseudo-marginal MCMC is to augment the model with latent variables such that the sample average of the likelihood, computed over draws from these latent variables, provides an unbiased estimator of the marginal likelihood. This approach is particularly valuable when the marginal likelihood is not available in closed form. Moreover, the same principles can be adapted to reduce the computational burden of evaluating the log-likelihood. For an excellent review of divide-and-conquer and subsampling-based approaches, see (Bardenet, Doucet, and Holmes 2017). 12.5.1 Divide-and-conquer methods In divide-and-conquer methods, the main idea is to partition the dataset and distribute the subsets across multiple computing machines/cores. An independent MCMC algorithm is then executed on each subset to obtain a corresponding subposterior distribution. The central challenge lies in accurately and efficiently combining these subposteriors into a single approximation of the full posterior distribution. Several approaches have been proposed to address this issue. For instance, Huang and Gelman (2005), Steven L. Scott et al. (2016), Rendell et al. (2020) and Steven L. Scott et al. (2022) introduce the Consensus Monte Carlo algorithm; Wang and Dunson (2013) develop a method based on the Weierstrass sampler for parallelizing MCMC; Minsker (2015) propose using the geometric median of posterior distributions; and Wu and Robert (2017) suggest combining rescaled subposteriors. In divide-and-conquer methods, the dataset is partitioned into \\(B\\) disjoint batches \\(\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_B\\), and the posterior is rewritten using the identity: \\[ \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\propto \\prod_{b=1}^B \\pi(\\boldsymbol{\\theta})^{1/B} p(\\mathbf{y}_b \\mid \\boldsymbol{\\theta}), \\] which implies that the full posterior is proportional to the product of appropriately rescaled subposteriors. Consensus Monte Carlo (CMC) operates by running separate Monte Carlo algorithms on each subset in parallel, and then averaging the resulting posterior draws. Specifically, given samples \\(\\boldsymbol{\\theta}_b^{(s)}\\), for \\(b = 1, 2, \\dots, B\\) and \\(s = 1, 2, \\dots, S\\), obtained independently from each batch \\(\\mathbf{y}_b\\), the \\(s\\)-th draw from the consensus posterior is computed as: \\[ \\boldsymbol{\\theta}^{(s)} = \\left( \\sum_{b=1}^B \\mathbf{w}_b \\right)^{-1} \\sum_{b=1}^B \\mathbf{w}_b \\boldsymbol{\\theta}_b^{(s)}, \\] where the optimal weight is the inverse covariance matrix of the subposterior, i.e., \\(\\mathbf{w}_b = \\operatorname{Var}^{-1}(\\boldsymbol{\\theta} \\mid \\mathbf{y}_b)\\). In practice, one may use the marginal variances of each parameter to simplify the computation, which can still yield good performance. When each subposterior \\(\\pi_b(\\boldsymbol{\\theta} \\mid \\mathbf{y}_b)\\) is Gaussian, the full posterior \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) is also Gaussian and can be recovered exactly by combining the subposteriors using simple rules based on their means and covariances (Steven L. Scott et al. 2016; Steven L. Scott et al. 2022). In the non-Gaussian case, standard asymptotic results in Bayesian inference (see Chapter 1) imply that the posterior distributions converge to a Gaussian distribution as the batch size increases. Alternative merging procedures that are more robust to non-Gaussianity have also been proposed (Neiswanger, Wang, and Xing 2013; Minsker et al. 2017); however, it remains difficult to quantify the approximation error in these approaches. Moreover, this procedure is limited to continuous parameter spaces and may exhibit small-sample bias; that is, when the dataset is divided into small batches, the subposterior distributions may be biased. In such cases, jackknife bias corrections are recommended to reduce the overall approximation error. In particular, we perform CMC using the following Algorithm (Steven L. Scott et al. 2016). Next, we compute the CMC posterior repeatedly, each time leaving out one of the \\(B\\) subsets. Let \\(\\pi_{-b}(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) denote the resulting posterior when subset \\(b\\) is excluded. The average of these leave-one-out posteriors is denoted by: \\[ \\bar{\\pi}_{-b}(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = \\frac{1}{B} \\sum_{b=1}^B \\pi_{-b}(\\boldsymbol{\\theta} \\mid \\mathbf{y}). \\] Then, the jackknife bias-corrected posterior is given by: \\[ \\pi_{\\text{jk}}(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = B \\cdot \\pi_{\\text{CMC}}(\\boldsymbol{\\theta} \\mid \\mathbf{y}) - (B - 1) \\cdot \\bar{\\pi}_{-b}(\\boldsymbol{\\theta} \\mid \\mathbf{y}), \\] where \\(\\pi_{\\text{CMC}}(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) is the original CMC posterior based on all \\(B\\) subsets. Algorithm: Consensus Monte Carlo Divide the dataset into \\(B\\) disjoint batches \\(\\mathbf{y}_1, \\mathbf{y}_2, \\dots, \\mathbf{y}_B\\) Run \\(B\\) separate MCMC algorithms to sample \\(\\boldsymbol{\\theta}_b^{(s)}\\sim \\pi(\\boldsymbol{\\theta}\\mid \\mathbf{y}_b)\\), \\(b=1,2,\\dots,B\\), and \\(s=1,2,\\dots,S\\) using the prior distribution \\(\\pi(\\boldsymbol{\\theta})^{1/B}\\) Combine the posterior draws using \\(\\boldsymbol{\\theta}^{(s)}=\\left(\\sum_{b=1}^B\\mathbf{w}_b\\right)^{-1} \\sum_{b=1}^B\\mathbf{w}_b \\boldsymbol{\\theta}^{(s)}_b\\) using \\(\\mathbf{w}_b = \\operatorname{Var}^{-1}(\\boldsymbol{\\theta} \\mid \\mathbf{y}_b)\\) The main difficulty is how to effectively merge the subposterior distributions, especially when their supports are not well-aligned. This misalignment can lead to poor scalability with an increasing number of batches. Moreover, most theoretical guarantees for these methods are asymptotic in the size of each batch, which may limit their performance in practice (Bardenet, Doucet, and Holmes 2017). 12.5.2 Subsampling-based algorithms An alternative to divide-and-conquer methods is to avoid evaluating the likelihood over all observations, which requires \\(O(N)\\) operations. Instead, the likelihood is approximated using a smaller subset of observations, \\(n \\ll N\\), in order to reduce the computational burden of the algorithm. The starting point is the log-likelihood function for \\(N\\) independent observations: \\[ \\log p(\\mathbf{y} \\mid \\boldsymbol{\\theta}) = \\sum_{i=1}^N \\log p(y_i \\mid \\boldsymbol{\\theta}). \\] The literature has focused on the log-likelihood because it is a sum over independent contributions, which is analogous to the problem of estimating a population total. A class of subsampling methods relies on estimating the marginal likelihood via the pseudo-marginal approach. Examples include the confidence sampler (Bardenet, Doucet, and Holmes 2014), the Firefly Monte Carlo algorithm (Maclaurin and Adams 2015), whose relationship to subsampling MCMC is formally established in Bardenet, Doucet, and Holmes (2017), and approaches using data-expanded and parameter-expanded control variates (Quiroz et al. 2019). The intuition behind the pseudo-marginal method is straightforward: introduce a set of auxiliary random variables \\(\\mathbf{z} \\sim p(\\mathbf{z})\\), such that the marginal likelihood can be written as an expectation with respect to \\(\\mathbf{z}\\): \\[ \\mathbb{E}_{\\mathbf{z}}[p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z})] = \\int_{\\mathcal{Z}} p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z}) \\, p(\\mathbf{z}) \\, d\\mathbf{z} = p(\\mathbf{y} \\mid \\boldsymbol{\\theta}). \\] This implies that \\[ \\hat{p}(\\mathbf{y} \\mid \\boldsymbol{\\theta}) = \\frac{1}{S} \\sum_{s=1}^S p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z}^{(s)}) \\] is an unbiased estimator of the marginal likelihood, \\(\\mathbf{z}^{(s)} \\sim p(\\mathbf{z})\\). As a result, the pseudo-marginal method enables exact simulation-based inference for \\(p(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) in settings where the likelihood cannot be evaluated analytically (Andrieu and Roberts 2009), for instance, in non-linear random effects models (see also approximate methods such as approximate Bayesian computation and Bayesian synthetic likelihood in Chapter 14). Andrieu and Roberts (2009) show that replacing the likelihood with an unbiased and positive estimator within the Metropolis–Hastings (MH) algorithm yields samples from the correct posterior distribution. The pseudo-marginal likelihood approach can also be applied in settings where the sample size is so large that evaluating the full likelihood at each iteration of an MCMC algorithm becomes computationally prohibitive. In such cases, the likelihood can be approximated using a small subset of observations, \\(n \\ll N\\). The choice of the subset size \\(n\\) is particularly important, as it directly affects the variance of the likelihood estimator, which in turn is critical to ensuring an efficient Metropolis–Hastings (MH) algorithm. In particular, a likelihood estimator with high variance may result in an accepted draw that overestimates the likelihood. As a consequence, subsequent proposals are unlikely to be accepted, causing the algorithm to become stuck and leading to a very low acceptance rate. Therefore, the choice of \\(n\\) determines the computational efficiency of the algorithm: a small \\(n\\) increases the estimator’s variance, which reduces the acceptance rate, whereas a large \\(n\\) increases the number of likelihood evaluations per iteration. Quiroz et al. (2018) recommend targeting a likelihood estimator variance between 1 and 3.3 to optimize computational efficiency, as supported by the findings of Pitt et al. (2012). Let \\(\\ell_i(y_i \\mid \\boldsymbol{\\theta}) = \\log p(y_i \\mid \\boldsymbol{\\theta})\\) denote the contribution of the \\(i\\)-th observation to the log-likelihood, and let \\(z_1, \\dots, z_N\\) be latent binary variables such that \\(z_i = 1\\) indicates that \\(y_i\\) is included in a subsample of size \\(n\\), selected without replacement. Then, an unbiased estimator of the log-likelihood is given by \\[ \\hat{\\ell}(\\mathbf{y} \\mid \\boldsymbol{\\theta}) = \\frac{N}{n} \\sum_{i=1}^N \\ell_i(y_i \\mid \\boldsymbol{\\theta}) z_i. \\] However, note that what we require is an unbiased estimator of the likelihood, not the log-likelihood. Consequently, a bias correction is needed: \\[ \\hat{p}(\\mathbf{y} \\mid \\boldsymbol{\\theta}) = \\exp\\left\\{ \\hat{\\ell}(\\mathbf{y} \\mid \\boldsymbol{\\theta}) - \\frac{1}{2} \\sigma^2_{\\hat{\\ell}}(\\boldsymbol{\\theta}) \\right\\}, \\] where \\(\\sigma^2_{\\hat{\\ell}}(\\boldsymbol{\\theta})\\) denotes the variance of \\(\\hat{\\ell}(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) (Ceperley and Dewing 1999). This correction is exact if \\(\\sigma^2_{\\hat{\\ell}}(\\boldsymbol{\\theta})\\) is known and \\(\\hat{\\ell}(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) follows a normal distribution. Given the importance of controlling the variance of the log-likelihood estimator in subsampling methods, and the limitations of simple random sampling in achieving low variability, Quiroz et al. (2019) propose a highly efficient, unbiased estimator of the log-likelihood based on control variates, specifically through data-expanded and parameter-expanded control variates. The key idea is to construct a function \\(q_i(\\boldsymbol{\\theta})\\) that is highly correlated with the log-likelihood contribution \\(\\ell_i(y_i \\mid \\boldsymbol{\\theta})\\), thereby stabilizing the log-likelihood estimator. In particular, Quiroz et al. (2019) introduce a difference estimator of the form: \\[ \\hat{\\ell}_{\\mathrm{DE}}(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z}) = \\sum_{i=1}^N q_i(\\boldsymbol{\\theta}) + \\frac{N}{n} \\sum_{i: z_i = 1} \\left( \\ell_i(y_i \\mid \\boldsymbol{\\theta}) - q_i(\\boldsymbol{\\theta}) \\right). \\] This estimator \\(\\hat{\\ell}_{\\mathrm{DE}}(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathbf{z})\\) is unbiased for the full log-likelihood \\(\\log p(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\). Quiroz et al. (2019) propose constructing \\(q_i(\\boldsymbol{\\theta})\\) using a second-order Taylor expansion of the log-likelihood around a central value of \\(\\boldsymbol{\\theta}\\), such as the mode. An alternative approach is to perform a second-order Taylor expansion around the nearest centroid of each observation, where the centroids are obtained from a pre-clustering of the data. The first approach can perform poorly when the current draw \\(\\boldsymbol{\\theta}\\) is far from the central expansion point, leading to inaccurate approximations. The second approach encounters difficulties in high-dimensional settings due to the curse of dimensionality: many observations will be far from their assigned centroid. To address these issues, the authors propose an adaptive strategy: initialize the algorithm using data-expanded control variates, and switch to parameter-expanded control variates once the sampler reaches a region closer to the center of the parameter space. It is important to note that this strategy targets an approximation to the posterior distribution, due to the small bias introduced by the difference estimator. However, this bias diminishes rapidly and has a negligible impact on the quality of the posterior inference. Once a good estimator of the log-likelihood is obtained, meaning it has low variance, the likelihood can be recovered using the appropriate bias correction. This corrected likelihood estimator is then used within the acceptance probability of the Metropolis–Hastings algorithm (see Section 4.1.2), resulting in the so-called pseudo-marginal Metropolis–Hastings (PMMH) method. This strategy significantly reduces computational cost in tall data settings. Another class of subsampling methods, which does not rely on the pseudo-marginal likelihood, consists of stochastic gradient MCMC algorithms. These methods are based on ideas from stochastic gradient descent (Robbins and Monro 1951) and Langevin diffusion-based stochastic differential equations. The starting point is the unnormalized posterior distribution: \\[\\begin{align*} \\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\propto \\pi(\\boldsymbol{\\theta}) \\prod_{i=1}^{N} p(y_i \\mid \\boldsymbol{\\theta}) &amp; = \\exp\\left\\{ \\sum_{i=1}^N \\left[ \\frac{1}{N} \\log \\pi(\\boldsymbol{\\theta}) + \\log p(y_i \\mid \\boldsymbol{\\theta}) \\right] \\right\\}\\\\ &amp; = \\exp\\left\\{ -\\sum_{i=1}^N U_i(\\boldsymbol{\\theta}) \\right\\}\\\\ &amp; = \\exp\\left\\{ -U(\\boldsymbol{\\theta}) \\right\\}, \\end{align*}\\] where \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^K\\), \\(U_i(\\boldsymbol{\\theta}) = -\\frac{1}{N} \\log \\pi(\\boldsymbol{\\theta}) - \\log p(y_i \\mid \\boldsymbol{\\theta})\\), and \\(U(\\boldsymbol{\\theta}) = \\sum_{i=1}^N U_i(\\boldsymbol{\\theta})\\) is assumed to be continuous and differentiable almost everywhere. The advantage of this formulation is that, under mild regularity conditions (Roberts and Tweedie 1996), the Langevin diffusion process \\[ d\\boldsymbol{\\theta}(s) = -\\frac{1}{2} \\nabla U(\\boldsymbol{\\theta}(s))\\,ds + d\\mathbf{B}_s, \\] has \\(\\pi(\\boldsymbol{\\theta} \\mid \\mathbf{y})\\) as its stationary distribution. Here, \\(\\nabla U(\\boldsymbol{\\theta}(s))\\) is the drift term, and \\(\\mathbf{B}_s\\) is a \\(K\\)-dimensional Brownian motion.3 Using an Euler-Maruyama discretization of the Langevin diffusion gives a proposal draw from the posterior: \\[ \\boldsymbol{\\theta}^{(c)} = \\boldsymbol{\\theta}^{(s)} - \\frac{\\epsilon}{2} \\nabla U(\\boldsymbol{\\theta}^{(s)}) + \\boldsymbol{\\psi}, \\] where \\(\\boldsymbol{\\psi} \\sim \\mathcal{N}(\\mathbf{0}, \\epsilon \\mathbf{I}_K)\\) and \\(\\epsilon &gt; 0\\) is a suitably chosen step size (learning rate). This proposal is used within a Metropolis–Hastings algorithm (see Section 4.1.2) to correct for the discretization error introduced by the Euler approximation. This method is known as the Metropolis-adjusted Langevin algorithm (MALA) (Roberts and Tweedie 1996). A simpler variant, known as the unadjusted Langevin algorithm (ULA), omits the acceptance step. As a result, ULA produces a biased approximation of the posterior distribution. However, a major computational bottleneck in both MALA and ULA is the requirement to evaluate the full gradient \\(\\nabla U(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\nabla U_i(\\boldsymbol{\\theta})\\) at every iteration, which becomes computationally prohibitive when \\(N\\) is large. To overcome this limitation, Welling and Teh (2011) proposed the Stochastic Gradient Langevin Dynamics (SGLD), which replaces the full gradient with an unbiased estimate computed using a mini-batch of data. Given a random sample of size \\(n \\ll N\\), the stochastic gradient estimate at iteration \\(s\\) is: \\[\\begin{equation} \\hat{\\nabla} U(\\boldsymbol{\\theta})^{(n)} = \\frac{N}{n} \\sum_{i \\in \\mathcal{S}_n} \\nabla U_i(\\boldsymbol{\\theta}), \\tag{12.1} \\end{equation}\\] where \\(\\mathcal{S}_n \\subset \\{1, 2, \\dots, N\\}\\) is a randomly selected subset of size \\(n\\), sampled without replacement. Therefore, \\[ \\boldsymbol{\\theta}^{(s+1)} = \\boldsymbol{\\theta}^{(s)} - \\frac{\\epsilon_s}{2} \\hat{\\nabla} U(\\boldsymbol{\\theta}^{(s)})^{(n)} + \\boldsymbol{\\psi}_s, \\] such that \\(\\sum_{s=1}^{\\infty}\\epsilon_s = \\infty\\) and \\(\\sum_{s=1}^{\\infty}\\epsilon_s^2 &lt; \\infty\\). These conditions guarantee almost sure convergence: the former ensures continued exploration of the parameter space (no premature convergence), and the latter ensures that the cumulative noise remains bounded. Teh, Thiery, and Vollmer (2016) formally show that, under verifiable assumptions, the SGLD algorithm is consistent. That is, given a test function \\(\\phi(\\boldsymbol{\\theta}): \\mathbb{R}^K \\rightarrow \\mathbb{R}\\), \\[ \\lim_{S \\rightarrow \\infty} \\frac{\\epsilon_1 \\phi(\\boldsymbol{\\theta}_1) + \\epsilon_2 \\phi(\\boldsymbol{\\theta}_2) + \\dots + \\epsilon_S \\phi(\\boldsymbol{\\theta}_S)}{\\sum_{s=1}^S \\epsilon_s} = \\int_{\\mathbb{R}^K} \\phi(\\boldsymbol{\\theta}) \\pi(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta}. \\] Moreover, the algorithm satisfies a central limit theorem: \\(\\lim_{S \\rightarrow \\infty} \\pi_S(\\phi(\\boldsymbol{\\theta})) = \\pi(\\phi(\\boldsymbol{\\theta}))\\), and its asymptotic bias–variance decomposition is characterized by a functional of \\(\\epsilon_s\\), such that the optimal step size that minimizes the asymptotic mean squared error is proportional to \\(s^{-1/3}\\). In the common practice of using a constant step size, it has been shown that the optimal choice to minimize the asymptotic mean squared error is of order \\(S^{-1/3}\\) (Vollmer, Zygalakis, and Teh 2016). However, we recommend tuning this parameter based on the specific application, guided by the theoretical results presented here. Importantly, this iterative process does not require the computation of acceptance probabilities, which significantly reduces the computational burden. Empirical evidence suggests that SGLD often outperforms the Metropolis–Hastings algorithm when applied to large datasets under a fixed computational budget (Li, Ahn, and Welling 2016). The following Algorithm summarizes the SGLD procedure (Nemeth and Fearnhead 2021). Algorithm: Stochastic gradient Langevin dynamic Set \\(\\boldsymbol{\\theta}^{(0)}\\) and the step size schedule \\(\\epsilon_s\\) For \\(s = 1, \\dots, S\\): Draw \\(\\mathcal{S}_n\\) of size \\(n\\) from \\(i=\\left\\{1,2,\\dots,N\\right\\}\\) without replacement Calculate \\(\\hat{\\nabla} U(\\boldsymbol{\\theta})^{(n)}\\) using \\(\\hat{\\nabla} U(\\boldsymbol{\\theta})^{(n)} = \\frac{N}{n} \\sum_{i \\in \\mathcal{S}_n} \\nabla U_i(\\boldsymbol{\\theta})\\) Draw \\(\\boldsymbol{\\psi}_s\\sim N(\\mathbf{0},\\epsilon_s\\mathbf{I}_K)\\) Update \\(\\boldsymbol{\\theta}^{(s+1)}\\leftarrow \\boldsymbol{\\theta}^{(s)} -\\frac{\\epsilon_s}{2}\\hat{\\nabla} U(\\boldsymbol{\\theta}^{(s)})^{(n)}+\\boldsymbol{\\psi}_s\\) End for A critical component of the SGLD algorithm is the estimation of the stochastic gradient (Equation (12.1)), particularly because high variability in this estimator can lead to algorithmic instability, a challenge also encountered in pseudo-marginal methods, as described previously. To mitigate this issue, the literature also employs control variates to reduce the variance of the estimator. The core idea is to construct a simple function \\(u_i(\\boldsymbol{\\theta})\\) that is highly correlated with \\(\\nabla U_i(\\boldsymbol{\\theta})\\) and has a known expectation. This correlation allows the fluctuations in \\(u_i(\\boldsymbol{\\theta})\\) to “cancel out” some of the noise in \\(\\nabla U_i(\\boldsymbol{\\theta})\\), thereby stabilizing the stochastic gradient estimates. Specifically, \\[ \\sum_{i=1}^N \\nabla U_i(\\boldsymbol{\\theta}) = \\sum_{i=1}^N u_i(\\boldsymbol{\\theta}) + \\sum_{i=1}^N \\left( \\nabla U_i(\\boldsymbol{\\theta}) - u_i(\\boldsymbol{\\theta}) \\right), \\] which leads to the following unbiased estimator: \\[ \\sum_{i=1}^N u_i(\\boldsymbol{\\theta}) + \\frac{N}{n} \\sum_{i \\in \\mathcal{S}_n} \\left( \\nabla U_i(\\boldsymbol{\\theta}) - u_i(\\boldsymbol{\\theta}) \\right). \\] To construct effective control variates, one common strategy is to first approximate the posterior mode \\(\\hat{\\boldsymbol{\\theta}}\\) using stochastic gradient descent (SGD), which serves as the initialization point for SGLD Algorithm. SGD proceeds via a stochastic approximation of the gradient: \\[ \\boldsymbol{\\theta}^{(s+1)} = \\boldsymbol{\\theta}^{(s)} - \\epsilon_s \\frac{1}{n} \\sum_{i \\in \\mathcal{S}_n} \\nabla U_i(\\boldsymbol{\\theta}^{(s)}). \\] This approximation introduces stochasticity into the updates but significantly reduces computational cost. Two commonly used learning rate (or step size) schedules are \\(\\epsilon_s = s^{-\\kappa}\\) and \\(\\epsilon_s = \\epsilon_0 / (1 + s / \\tau)^{\\kappa}\\), where \\(\\epsilon_0\\) is the initial learning rate, \\(\\tau\\) is a stability constant that slows down early decay (larger values lead to more stable early behavior), and \\(\\kappa \\in (0.5, 1]\\) controls the long-run decay rate. If \\(\\kappa\\) is too large, the learning rate decays too quickly and the algorithm may stagnate. Conversely, if \\(\\kappa\\) is too small, the algorithm may remain unstable or fail to converge. An important distinction to note is that SGLD operates with gradient sums, while SGD typically uses averages. This distinction affects how step sizes and noise scaling should be interpreted in practice. After convergence, we obtain a reliable estimate of the posterior mode \\(\\hat{\\boldsymbol{\\theta}}\\). Based on this, we define the control variate as \\(u_i(\\boldsymbol{\\theta}) = \\nabla U_i(\\hat{\\boldsymbol{\\theta}})\\). The resulting control variate estimator of the gradient is: \\[ \\hat{\\nabla}_{\\text{cv}} U(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\nabla U_i(\\hat{\\boldsymbol{\\theta}}) + \\frac{N}{n} \\sum_{i \\in \\mathcal{S}_n} \\left( \\nabla U_i(\\boldsymbol{\\theta}) - \\nabla U_i(\\hat{\\boldsymbol{\\theta}}) \\right). \\] Example: Simulation exercise to study the performance of CMC and SGLD In this example, we follow the logistic regression simulation setup introduced by Nemeth and Fearnhead (2021): \\[ P(y_i = 1 \\mid \\boldsymbol{\\beta}, \\mathbf{x}_i) = \\frac{\\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\}}{1 + \\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\}}, \\] with log-likelihood function given by: \\[ \\log p(\\mathbf{y} \\mid \\boldsymbol{\\beta}, \\mathbf{x}) = \\sum_{i=1}^N y_i \\left( \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - \\log \\left(1 + \\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\} \\right) \\right) + (1 - y_i) \\left( - \\log \\left(1 + \\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\} \\right) \\right), \\] which simplifies to: \\[ \\log p(\\mathbf{y} \\mid \\boldsymbol{\\beta}, \\mathbf{x}) = \\sum_{i=1}^N y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - \\log \\left(1 + \\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\} \\right). \\] This implies that the gradient vector is: \\[ \\nabla \\log p(\\mathbf{y} \\mid \\boldsymbol{\\beta}, \\mathbf{x}) = \\sum_{i=1}^N \\left(y_i - \\frac{\\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\}}{1+\\exp\\left\\{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right\\}}\\right)\\mathbf{x}_i. \\] We assume a prior distribution for \\(\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, 10 \\mathbf{I}_K)\\), leading to the log-prior: \\[ \\log \\pi(\\boldsymbol{\\beta}) = -\\frac{K}{2} \\log(2\\pi) - \\frac{1}{2} \\log\\left( \\lvert 10 \\mathbf{I}_K \\rvert \\right) - \\frac{1}{2} \\boldsymbol{\\beta}^{\\top} (10^{-1} \\mathbf{I}_K) \\boldsymbol{\\beta}. \\] The gradient of the log-prior is: \\[ \\nabla \\log \\pi(\\boldsymbol{\\beta}) = -\\frac{1}{10}\\boldsymbol{\\beta}. \\] Also note that: \\[\\begin{align*} \\pi(\\boldsymbol{\\beta})^{1/B} &amp; \\propto \\left\\{\\exp\\left(-\\frac{1}{2 \\cdot 10} \\boldsymbol{\\beta}^\\top \\boldsymbol{\\beta}\\right)\\right\\}^{1/B}\\\\ &amp; = \\exp\\left(-\\frac{1}{2 \\cdot 10 \\cdot B} \\boldsymbol{\\beta}^\\top \\boldsymbol{\\beta} \\right). \\end{align*}\\] This implies that, when implementing CMC, the prior variance must be scaled by the number of batches \\(B\\). That is, each subposterior should use a prior with variance \\(10 \\cdot B\\) so that the product of the \\(B\\) subposteriors reconstructs the correct full posterior. We set \\(K = 10\\), \\(\\boldsymbol{\\beta} = 0.5 \\cdot \\mathbf{i}_K\\), and \\(N = 10^5\\). The covariates \\(\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})\\), where the covariance matrix \\(\\boldsymbol{\\Sigma}^{(i,j)} = U[-\\rho, \\rho]^{|i-j|}\\) with \\(\\rho = 0.4\\), and \\(\\mathbf{i}_K\\) denotes a \\(K\\)-dimensional vector of ones. We run 2,000 MCMC iterations initialized at zero, and discard the first 500 as burn-in. We scale the regressors beforehand, as this is generally recommended to improve numerical stability and convergence. The following code simulates the model and sets the hyperparameters of the algorithms. The following code implements SMC Algorithm, running five parallel MCMC chains and combining the resulting subposteriors using three different weighting schemes: equal weights, weights based on marginal variances, and weights based on the full covariance matrices. By running the code, you can verify that the computational time of the CMC algorithm is lower than that of the Metropolis–Hastings algorithm. The first figure shows the posterior distributions of \\(\\beta_4\\) and \\(\\beta_5\\). We observe that all three weighting schemes perform reasonably well, yielding posterior modes similar to those obtained from the full-data MCMC algorithm. However, the consensus Monte Carlo (CMC) methods produce more dispersed draws, particularly when using equal weights. In contrast, the weighting schemes based on marginal variances and the full covariance matrices yield comparable and more concentrated posterior distributions. To implement the SGLD algorithm, we set \\(n = 0.01 \\cdot N\\), and the step size to \\(1 \\times 10^{-4}\\). The following code illustrates how to implement the SGLD algorithm.4 In Exercise 8, you are asked to implement the control variate version of SGLD. Begin by running 1,500 SGD iterations to locate the posterior mode. This mode should then be used as the initial value for a subsequent run of 1,000 SGLD iterations. By running the code, you can verify that the computational time of the SGLD algorithm is lower than that of the Metropolis–Hastings algorithm. The second figure shows the posterior distributions of the fifth location parameter obtained from SGLD and Metropolis–Hastings. We observe that both modes are centered around the true population value; however, the SGLD distribution exhibits greater dispersion compared to the Metropolis–Hastings distribution. ####### CMC and SDLD ####### #### Simulation rm(list = ls()); set.seed(10101) library(mvtnorm); library(MCMCpack) ## Warning: package &#39;MCMCpack&#39; was built under R version 4.5.2 ## Loading required package: coda ## ## Attaching package: &#39;coda&#39; ## The following object is masked from &#39;package:Boom&#39;: ## ## thin ## ## ## ## Markov Chain Monte Carlo Package (MCMCpack) ## ## Copyright (C) 2003-2025 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park ## ## ## ## Support provided by the U.S. National Science Foundation ## ## (Grants SES-0350646 and SES-0350613) ## ## ## ## Attaching package: &#39;MCMCpack&#39; ## The following objects are masked from &#39;package:Boom&#39;: ## ## ddirichlet, dinvgamma, rdirichlet, rinvgamma library(ggplot2); library(dplyr) library(parallel); library(GGally) ## Warning: package &#39;GGally&#39; was built under R version 4.5.1 ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ### Generate correlated covariates genCovMat &lt;- function(K, rho = 0.4) { Sigma0 &lt;- matrix(1, K, K) for (i in 2:K) { for (j in 1:(i - 1)) { Sigma0[i, j] &lt;- runif(1, -rho, rho)^(i - j) } } Sigma0 &lt;- Sigma0 * t(Sigma0) diag(Sigma0) &lt;- 1 return(Sigma0) } ### Simulate logistic regression data simulate_logit_data &lt;- function(K, N, beta_true) { Sigma0 &lt;- genCovMat(K) X &lt;- rmvnorm(N, mean = rep(0, K), sigma = Sigma0) linpred &lt;- X %*% beta_true p &lt;- 1 / (1 + exp(-linpred)) y &lt;- rbinom(N, 1, p) list(y = y, X = X) } ### Parameters K &lt;- 10 N &lt;- 100000 beta_true &lt;- rep(0.5, K) B &lt;- 5 batch_prop &lt;- 0.01 Prior_prec &lt;- 0.1 n_iter &lt;- 2000 burnin &lt;- 500 stepsize &lt;- 1e-4 k_target1 &lt;- 4 # beta5 k_target2 &lt;- 5 # beta5 ks &lt;- k_target1:k_target2 #--- Simulate data sim_data &lt;- simulate_logit_data(K, N, beta_true) y &lt;- sim_data$y X &lt;- scale(sim_data$X) ### Run MCMCpack logit df &lt;- as.data.frame(X) colnames(df) &lt;- paste0(&quot;X&quot;, 1:K) df$y &lt;- y formula &lt;- as.formula(paste(&quot;y ~&quot;, paste(colnames(df)[1:K], collapse = &quot; + &quot;), &quot;-1&quot;)) posterior_mh &lt;- MCMClogit(formula, data = df, b0 = 0, B0 = Prior_prec, burnin = burnin, mcmc = n_iter) full_posterior &lt;- as.matrix(posterior_mh)[, 1:K] #### CMC ### Split data batch_ids &lt;- split(1:N, sort(rep(1:B, length.out = N))) ### Function to run MCMC on a subset mcmc_batch &lt;- function(batch_index, X, y, n_iter, burnin) { ids &lt;- batch_ids[[batch_index]] X_b &lt;- X[ids, ] y_b &lt;- y[ids] mcmc_out &lt;- MCMClogit(y_b ~ X_b - 1, burnin = burnin, mcmc = n_iter, verbose = 0, b0 = 0, B0 = Prior_prec * (1/B)) return(mcmc_out) } ### Run in parallel cl &lt;- makeCluster(B) clusterExport(cl, c(&quot;X&quot;, &quot;y&quot;, &quot;batch_ids&quot;, &quot;n_iter&quot;, &quot;burnin&quot;, &quot;mcmc_batch&quot;, &quot;Prior_prec&quot;, &quot;B&quot;)) clusterEvalQ(cl, library(MCMCpack)) ## [[1]] ## [1] &quot;MCMCpack&quot; &quot;MASS&quot; &quot;coda&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; ## ## [[2]] ## [1] &quot;MCMCpack&quot; &quot;MASS&quot; &quot;coda&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; ## ## [[3]] ## [1] &quot;MCMCpack&quot; &quot;MASS&quot; &quot;coda&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; ## ## [[4]] ## [1] &quot;MCMCpack&quot; &quot;MASS&quot; &quot;coda&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; ## ## [[5]] ## [1] &quot;MCMCpack&quot; &quot;MASS&quot; &quot;coda&quot; &quot;stats&quot; &quot;graphics&quot; &quot;grDevices&quot; ## [7] &quot;utils&quot; &quot;datasets&quot; &quot;methods&quot; &quot;base&quot; chains &lt;- parLapply(cl, 1:B, function(b) mcmc_batch(b, X, y, n_iter, burnin)) stopCluster(cl) # Stack MCMC results posteriors &lt;- lapply(chains, function(x) x[, 1:K]) # CMC posteriors equal_cmc &lt;- Reduce(&quot;+&quot;, posteriors) / B invvar_cmc &lt;- { vars &lt;- lapply(posteriors, function(x) apply(x, 2, var)) weights &lt;- lapply(vars, function(v) 1 / v) weights_sum &lt;- Reduce(&quot;+&quot;, weights) weighted_post &lt;- Reduce(&quot;+&quot;, Map(function(x, w) sweep(x, 2, w, &quot;*&quot;), posteriors, weights)) sweep(weighted_post, 2, weights_sum, &quot;/&quot;) } invmat_cmc &lt;- { covs &lt;- lapply(posteriors, cov) invs &lt;- lapply(covs, solve) weight_sum &lt;- Reduce(&quot;+&quot;, invs) consensus &lt;- matrix(NA, nrow = n_iter, ncol = K) for (i in 1:n_iter) { draws &lt;- lapply(posteriors, function(p) matrix(p[i, ], ncol = 1)) weighted_sum &lt;- Reduce(&quot;+&quot;, Map(function(w, d) w %*% d, invs, draws)) consensus[i, ] &lt;- as.vector(solve(weight_sum, weighted_sum)) } consensus } # Combine all for plotting build_df &lt;- function(mat, method) { df &lt;- as.data.frame(mat) colnames(df) &lt;- paste0(&quot;x&quot;, ks) df$method &lt;- method return(df) } df_full &lt;- build_df(full_posterior[,ks], &quot;overall&quot;) df_equal &lt;- build_df(equal_cmc[,ks], &quot;equal&quot;) df_scalar &lt;- build_df(invvar_cmc[,ks], &quot;scalar&quot;) df_matrix &lt;- build_df(invmat_cmc[,ks], &quot;matrix&quot;) df_plot &lt;- rbind(df_full, df_matrix, df_scalar, df_equal) # Plot ggpairs(df_plot, aes(color = method, fill = method, alpha = 0.4), upper = list(continuous = GGally::wrap(&quot;density&quot;, alpha = 0.4)), lower = list(continuous = GGally::wrap(&quot;density&quot;, alpha = 0.4)), diag = list(continuous = GGally::wrap(&quot;densityDiag&quot;, alpha = 0.4))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #### SGLD SGLD_step &lt;- function(beta, y, X, stepsize, batch_size, prior_var = 10) { N &lt;- nrow(X); K &lt;- length(beta) ids &lt;- sample(1:N, size = batch_size, replace = FALSE) grad &lt;- rep(0, K) for (i in ids) { xi &lt;- X[i, ] eta &lt;- sum(xi * beta) pi &lt;- 1 / (1 + exp(-eta)) grad_i &lt;- -(y[i] - pi) * xi grad &lt;- grad + grad_i } grad &lt;- grad / batch_size * N grad &lt;- grad + beta / prior_var noise &lt;- rnorm(K, 0, sqrt(stepsize)) beta_new &lt;- beta - 0.5 * stepsize * grad + noise return(beta_new) } ### SGLD algorithm run_SGLD &lt;- function(y, X, stepsize, batch_prop, n_iter, burnin, beta_init = NULL) { N &lt;- nrow(X) K &lt;- ncol(X) batch_size &lt;- round(batch_prop * N) beta_mat &lt;- matrix(0, n_iter + burnin, K) beta_mat[1, ] &lt;- if (is.null(beta_init)) rep(0, K) else beta_init for (s in 2:(n_iter + burnin)) { beta_mat[s, ] &lt;- SGLD_step(beta_mat[s - 1, ], y, X, stepsize, batch_size) } beta_mat[(burnin + 1):(n_iter + burnin), ] } ### Run SGLD posterior_sgld &lt;- run_SGLD(y = y, X = X, stepsize, batch_prop, n_iter, burnin) ### Compare densities for beta5 df_plot &lt;- data.frame( value = c(posterior_sgld[, k_target2], posterior_mh[, k_target2]), method = rep(c(&quot;SGLD&quot;, &quot;MCMC&quot;), each = n_iter) ) ggplot(df_plot, aes(x = value, fill = method, color = method)) + geom_density(alpha = 0.4) + geom_vline(xintercept = beta_true[k_target2], linetype = &quot;dashed&quot;, color = &quot;black&quot;) + labs(title = expression(paste(&quot;Posterior density of &quot;, beta[5])), x = expression(beta[5]), y = &quot;Density&quot;) + theme_minimal() References Andrieu, Christophe, and Gareth O. Roberts. 2009. “The Pseudo-Marginal Approach for Efficient Monte Carlo Computations.” Annals of Statistics 37 (2): 697–725. https://doi.org/10.1214/07-AOS574. Baker, Jack, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. 2019. “Sgmcmc: An r Package for Stochastic Gradient Markov Chain Monte Carlo.” Journal of Statistical Software 91: 1–27. Bardenet, Rémi, Arnaud Doucet, and Chris Holmes. 2014. “Towards Scaling up Markov Chain Monte Carlo: An Adaptive Subsampling Approach.” In International Conference on Machine Learning, 405–13. PMLR. ———. 2017. “On Markov Chain Monte Carlo Methods for Tall Data.” Journal of Machine Learning Research 18 (47): 1–43. Ceperley, DM, and Mark Dewing. 1999. “The Penalty Method for Random Walks with Uncertain Energies.” The Journal of Chemical Physics 110 (20): 9812–20. Huang, Zhen, and Andrew Gelman. 2005. “Sampling for Bayesian Computation with Large Datasets.” Technical Report. Department of Statistics, Columbia University. Li, Wei, Sangwoo Ahn, and Max Welling. 2016. “Scalable MCMC for Mixed Membership Stochastic Blockmodels.” In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS), edited by JMLR Workshop and Conference Proceedings, 51:723–31. Maclaurin, Dougal, and Ryan P. Adams. 2015. “Firefly Monte Carlo: Exact MCMC with Subsets of Data.” In The Twenty-Fourth International Joint Conference on Artificial Intelligence, 4279–95. IJCAI. Minsker, Stanislav. 2015. “Scalable and Robust Bayesian Inference via the Median Posterior.” Edited by Francis Bach and David Blei. Proceedings of the 31st International Conference on Machine Learning (ICML) 37: 1656–64. Minsker, Stanislav, Subhabrata Srivastava, Lin Lin, and David B. Dunson. 2017. “Robust and Scalable Bayes via a Median of Subset Posterior Measures.” Journal of Machine Learning Research 18 (1): 4488–4527. Neiswanger, Willie, Chong Wang, and Eric P Xing. 2013. “Asymptotically Exact, Embarrassingly Parallel MCMC.” In Proceedings of the Thirtieth International Conference on Machine Learning, 623–32. PMLR. Nemeth, Christopher, and Paul Fearnhead. 2021. “Stochastic Gradient Markov Chain Monte Carlo.” Journal of the American Statistical Association 116 (533): 433–50. Pitt, Michael K, Ralph dos Santos Silva, Paolo Giordani, and Robert Kohn. 2012. “On Some Properties of Markov Chain Monte Carlo Simulation Methods Based on the Particle Filter.” Journal of Econometrics 171 (2): 134–51. Quiroz, Matias, Robert Kohn, Mattias Villani, and Minh-Ngoc Tran. 2019. “Speeding up MCMC by Efficient Data Subsampling.” Journal of the American Statistical Association. Quiroz, Matias, Mattias Villani, Robert Kohn, Minh-Ngoc Tran, and Khue-Dung Dang. 2018. “Subsampling MCMC—a Review for the Survey Statistician.” arXiv Preprint arXiv:1807.08409. Rendell, Lewis J, Adam M Johansen, Anthony Lee, and Nick Whiteley. 2020. “Global Consensus Monte Carlo.” Journal of Computational and Graphical Statistics 30 (2): 249–59. Robbins, Herbert, and Sutton Monro. 1951. “A Stochastic Approximation Method.” The Annals of Mathematical Statistics, 400–407. Roberts, Gareth O., and Richard L. Tweedie. 1996. “Exponential Convergence of Langevin Distributions and Their Discrete Approximations.” Bernoulli 2 (4): 341–63. https://doi.org/10.2307/3318418. Scott, Steven L., Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. 2016. “Bayes and Big Data: The Consensus Monte Carlo Algorithm.” International Journal of Management Science and Engineering Management 11 (2): 78–88. https://doi.org/10.1080/17509653.2016.1142191. Scott, Steven L, Alexander W Blocker, Fernando V Bonassi, Hugh A Chipman, Edward I George, and Robert E McCulloch. 2022. “Bayes and Big Data: The Consensus Monte Carlo Algorithm.” In Big Data and Information Theory, 8–18. Routledge. Teh, Yee Whye, Alexandre H. Thiery, and Sebastian J. Vollmer. 2016. “Consistency and Fluctuations for Stochastic Gradient Langevin Dynamics.” Journal of Machine Learning Research 17 (1): 193–225. Vollmer, Sebastian J., Konstantinos C. Zygalakis, and Yee Whye Teh. 2016. “Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics.” Journal of Machine Learning Research 17 (159): 1–48. Wang, Xiangyu, and David B Dunson. 2013. “Parallelizing MCMC via Weierstrass Sampler.” arXiv Preprint arXiv:1312.4605. Welling, Max, and Yee W Teh. 2011. “Bayesian Learning via Stochastic Gradient Langevin Dynamics.” In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 681–88. Citeseer. Wu, Changye, and Christian P Robert. 2017. “Average of Recentered Parallel MCMC for Big Data.” arXiv Preprint arXiv:1706.04780. A Brownian motion is a continuous-time stochastic process that starts at zero, has independent increments with \\(B(s) - B(t) \\sim \\mathcal{N}(0, s - t)\\), and is continuous almost surely but nowhere differentiable.↩︎ There is an R package called sgmcmc, developed by Baker et al. (2019), which provides implementations of various stochastic gradient MCMC methods, including SGLD and SGHMC. However, this package depends on version 1 of the tensorflow package, while the current version is 2, and sgmcmc has not been updated on CRAN. We attempted to install the package from its GitHub repository using the devtools::install\\_github(\"STOR-i/sgmcmc\" command, but encountered compatibility issues due to conflicting TensorFlow versions.↩︎ "],["id_13_6.html", "12.6 Summary", " 12.6 Summary In this chapter, we introduced several Bayesian machine learning methods designed to address the challenges posed by wide and tall data. However, the field of Bayesian machine learning is rapidly evolving, and the material presented here should be viewed as an introductory overview. Many important topics were not covered but are highly relevant, such as Bayesian neural networks (Neal 2012) and neural posterior estimation (Papamakarios and Murray 2016; Lueckmann et al. 2017; Greenberg, Nonnenmacher, and Macke 2019). Other key approaches, such as Variational Bayes, particularly in its stochastic implementations, which are rooted in machine learning and offer scalable solutions for tall data, are introduced in Chapter 14 (Wainwright, Jordan, et al. 2008). References Greenberg, David S, Marcel Nonnenmacher, and Jakob H Macke. 2019. “Automatic Posterior Transformation for Likelihood-Free Inference.” In International Conference on Machine Learning, 2404–14. Lueckmann, Jan-Matthis, Pedro J Goncalves, Gabriele Bassetto, Kaan Öcal, Marcel Nonnenmacher, and Jakob H Macke. 2017. “Flexible Statistical Inference for Mechanistic Models of Neural Dynamics.” Advances in Neural Information Processing Systems 30. Neal, Radford M. 2012. Bayesian Learning for Neural Networks. Vol. 118. Springer Science &amp; Business Media. Papamakarios, George, and Iain Murray. 2016. “Fast \\(\\epsilon\\)-Free Inference of Simulation Models with Bayesian Conditional Density Estimation.” In Advances in Neural Information Processing Systems. Vol. 29. Wainwright, Martin J, Michael I Jordan, et al. 2008. “Graphical Models, Exponential Families, and Variational Inference.” Foundations and Trends in Machine Learning 1 (1–2): 1–305. "],["id_13_7.html", "12.7 Exercises", " 12.7 Exercises Simulation exercise: the Bayesian LASSO continues Program the Gibbs sampler for the Bayesian LASSO from scratch, assuming a hierarchical structure for the global shrinkage parameter, where both the shape and rate parameters are set to 1. Perform inference using this sampler in the Bayesian LASSO simulation exercise and compare the results with those obtained using the monomvn package. Jetter et al. (2022) employ SSVS to identify the main drivers of civil conflict in the post-Cold War era, considering a set of 35 potential determinants across 175 countries worldwide. We use a subset of their dataset provided in Conflict.csv, where the dependent variable is conflictcw, a binary indicator of civil conflict. Perform SSVS using the BoomSpikeSlab package, specifically the lm.spike function, to identify the best subset of models. Tüchler (2008) proposes an SSVS approach for binary response models. Use the dataset Conflict.csv, where the dependent variable is conflictcw, to perform SSVS using the BoomSpikeSlab package, specifically the logit.spike function, in order to identify the best subset of models. Compare the results with those obtained in Exercise 2. Example: Simulation exercise \\(K &gt; N\\) Use the simulation setting from the Bayesian LASSO and SSVS examples, but now assume there are 600 inputs. This setup implies that the number of inputs exceeds the sample size. In such a scenario, there is no unique solution to the least squares estimator because the determinant of \\(\\mathbf{W}^{\\top} \\mathbf{W}\\) is zero. This means the matrix is not invertible, and consequently, standard inference procedures based on the least squares estimator cannot be applied. On the other hand, Bayesian inference in this setup is well-defined because the prior helps regularize the problem, which is a key motivation for these methods. Simulation exercise: the BART model continues Compute Friedman’s partial dependence functions (Friedman 2001) for all variables in the BART model simulation example, and plot the posterior mean along with the 95% credible intervals. Chipman, George, and McCulloch (2010) present BART probit for classification. This method can be implemented using the BART package through the function pbart. Use the file Conflict.csv, where the dependent variable is conflictcw, to perform BART probit, implementing k-fold cross-validation to select the threshold that maximizes the sum of the true positive and true negative rates. Additionally, identify the most important predictors by evaluating different numbers of trees. Simulation exercise: The Gaussian Process simulation continues Simulate the process \\[ f_i = \\sin(2\\pi x_{i1}) + \\cos(2\\pi x_{i2}) + \\sin(x_{i1} x_{i2}) + \\mu_i, \\] where \\(\\mu_i \\overset{\\text{i.i.d.}}{\\sim} N(0, 0.1^2)\\), \\(x_{ik} \\sim U(0,1)\\) for \\(k = 1, 2\\), and the sample size is 500. Define a grid of 20 evenly spaced values between 0 and 1 for each covariate \\(x_{ik}\\), and use this grid to perform prediction. Estimate the hyperparameters of the Gaussian Process by maximizing the log marginal likelihood. Then, use the km function from the DiceKriging package to fit the Gaussian Process, fixing the noise variance at the value that maximizes the log marginal likelihood. Finally, use the fitted model to predict the outputs on the grid points, and produce a 3D plot showing the predicted surface along with the training data points. Simulation exercise: Stochastic gradient MCMC continues Program from scratch the stochastic gradient Langevin dynamics algorithm for the logit simulation exercise implementing the control variate version, performing 1,500 stochastic gradient descent iterations to locate the posterior mode, which should then be used as the initial value for 1,000 subsequent MCMC iterations using a step size set to \\(1 \\times 10^{-4}\\). Perform the simulation according to the model \\[ y_i = 1 - 2 x_{i1} + 0.5 x_{i2} + \\mu_i, \\] where \\(\\mu_i \\sim N(0,1)\\), the sample size is 100,000, and the covariates \\(\\mathbf{x}_i \\sim N(\\mathbf{0}, \\mathbf{I}_2)\\). Use 5,000 MCMC iterations and a batch size of 1,000 to implement the SGLD algorithm. Set a learning rate schedule that yields sensible results. Assume independent priors \\(\\pi(\\boldsymbol{\\beta}, \\sigma^2) = \\pi(\\boldsymbol{\\beta}) \\times \\pi(\\sigma^2)\\), with \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0}, \\mathbf{I}_3)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\), where \\(\\alpha_0 = \\delta_0 = 0.01\\). References ———. 2010. “BART: Bayesian Additive Regression Trees.” The Annals of Applied Statistics 4 (1): 266–98. ———. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics 29 (5): 1189–1232. Jetter, Michael, Rafat Mahmood, Christopher F. Parmeter, and Andrés Ramı́rez-Hassan. 2022. “Post-Cold War Civil Conflict and the Role of History and Religion: A Stochastic Search Variable Selection Approach.” Economic Modelling 114: 105907. https://doi.org/10.1016/j.econmod.2022.105907. Tüchler, Regina. 2008. “Bayesian Variable Selection for Logistic Models Using Auxiliary Mixture Sampling.” Journal of Computational and Graphical Statistics 17 (1): 76–94. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
