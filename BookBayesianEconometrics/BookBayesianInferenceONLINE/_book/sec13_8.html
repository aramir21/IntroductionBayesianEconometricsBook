<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.8 Bayesian exponentially tilted empirical likelihood | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec13_7.html"/>
<link rel="next" href="sec13_10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Identification setting</a></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Randomized controlled trial (RCT)</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Conditional independence assumption (CIA)</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Instrumental variables (IV)</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference-in-differences design (DiD)</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="sec13_5.html"><a href="sec13_5.html#sec13_51"><i class="fa fa-check"></i><b>13.5.1</b> Classic DiD: two-group and two-period (<span class="math inline">\(2\times2\)</span>) setup</a></li>
<li class="chapter" data-level="13.5.2" data-path="sec13_5.html"><a href="sec13_5.html#sec13_52"><i class="fa fa-check"></i><b>13.5.2</b> Staggered (SDiD): G-group and T-period (<span class="math inline">\(G\times T\)</span>) setup</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Regression discontinuity design (RD)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="sec13_6.html"><a href="sec13_6.html#sec13_61"><i class="fa fa-check"></i><b>13.6.1</b> Sharp regression discontinuity design (SRD)</a></li>
<li class="chapter" data-level="13.6.2" data-path="sec13_6.html"><a href="sec13_6.html#sec13_62"><i class="fa fa-check"></i><b>13.6.2</b> Fuzzy regression discontinuity design (FRD)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Sample selection</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Bayesian exponentially tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.9" data-path="sec13_10.html"><a href="sec13_10.html"><i class="fa fa-check"></i><b>13.9</b> Doubly robust Bayesian inferential framework (DRB)</a></li>
<li class="chapter" data-level="13.10" data-path="sec13_11.html"><a href="sec13_11.html"><i class="fa fa-check"></i><b>13.10</b> Summary</a></li>
<li class="chapter" data-level="13.11" data-path="sec13_12.html"><a href="sec13_12.html"><i class="fa fa-check"></i><b>13.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec13_8" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> Bayesian exponentially tilted empirical likelihood<a href="sec13_8.html#sec13_8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian parametric approaches are often criticized because they require distributional assumptions that may be arbitrary or remain unchecked. For example, in this chapter we model a continuous outcome as normally distributed. This choice is defensible: among all distributions with a fixed mean and variance, the normal imposes the least prior structure (it maximizes entropy; see Exercise 2), and the same reasoning extends to regression via Gaussian errors with fixed conditional variance <span class="citation">(<a href="#ref-zellner1996bmom">Zellner 1996</a>)</span>. Nevertheless, in some settings it may be preferable to use partial information methods that rely only on moment conditions rather than full distributional assumptions. The trade-off is familiar: unless the parametric model is correctly specified, these semiparametric approaches typically reduce efficiency relative to a well-specified parametric model.</p>
<p>The point of departure is a set of moment conditions</p>
<p><span class="math display">\[
\mathbb{E}\!\big[\mathbf{g}(\mathbf{W},\boldsymbol{\theta})\big]=\mathbf{0}_{d},
\]</span></p>
<p>where the expectation is with respect to the population distribution, <span class="math inline">\(\mathbf{W}_{1:N}:=[\mathbf{W}_1 \ \mathbf{W}_2 \ \dots \ \mathbf{W}_N]\)</span> is a random sample from <span class="math inline">\(\mathbf{W}\subset \mathbb{R}^{d_w}\)</span>, <span class="math inline">\(\mathbf{g}:\mathbb{R}^{d_w}\times\boldsymbol{\Theta}\to\mathbb{R}^{d}\)</span> is a vector of known functions, and
<span class="math inline">\(\boldsymbol{\theta}=[\theta_{1}\ \theta_{2}\ \dots\ \theta_{p}]^{\top}\in\boldsymbol{\Theta}\subset\mathbb{R}^{p}\)</span>.
If <span class="math inline">\(d&gt;p\)</span> the model is <em>over-identified</em>; if <span class="math inline">\(d=p\)</span> it is <em>exactly identified</em> (and if <span class="math inline">\(d&lt;p\)</span> it is <em>under-identified</em>).</p>
<p><strong>Example: Linear regression</strong></p>
<p>Let</p>
<p><span class="math display">\[
y_i=\mathbf{X}_i^{\top}\boldsymbol{\beta}+\mu_i,\qquad \mathbb{E}[\mu_i\mid \mathbf{X}_i]=0,
\]</span></p>
<p>with <span class="math inline">\(\mathbf{X}_i\in\mathbb{R}^{p}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\in\mathbb{R}^{p}\)</span>. Then the unconditional moment conditions are</p>
<p><span class="math display">\[
\mathbb{E}\!\left[\mathbf{X}_i\,\mu_i\right]
=\mathbb{E}\!\left[\mathbf{X}_i\,(y_i-\mathbf{X}_i^{\top}\boldsymbol{\beta})\right]
=\mathbf{0}_{p}.
\]</span></p>
<p><strong>Example: Instrumental variables</strong></p>
<p>If there is endogeneity, <span class="math inline">\(\mathbb{E}[\mu_i\mid \mathbf{X}_i]\neq 0\)</span>, but there exist instruments <span class="math inline">\(\mathbf{Z}_i\in\mathbb{R}^{d}\)</span> that are exogenous, <span class="math inline">\(\mathbb{E}[\mu_i\mid \mathbf{Z}_i]=0\)</span>, and relevant, <span class="math inline">\(\operatorname{rank}\!\big(\mathbb{E}[\mathbf{Z}_i\mathbf{X}_i^{\top}]\big)=p\)</span>, then</p>
<p><span class="math display">\[
\mathbb{E}\!\left[\mathbf{Z}_i\,\mu_i\right]
=\mathbb{E}\!\left[\mathbf{Z}_i\,(y_i-\mathbf{X}_i^{\top}\boldsymbol{\beta})\right]
=\mathbf{0}_{d},
\]</span></p>
<p>with <span class="math inline">\(d\ge p\)</span>.</p>
<p>Moment conditions can be used in a Bayesian framework via <em>Bayesian Empirical Likelihood</em> (BEL) <span class="citation">(<a href="#ref-lazar2003bel">Lazar 2003</a>)</span> and <em>Bayesian Exponentially Tilted Empirical Likelihood</em> (BETEL) <span class="citation">(<a href="#ref-schennach2005betel">Schennach 2005</a>)</span>. We focus on BETEL because, while BEL inherits the attractive properties of empirical likelihood under correct specification, it can lose them under model misspecification. In contrast, Exponentially Tilted Empirical Likelihood (ETEL) remains well behaved under misspecification and retains root-<span class="math inline">\(n\)</span> consistency and asymptotic normality <span class="citation">(<a href="#ref-schennach2007etel">Schennach 2007</a>)</span>.</p>
<p>Thus, the posterior distribution is</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}\mid \mathbf{W}_{1:N})
\;\propto\;
\pi(\boldsymbol{\theta})\; L_{\mathrm{ETEL}}(\boldsymbol{\theta}),
\qquad
L_{\mathrm{ETEL}}(\boldsymbol{\theta})=\prod_{i=1}^N p_i^{*}(\boldsymbol{\theta}),
\]</span></p>
<p>where <span class="math inline">\(\pi(\boldsymbol{\theta})\)</span> is the prior and <span class="math inline">\(L_{\mathrm{ETEL}}\)</span> is the exponentially tilted empirical likelihood. The weights
<span class="math inline">\(\big(p_1^{*}(\boldsymbol{\theta}),\dots,p_N^{*}(\boldsymbol{\theta})\big)\)</span> are obtained from the maximum-entropy problem</p>
<p><span class="math display">\[
\max_{\{p_i\}_{i=1}^N}\;\Big\{-\sum_{i=1}^N p_i\log p_i\Big\}
\quad\text{subject to}\quad
\sum_{i=1}^N p_i=1,\;\; p_i\ge 0,\;\;
\sum_{i=1}^N p_i\,\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})=\mathbf{0}_d.
\]</span></p>
<p>Equivalently (dual/saddlepoint form; see <span class="citation">Schennach (<a href="#ref-schennach2005betel">2005</a>)</span>;<span class="citation">Schennach (<a href="#ref-schennach2007etel">2007</a>)</span>;<span class="citation">Chib, Shin, and Simoni (<a href="#ref-chib2018moment">2018</a>)</span>),</p>
<p><span class="math display">\[
p_i^{*}(\boldsymbol{\theta})
=\frac{\exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)}
{\sum_{j=1}^N \exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_j,\boldsymbol{\theta})\big)},
\quad\text{where}\quad
\sum_{i=1}^N p_i^{*}(\boldsymbol{\theta})\,\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})=\mathbf{0}_d.
\]</span></p>
<p>Equivalently, <span class="math inline">\(\boldsymbol{\lambda}(\boldsymbol{\theta})\)</span> can be characterized as</p>
<p><span class="math display">\[
\boldsymbol{\lambda}(\boldsymbol{\theta})
=\arg\min_{\boldsymbol{\lambda}\in\mathbb{R}^{d}}
\;\log\!\left(\frac{1}{N}\sum_{i=1}^N
\exp\!\big(\boldsymbol{\lambda}^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)\right),
\]</span></p>
<p>whose gradient condition is precisely the moment constraint above. Therefore, the BETEL posterior is</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}\mid \mathbf{w}_{1:N})
\;\propto\;
\pi(\boldsymbol{\theta})\;
\prod_{i=1}^N
\frac{\exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_i,\boldsymbol{\theta})\big)}
{\sum_{j=1}^N \exp\!\big(\boldsymbol{\lambda}(\boldsymbol{\theta})^{\top}\mathbf{g}(\mathbf{W}_j,\boldsymbol{\theta})\big)}.
\]</span></p>
<p>Posterior inference of the BETEL can be performed using a Metropolis-Hastings algorithm where the proposal distribution is <span class="math inline">\(q(\boldsymbol{\theta}\mid \mathbf{W}_{1:N})\)</span>. See the following algorithm <span class="citation">(<a href="#ref-chib2018moment">Chib, Shin, and Simoni 2018</a>)</span>.</p>
<figcaption><b>Algorithm: Bayesian Exponentially Tilted Empirical Likelihood (BETEL) – Metropolis-Hastings</b></figcaption>
<figure id="alg:BETEL">
<pre>
Set θ<sup>(0)</sup> in the support of π(θ|W<sub>1:N</sub>) 
For s=1,2,...,S do
  Propose θ<sup>c</sup> from q(θ|W<sub>1:N</sub>)
  Solve for p<sub>i</sub><sup>*</sup>(θ<sup>c</sup>), i=1,...,N using the equation that characterized λ
  Calculate α(θ<sup>(s-1)</sup>,θ<sup>c</sup>|W<sub>1:N</sub>) =
      min(1, [π(θ<sup>c</sup>|W<sub>1:N</sub>)/π(θ<sup>(s-1)</sup>|W<sub>1:N</sub>)] × 
                [q(θ<sup>(s-1)</sup>|W<sub>1:N</sub>)/q(θ<sup>c</sup>|W<sub>1:N</sub>)])
  Draw U ~ U(0,1)
  θ<sup>(s)</sup> = θ<sup>c</sup> if U ≤ α(θ<sup>(s-1)</sup>,θ<sup>c</sup>|W<sub>1:N</sub>)
  θ<sup>(s)</sup> = θ<sup>(s-1)</sup> otherwise
End for
</pre>
</figure>
<p><strong>Example: Classical measurement error in regressor</strong></p>
<p>Let’s set the unobserved (latent) regressor <span class="math inline">\(X_i^*\)</span> such that</p>
<p><span class="math display">\[
X_i = X_i^* + \nu_i,
\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> is the observed regressor, and <span class="math inline">\(\nu_i\)</span> is a <em>classical measurement error</em> such that
<span class="math inline">\(\mathbb{E}[\nu_i]=0\)</span> and <span class="math inline">\(\nu_i \perp \{X_i^*,\mu_i\}\)</span>, where <span class="math inline">\(\mu_i\)</span> is the stochastic error of the structural model</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1X_i^*+\mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[\mu_i]=0\)</span> and <span class="math inline">\(\mu_i\perp X_i^*\)</span>.</p>
<p>If we perform the regression using the observed regressor, then</p>
<p><span class="math display">\[
Y_i=\beta_0+\beta_1(X_i-\nu_i)+\mu_i
=\beta_0+\beta_1X_i+\underbrace{\mu_i-\beta_1\nu_i}_{\epsilon_i},
\]</span></p>
<p>where the new error term is <span class="math inline">\(\epsilon_i=\mu_i-\beta_1\nu_i\)</span>. We will show that</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i\mid X_i]\neq 0.
\]</span></p>
<p>By the law of iterated expectations,</p>
<p><span class="math display">\[
\mathbb{E}[\nu_i X_i] = \mathbb{E}\!\left[X_i \, \mathbb{E}[\nu_i \mid X_i]\right].
\]</span></p>
<p>This implies:</p>
<ul>
<li><p>If <span class="math inline">\(\mathbb{E}[\nu_i \mid X_i] = 0\)</span> almost surely, then <span class="math inline">\(\mathbb{E}[\nu_i X_i] = 0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\mathbb{E}[\nu_i X_i] \neq 0\)</span>, then <span class="math inline">\(\mathbb{E}[\nu_i \mid X_i]\)</span> cannot be equal to zero almost surely.</p></li>
</ul>
<p>Now compute</p>
<p><span class="math display">\[
\mathbb{E}[\nu_iX_i]
= \mathbb{E}[\nu_i(X_i^*+\nu_i)]
= \underbrace{\mathbb{E}[\nu_i X_i^*]}_{=0} + \mathbb{E}[\nu_i^2]
= \sigma^2_{\nu} \neq 0.
\]</span></p>
<p>Hence it must be that <span class="math inline">\(\mathbb{E}[\nu_i\mid X_i]\neq 0\)</span>. Therefore,</p>
<p><span class="math display">\[
\mathbb{E}[\epsilon_i\mid X_i]
=\underbrace{\mathbb{E}[\mu_i\mid X_i]}_{=0}
- \beta_1\underbrace{\mathbb{E}[\nu_i\mid X_i]}_{\neq 0}
\neq 0,
\]</span></p>
<p>that is, the regressor is not exogenous, and consequently,</p>
<p><span class="math display">\[
\mathbb{E}[X_i\underbrace{(Y_i-\beta_0-\beta_1X_i)}_{\epsilon_i}]\neq 0,
\]</span></p>
<p>thus, we need a relevant (strong) (<span class="math inline">\(\mathbb{E}[Z_iX_i^*]\neq 0\)</span>) and exogeneous (<span class="math inline">\(\mathbb{E}[Z_i\mid \mu_i]=\mathbb{E}[Z_i\mid \nu_i]= 0\)</span>) instrument to identify the causal effect. Therefore,</p>
<p><span class="math display">\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    Z_i
\end{bmatrix}(Y_i-\beta_0-\beta_1X_i)\right]=\mathbf{0}.
\]</span></p>
<p>The following DAG illustrates the situation of measurement error, and how the instrument helps to identify the causal effect. The instrument solves the endogeneity problem because it exploits variation in the regressor <span class="math inline">\(X_i\)</span> that is correlated with the true latent regressor <span class="math inline">\(X_i^*\)</span> but uncorrelated with the measurement error <span class="math inline">\(\nu_i\)</span>. See Chapter 9 in <span class="citation">Hernán and Robins (<a href="#ref-hernan2020causal">2020</a>)</span> for a nice review of the effects of measurement error in causal inference.`</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="figures/FigChap13_13.png" alt="DAG with measurement error and instrument $Z$. Observed $X$ depends on the latent $X^{*}$ and error $v$. Because the composed error in the estimated equation includes $v$, $X$ is endogenous when regressed on $Y$. Instrument $Z$ isolates exogenous variation in $X^{*}$." width="500px" />
<p class="caption">
Figure 13.13: DAG with measurement error and instrument <span class="math inline">\(Z\)</span>. Observed <span class="math inline">\(X\)</span> depends on the latent <span class="math inline">\(X^{*}\)</span> and error <span class="math inline">\(v\)</span>. Because the composed error in the estimated equation includes <span class="math inline">\(v\)</span>, <span class="math inline">\(X\)</span> is endogenous when regressed on <span class="math inline">\(Y\)</span>. Instrument <span class="math inline">\(Z\)</span> isolates exogenous variation in <span class="math inline">\(X^{*}\)</span>.
</p>
</div>
<p>We simulate the latent process <span class="math inline">\(X_i^* = 0.5Z_i + e_i\)</span>, with the observed regressor defined as <span class="math inline">\(X_i = X_i^* + \nu_i\)</span>, and the outcome equation specified as <span class="math inline">\(Y_i = 1 + 1.2X_i^* + \mu_i\)</span>. The variables <span class="math inline">\(Z_i\)</span>, <span class="math inline">\(e_i\)</span>, and <span class="math inline">\(\nu_i\)</span> are standard normal, while <span class="math inline">\(\mu_i\)</span> follows a mixture of two normal distributions with means <span class="math inline">\(0.5\)</span> and <span class="math inline">\(-0.5\)</span>, and standard deviations <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1.2\)</span>. The sample size is 2,000, the burn-in is 1,000, and the number of MCMC draws retained after burn-in is 10,000.</p>
<p>We perform Bayesian exponentially tilted empirical likelihood (BETEL) using the package <em>betel</em>, and adopt the default hyperparameter values provided there.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> This package implements Bayesian estimation and marginal likelihood computation for moment condition models, employing a Student-t prior distribution and a Student-t proposal distribution following <span class="citation">Chib, Shin, and Simoni (<a href="#ref-chib2018moment">2018</a>)</span>. We also compare the results with a model that assumes <span class="math inline">\(X_i\)</span> is exogenous, and with an instrumental Gibbs sampler that assumes <span class="math inline">\(Y_i\)</span> is normally distributed. We also use the default hyperparameters of the packages employed. The following code shows the implementation.</p>
<p>The figure displays the posterior distributions. We see that ignoring measurement error produces a biased posterior distribution. In particular, the absolute value of the causal effect is smaller than the population value; this is called <em>attenuation bias</em>. By contrast, methods that account for measurement error and use valid instruments yield well-centered posterior distributions.</p>
<p>``<code>{r</code>, eval = FALSE}
rm(list = ls()); set.seed(10101)
library(betel); library(ucminf)
# Simulate data
N &lt;- 2000; d &lt;- 2; k &lt;- 2
gamma &lt;- 0.5; beta &lt;- c(1, 1.2)
# Mixture
mum1 &lt;- 1/2; mum2 &lt;- -1/2
mu1 &lt;- rnorm(N, mum1, 0.5); mu2 &lt;- rnorm(N, mum2, 1.2)
mu &lt;- sapply(1:N, function(i){sample(c(mu1[i], mu2[i]), 1, prob = c(0.5, 0.5))})
e &lt;- rnorm(N)
z &lt;- rnorm(N) # Instrument
xlat &lt;- gamma<em>z + e # Unobserved regressor
nu &lt;- rnorm(N) # Measurement error
x &lt;- xlat + nu # Observed regressor
Xlat &lt;- cbind(1, xlat)
y &lt;- Xlat%</em>%beta + mu
dat &lt;- cbind(1, x, z) # Data
# Function g_i by row in BETEL
gfunc &lt;- function(psi = psi, y = y, dat = dat) {
X &lt;- dat[,1:2]
e &lt;- y - X %<em>% psi
E &lt;- e %</em>% rep(1,d)
Z &lt;- dat[,c(1,3)]
G &lt;- E * Z;
return(G)
}
nt &lt;- round(N * 0.1, 0); # training sample size for prior
psi0 &lt;- lm(y[1:nt]~x[1:nt])$coefficients # Starting value of psi = (theta, v), v is the slack parameter in CSS (2018)
names(psi0) &lt;- c(“alpha”,“beta”)
psi0_ &lt;- as.matrix(psi0) # Prior mean of psi
Psi0_ &lt;- 5<em>rep(1,k) # Prior dispersions of psi
lam0 &lt;- .5</em>rnorm(d) # Starting value of lambda
nu &lt;- 2.5 # df of the prior student-t
nuprop &lt;- 15 # df of the student-t proposal
n0 &lt;- 1000 # burn-in
m &lt;- 10000 # iterations beyond burn-in
# MCMC ESTIMATION BY THE CSS (2018) method
psim &lt;- betel::bayesetel(gfunc = gfunc, y = y[-(1:nt)], dat = dat[-(1:nt),], psi0 = psi0, lam0 = lam0, psi0_ = psi0_, Psi0_ = Psi0_, nu = nu, nuprop = nuprop, controlpsi = list(maxiterpsi = 50, mingrpsi = 1.0e-8), # list of parameters in maximizing likelihood over psi
controllam = list(maxiterlam = 50, # list of parameters in minimizing dual over lambda
mingrlam = 1.0e-7), n0 = n0, m = m,printstep = 5000)
MCMCexg &lt;- MCMCpack::MCMCregress(y ~ x, burnin = n0, mcmc = m)
Data &lt;- list(y = c(y), x = x, z = matrix(z, N, 1), w = matrix(rep(1, N), N, 1))
Mcmc &lt;- list(R = m, nprint = 0)
MCMCivr &lt;- bayesm::rivGibbs(Data, Mcmc = Mcmc)
dfplot &lt;- data.frame(betel = psim[,2], iv = MCMCivr[[“betadraw”]], exo = MCMCexg[,2])
colnames(dfplot) &lt;- c(“betel”, “iv”, “exo”)
library(tidyr)
library(dplyr)
library(ggplot2)</p>
<p>df_long &lt;- dfplot |&gt;
pivot_longer(everything(), names_to = “Method”, values_to = “Posterior”) |&gt;
mutate(Method = factor(Method,
levels = c(“betel”,“iv”,“exo”),
labels = c(“betel”,“rivGibbs”,“MCMCregress”)))</p>
<p>ggplot(df_long, aes(x = Posterior, color = Method, fill = Method)) +
geom_density(alpha = 0.3, linewidth = 1) +
geom_vline(xintercept = 1.2, linetype = “dashed”, linewidth = 1, color = “black”) +
labs(
title = “Posterior Densities with Population Value”,
x = expression(beta[1]),
y = “Density”
) +
theme_minimal(base_size = 14) +
theme(
legend.position = “top”,
legend.title = element_blank()
)</p>
<pre><code>
**Example: Omission of relevant correlated regressor**

Consider the true model

\[
Y_i = \beta_0 + \beta_1 X_i + \gamma W_i + \mu_i,
\qquad \mathbb{E}[\mu_i \mid X_i, W_i] = 0.
\]

Suppose that the relevant regressor $W_i$ is omitted from the specification, with $\gamma \neq 0$ and $\operatorname{Cov}(X_i, W_i) \neq 0$.

Thus, if we regress $Y_i$ only on $X_i$, the composite error is

\[
\epsilon_i = \gamma W_i + \mu_i.
\]

Therefore,

\[
\mathbb{E}[\epsilon_i \mid X_i] 
= \gamma \, \mathbb{E}[W_i \mid X_i] + \mathbb{E}[\mu_i \mid X_i]
= \gamma \, \mathbb{E}[W_i \mid X_i] \neq 0,
\]

since $\mathbb{E}[W_i \mid X_i]$ is nonconstant whenever $X_i$ and $W_i$ are correlated, that is, the expected value of $W_i$ is a function of $X_i$.

Consequently,

\[
\mathbb{E}[\epsilon_i X_i] 
= \gamma \, \mathbb{E}[W_i X_i] + \mathbb{E}[\mu_i X_i].
\]

Because $\mathbb{E}[\mu_i X_i]=0$, we obtain

\[
\mathbb{E}[\epsilon_i X_i] = \gamma (\operatorname{Cov}(W_i,X_i) 
+ \mathbb{E}[W_i] \, \mathbb{E}[X_i])\neq 0.
\]

In this situation, we can use an instrument that is relevant ($\mathbb{E}[Z_i X_i] \neq 0$) and exogenous ($\mathbb{E}[Z_i \mid \mu_i] = \mathbb{E}[Z_i \mid W_i] = 0$) to address the endogeneity problem, and identify the causal effect.^[Another potential solution for the omission of relevant variables is the use of *proxy variables*; see @wooldridge2010econometric for details.] Therefore,

\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    Z_i
\end{bmatrix}(Y_i-\beta_0-\beta_1X_i)\right]=\mathbf{0}.
\] 

The figure illustrates the case of an omitted relevant regressor that is correlated with $X_i$, and how an instrument can be used to identify the causal effect. The instrument resolves the endogeneity problem by exploiting variation in $X_i$ that is driven by $Z_i$, which is orthogonal to the problematic error term. In other words, it replaces &quot;bad&quot; correlation with &quot;clean&quot; variation.

&lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;figures/FigChap13_14.png&quot; alt=&quot;DAG with omitted relevant regressor $W$ (dashed, unobserved) that is correlated with $X$ and affects $Y$, inducing endogeneity of $X$ in a regression of $Y$ on $X$. A valid instrument $Z$ affects $X$ (relevance) but has no direct path to $Y$ and is independent of $W$ and $u$ (exclusion/independence), enabling identification of the causal effect $X ightarrow Y$.&quot; width=&quot;400px&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:unnamed-chunk-22)DAG with omitted relevant regressor $W$ (dashed, unobserved) that is correlated with $X$ and affects $Y$, inducing endogeneity of $X$ in a regression of $Y$ on $X$. A valid instrument $Z$ affects $X$ (relevance) but has no direct path to $Y$ and is independent of $W$ and $u$ (exclusion/independence), enabling identification of the causal effect $X ightarrow Y$.&lt;/p&gt;
&lt;/div&gt;

We ask in Exercise 10 to perform a simulation exercise to assess the ability of BETEL to identify the causal effect when an instrument is used to address the omission of relevant regressors.  

**Example: Simultaneous causality**

This example is based on @imbens2014ivperspective, where Professor Imbens illustrates the problem of analyzing the causal effect on traded quantity of a tax of $100 \times r\%$ in a market. He defines the average causal effect on the logarithm of traded quantity as

\[
\tau = \mathbb{E}[q_i(r) - q_i(0)],
\]

where $q_i(r) = \log Q_i(r)$ and $Q_i(r)$ denotes the potential traded quantity if the tax were $r$\%.

This situation is more challenging than in the standard treatment effects literature, because we do not observe any unit facing the tax. Instead, we only observe all units facing no tax, that is, $Q_i^{\text{Obs}} = Q_i(0)$. This setting requires the use of a *structural model* to define the counterfactual scenarios of the potential outcomes.

The starting point for inference on the treatment effect of this new tax is the price determination mechanism, that is, the assignment mechanism in the potential outcome framework. We specify a *structural demand function* that defines the potential demand given the price (treatment) and other exogenous variables:

\[
q_i^d(p) = \beta_1 + \beta_2 p + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1},
\]

where $q^d$ is demand, and $p$, $y$, $pc$, and $ps$ are the logarithms of price, income, the price of a complementary good, and the price of a substitute good, respectively. All coefficients are interpreted as demand elasticities. The term $u_{i1}$ represents unobserved demand factors such that $\mathbb{E}[u_{i1}]=0$. Therefore,

\[
\mathbb{E}[q_i^d(p)\mid p, y_i, pc_i, ps_i] = \beta_1 + \beta_2 p + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i.
\]

This expectation does not represent the conditional expectation of the observed quantity in markets where the observed price equals $p$. Rather, it is the expectation of potential demand functions given $p$ and other exogenous controls, irrespective of the realized market price.

Similarly, the assignment mechanism requires specifying the *structural supply function*:

\[
q_i^s(p) = \alpha_1 + \alpha_2 p + \alpha_3 er_i + u_{i2},
\]

which represents the quantity that sellers are willing to supply given the price and the (exogenous) exchange rate $er_i$. The unobserved supply factors satisfy $\mathbb{E}[u_{i2}]=0$, so that

\[
\mathbb{E}[q_i^s(p)\mid p, er_i] = \alpha_1 + \alpha_2 p + \alpha_3 er_i.
\]

This expectation represents the average of all potential supply functions given the price and exogenous controls, again irrespective of the realized market price.

Thus, the assignment mechanism is given by the market equilibrium where the observed price is such that the observed quantity is equal to the demand and supply potential outcomes at the observed price,

\[
q_i^{Obs}=q_i^d(p_i^{Obs})=q_i^s(p_i^{Obs}).
\] 

Using this market equilibrium condition, we get

\[
p_i^{Obs}=\pi_1+\pi_2 er_i + \pi_3 y_i + \pi_4 pc_i + \pi_5 ps_i + v_{i1},
\]

where $\pi_1=\frac{\alpha_1-\beta_1}{\beta_2-\alpha_2}$, $\pi_2=\frac{\alpha_3}{\beta_2-\alpha_2}$, $\pi_3=\frac{-\beta_3}{\beta_2-\alpha_2}$, $\pi_4=\frac{-\beta_4}{\beta_2-\alpha_2}$, $\pi_5=\frac{-\beta_5}{\beta_2-\alpha_2}$, and $v_{i1}=\frac{u_{i2}-u_{i1}}{\beta_2-\alpha_2}$ given $\beta_2\neq\alpha_2$, that is, the equations should be independent. This condition is given by economic theory due to $\beta_2&lt;0$ and $\alpha_2&gt;0$, the effect of price on demand and supply should be negative and positive, respectively.

The equation of price into the demand equation gives

\[
q_i^{Obs}=\tau_1+\tau_2 er_i + \tau_3 y_i + \tau_4 pc_i + \tau_5 ps_i + v_{i2},
\]

where $\tau_1=\beta_1+\beta_2\pi_1$, $\tau_2=\beta_2\pi_2$, $\tau_3=\beta_2\pi_3+\beta_3$, $\tau_4=\beta_2\pi_4+\beta_4$, $\tau_5=\beta_2\pi_5+\beta_5$, and $v_{i2}=\beta_2v_{i1}+u_{i1}$.

The expressions for $p_i^{Obs}$ and $q_i^{Obs}$ are called the *reduced-form* representations. In Section \@ref(sec71), we presented the order condition, which is necessary, and the rank condition, which is both necessary and sufficient, to identify the structural parameters from the reduced-form parameters. A key point to note is that only the prior distribution of the reduced-form parameters is updated by the sample information, while the updating of the structural parameters occurs solely through the reduced-form parameters, that is,

\[
\pi(\boldsymbol{\beta},\boldsymbol{\alpha}\mid \boldsymbol{\gamma},\boldsymbol{\pi},\mathbf{W}_{1:N})
\;\propto\;
\pi(\boldsymbol{\beta},\boldsymbol{\alpha}\mid \boldsymbol{\gamma},\boldsymbol{\pi}).
\]

See Section 9.3 in @zellner1996introduction for details about identification in Bayesian inference. This implies that in *under-identified* models, the posterior distribution of the structural parameters is not concentrated at a single point, but rather remains spread out (e.g., uniformly) over a range of values.^[This identification issue is not peculiar to simultaneous equation models, it arises in other econometric/statistical models [@zellner1996introduction]. See for instance the example of the effects of vitamin A.]

To analyze the causal effects of the new tax, we can find the new equilibrium,

\[
q_i^d(P_i(r)\times (1+r))=q_i^s(P_i(r)),
\] 

where $P_i(r)$ is the price level ($p_i(r)=\log P_i(r)$) that sellers get, and $(P_i(r)\times (1+r)$ is the price that buyers pay.

Let&#39;s define the (log) price that buyers pay $p_b = p + \log(1+r)$ while sellers receive $p$. The demand function becomes

\[
q_i^d\big(p + \log(1+r)\big) = \beta_1 + \beta_2\left(p + \log(1+r)\right) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1},
\]

and the supply function remains

\[
q_i^s(p) = \alpha_1 + \alpha_2 p + \alpha_3 er_i + u_{i2}.
\]

Thus, the equilibrium price is given by

\[
p_i^*(r) = \frac{(\beta_1 - \alpha_1) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i - \alpha_3 er_i + (u_{i1} - u_{i2}) + \beta_2 \log(1+r)}{\alpha_2 - \beta_2}.
\]

The equilibrium quantity is

\[
q_i^*(r) = \beta_1 + \beta_2\left(p_i^*(r) + \log(1+r)\right) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1}.
\]

Thus, the expected equilibrium price and quantity are

\[
\mathbb{E}[p_i^*(r)\mid y_i, pc_i, ps_i, er_i]
=
\frac{(\beta_1 - \alpha_1) + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i - \alpha_3 er_i + \beta_2 \log(1+r)}{\alpha_2 - \beta_2},
\]

\[
\mathbb{E}[q_i^*(r)\mid y_i, pc_i, ps_i, er_i]
=
\alpha_1 + \alpha_2 \,\mathbb{E}[p_i^*(r)\mid \cdot] + \alpha_3 er_i.
\]

We can see that given $\beta_2 &lt; 0 &lt; \alpha_2$, we have:

\[
\frac{d p^*(r)}{dr} = \frac{\beta_2}{(\alpha_2 - \beta_2)(1+r)} &lt; 0, \quad
\frac{d p_b^*(r)}{dr} = \frac{\alpha_2}{(\alpha_2 - \beta_2)(1+r)} &gt; 0, \quad
\frac{d q^*(r)}{dr} = \alpha_2 \cdot \frac{d p^*(r)}{dr} &lt; 0.
\]

Thus, the price received by sellers decreases with the tax, the price paid by buyers increases, and the equilibrium quantity falls.
  
Thus, the average causal effect on the logarithm of traded quantity is

\[
\tau = \mathbb{E}[q_i(r) - q_i(0)]= \frac{\alpha_2 \, \beta_2}{\alpha_2 - \beta_2}\,\log(1+r).
\]

Note that the treatment effect depends on the price elasticities of supply and demand. Therefore, we need to identify the demand and supply functions using instruments, since the observed quantities and prices cannot be used directly due to the issue of simultaneous causality. In particular, given the assumption of exogeneity of the other control variables, the only part of $p_i^{Obs}$ that can correlate with $u_{i1}$ or $u_{i2}$ is the error component

\[
\frac{u_{i2}-u_{i1}}{\beta_2-\alpha_2}.
\]

Hence

\[
\mathbb{E}[u_{i1}p_i^{Obs}]
=\frac{1}{\beta_2-\alpha_2}\,\mathbb{E}\!\big[u_{i1}(u_{i2}-u_{i1})\big]
=\frac{\operatorname{Cov}(u_{i1},u_{i2})-\operatorname{Var}(u_{i1})}{\beta_2-\alpha_2}\neq 0,
\]

and

\[
\mathbb{E}[u_{i2}p_i^{Obs}]
=\frac{1}{\beta_2-\alpha_2}\,\mathbb{E}\!\big[u_{i2}(u_{i2}-u_{i1})\big]
=\frac{\operatorname{Var}(u_{i2})-\operatorname{Cov}(u_{i1},u_{i2})}{\beta_2-\alpha_2}\neq 0,
\]

due to the usual slopes $\beta_2&lt;0&lt;\alpha_2$ so that $\alpha_2-\beta_2&gt;0$.

We can use supply shifters to identify the demand, and demand shifters to identify the supply. In this setting, the exchange rate serves as an instrument to identify the demand, while income together with the prices of complementary and substitute goods serve as instruments to identify the supply. Thus, we can use the moment conditions,

\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    y_i\\
    pc_i\\
    ps_i\\
    er_i
\end{bmatrix}(q_i^{Obs}-\beta_1-\beta_2p_i^{Obs}-\beta_3y_i-\beta_4pc_i-\beta_5ps_i)\right]=\mathbf{0},
\] 

and 

\[
\mathbb{E}\left[\begin{bmatrix}
    1\\
    y_i\\
    pc_i\\
    ps_i\\
    er_i
\end{bmatrix}(q_i^{Obs}-\alpha_1-\alpha_2p_i^{Obs}-\alpha_3er_i)\right]=\mathbf{0}
\]

to identify the demand and supply functions. Note that the demand equation is *exactly identified*, whereas the supply equation is *over-identified*.

We perform a simulation exercise to analyze the hypothetical causal effects of a new tax rate of 10% simulating the demand and supply equations using the following structural parameters $\boldsymbol{\beta} = \left[ 5 \ -0.5 \ 0.8 \ -0.4 \ 0.7 \right]^{\top}$, $\boldsymbol{\alpha} = \left[ -2 \ 0.5 \ -0.4 \right]^{\top}$, $u_1 \sim N(0, 0.5^2)$, and $u_2 \sim N(0, 0.5^2)$ assuming a sample size equal to 5,000. Additionally, assume that $y \sim N(10, 1)$, $pc \sim N(5, 1)$, $ps \sim N(5, 1)$, and $er \sim N(15, 1)$. 

The population causal effect of the tax on quantity is

\[
\tau=\frac{(-0.5)\times 0.5}{0.5-(-0.5)}\log(1+0.1)\approx -0.0238,
\]

which implies that a 10% tax reduces traded quantity by approximately 2.4%.

In Exercise 11, you are asked to program a BETEL algorithm from scratch to perform inference in this example. Th following figure displays the posterior distribution of the causal effect. The 95% credible interval contains the population value, and the posterior mean lies close to it.


&lt;div class=&quot;figure&quot; style=&quot;text-align: center&quot;&gt;
&lt;img src=&quot;figures/FigChap13_15.png&quot; alt=&quot;Posterior distribution of the causal effect of a tax: BETEL&quot; width=&quot;600px&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:unnamed-chunk-23)Posterior distribution of the causal effect of a tax: BETEL&lt;/p&gt;
&lt;/div&gt;

@chib2018moment propose a unified framework based on marginal likelihoods and Bayes factors for comparing different moment-restricted models and for discarding misspecified restrictions. They demonstrate the model selection consistency of the marginal likelihood, showing that it favors the specification with the fewest parameters and the largest number of valid moment restrictions. When models are misspecified, the marginal likelihood procedure selects the model that is closest to the (unknown) true data-generating process in terms of Kullback–Leibler divergence. See @lazar2021review and @liu2023review for comprehensive reviews of recent advances in empirical likelihood and exponentially tilted empirical likelihood methods, including their Bayesian variants.

## General Bayes posteriors {#sec13_9}

It is well known that, under model misspecification, that is, when the assumed likelihood function does not coincide with the true data-generating likelihood, the posterior concentrates its mass near those points in the support of the prior that minimize the Kullback--Leibler divergence with respect to the true model [@Kleijn2006]. However, this updating process may yield credible sets that fail to achieve the desired coverage, Bayes factors that can be misleading, and predictions that may remain approximately valid but require careful checking. In addition, the misspecified posterior distribution may exhibit suboptimal risk performance. In this setting, a modified posterior directly linked to the risk function of interest, known as the *Gibbs posterior*, can still perform very well [@Jiang2008],

\begin{equation}
    \hat{\pi}(\boldsymbol{\theta}\mid\mathbf{y})
    =\frac{\exp\left\{-Nw\,R_N(\boldsymbol{\theta},\mathbf{y})\right\}\pi(\boldsymbol{\theta})}
    {\int_{\boldsymbol{\Theta}}\exp\left\{-Nw\,R_N(\boldsymbol{\theta},\mathbf{y})\right\}\pi(\boldsymbol{\theta})\,d\boldsymbol{\theta}},
    (\#eq:GibbsPost)
\end{equation}

where $w&gt;0$ is the *learning rate* (or the inverse temperature in simulated annealing), which balances the information in the data with that in the prior, $N$ is the sample size, and

\[
R_N(\boldsymbol{\theta},\mathbf{y})=\frac{1}{N}\sum_{i=1}^N l(\boldsymbol{\theta},\mathbf{y}_i)
\]

is the empirical risk associated with the loss function $l(\boldsymbol{\theta},\mathbf{y}_i)$.

Note that Equation \@ref(eq:GibbsPost) reduces to the ordinary Bayesian posterior when the loss is chosen as the negative log-likelihood, $l(\boldsymbol{\theta},y_i)=-\log p(y_i\mid\boldsymbol{\theta})$, under i.i.d. sampling and with $w=1$. In this case, we are asserting knowledge of the data-generating process $p(y_i\mid \boldsymbol{\theta})$. More generally, however, $R_N(\boldsymbol{\theta},\mathbf{y})$ can be based on any loss function that satisfies the required conditions for coherent belief updating: non-negativity, existence of a well-defined expectation, identifiability, and additivity across observations. Importantly, correctness of the parametric model is not required, since

\[
\frac{1}{N}\sum_{i=1}^N l(\boldsymbol{\theta},\mathbf{y}_i)\;\stackrel{p}{\longrightarrow}\;\int_{\mathcal{Y}} l(\boldsymbol{\theta},\mathbf{y})\,p(\mathbf{y}\mid \boldsymbol{\theta})\,d\mathbf{y},
\]

by the law of large numbers as $N\to\infty$.

Thus, the Gibbs posterior provides inference on the parameter values that minimize the chosen risk function, with minimal modeling assumptions. In contrast, the standard Bayesian posterior allows inference on essentially any feature of the data-generating distribution, but at the cost of strong model assumptions [@Syring2019].

@bissiri2016general show that Equation \@ref(eq:GibbsPost) is a valid, coherent mechanism to update prior beliefs. In particular, there must exist a mapping $h$ such that

\[
\pi(\boldsymbol{\theta}\mid \mathbf{y}) = h\!\big(l(\boldsymbol{\theta},\mathbf{y}),\pi(\boldsymbol{\theta})\big),
\]

where $h$ satisfies the coherence property

\[
h\!\left[l(\boldsymbol{\theta},y_2),\,h\!\big(l(\boldsymbol{\theta},y_1),\pi(\boldsymbol{\theta})\big)\right]
= h\!\big(l(\boldsymbol{\theta},y_2)+l(\boldsymbol{\theta},y_1),\pi(\boldsymbol{\theta})\big).
\]

This ensures that the updated posterior $\pi(\boldsymbol{\theta}\mid y_1,y_2)$ is the same whether we update with $(y_1,y_2)$ jointly or sequentially.

Equation \@ref(eq:GibbsPost) is the solution of minimizing the loss function $L(\nu;\pi,\mathbf{y})$ on the space of probability measures on $\theta$-space,

\[
\hat{\pi}=\arg\min_{\nu} L(\nu;\pi,\mathbf{y}),
\]

such that $\hat{\pi}$ is the representation of beliefs about $\boldsymbol{\theta}$ given prior beliefs ($\pi$) and data ($\mathbf{y}$). Given that the prior beliefs and the data are two independent pieces of information, it makes sense that the loss function is additive in these two arguments, 

\[
L(\nu;\pi,\mathbf{y})=h_1(\nu,\mathbf{y})+w^{-1}h_2(\nu,\pi),
\] 

where $h_1(\cdot)$ and $h_2(\cdot)$ are loss functions in their arguments.  
The coherence requirement implies that

\[
h_2(\nu,\pi)=d_{KL}(\nu,\pi)=\int \log\frac{\nu(d\boldsymbol{\theta})}{\pi(d\boldsymbol{\theta})} \nu(d\boldsymbol{\theta}),
\]

that is, $h_2(\nu,\pi)$ must be the Kullback-Leibler divergence [@bissiri2016general].

On the other hand, 

\[
h_1(\nu,\mathbf{y})=\int l(\boldsymbol{\theta},\mathbf{y})\nu(d\boldsymbol{\theta})
\]  

is the expected loss of the action with respect to the data.

Therefore, the minimizer of

\[
L(\nu;\pi,\mathbf{y})=\int l(\boldsymbol{\theta},\mathbf{y})\nu(d\boldsymbol{\theta})+d_{KL}(\nu,\pi),
\] 

is given by Equation \@ref(eq:GibbsPost) [@Zhang2006KLentropy; @Jiang2008; @bissiri2016general].

A very important parameter in the Gibbs posterior [@Jiang2008], or *general Bayes posterior* [@bissiri2016general], is the learning rate, as it determines the asymptotic sampling properties of $\hat{\pi}(\boldsymbol{\theta}\mid\mathbf{y})$ used to perform inference on $\boldsymbol{\theta}$. For instance, @bissiri2016general propose different strategies to set this parameter, and @Syring2019 propose a Monte Carlo algorithm that selects the learning rate so that the resulting credible region attains the nominal Frequentist coverage probability.

@chernozhukov2003mcmc introduce the *Laplace-type estimator* (LTE) or *quasi-posterior* distribution, which can be interpreted as a special case of the *general Bayes* update of @bissiri2016general, taking as loss a scaled sample criterion based on the moment conditions (e.g., the GMM quadratic form).

In particular, given the moment conditions
\[
\mathbb{E}\!\big[\mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})\big]=\mathbf{0}_{d} 
\quad \text{if and only if } \boldsymbol{\theta}=\boldsymbol{\theta}_0,
\]
where the expectation is taken with respect to the population distribution,  
$\mathbf{w}_{1:N}:=[\mathbf{w}_1 \ \mathbf{w}_2 \ \dots \ \mathbf{w}_N]$ is a random sample from $\mathbf{W}\subset \mathbb{R}^{d_w}$,  
$\mathbf{g}:\mathbb{R}^{d_w}\times\boldsymbol{\Theta}\to\mathbb{R}^{d}$ is a vector of known functions, and  
$\boldsymbol{\theta}=[\theta_{1}\ \theta_{2}\ \dots\ \theta_{p}]^{\top}\in\boldsymbol{\Theta}\subset\mathbb{R}^{p}$ with $d\geq p$, the risk function can be defined as
\[
R_N(\boldsymbol{\theta},\mathbf{w})=\tfrac{1}{2}\left(\underbrace{\frac{1}{N}\sum_{i=1}^N \mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})}_{\mathbf{g}_N(\boldsymbol{\theta})}\right)^{\top}\mathbf{W}_N\left(\underbrace{\frac{1}{N}\sum_{i=1}^N\mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})}_{\mathbf{g}_N(\boldsymbol{\theta})}\right)
\] (\#eq:RiskLTE)

where $\mathbf{W}_N$ is a positive semi-definite weighting matrix such that
\[
\mathbf{W}_N \;\to\; 
\Bigg(\text{Var}\left[\sqrt{N}\left(\tfrac{1}{N}\sum_{i=1}^N \mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta}_0)\right)\right]\Bigg)^{-1}
\quad \text{as } N\rightarrow \infty.
\]

Then, the *quasi-posterior* in @chernozhukov2003mcmc is similar to \@ref(eq:GibbsPost) with $w=1$.

The following algorithm shows the Metropolis–Hastings to perform inference using the general Bayes posterior.

```{=html}
&lt;figcaption&gt;&lt;b&gt;Algorithm: General Bayes posterior — Metropolis-Hastings&lt;/b&gt;&lt;/figcaption&gt;
&lt;figure id=&quot;alg:GibbsPosterior&quot;&gt;
&lt;pre&gt;
Set θ(0) in the support of π̂(θ|y)
For s=1,2,...,S do
  Draw θᶜ from q(θᶜ | θ(s-1))
  Calculate α(θ(s-1), θᶜ) = min( ( q(θ(s-1)|θᶜ) exp{-Nw R&lt;sub&gt;N&lt;/sub&gt;(θᶜ,y)} π(θᶜ) ) /
                               ( q(θᶜ|θ(s-1)) exp{-Nw R&lt;sub&gt;N&lt;/sub&gt;(θ(s-1),y)} π(θ(s-1)) ), 1 )
  Draw U from U(0,1)
  θ(s) = θᶜ if U ≤ α(θ(s-1), θᶜ)
  θ(s) = θ(s-1) otherwise
End for
&lt;/pre&gt;
&lt;/figure&gt;</code></pre>
<p>Under suitable regularity conditions, <span class="citation">Chernozhukov and Hong (<a href="#ref-chernozhukov2003mcmc">2003</a>)</span> show that the LTE posterior mean is first-order equivalent to the efficient GMM estimator and that posterior quantiles yield confidence sets with asymptotically correct Frequentist coverage. Relatedly, <span class="citation">Müller (<a href="#ref-Muller2013SandwichBayes">2013</a>)</span> demonstrates that, under model misspecification, replacing the original likelihood with a curvature-adjusted (“sandwich”) log-likelihood can improve Frequentist risk, that is, lower the average loss over repeated samples when making inference on the parameters. Moreover, <span class="citation">Chen, Christensen, and Tamer (<a href="#ref-ChenChristensenTamer2018">2018</a>)</span> develop confidence sets based on quasi-posteriors that achieve exact asymptotic Frequentist coverage for identified sets of parameters in complex nonlinear structural models, regardless of whether the parameters are point identified. Finally, <span class="citation">Andrews and Mikusheva (<a href="#ref-andrews2022weakgmm">2022</a>)</span> study decision rules for weak GMM and provide support for quasi-Bayesian procedures built from the GMM quadratic form, in line with the spirit of the LTE in settings with weak identification.</p>
<p><strong>Example: Instrumental variable quantile regression (IVQR)</strong></p>
<p><span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> propose an instrumental variable model of quantile treatment effects to characterize the heterogeneous impact of treatments across different points of the outcome distribution. Their model requires conditions that restrict the evolution of ranks across treatment states. Using these conditions, they address the endogeneity problem and recover quantile treatment effects using instrumental variables for the entire population, not only for compliers.</p>
<p>Assume a binary treatment variable <span class="math inline">\(D_i\in\{0,1\}\)</span>, with potential outcomes <span class="math inline">\(\{Y_i(0),Y_i(1)\}\)</span>. The estimand of interest is the quantile treatment effect, which summarizes the differences in the quantiles of potential outcomes across treatment states,</p>
<p><span class="math display">\[
q(D_i=1,\mathbf{X}_i=\mathbf{x}_i,\tau)-q(D=0,\mathbf{X}_i=\mathbf{x}_i,\tau),
\]</span></p>
<p>where <span class="math inline">\(q(D_i=d,\mathbf{X}_i=\mathbf{x}_i,\tau)\)</span> denotes the <span class="math inline">\(\tau\)</span>-th quantile treatment response function.</p>
<p>The potential outcomes are related to the quantile treatment response via</p>
<p><span class="math display">\[
Y_i(d)=q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,U(d_i)),
\]</span></p>
<p>where <span class="math inline">\(U(d_i)\sim U(0,1)\)</span> is the <em>rank variable</em>. This variable captures unobserved heterogeneity that explains differences in outcomes given observed characteristics <span class="math inline">\(\mathbf{x}_i\)</span> and treatment <span class="math inline">\(d_i\)</span>.</p>
<p>For instance, consider a retirement savings model, where the potential outcome is net financial assets under different retirement plan statuses <span class="math inline">\(d\)</span>, and <span class="math inline">\(q(D=d,\mathbf{X}=\mathbf{x},\tau)\)</span> is the net financial asset function describing how an individual with retirement status <span class="math inline">\(d\)</span> and “financial ability” <span class="math inline">\(\tau\)</span> is rewarded in the financial market. Because the function depends on <span class="math inline">\(\tau\)</span>, treatment effects are heterogeneous.</p>
<p>The identification conditions for the IVQR model are stated in <span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> as follows:</p>
<p><em>1. Potential outcomes:</em><br />
<span class="math display">\[
Y_i(d)=q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,U(d_i)),
\]</span><br />
where <span class="math inline">\(q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,\tau)\)</span> is strictly increasing in <span class="math inline">\(\tau\)</span> and <span class="math inline">\(U(d_i)\sim U(0,1)\)</span>.</p>
<p><em>2. Independence:</em> Conditional on <span class="math inline">\(\mathbf{X}_i=\mathbf{x}_i\)</span>, the rank variables <span class="math inline">\(\{U(d_i)\}\)</span> are independent of the instruments <span class="math inline">\(\mathbf{Z}_i\)</span>.</p>
<p><em>3. Selection:</em> The treatment assignment is given by <span class="math inline">\(D_i=\delta(\mathbf{Z}_i,\mathbf{X}_i,\mathbf{V}_i)\)</span> for some unknown function <span class="math inline">\(\delta\)</span> and unobserved heterogeneity <span class="math inline">\(\mathbf{V}_i\)</span>.</p>
<p><em>4. Rank invariance:</em> Conditional on <span class="math inline">\(\mathbf{X}_i=\mathbf{x}_i,\mathbf{Z}_i=\mathbf{z}_i\)</span>,</p>
<ul>
<li>either <span class="math inline">\(\{U(d_i)\}\)</span> coincide (<span class="math inline">\(U(d_i)=U\)</span> for all <span class="math inline">\(d\)</span>), or<br />
</li>
<li><span class="math inline">\(\{U(d_i)\}\)</span> are identically distributed conditional on <span class="math inline">\(\mathbf{V}_i\)</span>.</li>
</ul>
<p><em>5. Observables:</em> The observed data consist of <span class="math inline">\(Y_i=q(D_i,\mathbf{X}_i,U(D_i))\)</span>, <span class="math inline">\(D_i\)</span>, <span class="math inline">\(\mathbf{X}_i\)</span>, and <span class="math inline">\(\mathbf{Z}_i\)</span>, <span class="math inline">\(i=1,2,\dots,N\)</span>.</p>
<p>The most important identification restriction is <em>rank invariance</em>, which implies that individuals with higher unobserved rank remain higher-ranked regardless of treatment status. This condition accommodates more general selection mechanisms than the monotonicity assumption used in the LATE framework, while being less restrictive than full independence assumptions between instruments (<span class="math inline">\(\mathbf{Z}\)</span>) and unobserved variables in the selection equation (<span class="math inline">\(\mathbf{V}\)</span>) in other common models (see <span class="citation">Chernozhukov and Hansen (<a href="#ref-chernozhukov2004effects">2004</a>)</span>; <span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> for details).<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>The main testable implication of the identification restrictions is that</p>
<p><span class="math display">\[
P\!\left(Y_i \leq q(D_i,\mathbf{X}_i,\tau)\mid \mathbf{X}_i,\mathbf{Z}_i\right)
= P\!\left(Y_i &lt; q(D_i,\mathbf{X}_i,\tau)\mid \mathbf{X}_i,\mathbf{Z}_i\right)
= \tau,
\]</span></p>
<p>for all <span class="math inline">\(\tau\)</span> almost surely, and <span class="math inline">\(U(D_i)\sim U(0,1)\)</span> conditional on <span class="math inline">\((\mathbf{X}_i,\mathbf{Z}_i)\)</span>.</p>
<p>This conditional moment restriction implies the unconditional moment conditions that form the basis for estimation and inference in the IVQR model <span class="citation">(<a href="#ref-chernozhukov2003mcmc">Chernozhukov and Hong 2003</a>; <a href="#ref-chernozhukov2004effects">Chernozhukov and Hansen 2004</a>)</span>,</p>
<p><span class="math display">\[
\mathbf{g}_N(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^N (\tau-\mathbf{1}(Y_i\leq \alpha_{\tau}D_i+\mathbf{X}_i^{\top}\boldsymbol{\beta}_{\tau}))
\begin{bmatrix}
\mathbf{X}_i\\
\mathbf{Z}_i
\end{bmatrix}.
\]</span></p>
<p>Thus, the empirical risk function is given by</p>
<p><span class="math display">\[
R_N(\boldsymbol{\theta},\mathbf{w})=\tfrac{1}{2}\mathbf{g}_N(\boldsymbol{\theta})^{\top}\mathbf{W}_N\mathbf{g}_N(\boldsymbol{\theta}),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{W}_N=\frac{1}{\tau(1-\tau)}\left(\frac{1}{N}\sum_{i=1}^N \mathbf{Z}_i\mathbf{Z}_i^{\top}\right)^{-1}.
\]</span></p>
<p>The following code implements the previous algorithm to perform inference on the quantile treatment effect of participation in the 401(k) retirement program on net financial assets, using eligibility as an instrument (see the 401(k) treatment effects example in Section~@ref(sec13_4) for details of the data).</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="sec13_8.html#cb10-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb10-2"><a href="sec13_8.html#cb10-2" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb10-3"><a href="sec13_8.html#cb10-3" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv&quot;</span>,</span>
<span id="cb10-4"><a href="sec13_8.html#cb10-4" tabindex="-1"></a><span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb10-5"><a href="sec13_8.html#cb10-5" tabindex="-1"></a><span class="co"># Attach variables</span></span>
<span id="cb10-6"><a href="sec13_8.html#cb10-6" tabindex="-1"></a><span class="fu">attach</span>(df)</span>
<span id="cb10-7"><a href="sec13_8.html#cb10-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> net_tfa<span class="sc">/</span><span class="dv">1000</span>    <span class="co"># Outcome: net financial assets</span></span>
<span id="cb10-8"><a href="sec13_8.html#cb10-8" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(p401) <span class="co"># Endogenous regressor: participation</span></span>
<span id="cb10-9"><a href="sec13_8.html#cb10-9" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(age, inc, fsize, educ, marr, twoearn, db, pira, hown))  <span class="co"># Exogenous regressors</span></span>
<span id="cb10-10"><a href="sec13_8.html#cb10-10" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(e401)  <span class="co"># Instrument: eligibility (NO intercept here)</span></span>
<span id="cb10-11"><a href="sec13_8.html#cb10-11" tabindex="-1"></a><span class="fu">library</span>(quantreg)</span>
<span id="cb10-12"><a href="sec13_8.html#cb10-12" tabindex="-1"></a>tau <span class="ot">&lt;-</span> <span class="fl">0.5</span> </span>
<span id="cb10-13"><a href="sec13_8.html#cb10-13" tabindex="-1"></a>QuanReg <span class="ot">&lt;-</span> <span class="fu">rq</span>(y <span class="sc">~</span> p401 <span class="sc">+</span> age <span class="sc">+</span> inc <span class="sc">+</span> fsize <span class="sc">+</span> educ <span class="sc">+</span> marr <span class="sc">+</span> twoearn <span class="sc">+</span> db <span class="sc">+</span> pira <span class="sc">+</span> hown, <span class="at">tau =</span> tau, <span class="at">data =</span> df)</span>
<span id="cb10-14"><a href="sec13_8.html#cb10-14" tabindex="-1"></a><span class="fu">summary</span>(QuanReg)</span>
<span id="cb10-15"><a href="sec13_8.html#cb10-15" tabindex="-1"></a>Reg <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">MCMCquantreg</span>(y <span class="sc">~</span> x <span class="sc">+</span> w, <span class="at">data =</span> df, <span class="at">tau =</span> tau)</span>
<span id="cb10-16"><a href="sec13_8.html#cb10-16" tabindex="-1"></a>BayesExo <span class="ot">&lt;-</span> <span class="fu">summary</span>(Reg)</span>
<span id="cb10-17"><a href="sec13_8.html#cb10-17" tabindex="-1"></a>LossFunct <span class="ot">&lt;-</span> <span class="cf">function</span>(par, tau, y, z, x, w){</span>
<span id="cb10-18"><a href="sec13_8.html#cb10-18" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb10-19"><a href="sec13_8.html#cb10-19" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x, w)</span>
<span id="cb10-20"><a href="sec13_8.html#cb10-20" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z, w)</span>
<span id="cb10-21"><a href="sec13_8.html#cb10-21" tabindex="-1"></a>    Ind <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(y <span class="sc">&lt;=</span> X<span class="sc">%*%</span>par) </span>
<span id="cb10-22"><a href="sec13_8.html#cb10-22" tabindex="-1"></a>    gn <span class="ot">&lt;-</span> <span class="fu">colMeans</span>((tau <span class="sc">-</span> Ind) <span class="sc">*</span> Z)</span>
<span id="cb10-23"><a href="sec13_8.html#cb10-23" tabindex="-1"></a>    Wni <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="cf">function</span>(i) {Z[i,] <span class="sc">%*%</span> <span class="fu">t</span>(Z[i,])}) </span>
<span id="cb10-24"><a href="sec13_8.html#cb10-24" tabindex="-1"></a>    Wn <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (tau <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> tau)) <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, Wni)<span class="sc">/</span>n)</span>
<span id="cb10-25"><a href="sec13_8.html#cb10-25" tabindex="-1"></a>    Ln <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.5</span> <span class="sc">*</span> n <span class="sc">*</span> <span class="fu">t</span>(gn) <span class="sc">%*%</span> Wn <span class="sc">%*%</span> gn</span>
<span id="cb10-26"><a href="sec13_8.html#cb10-26" tabindex="-1"></a>    <span class="fu">return</span>(Ln)</span>
<span id="cb10-27"><a href="sec13_8.html#cb10-27" tabindex="-1"></a>}</span>
<span id="cb10-28"><a href="sec13_8.html#cb10-28" tabindex="-1"></a>par0 <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(Reg)</span>
<span id="cb10-29"><a href="sec13_8.html#cb10-29" tabindex="-1"></a><span class="fu">LossFunct</span>(<span class="at">par =</span> par0, <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb10-30"><a href="sec13_8.html#cb10-30" tabindex="-1"></a><span class="co"># ----- MH using Ln -----</span></span>
<span id="cb10-31"><a href="sec13_8.html#cb10-31" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">length</span>(par0); b0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, k); B0 <span class="ot">&lt;-</span> <span class="dv">1000</span><span class="sc">*</span><span class="fu">diag</span>(k)</span>
<span id="cb10-32"><a href="sec13_8.html#cb10-32" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">10000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">10000</span>; thin <span class="ot">&lt;-</span> <span class="dv">1</span>; tot <span class="ot">&lt;-</span> S <span class="sc">+</span> burnin</span>
<span id="cb10-33"><a href="sec13_8.html#cb10-33" tabindex="-1"></a>BETA <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, tot, k); accept <span class="ot">&lt;-</span> <span class="fu">logical</span>(tot)</span>
<span id="cb10-34"><a href="sec13_8.html#cb10-34" tabindex="-1"></a>SIGMA <span class="ot">&lt;-</span> <span class="fu">diag</span>(BayesExo[[<span class="st">&quot;statistics&quot;</span>]][,<span class="dv">2</span>]); tune <span class="ot">&lt;-</span> <span class="fl">2.4</span> <span class="sc">/</span> <span class="fu">sqrt</span>(k)</span>
<span id="cb10-35"><a href="sec13_8.html#cb10-35" tabindex="-1"></a>BETA[<span class="dv">1</span>,] <span class="ot">&lt;-</span> par0</span>
<span id="cb10-36"><a href="sec13_8.html#cb10-36" tabindex="-1"></a>LL <span class="ot">&lt;-</span> <span class="fu">LossFunct</span>(<span class="at">par =</span> BETA[<span class="dv">1</span>,], <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb10-37"><a href="sec13_8.html#cb10-37" tabindex="-1"></a>pb <span class="ot">&lt;-</span> <span class="fu">txtProgressBar</span>(<span class="at">min=</span><span class="dv">0</span>, <span class="at">max=</span>tot, <span class="at">style=</span><span class="dv">3</span>)</span>
<span id="cb10-38"><a href="sec13_8.html#cb10-38" tabindex="-1"></a><span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>tot){</span>
<span id="cb10-39"><a href="sec13_8.html#cb10-39" tabindex="-1"></a>    cand <span class="ot">&lt;-</span> BETA[s<span class="dv">-1</span>,] <span class="sc">+</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, k), tune<span class="sc">*</span>SIGMA)</span>
<span id="cb10-40"><a href="sec13_8.html#cb10-40" tabindex="-1"></a>    LLc  <span class="ot">&lt;-</span> <span class="fu">LossFunct</span>(<span class="at">par =</span> cand, <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb10-41"><a href="sec13_8.html#cb10-41" tabindex="-1"></a>    priorRat <span class="ot">&lt;-</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(cand, b0, B0, <span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">-</span></span>
<span id="cb10-42"><a href="sec13_8.html#cb10-42" tabindex="-1"></a>    mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(BETA[s<span class="dv">-1</span>,], b0, B0, <span class="at">log=</span><span class="cn">TRUE</span>)</span>
<span id="cb10-43"><a href="sec13_8.html#cb10-43" tabindex="-1"></a>    loga <span class="ot">&lt;-</span> (LLc <span class="sc">-</span> LL) <span class="sc">+</span> priorRat</span>
<span id="cb10-44"><a href="sec13_8.html#cb10-44" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">is.finite</span>(loga) <span class="sc">&amp;&amp;</span> <span class="fu">log</span>(<span class="fu">runif</span>(<span class="dv">1</span>)) <span class="sc">&lt;=</span> loga) {</span>
<span id="cb10-45"><a href="sec13_8.html#cb10-45" tabindex="-1"></a>        BETA[s,] <span class="ot">&lt;-</span> cand; LL <span class="ot">&lt;-</span> LLc; accept[s] <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb10-46"><a href="sec13_8.html#cb10-46" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb10-47"><a href="sec13_8.html#cb10-47" tabindex="-1"></a>        BETA[s,] <span class="ot">&lt;-</span> BETA[s<span class="dv">-1</span>,]; accept[s] <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb10-48"><a href="sec13_8.html#cb10-48" tabindex="-1"></a>    }</span>
<span id="cb10-49"><a href="sec13_8.html#cb10-49" tabindex="-1"></a>    <span class="cf">if</span> (s <span class="sc">&lt;=</span> burnin <span class="sc">&amp;&amp;</span> s <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {    </span>
<span id="cb10-50"><a href="sec13_8.html#cb10-50" tabindex="-1"></a>        acc <span class="ot">&lt;-</span> <span class="fu">mean</span>(accept[(s<span class="dv">-99</span>)<span class="sc">:</span>s])</span>
<span id="cb10-51"><a href="sec13_8.html#cb10-51" tabindex="-1"></a>        <span class="cf">if</span> (acc <span class="sc">&gt;</span> <span class="fl">0.35</span>) tune <span class="ot">&lt;-</span> tune <span class="sc">*</span> <span class="fl">1.25</span></span>
<span id="cb10-52"><a href="sec13_8.html#cb10-52" tabindex="-1"></a>        <span class="cf">if</span> (acc <span class="sc">&lt;</span> <span class="fl">0.15</span>) tune <span class="ot">&lt;-</span> tune <span class="sc">/</span> <span class="fl">1.25</span></span>
<span id="cb10-53"><a href="sec13_8.html#cb10-53" tabindex="-1"></a>    }</span>
<span id="cb10-54"><a href="sec13_8.html#cb10-54" tabindex="-1"></a>    <span class="fu">setTxtProgressBar</span>(pb, s)</span>
<span id="cb10-55"><a href="sec13_8.html#cb10-55" tabindex="-1"></a>}</span>
<span id="cb10-56"><a href="sec13_8.html#cb10-56" tabindex="-1"></a><span class="fu">close</span>(pb)</span>
<span id="cb10-57"><a href="sec13_8.html#cb10-57" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Acceptance rate:&quot;</span>, <span class="fu">mean</span>(accept), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-58"><a href="sec13_8.html#cb10-58" tabindex="-1"></a>keep <span class="ot">&lt;-</span> <span class="fu">seq</span>(burnin, tot, <span class="at">by =</span> thin)</span>
<span id="cb10-59"><a href="sec13_8.html#cb10-59" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Acceptance rate:&quot;</span>, <span class="fu">mean</span>(accept[keep]), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-60"><a href="sec13_8.html#cb10-60" tabindex="-1"></a>post <span class="ot">&lt;-</span> BETA[keep, , drop<span class="ot">=</span><span class="cn">FALSE</span>]   <span class="co"># posterior draws in scaled space</span></span>
<span id="cb10-61"><a href="sec13_8.html#cb10-61" tabindex="-1"></a><span class="fu">colnames</span>(post) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Int&quot;</span>, <span class="st">&quot;p401&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;inc&quot;</span>, <span class="st">&quot;fsize&quot;</span>, <span class="st">&quot;educ&quot;</span>, <span class="st">&quot;marr&quot;</span>, <span class="st">&quot;twoearn&quot;</span>, <span class="st">&quot;db&quot;</span>, <span class="st">&quot;pira&quot;</span>, <span class="st">&quot;hown&quot;</span>)</span>
<span id="cb10-62"><a href="sec13_8.html#cb10-62" tabindex="-1"></a><span class="co"># Posterior summaries</span></span>
<span id="cb10-63"><a href="sec13_8.html#cb10-63" tabindex="-1"></a>post_mean <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(post)</span>
<span id="cb10-64"><a href="sec13_8.html#cb10-64" tabindex="-1"></a>post_ci   <span class="ot">&lt;-</span> <span class="fu">apply</span>(post, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb10-65"><a href="sec13_8.html#cb10-65" tabindex="-1"></a><span class="fu">round</span>(post_mean, <span class="dv">3</span>); <span class="fu">round</span>(post_ci, <span class="dv">3</span>)</span></code></pre></div>
<p>Setting <span class="math inline">\(\tau = 0.5\)</span> (the median), the quantile treatment effect is USD 6,719, with a 95% credible interval of (USD 6,612, USD 6,806). For <span class="math inline">\(\tau = 0.9\)</span>, the treatment effect is USD 19,318, with a 95% credible interval of (USD 18,634, USD 20,085). Note that, in the instrumental variable example, the posterior mean of the LATE was USD 8,520, which is higher than the treatment effect at the median (<span class="math inline">\(\tau = 0.5\)</span>). This suggests that the LATE overstates the causal effect of 401(k) participation due to the large impact at higher quantiles of the outcome distribution.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-andrews2022weakgmm" class="csl-entry">
Andrews, Isaiah, and Anna Mikusheva. 2022. <span>“Optimal Decision Rules for Weak <span>GMM</span>.”</span> <em>Econometrica</em> 90 (2): 715–48. <a href="https://doi.org/10.3982/ECTA18678">https://doi.org/10.3982/ECTA18678</a>.
</div>
<div id="ref-ChenChristensenTamer2018" class="csl-entry">
Chen, Xiaohong, Timothy M. Christensen, and Elie Tamer. 2018. <span>“Monte Carlo Confidence Sets for Identified Sets.”</span> <em>Econometrica</em> 86 (6): 1965–2018. <a href="https://doi.org/10.3982/ECTA14525">https://doi.org/10.3982/ECTA14525</a>.
</div>
<div id="ref-chernozhukov2004effects" class="csl-entry">
Chernozhukov, Victor, and Christian Hansen. 2004. <span>“The Effects of 401(k) Participation on the Wealth Distribution: An Instrumental Quantile Regression Analysis.”</span> <em>The Review of Economics and Statistics</em> 86 (3): 735–51. <a href="https://doi.org/10.1162/0034653041811749">https://doi.org/10.1162/0034653041811749</a>.
</div>
<div id="ref-Chernozhukov2005" class="csl-entry">
———. 2005. <span>“An IV Model of Quantile Treatment Effects.”</span> <em>Econometrica</em> 73 (1): 245–61. <a href="https://doi.org/10.1111/j.1468-0262.2005.00570.x">https://doi.org/10.1111/j.1468-0262.2005.00570.x</a>.
</div>
<div id="ref-chernozhukov2003mcmc" class="csl-entry">
Chernozhukov, Victor, and Han Hong. 2003. <span>“An MCMC Approach to Classical Estimation.”</span> <em>Journal of Econometrics</em> 115 (2): 293–346.
</div>
<div id="ref-chib2018moment" class="csl-entry">
Chib, Siddhartha, Minchul Shin, and Anna Simoni. 2018. <span>“Bayesian Estimation and Comparison of Moment Condition Models.”</span> <em>Journal of the American Statistical Association</em> 113 (524): 1656–68. <a href="https://doi.org/10.1080/01621459.2017.1358172">https://doi.org/10.1080/01621459.2017.1358172</a>.
</div>
<div id="ref-hernan2020causal" class="csl-entry">
Hernán, Miguel A., and James M. Robins. 2020. <em>Causal Inference: What If</em>. Boca Raton, FL: Chapman &amp; Hall/CRC. <a href="https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/">https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/</a>.
</div>
<div id="ref-lazar2003bel" class="csl-entry">
Lazar, Nicole A. 2003. <span>“Bayesian Empirical Likelihood.”</span> <em>Biometrika</em> 90 (2): 319–26. <a href="https://doi.org/10.1093/biomet/90.2.319">https://doi.org/10.1093/biomet/90.2.319</a>.
</div>
<div id="ref-Muller2013SandwichBayes" class="csl-entry">
Müller, Ulrich K. 2013. <span>“Risk of <span>Bayesian</span> Inference in Misspecified Models, and the Sandwich Covariance Matrix.”</span> <em>Econometrica</em> 81 (5): 1805–49. <a href="https://doi.org/10.3982/ECTA9097">https://doi.org/10.3982/ECTA9097</a>.
</div>
<div id="ref-schennach2005betel" class="csl-entry">
Schennach, Susanne M. 2005. <span>“Bayesian Exponentially Tilted Empirical Likelihood.”</span> <em>Biometrika</em> 92 (1): 31–46. <a href="https://doi.org/10.1093/biomet/92.1.31">https://doi.org/10.1093/biomet/92.1.31</a>.
</div>
<div id="ref-schennach2007etel" class="csl-entry">
———. 2007. <span>“Point Estimation with Exponentially Tilted Empirical Likelihood.”</span> <em>The Annals of Statistics</em> 35 (2): 634–72. <a href="https://doi.org/10.1214/009053606000001208">https://doi.org/10.1214/009053606000001208</a>.
</div>
<div id="ref-zellner1996bmom" class="csl-entry">
Zellner, Arnold. 1996. <span>“Bayesian Method of Moments (BMOM) Analysis of Mean and Regression Models.”</span> In <em>Modelling and Prediction Honoring Seymour Geisser</em>, edited by Jack C. Lee, Wesley O. Johnson, and Arnold Zellner, 61–72. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-4612-2414-3_4">https://doi.org/10.1007/978-1-4612-2414-3_4</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Available at <em><a href="https://apps.olin.wustl.edu/faculty/chib/rpackages/betel/" class="uri">https://apps.olin.wustl.edu/faculty/chib/rpackages/betel/</a></em>.<a href="sec13_8.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p><span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span>’s proposal allows for both discrete and continuous <span class="math inline">\(D\)</span> and <span class="math inline">\(\mathbf{Z}\)</span>.<a href="sec13_8.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec13_7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec13_10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/10-Diagnostics.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
