<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.1 The Bayes’ rule | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1.1 The Bayes’ rule | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.1 The Bayes’ rule | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-03-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Chap1.html"/>
<link rel="next" href="sec12.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec11" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> The Bayes’ rule<a href="sec11.html#sec11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As expected, the starting point for performing Bayesian inference is Bayes’ rule, which provides the solution to the inverse problem of determining causes from observed effects. This rule combines prior beliefs with objective probabilities based on repeatable experiments, allowing us to move from observations to probable causes.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Formally, the conditional probability of <span class="math inline">\(A_i\)</span> given <span class="math inline">\(B\)</span> is equal to the conditional probability of <span class="math inline">\(B\)</span> given <span class="math inline">\(A_i\)</span>, multiplied by the marginal probability of <span class="math inline">\(A_i\)</span>, divided by the marginal probability of <span class="math inline">\(B\)</span>:</p>
<p><span class="math display" id="eq:111">\[\begin{align}
  P(A_i|B)&amp;=\frac{P(A_i,B)}{P(B)}\\
        &amp;=\frac{P(B|A_i) \times P(A_i)}{P(B)},
  \tag{1.1}
\end{align}\]</span>
where equation <a href="sec11.html#eq:111">(1.1)</a> is Bayes’ rule.</p>
<p>By the law of total probability, <span class="math inline">\(P(B) = \sum_i P(B \mid A_i) P(A_i) \neq 0\)</span>, and <span class="math inline">\(\{ A_i, i = 1, 2, \dots \}\)</span> is a finite or countably infinite partition of the sample space.</p>
<p>In the Bayesian framework, <span class="math inline">\(B\)</span> represents sample information that updates a probabilistic statement about an unknown object <span class="math inline">\(A_i\)</span> according to probability rules. This is done using Bayes’ rule, which incorporates prior “beliefs” about <span class="math inline">\(A_i\)</span>, i.e., <span class="math inline">\(P(A_i)\)</span>, sample information relating <span class="math inline">\(B\)</span> to the particular state of nature <span class="math inline">\(A_i\)</span> through a probabilistic statement, <span class="math inline">\(P(B \mid A_i)\)</span>, and the probability of observing that specific sample information, <span class="math inline">\(P(B)\)</span>.</p>
<p>Let’s consider a simple example, <em>the base rate fallacy</em>:</p>
<p>Assume that the sample information comes from a positive result from a test whose true positive rate (sensitivity) is 98%, i.e., <span class="math inline">\(P(+ \mid \text{disease}) = 0.98\)</span>. On the other hand, the prior information regarding being infected with this disease comes from a base incidence rate of 0.002, i.e., <span class="math inline">\(P(\text{disease}) = 0.002\)</span>. The question is: <em>What is the probability of actually being infected, given a positive test result?</em></p>
<p>This is an example of <em>the base rate fallacy</em>, where a positive test result for a disease with a very low base incidence rate still gives a low probability of actually having the disease.</p>
<p>The key to answering this question lies in understanding the difference between the probability of having the disease given a positive test result, <span class="math inline">\(P(\text{disease} \mid +)\)</span>, and the probability of a positive result given the disease, <span class="math inline">\(P(+ \mid \text{disease})\)</span>. The former is the crucial result, and Bayes’ rule helps us to compute it. Using Bayes’ rule (equation <a href="sec11.html#eq:111">(1.1)</a>):</p>
<p><span class="math display">\[
P(\text{disease} \mid +) = \frac{P(+ \mid \text{disease}) \times P(\text{disease})}{P(+)} = \frac{0.98 \times 0.002}{0.98 \times 0.002 + (1-0.98) \times (1-0.002)} = 0.09
\]</span></p>
<p>where <span class="math inline">\(P(+) = P(+ \mid \text{disease}) \times P(\text{disease}) + P(+ \mid \lnot \text{disease}) \times P(\lnot \text{disease})\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>The following code shows how to perform this exercise in <strong>R</strong>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="sec11.html#cb1-1" tabindex="-1"></a>PD <span class="ot">&lt;-</span> <span class="fl">0.002</span> <span class="co"># Probability of disease</span></span>
<span id="cb1-2"><a href="sec11.html#cb1-2" tabindex="-1"></a>PPD <span class="ot">&lt;-</span> <span class="fl">0.98</span> <span class="co"># True positive (Sensitivity)</span></span>
<span id="cb1-3"><a href="sec11.html#cb1-3" tabindex="-1"></a>PDP <span class="ot">&lt;-</span> PD <span class="sc">*</span> PPD <span class="sc">/</span> (PD <span class="sc">*</span> PPD <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> PD)<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> PPD))</span>
<span id="cb1-4"><a href="sec11.html#cb1-4" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Probability of disease given a positive test is&quot;</span>, <span class="at">sep =</span> <span class="st">&quot; &quot;</span>, <span class="fu">round</span>(PDP, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of disease given a positive test is 0.09&quot;</code></pre>
<p>We observe that despite having a positive result, the probability of actually having the disease remains low. This is due to the base rate being so small.</p>
<p>Another interesting example, which lies at the heart of the origin of Bayes’ theorem <span class="citation">(<a href="#ref-bayes1763lii">Bayes 1763</a>)</span>, is related to the existence of God <span class="citation">(<a href="#ref-stigler2018richard">Stigler 2018</a>)</span>. In Section X of David Hume’s <em>“An Inquiry concerning Human Understanding”</em> (1748), titled <em>Of Miracles</em>, Hume argues that when someone claims to have seen a miracle, this provides poor evidence that the event actually occurred, as it contradicts our everyday observations. In response, Richard Price, who finished and published <em>“An Essay Towards Solving a Problem in the Doctrine of Chances”</em> in 1763 (after Bayes’ death in 1761), argues against Hume by highlighting the difference between <em>impossibility</em> in casual conversation and <em>physical impossibility</em>. Price used an example of a die with a million sides, where <em>impossibility</em> refers to rolling a specific side, and <em>physical impossibility</em> refers to rolling a side that does not exist. In millions of throws, the latter would never happen, while the former would eventually occur.</p>
<p>Now, let’s consider a scenario involving two cases of resurrection (Res): Jesus Christ and Elvis. The total number of people who have ever lived is approximately 108.5 billion,<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> so the prior base rate is given by <span class="math inline">\(\frac{2}{108.5 \times 10^9}\)</span>. On the other hand, suppose the sample information comes from a highly reliable witness with a true positive rate of 0.9999999. The question then is: <em>What is the probability of this miracle occurring?</em><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>Using Bayes’ rule:</p>
<p><span class="math display">\[\begin{align*}
    P(\text{Res}\mid \text{Witness}) &amp; =  \frac{P(\text{Witness}\mid \text{Res})\times P(\text{Res})}{P(\text{Witness})}\\
    &amp; =\frac{2/(108.5 * 10^9) \times 0.9999999}{2/(108.5 * 10^9) \times 0.9999999 + (1-2/(108.5 * 10^9)) \times (1-0.9999999)}\\
    &amp; = 0.000184297806959661
\end{align*}\]</span></p>
<p>where <span class="math inline">\(P(\text{Witness}) = P(\text{Witness} \mid \text{Res}) \times P(\text{Res}) + (1 - P(\text{Witness} \mid \text{Res})) \times (1 - P(\text{Res}))\)</span>.</p>
<p>Thus, the probability of a resurrection, given a very reliable witness, is approximately <span class="math inline">\(1.843 \times 10^{-4}\)</span>.</p>
<p>The following code shows how to perform this exercise in <strong>R</strong>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="sec11.html#cb3-1" tabindex="-1"></a><span class="co"># Probability of resurrection</span></span>
<span id="cb3-2"><a href="sec11.html#cb3-2" tabindex="-1"></a>PR <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span>(<span class="fl">108.5</span> <span class="sc">*</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">9</span>) </span>
<span id="cb3-3"><a href="sec11.html#cb3-3" tabindex="-1"></a>PWR <span class="ot">&lt;-</span> <span class="fl">0.9999999</span> <span class="co"># True positive rate</span></span>
<span id="cb3-4"><a href="sec11.html#cb3-4" tabindex="-1"></a>PRW <span class="ot">&lt;-</span> PR <span class="sc">*</span> PWR <span class="sc">/</span> (PR <span class="sc">*</span> PWR <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> PR)<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> PWR)) </span>
<span id="cb3-5"><a href="sec11.html#cb3-5" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Probability of resurrection given witness is&quot;</span>, <span class="at">sep =</span> <span class="st">&quot; &quot;</span>, PRW)</span></code></pre></div>
<pre><code>## [1] &quot;Probability of resurrection given witness is 0.000184297806959661&quot;</code></pre>
<p>Observe that we can condition on multiple events in Bayes’ rule. Let’s consider two conditioning events, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>. Then, equation <a href="sec11.html#eq:111">(1.1)</a> becomes</p>
<p><span class="math display" id="eq:112">\[\begin{align}
    P(A_i\mid B,C)&amp;=\frac{P(A_i,B,C)}{P(B,C)}\nonumber\\
    &amp;=\frac{P(B\mid A_i,C) \times P(A_i\mid C) \times P(C)}{P(B\mid C)P(C)}.
    \tag{1.2}
\end{align}\]</span></p>
<p>Let’s use this rule in one of the most intriguing statistical puzzles, <em>the Monty Hall problem</em>, to illustrate how to use equation <a href="sec11.html#eq:112">(1.2)</a> <span class="citation">(<a href="#ref-selvin1975problem">Selvin 1975</a>; <a href="#ref-morgan1991let">Morgan et al. 1991</a>)</span>. This was the situation faced by a contestant in the American television game show <em>Let’s Make a Deal</em>. In this game, the contestant was asked to choose a door, behind one of which there is a car, and behind the others, goats.</p>
<p>Let’s say the contestant picks door No. 1, and the host (Monty Hall), who knows what is behind each door, opens door No. 3, revealing a goat. Then, the host asks the tricky question: <em>Do you want to pick door No. 2?</em></p>
<p><img src="figures/MHproblemNew.png" width="400" style="display: block; margin: auto;" /></p>
<p>Let’s define the following events:</p>
<ul>
<li><span class="math inline">\(P_i\)</span>: the event <em>contestant picks door No. <span class="math inline">\(i\)</span></em>, which stays closed,</li>
<li><span class="math inline">\(H_i\)</span>: the event <em>host picks door No. <span class="math inline">\(i\)</span></em>, which is open and contains a goat,</li>
<li><span class="math inline">\(C_i\)</span>: the event <em>car is behind door No. <span class="math inline">\(i\)</span></em>.</li>
</ul>
<p>In this particular setting, the contestant is interested in the probability of the event <span class="math inline">\(P(C_2 \mid H_3, P_1)\)</span>. A naive answer would be that it is irrelevant, as initially, <span class="math inline">\(P(C_i) = \frac{1}{3}, \ i = 1, 2, 3\)</span>, and now <span class="math inline">\(P(C_i \mid H_3) = \frac{1}{2}, \ i = 1, 2\)</span>, since the host opened door No. 3. So, why bother changing the initial guess if the odds are the same (1:1)?</p>
<p>The important point here is that the host knows what is behind each door and always picks a door where there is a goat, given the contestant’s choice. In this particular setting:</p>
<p><span class="math display">\[
P(H_3 \mid C_3, P_1) = 0, \quad P(H_3 \mid C_2, P_1) = 1, \quad P(H_3 \mid C_1, P_1) = \frac{1}{2}.
\]</span></p>
<p>Then, using equation <a href="sec11.html#eq:112">(1.2)</a>, we can calculate the posterior probability.</p>
<p><span class="math display">\[\begin{align*}
    P(C_2\mid H_3,P_1)&amp;= \frac{P(C_2,H_3,P_1)}{P(H_3,P_1)}\\
    &amp;= \frac{P(H_3\mid C_2,P_1)P(C_2\mid P_1)P(P_1)}{P(H_3\mid P_1)\times P(P_1)}\\
    &amp;= \frac{P(H_3\mid C_2,P_1)P(C_2)}{P(H_3\mid P_1)}\\
    &amp;=\frac{1\times 1/3}{1/2},
\end{align*}\]</span></p>
<p>Where the third equation uses the fact that <span class="math inline">\(C_i\)</span> and <span class="math inline">\(P_i\)</span> are independent events, and <span class="math inline">\(P(H_3 \mid P_1) = \frac{1}{2}\)</span> because this depends only on <span class="math inline">\(P_1\)</span> (not on <span class="math inline">\(C_2\)</span>).</p>
<p>Therefore, changing the initial decision increases the probability of getting the car from <span class="math inline">\(\frac{1}{3}\)</span> to <span class="math inline">\(\frac{2}{3}\)</span>! Thus, it is always a good idea to change the door.</p>
<p>Let’s see a simulation exercise in <strong>R</strong> to check this answer:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="sec11.html#cb5-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">0101</span>) <span class="co"># Set simulation seed</span></span>
<span id="cb5-2"><a href="sec11.html#cb5-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">100000</span> <span class="co"># Simulations</span></span>
<span id="cb5-3"><a href="sec11.html#cb5-3" tabindex="-1"></a>Game <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">switch =</span> <span class="dv">0</span>){</span>
<span id="cb5-4"><a href="sec11.html#cb5-4" tabindex="-1"></a>    <span class="co"># switch = 0 is not change  </span></span>
<span id="cb5-5"><a href="sec11.html#cb5-5" tabindex="-1"></a>    <span class="co"># switch = 1 is to change</span></span>
<span id="cb5-6"><a href="sec11.html#cb5-6" tabindex="-1"></a>    opts <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span> </span>
<span id="cb5-7"><a href="sec11.html#cb5-7" tabindex="-1"></a>    car <span class="ot">&lt;-</span> <span class="fu">sample</span>(opts, <span class="dv">1</span>) <span class="co"># car location</span></span>
<span id="cb5-8"><a href="sec11.html#cb5-8" tabindex="-1"></a>    guess1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(opts, <span class="dv">1</span>) <span class="co"># Initial guess </span></span>
<span id="cb5-9"><a href="sec11.html#cb5-9" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="sec11.html#cb5-10" tabindex="-1"></a>    <span class="cf">if</span>(car <span class="sc">!=</span> guess1) {</span>
<span id="cb5-11"><a href="sec11.html#cb5-11" tabindex="-1"></a>     host <span class="ot">&lt;-</span> opts[<span class="sc">-</span><span class="fu">c</span>(car, guess1)]</span>
<span id="cb5-12"><a href="sec11.html#cb5-12" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb5-13"><a href="sec11.html#cb5-13" tabindex="-1"></a>     host <span class="ot">&lt;-</span> <span class="fu">sample</span>(opts[<span class="sc">-</span><span class="fu">c</span>(car, guess1)], <span class="dv">1</span>)</span>
<span id="cb5-14"><a href="sec11.html#cb5-14" tabindex="-1"></a>    }   </span>
<span id="cb5-15"><a href="sec11.html#cb5-15" tabindex="-1"></a>    win1 <span class="ot">&lt;-</span> guess1 <span class="sc">==</span> car <span class="co"># Win no change</span></span>
<span id="cb5-16"><a href="sec11.html#cb5-16" tabindex="-1"></a>    guess2 <span class="ot">&lt;-</span> opts[<span class="sc">-</span><span class="fu">c</span>(host, guess1)]    </span>
<span id="cb5-17"><a href="sec11.html#cb5-17" tabindex="-1"></a>    win2 <span class="ot">&lt;-</span> guess2 <span class="sc">==</span> car <span class="co"># Win change</span></span>
<span id="cb5-18"><a href="sec11.html#cb5-18" tabindex="-1"></a>    <span class="cf">if</span>(<span class="cf">switch</span> <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb5-19"><a href="sec11.html#cb5-19" tabindex="-1"></a>        win <span class="ot">&lt;-</span> win1</span>
<span id="cb5-20"><a href="sec11.html#cb5-20" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb5-21"><a href="sec11.html#cb5-21" tabindex="-1"></a>        win <span class="ot">&lt;-</span> win2</span>
<span id="cb5-22"><a href="sec11.html#cb5-22" tabindex="-1"></a>    }</span>
<span id="cb5-23"><a href="sec11.html#cb5-23" tabindex="-1"></a>    <span class="fu">return</span>(win)</span>
<span id="cb5-24"><a href="sec11.html#cb5-24" tabindex="-1"></a>}</span>
<span id="cb5-25"><a href="sec11.html#cb5-25" tabindex="-1"></a></span>
<span id="cb5-26"><a href="sec11.html#cb5-26" tabindex="-1"></a><span class="co">#Win probabilities not changing</span></span>
<span id="cb5-27"><a href="sec11.html#cb5-27" tabindex="-1"></a>Prob <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">replicate</span>(S, <span class="fu">Game</span>(<span class="at">switch =</span> <span class="dv">0</span>))) </span>
<span id="cb5-28"><a href="sec11.html#cb5-28" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Winning probabilities no changing door is&quot;</span>, Prob, <span class="at">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Winning probabilities no changing door is 0.3334&quot;</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="sec11.html#cb7-1" tabindex="-1"></a><span class="co">#Win probabilities changing</span></span>
<span id="cb7-2"><a href="sec11.html#cb7-2" tabindex="-1"></a>Prob <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">replicate</span>(S, <span class="fu">Game</span>(<span class="at">switch =</span> <span class="dv">1</span>))) </span>
<span id="cb7-3"><a href="sec11.html#cb7-3" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Winning probabilities changing door is&quot;</span>, Prob, <span class="at">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Winning probabilities changing door is 0.6654&quot;</code></pre>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bayes1763lii" class="csl-entry">
Bayes, Thomas. 1763. <span>“LII. An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, <span>FRS</span> Communicated by <span>M</span>r. <span>P</span>rice, in a Letter to <span>J</span>ohn <span>C</span>anton, <span>AMFR S</span>.”</span> <em>Philosophical Transactions of the Royal Society of London</em>, no. 53: 370–418.
</div>
<div id="ref-laplace1774memoire" class="csl-entry">
Laplace, Pierre Simon. 1774. <span>“M<span>é</span>moire Sur La Probabilit<span>é</span> de Causes Par Les <span class="nocase">é</span>venements.”</span> <em>M<span>é</span>moire de l’acad<span>é</span>mie Royale Des Sciences</em>.
</div>
<div id="ref-morgan1991let" class="csl-entry">
Morgan, John P, N Rao Chaganty, Ram C Dahiya, and Michael J Doviak. 1991. <span>“Let’s Make a Deal: The Player’s Dilemma.”</span> <em>The American Statistician</em> 45 (4): 284–87.
</div>
<div id="ref-selvin1975problem" class="csl-entry">
Selvin, Steve. 1975. <span>“A Problem in Probability (Letter to the Editor).”</span> <em>The American Statistician</em> 11 (1): 67–71.
</div>
<div id="ref-stigler2018richard" class="csl-entry">
Stigler, Stephen. 2018. <span>“Richard Price, the First Bayesian.”</span> <em>Statistical Science</em> 33 (1): 117–25.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Note that I use the term “Bayes’ rule” rather than “Bayes’ theorem.” It was Laplace <span class="citation">(<a href="#ref-laplace1774memoire">Laplace 1774</a>)</span> who generalized Bayes’ theorem <span class="citation">(<a href="#ref-bayes1763lii">Bayes 1763</a>)</span>, and his generalization is referred to as Bayes’ rule.<a href="sec11.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><span class="math inline">\(\lnot\)</span> is the negation symbol. In addition, we have that <span class="math inline">\(P(B \mid A) = 1 - P(B \mid A^c)\)</span> in this example, where <span class="math inline">\(A^c\)</span> is the complement of <span class="math inline">\(A\)</span>. However, it is not always the case that <span class="math inline">\(P(B \mid A) \neq 1 - P(B \mid A^c)\)</span>.<a href="sec11.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://www.wolframalpha.com/input/?i=number+of+people+who+have+ever+lived+on+Earth">Source</a>.<a href="sec11.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://www.r-bloggers.com/2019/04/base-rate-fallacy-or-why-no-one-is-justified-to-believe-that-jesus-rose/">Source</a>.<a href="sec11.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Chap1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec12.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
