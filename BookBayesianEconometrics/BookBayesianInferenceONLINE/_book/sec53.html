<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Particle filtering | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Particle filtering | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Particle filtering | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec52.html"/>
<link rel="next" href="sec54.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Dirichlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Non-parametric generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec53" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Particle filtering<a href="sec53.html#sec53" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, we consider the scenario where we need to sample from a posterior distribution whose dimension increases over time, <span class="math inline">\(\pi(\boldsymbol{\theta}_{0:t}\mid \boldsymbol{y}_{0:t})\)</span>, for <span class="math inline">\(t = 0, 1, \dots\)</span>. The challenge arises from the fact that, even if this posterior distribution is known, the computational complexity of implementing a sampling scheme in this context increases linearly with <span class="math inline">\(t\)</span>.</p>
<p>This makes MCMC methods, which operate in batch mode and require a complete re-run whenever new information becomes available, less optimal. Consequently, we present sequential algorithms, which operate incrementally as new data becomes available, and are often a better alternative. These algorithms are typically faster and are well-suited for scenarios requiring real-time updates, commonly referred to as online mode.</p>
<p>Specifically, we consider the dynamic system in the <em>state-space</em> representation. This is a system where there is an <em>unobservable state vector</em> <span class="math inline">\(\boldsymbol{\theta}_t\in\mathbb{R}^K\)</span>, and an observed variable <span class="math inline">\(\boldsymbol{Y}_t\)</span>, <span class="math inline">\(t=0,1,\dots\)</span> such that:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\boldsymbol{\theta}_t\)</span> is a <em>Markov process</em>, that is,<br />
<span class="math display">\[
\pi(\boldsymbol{\theta}_{t}\mid \boldsymbol{\theta}_{1:t-1})=\pi(\boldsymbol{\theta}_{t}\mid \boldsymbol{\theta}_{t-1}),
\]</span>
for <span class="math inline">\(t=1,2,\dots\)</span>. All the relevant information to define <span class="math inline">\(\boldsymbol{\theta}_{t}\)</span> is in <span class="math inline">\(\boldsymbol{\theta}_{t-1}\)</span>.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a></p></li>
<li><p><span class="math inline">\(\boldsymbol{Y}_t\perp \boldsymbol{Y}_s\mid \boldsymbol{\theta}_{t}\)</span>, for <span class="math inline">\(s&lt;t\)</span>. That is, there is independence between observable variables regarding their history conditional on the actual state vector.</p></li>
</ol>
<p>We can see in the next figure a graphical representation of the dynamic system.</p>
<p><img src="figures/SSfig.png" width="600px" height="350px" style="display: block; margin: auto;" /></p>
<p>Formally,</p>
<p><span class="math display">\[
\begin{aligned}
    \boldsymbol{\theta}_t &amp;= h(\boldsymbol{\theta}_{t-1}, \boldsymbol{w}_t) &amp; \text{(State equations)}\\
    Y_t &amp; = f(\boldsymbol{\theta}_t, \mu_t)&amp; \text{(Observation equation)},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{w}_t\)</span> and <span class="math inline">\(\mu_t\)</span> are stochastic errors such that their probability distributions define the transition density <span class="math inline">\(\pi(\boldsymbol{\theta}_t\mid \boldsymbol{\theta}_{t-1})\)</span> and observation density <span class="math inline">\(p(Y_t\mid \boldsymbol{\theta}_t)\)</span>.</p>
<p>We present <em>particle filtering</em>, a specific case of <em>sequential Monte Carlo</em> (SMC), which is one of the most commonly used algorithms for scenarios requiring sequential updates of the posterior distribution as described by the <em>state-space</em> model.</p>
<p>The starting point is <em>sequential importance sampling</em> (SIS), originally proposed by <span class="citation">Handschin and Mayne (<a href="#ref-handschin1969monte">1969</a>)</span>, which is a modification of IS to compute an estimate of <span class="math inline">\(\pi(\boldsymbol{\theta}_{0:t}\mid \boldsymbol{y}_{0:t})\)</span> without altering the past trajectories <span class="math inline">\(\left\{\boldsymbol{\theta}^{(s)}_{1:t-1}, s=1,2,\dots,S\right\}\)</span>. The key idea is to use a proposal density that takes the form</p>
<p><span class="math display">\[
\begin{aligned}
    q(\boldsymbol{\theta}_{0:t}\mid \boldsymbol{y}_{0:t}) &amp;= q(\boldsymbol{\theta}_{0:t-1}\mid \boldsymbol{y}_{1:t-1})q(\boldsymbol{\theta}_t\mid \boldsymbol{\theta}_{t-1},\boldsymbol{y}_{t}) \\
    &amp;= q(\boldsymbol{\theta}_0)\prod_{h=1}^{t}q(\boldsymbol{\theta}_h\mid \boldsymbol{\theta}_{h-1},\boldsymbol{y}_{h}).
\end{aligned}
\]</span></p>
<p>This proposal density allows calculating the weights sequentially,</p>
<p><span class="math display">\[
\begin{aligned}
    w_{t}(\boldsymbol{\theta}^{(s)}_{0:t})&amp;=\frac{\pi(\boldsymbol{\theta}_{0:t}^{(s)}\mid \boldsymbol{y}_{0:t})}{q(\boldsymbol{\theta}_{0:t}^{(s)}\mid \boldsymbol{y}_{0:t})}\\
    &amp;=\frac{p(\boldsymbol{y}_{0:t}\mid \boldsymbol{\theta}_{0:t}^{(s)})\pi(\boldsymbol{\theta}_{0:t}^{(s)})}{p(\boldsymbol{y}_{0:t})q(\boldsymbol{\theta}_{0:t}^{(s)}\mid \boldsymbol{y}_{0:t})}\\
    &amp;=\frac{p(\boldsymbol{y}_{t}\mid \boldsymbol{\theta}_{t}^{(s)})p(\boldsymbol{y}_{1:t-1}\mid \boldsymbol{\theta}_{0:t-1}^{(s)})\pi(\boldsymbol{\theta}_{t}^{(s)}\mid \boldsymbol{\theta}_{t-1}^{(s)})\pi(\boldsymbol{\theta}_{0:t-1}^{(s)})}{p(\boldsymbol{y}_{0:t})q(\boldsymbol{\theta}_{t}^{(s)}\mid \boldsymbol{\theta}_{t-1},\boldsymbol{y}_{t}^{(s)})q(\boldsymbol{\theta}_{0:t-1}^{(s)}\mid \boldsymbol{y}_{1:t-1})}\\
    &amp;\propto w_{t-1}^*(\boldsymbol{\theta}^{(s)}_{0:t-1})\frac{p(\boldsymbol{y}_{t}\mid \boldsymbol{\theta}_{t}^{(s)})\pi(\boldsymbol{\theta}_{t}\mid \boldsymbol{\theta}_{t-1}^{(s)})}{q(\boldsymbol{\theta}_t^{(s)}\mid \boldsymbol{\theta}_{t-1}^{(s)},\boldsymbol{y}_{t})}.
\end{aligned}
\]</span></p>
<p>Take into account that <span class="math inline">\(p(\boldsymbol{y}_{0:t})\)</span> does not depend on <span class="math inline">\(\boldsymbol{\theta}^{(s)}_{0:t}\)</span>. The term <span class="math inline">\(\alpha_t(\boldsymbol{\theta}_{0:t}^{(s)})=\frac{p(\boldsymbol{y}_{t}\mid \boldsymbol{\theta}_{t}^{(s)})\pi(\boldsymbol{\theta}_{t}\mid \boldsymbol{\theta}_{t-1}^{(s)})}{q(\boldsymbol{\theta}_t^{(s)}\mid \boldsymbol{\theta}_{t-1}^{(s)},\boldsymbol{y}_{t})}\)</span> is called the <em>incremental importance weight</em>, and implies that</p>
<p><span class="math display">\[
w_t(\boldsymbol{\theta}^{s}_{0:t})=w_0(\boldsymbol{\theta}^{s}_{0})\prod_{h=1}^{t}\alpha_h(\boldsymbol{\theta}_{1:h}^{(s)}).
\]</span></p>
<p>This algorithm possesses the desirable property of maintaining fixed computational complexity. Consequently, we sequentially obtain draws <span class="math inline">\(\boldsymbol{\theta}_t^{(s)}\)</span>, referred to as particles: <span class="math inline">\(\boldsymbol{\theta}_0^{(s)}\)</span> is drawn from <span class="math inline">\(q(\boldsymbol{\theta}_0)\)</span> at <span class="math inline">\(t=0\)</span>, and subsequently, <span class="math inline">\(\boldsymbol{\theta}_h^{(s)}\)</span> is drawn from <span class="math inline">\(q(\boldsymbol{\theta}_h\mid \boldsymbol{\theta}_{h-1},\boldsymbol{y}_{h})\)</span> at <span class="math inline">\(t=h\)</span> <span class="citation">(<a href="#ref-doucet2001introduction">Doucet, De Freitas, and Gordon 2001</a>; <a href="#ref-cappe2007overview">Olivier Cappé, Godsill, and Moulines 2007</a>)</span>.</p>
<p>A relevant case is when the proposal distribution takes the form of the prior distribution, that is,</p>
<p><span class="math display">\[
q(\boldsymbol{\theta}_{0:t}\mid \boldsymbol{y}_{0:t}) = \pi(\boldsymbol{\theta}_{0:t}) = \pi(\boldsymbol{\theta}_0)\prod_{h=1}^{t}\pi(\boldsymbol{\theta}_h\mid \boldsymbol{\theta}_{h-1}).
\]</span></p>
<p>This implies that</p>
<p><span class="math display">\[
w_{t}(\boldsymbol{\theta}^{(s)}_{0:t})\propto w_{t-1}^*(\boldsymbol{\theta}^{(s)}_{0:t-1})p(\boldsymbol{y}_{t}\mid \boldsymbol{\theta}_{t}^{(s)}),
\]</span></p>
<p>which means that the <em>incremental importance weight</em> is given by <span class="math inline">\(p(\boldsymbol{y}_{t}\mid \boldsymbol{\theta}_{t}^{(s)})\)</span>.</p>
<p>Algorithm <a href="#algSIS">4</a> shows how to perform SIS <span class="citation">(<a href="#ref-cappe2007overview">Olivier Cappé, Godsill, and Moulines 2007</a>)</span>. We set <span class="math inline">\(w_t^{(s)}:=w_t(\boldsymbol{\theta}_{0:t}^{(s)})\)</span> to simplify notation.</p>
<figcaption><b>Algorithm: Sequential importance sampling</b></figcaption>
<figure id="alg:SIS">
  <pre>
For s=1,2,...,S do
  Sample θ<sub>0</sub><sup>(s)</sup> from q(θ<sub>0</sub>|<b>y<sub>0</sub></b>)
  Calculate the importance weights w<sub>0</sub><sup>(s)</sup> ∝(p(<b>y<sub>0</sub></b>|θ<sub>0</sub><sup>(s)</sup>)π(θ<sub>0</sub><sup>(s)</sup>))/q(θ<sub>0</sub><sup>(s)</sup>|<b>y<sub>0</sub></b>)
End for
for t=1,2,...,T do
  for s=1,2,...,S do
    Draw particles θ<sub>t</sub><sup>(s)</sup> from q<sub>t</sub>(θ<sub>t</sub>|θ<sub>t-1</sub>,<b>y<sub>0</sub></b>)
    Compute the weights 
    w<sub>t</sub><sup>(s)</sup> ∝ w<sub>t-1</sub><sup>*(s)</sup> (p(<b>y<sub>t</sub></b>|θ<sub>t</sub><sup>(s)</sup>)π(θ<sub>t</sub><sup>(s)</sup>|θ<sub>t-1</sub><sup>(s)</sup>))/q(θ<sub>t</sub><sup>(s)</sup>|θ<sub>t-1</sub><sup>(s)</sup>,<b>y<sub>t</sub></b>) 
  End for
  Standardize the weights w<sub>t</sub><sup>*(s)</sup> = w<sub>t</sub><sup>(s)</sup>/(∑<sub>h</sub>w<sub>t</sub><sup>(h)</sup>), s=1,2,...,S
End for
  </pre>
</figure>
<p><strong>Example: Dynamic linear model</strong></p>
<p>Let’s assume that the state-space representation is<br />
<span class="math display">\[
\theta_t = \theta_{t-1} + w_t \quad \text{(State equation)} \\
Y_t = \phi \theta_t + \mu_t \quad \text{(Observation equation)},
\]</span>
where <span class="math inline">\(w_t \sim N(0, \sigma_w^2)\)</span> and <span class="math inline">\(\mu_t \sim N(0, \sigma_{\mu}^2)\)</span>, <span class="math inline">\(t = 1, 2, \dots, 50\)</span>. In addition, we use the proposal distribution <span class="math inline">\(q(\theta_t \mid y_t) = \pi(\theta_t)\)</span>, which is normal with mean <span class="math inline">\(\theta_{t-1}\)</span> and variance <span class="math inline">\(\sigma_w^2\)</span>. Then, the weights are given by the recursion<br />
<span class="math display">\[
w_t^{(s)} \propto w_{t-1}^{*(s)} p(y_t \mid \theta_t, \sigma_{\mu}^2),
\]</span><br />
where <span class="math inline">\(p(y_t \mid \theta_t, \sigma_{\mu}^2)\)</span> is <span class="math inline">\(N(\phi \theta_t, \sigma_{\mu}^2)\)</span>.</p>
<p>We can compute the mean and standard deviation of the state at each <span class="math inline">\(t\)</span> using<br />
<span class="math display">\[
\hat{\theta}_t = \sum_{s=1}^S w_t^{*(s)} \theta_t^{(s)}
\]</span><br />
and<br />
<span class="math display">\[
\hat{\sigma}_{\theta} = \left(\sum_{s=1}^S w_t^{*(s)} \theta_t^{2(s)} - \hat{\theta}_t^2\right)^{1/2}.
\]</span></p>
<p>The following code demonstrates the implementation of this algorithm, setting <span class="math inline">\(\sigma_w^2 = \sigma_{\mu}^2 = 1\)</span> and <span class="math inline">\(\phi = 0.5\)</span>. First, we simulate the process, and then we implement the SIS algorithm.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="sec53.html#cb141-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb141-2"><a href="sec53.html#cb141-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">50000</span> <span class="co"># Number of particles</span></span>
<span id="cb141-3"><a href="sec53.html#cb141-3" tabindex="-1"></a>sigma_w <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># State noise</span></span>
<span id="cb141-4"><a href="sec53.html#cb141-4" tabindex="-1"></a>sigma_mu <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># Observation noise</span></span>
<span id="cb141-5"><a href="sec53.html#cb141-5" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="co"># Coefficient in observation equation</span></span>
<span id="cb141-6"><a href="sec53.html#cb141-6" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="co"># Sample size</span></span>
<span id="cb141-7"><a href="sec53.html#cb141-7" tabindex="-1"></a><span class="co"># Simulate true states and observations</span></span>
<span id="cb141-8"><a href="sec53.html#cb141-8" tabindex="-1"></a>theta_true <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T); y_obs <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb141-9"><a href="sec53.html#cb141-9" tabindex="-1"></a>theta_true[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_w)  <span class="co"># Initial state</span></span>
<span id="cb141-10"><a href="sec53.html#cb141-10" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T) {</span>
<span id="cb141-11"><a href="sec53.html#cb141-11" tabindex="-1"></a>    theta_true[t] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> theta_true[t<span class="dv">-1</span>], <span class="at">sd =</span> sigma_w)</span>
<span id="cb141-12"><a href="sec53.html#cb141-12" tabindex="-1"></a>}</span>
<span id="cb141-13"><a href="sec53.html#cb141-13" tabindex="-1"></a>y_obs <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(T, <span class="at">mean =</span> phi<span class="sc">*</span>theta_true, <span class="at">sd =</span> sigma_mu)</span>
<span id="cb141-14"><a href="sec53.html#cb141-14" tabindex="-1"></a><span class="co"># Sequential Importance Sampling (SIS)</span></span>
<span id="cb141-15"><a href="sec53.html#cb141-15" tabindex="-1"></a>particles <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T) </span>
<span id="cb141-16"><a href="sec53.html#cb141-16" tabindex="-1"></a>weights <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T) </span>
<span id="cb141-17"><a href="sec53.html#cb141-17" tabindex="-1"></a>weightsSt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T) </span>
<span id="cb141-18"><a href="sec53.html#cb141-18" tabindex="-1"></a><span class="co"># Initialization</span></span>
<span id="cb141-19"><a href="sec53.html#cb141-19" tabindex="-1"></a>particles[, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_w)  <span class="co"># Sample initial particles</span></span>
<span id="cb141-20"><a href="sec53.html#cb141-20" tabindex="-1"></a>weights[, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(y_obs[<span class="dv">1</span>], <span class="at">mean =</span> phi<span class="sc">*</span>particles[, <span class="dv">1</span>], <span class="at">sd =</span> sigma_mu)  <span class="co"># Importance weights</span></span>
<span id="cb141-21"><a href="sec53.html#cb141-21" tabindex="-1"></a>weightsSt[, <span class="dv">1</span>] <span class="ot">&lt;-</span> weights[, <span class="dv">1</span>] <span class="sc">/</span> <span class="fu">sum</span>(weights[, <span class="dv">1</span>])  <span class="co"># Standardized weights</span></span>
<span id="cb141-22"><a href="sec53.html#cb141-22" tabindex="-1"></a><span class="co"># Sequential updating</span></span>
<span id="cb141-23"><a href="sec53.html#cb141-23" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T) {</span>
<span id="cb141-24"><a href="sec53.html#cb141-24" tabindex="-1"></a>    <span class="co"># Propagate particles</span></span>
<span id="cb141-25"><a href="sec53.html#cb141-25" tabindex="-1"></a>    particles[, t] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="at">mean =</span> particles[, t<span class="dv">-1</span>], <span class="at">sd =</span> sigma_w)</span>
<span id="cb141-26"><a href="sec53.html#cb141-26" tabindex="-1"></a>    <span class="co"># Compute weights</span></span>
<span id="cb141-27"><a href="sec53.html#cb141-27" tabindex="-1"></a>    weights[, t] <span class="ot">&lt;-</span> weightsSt[, t<span class="dv">-1</span>] <span class="sc">*</span> <span class="fu">dnorm</span>(y_obs[t], <span class="at">mean =</span> phi<span class="sc">*</span>particles[, t], <span class="at">sd =</span> sigma_mu) <span class="co"># Recursive weight update</span></span>
<span id="cb141-28"><a href="sec53.html#cb141-28" tabindex="-1"></a>    weightsSt[, t] <span class="ot">&lt;-</span> weights[, t] <span class="sc">/</span> <span class="fu">sum</span>(weights[, t])  <span class="co"># Normalize weights</span></span>
<span id="cb141-29"><a href="sec53.html#cb141-29" tabindex="-1"></a>}</span>
<span id="cb141-30"><a href="sec53.html#cb141-30" tabindex="-1"></a><span class="co"># Estimate the states (weighted mean)</span></span>
<span id="cb141-31"><a href="sec53.html#cb141-31" tabindex="-1"></a>FilterDist <span class="ot">&lt;-</span> <span class="fu">colSums</span>(particles <span class="sc">*</span> weightsSt)</span>
<span id="cb141-32"><a href="sec53.html#cb141-32" tabindex="-1"></a>SDFilterDist <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(particles<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> weightsSt) <span class="sc">-</span> FilterDist<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb141-33"><a href="sec53.html#cb141-33" tabindex="-1"></a><span class="fu">library</span>(dplyr); <span class="fu">library</span>(ggplot2); <span class="fu">library</span>(latex2exp)</span>
<span id="cb141-34"><a href="sec53.html#cb141-34" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_set</span>(<span class="fu">theme_bw</span>())</span>
<span id="cb141-35"><a href="sec53.html#cb141-35" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">t =</span> <span class="dv">1</span><span class="sc">:</span>T, <span class="at">mean =</span> FilterDist, <span class="at">lower =</span> FilterDist <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDist, <span class="at">upper =</span> FilterDist <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDist, <span class="at">theta_true =</span> theta_true)</span>
<span id="cb141-36"><a href="sec53.html#cb141-36" tabindex="-1"></a><span class="co"># Function to plot</span></span>
<span id="cb141-37"><a href="sec53.html#cb141-37" tabindex="-1"></a>plot_filtering_estimates <span class="ot">&lt;-</span> <span class="cf">function</span>(df) {</span>
<span id="cb141-38"><a href="sec53.html#cb141-38" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> t)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper), <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> theta_true), <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean), <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">theta_{t}$&quot;</span>)) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Time&quot;</span>)</span>
<span id="cb141-39"><a href="sec53.html#cb141-39" tabindex="-1"></a>    <span class="fu">print</span>(p)</span>
<span id="cb141-40"><a href="sec53.html#cb141-40" tabindex="-1"></a>}</span>
<span id="cb141-41"><a href="sec53.html#cb141-41" tabindex="-1"></a><span class="fu">plot_filtering_estimates</span>(df)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-22-1.svg" width="672" /></p>
<p>The figure shows the trajectory of the true state vector (black line), the posterior mean (blue line), and the area defined by <span class="math inline">\(\pm2\hat{\sigma}_{\theta}\)</span> (light blue shaded area).</p>
<p>Sequential importance sampling is effective for sampling from the posterior distribution in the short term. However, it is important to note that SIS is a particular case of IS and, consequently, inherits the drawbacks of importance sampling. In particular, the variance of the weights increases exponentially with <span class="math inline">\(t\)</span> <span class="citation">(<a href="#ref-kong1994sequential">Kong, Liu, and Wong 1994</a>)</span>. This implies that, as <span class="math inline">\(t\)</span> increases, the importance weights tend to degenerate in the long run; that is, all probability mass concentrates on a few weights, a phenomenon known as sample impoverishment or weight degeneracy. This is because it is impossible to accurately represent a distribution on a space of arbitrarily high dimension with a sample of fixed, finite size. This phenomenon can be observed, for instance, in the dynamic linear model example, where the highest standardized weight at <span class="math inline">\(t = 50\)</span> is 53%, and 7 out of 50,000 particles account for 87% of the total probability.</p>
<p>Given that, in practice, we are often interested in lower-dimensional marginal distributions, ideas from sampling/importance resampling can be employed. This strategy avoids the accumulation of errors due to resetting the system, although resampling introduces some additional Monte Carlo variation. <span class="citation">Gordon, Salmond, and Smith (<a href="#ref-Gordon1993">1993</a>)</span> proposed the <em>Bootstrap filter</em>, where, at each time step, resampling is performed by drawing <span class="math inline">\(S\)</span> particles from the current set using the standardized weights as probabilities of selection. This ensures that particles with small weights have a low probability of being selected. After resampling, the standardized weights are set equal to <span class="math inline">\(1/S\)</span>. Note that the <em>Bootstrap filter</em> involves multiple iterations of the SIR algorithm, which implies that the resampled trajectories are no longer independent. This multinomial resampling provides an unbiased approximation to the posterior distribution obtained by SIS <span class="citation">(<a href="#ref-doucet2009tutorial">Doucet, Johansen, et al. 2009</a>)</span>.</p>
<p>Algorithm <a href="#algPF">5</a> shows how to perform the <em>particle filter</em>. We set <span class="math inline">\(w_t^{(s)} := w_t(\boldsymbol{\theta}_{0:t}^{(s)})\)</span> to simplify notation <span class="citation">(<a href="#ref-doucet2009tutorial">Doucet, Johansen, et al. 2009</a>)</span>.</p>
<figcaption><b>Algorithm: Particle filter</b></figcaption>
<figure id="alg:PF">
  <pre>
For s=1,2,...,S do
  Sample θ<sub>0</sub><sup>(s)</sup> from q(θ<sub>0</sub>|<b>y<sub>0</sub></b>)
  Calculate the importance weights w<sub>0</sub><sup>(s)</sup> ∝(p(<b>y<sub>0</sub></b>|θ<sub>0</sub><sup>(s)</sup>)π(θ<sub>0</sub><sup>(s)</sup>))/q(θ<sub>0</sub><sup>(s)</sup>|<b>y<sub>0</sub></b>)
End for
Standardize the weights w<sub>0</sub><sup>*(s)</sup> = w<sub>0</sub><sup>(s)</sup>/(∑<sub>h</sub>w<sub>0</sub><sup>(h)</sup>), s=1,2,...,S
Select S particle from {θ<sub>0</sub><sup>(s)</sup>,w<sub>0</sub><sup>*(s)</sup>} to obtain {θ<sub>0</sub><sup>r(s)</sup>,1/S}
for t=1,2,...,T do
  for s=1,2,...,S do
    Draw particles θ<sub>t</sub><sup>(s)</sup> from q<sub>t</sub>(θ<sub>t</sub>|θ<sub>t-1</sub>,<b>y<sub>0</sub></b>)
    Set θ<sub>1:t</sub><sup>(s)</sup> ← (θ<sub>1:t-1</sub><sup>r(s)</sup>, θ<sub>t</sub><sup>(s)</sup>) 
    Compute the weights 
    α<sub>t</sub><sup>(s)</sup> = (p(<b>y<sub>t</sub></b>|θ<sub>t</sub><sup>(s)</sup>)π(θ<sub>t</sub><sup>(s)</sup>|θ<sub>t-1</sub><sup>(s)</sup>))/q(θ<sub>t</sub><sup>(s)</sup>|θ<sub>t-1</sub><sup>(s)</sup>,<b>y<sub>t</sub></b>) 
  End for
  Standardize the weights w<sub>t</sub><sup>*(s)</sup> = w<sub>t</sub><sup>(s)</sup>/(∑<sub>h</sub>w<sub>t</sub><sup>(h)</sup>), s=1,2,...,S
  Select S particle from {θ<sub>1:t</sub><sup>(s)</sup>,w<sub>t</sub><sup>*(s)</sup>} to obtain {θ<sub>1:t</sub><sup>r(s)</sup>,1/S} 
End for
  </pre>
</figure>
<p><strong>Example: Dynamic linear model continues</strong></p>
<p>If we apply the SIS algorithm to the dynamic linear model with a sample size of 200, the algorithm’s performance deteriorates as <span class="math inline">\(t\)</span> increases. This is due to particle degeneration; at <span class="math inline">\(t=200\)</span>, a single particle holds a weight close to 100%.</p>
<p>Let’s perform particle filtering in this example. The following code illustrate the procedure. The figure shows the performance of particle filtering in this example. There is the true state vector (black line), the means based on <span class="math inline">\(\left\{\boldsymbol{\theta}_{1:t}^{(s)},w_t^{*(s)}\right\}\)</span> (blue line) and <span class="math inline">\(\left\{\boldsymbol{\theta}_{1:t}^{r(s)},1/S\right\}\)</span> (purple line), and the area defined by <span class="math inline">\(\pm2\hat{\sigma}_{\theta}\)</span> based on the former (light blue shaded area). Note that the particle filtering algorithm has better performance than the SIS algorithm.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="sec53.html#cb142-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb142-2"><a href="sec53.html#cb142-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">50000</span> <span class="co"># Number of particles</span></span>
<span id="cb142-3"><a href="sec53.html#cb142-3" tabindex="-1"></a>sigma_w <span class="ot">&lt;-</span> <span class="dv">1</span>; sigma_mu <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co">#  # State and observation noises</span></span>
<span id="cb142-4"><a href="sec53.html#cb142-4" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="co"># Coefficient in observation equation</span></span>
<span id="cb142-5"><a href="sec53.html#cb142-5" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">200</span> <span class="co"># Sample size</span></span>
<span id="cb142-6"><a href="sec53.html#cb142-6" tabindex="-1"></a><span class="co"># Simulate true states and observations</span></span>
<span id="cb142-7"><a href="sec53.html#cb142-7" tabindex="-1"></a>theta_true <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T); y_obs <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb142-8"><a href="sec53.html#cb142-8" tabindex="-1"></a>theta_true[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_w) </span>
<span id="cb142-9"><a href="sec53.html#cb142-9" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T) {</span>
<span id="cb142-10"><a href="sec53.html#cb142-10" tabindex="-1"></a>    theta_true[t] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> theta_true[t<span class="dv">-1</span>], <span class="at">sd =</span> sigma_w)</span>
<span id="cb142-11"><a href="sec53.html#cb142-11" tabindex="-1"></a>}</span>
<span id="cb142-12"><a href="sec53.html#cb142-12" tabindex="-1"></a>y_obs <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(T, <span class="at">mean =</span> phi<span class="sc">*</span>theta_true, <span class="at">sd =</span> sigma_mu)</span>
<span id="cb142-13"><a href="sec53.html#cb142-13" tabindex="-1"></a><span class="co"># Particle filtering</span></span>
<span id="cb142-14"><a href="sec53.html#cb142-14" tabindex="-1"></a>particles <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)  <span class="co"># Store particles</span></span>
<span id="cb142-15"><a href="sec53.html#cb142-15" tabindex="-1"></a>particlesT <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)  <span class="co"># Store resampling particles</span></span>
<span id="cb142-16"><a href="sec53.html#cb142-16" tabindex="-1"></a>weights <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)   <span class="co"># Store weights</span></span>
<span id="cb142-17"><a href="sec53.html#cb142-17" tabindex="-1"></a>weightsSt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)   <span class="co"># Store standardized weights</span></span>
<span id="cb142-18"><a href="sec53.html#cb142-18" tabindex="-1"></a>weightsSTT <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">/</span>S, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)   <span class="co"># Store standardized weights</span></span>
<span id="cb142-19"><a href="sec53.html#cb142-19" tabindex="-1"></a>logalphas <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> S, <span class="at">ncol =</span> T)   <span class="co"># Store log incremental weights</span></span>
<span id="cb142-20"><a href="sec53.html#cb142-20" tabindex="-1"></a>particles[, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_w) </span>
<span id="cb142-21"><a href="sec53.html#cb142-21" tabindex="-1"></a>weights[, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(y_obs[<span class="dv">1</span>], <span class="at">mean =</span> phi<span class="sc">*</span>particles[, <span class="dv">1</span>], <span class="at">sd =</span> sigma_mu)  <span class="co"># Importance weights</span></span>
<span id="cb142-22"><a href="sec53.html#cb142-22" tabindex="-1"></a>weightsSt[, <span class="dv">1</span>] <span class="ot">&lt;-</span> weights[, <span class="dv">1</span>] <span class="sc">/</span> <span class="fu">sum</span>(weights[, <span class="dv">1</span>])  <span class="co"># Normalize weights</span></span>
<span id="cb142-23"><a href="sec53.html#cb142-23" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="at">size =</span> S, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> weightsSt[, <span class="dv">1</span>]) <span class="co"># Resample </span></span>
<span id="cb142-24"><a href="sec53.html#cb142-24" tabindex="-1"></a>particles[, <span class="dv">1</span>] <span class="ot">&lt;-</span> particles[ind, <span class="dv">1</span>] <span class="co"># Resampled particles</span></span>
<span id="cb142-25"><a href="sec53.html#cb142-25" tabindex="-1"></a>particlesT[, <span class="dv">1</span>] <span class="ot">&lt;-</span> particles[, <span class="dv">1</span>] <span class="co"># Resampled particles</span></span>
<span id="cb142-26"><a href="sec53.html#cb142-26" tabindex="-1"></a><span class="co"># Sequential updating</span></span>
<span id="cb142-27"><a href="sec53.html#cb142-27" tabindex="-1"></a>pb <span class="ot">&lt;-</span> <span class="fu">winProgressBar</span>(<span class="at">title =</span> <span class="st">&quot;progress bar&quot;</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> T, <span class="at">width =</span> <span class="dv">300</span>)</span>
<span id="cb142-28"><a href="sec53.html#cb142-28" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T) {</span>
<span id="cb142-29"><a href="sec53.html#cb142-29" tabindex="-1"></a>    particles[, t] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="at">mean =</span> particles[, t<span class="dv">-1</span>], <span class="at">sd =</span> sigma_w)</span>
<span id="cb142-30"><a href="sec53.html#cb142-30" tabindex="-1"></a>    logalphas[, t] <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(y_obs[t], <span class="at">mean =</span> phi<span class="sc">*</span>particles[, t], <span class="at">sd =</span> sigma_mu, <span class="at">log =</span> <span class="cn">TRUE</span>) </span>
<span id="cb142-31"><a href="sec53.html#cb142-31" tabindex="-1"></a>    weights[, t] <span class="ot">&lt;-</span> <span class="fu">exp</span>(logalphas[, t])</span>
<span id="cb142-32"><a href="sec53.html#cb142-32" tabindex="-1"></a>    weightsSt[, t] <span class="ot">&lt;-</span> weights[, t] <span class="sc">/</span> <span class="fu">sum</span>(weights[, t])</span>
<span id="cb142-33"><a href="sec53.html#cb142-33" tabindex="-1"></a>    <span class="cf">if</span>(t <span class="sc">&lt;</span> T){</span>
<span id="cb142-34"><a href="sec53.html#cb142-34" tabindex="-1"></a>        ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="at">size =</span> S, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> weightsSt[, t])</span>
<span id="cb142-35"><a href="sec53.html#cb142-35" tabindex="-1"></a>        particles[, <span class="dv">1</span><span class="sc">:</span>t] <span class="ot">&lt;-</span> particles[ind, <span class="dv">1</span><span class="sc">:</span>t]</span>
<span id="cb142-36"><a href="sec53.html#cb142-36" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb142-37"><a href="sec53.html#cb142-37" tabindex="-1"></a>        ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="at">size =</span> S, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> weightsSt[, t])</span>
<span id="cb142-38"><a href="sec53.html#cb142-38" tabindex="-1"></a>        particlesT[, <span class="dv">1</span><span class="sc">:</span>t] <span class="ot">&lt;-</span> particles[ind, <span class="dv">1</span><span class="sc">:</span>t]</span>
<span id="cb142-39"><a href="sec53.html#cb142-39" tabindex="-1"></a>    }</span>
<span id="cb142-40"><a href="sec53.html#cb142-40" tabindex="-1"></a>    <span class="fu">setWinProgressBar</span>(pb, t, <span class="at">title=</span><span class="fu">paste</span>( <span class="fu">round</span>(t<span class="sc">/</span>T<span class="sc">*</span><span class="dv">100</span>, <span class="dv">0</span>), <span class="st">&quot;% done&quot;</span>))</span>
<span id="cb142-41"><a href="sec53.html#cb142-41" tabindex="-1"></a>}</span>
<span id="cb142-42"><a href="sec53.html#cb142-42" tabindex="-1"></a><span class="fu">close</span>(pb)</span></code></pre></div>
<pre><code>## NULL</code></pre>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="sec53.html#cb144-1" tabindex="-1"></a>FilterDist <span class="ot">&lt;-</span> <span class="fu">colSums</span>(particles <span class="sc">*</span> weightsSt)</span>
<span id="cb144-2"><a href="sec53.html#cb144-2" tabindex="-1"></a>SDFilterDist <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(particles<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> weightsSt) <span class="sc">-</span> FilterDist<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb144-3"><a href="sec53.html#cb144-3" tabindex="-1"></a>FilterDistT <span class="ot">&lt;-</span> <span class="fu">colSums</span>(particlesT <span class="sc">*</span> weightsSTT)</span>
<span id="cb144-4"><a href="sec53.html#cb144-4" tabindex="-1"></a>SDFilterDistT <span class="ot">&lt;-</span> (<span class="fu">colSums</span>(particlesT<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> weightsSTT) <span class="sc">-</span> FilterDistT<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb144-5"><a href="sec53.html#cb144-5" tabindex="-1"></a>MargLik <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(weights)</span>
<span id="cb144-6"><a href="sec53.html#cb144-6" tabindex="-1"></a><span class="fu">plot</span>(MargLik, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-23-1.svg" width="672" /></p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="sec53.html#cb145-1" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb145-2"><a href="sec53.html#cb145-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb145-3"><a href="sec53.html#cb145-3" tabindex="-1"></a><span class="fu">require</span>(latex2exp)</span>
<span id="cb145-4"><a href="sec53.html#cb145-4" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_set</span>(<span class="fu">theme_bw</span>())</span>
<span id="cb145-5"><a href="sec53.html#cb145-5" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">t =</span> <span class="dv">1</span><span class="sc">:</span>T, <span class="at">mean =</span> FilterDist, <span class="at">lower =</span> FilterDist <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDist, <span class="at">upper =</span> FilterDist<span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDist, <span class="at">meanT =</span> FilterDistT, <span class="at">lowerT =</span> FilterDistT <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDistT,</span>
<span id="cb145-6"><a href="sec53.html#cb145-6" tabindex="-1"></a><span class="at">upperT =</span> FilterDistT <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>SDFilterDistT, <span class="at">x_true =</span> theta_true)</span>
<span id="cb145-7"><a href="sec53.html#cb145-7" tabindex="-1"></a>plot_filtering_estimates <span class="ot">&lt;-</span> <span class="cf">function</span>(df) {</span>
<span id="cb145-8"><a href="sec53.html#cb145-8" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> t)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper), <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> x_true), <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean), <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb145-9"><a href="sec53.html#cb145-9" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> meanT), <span class="at">colour =</span> <span class="st">&quot;purple&quot;</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span>     <span class="fu">ylab</span>(<span class="fu">TeX</span>(<span class="st">&quot;$</span><span class="sc">\\</span><span class="st">theta_{t}$&quot;</span>)) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Time&quot;</span>)</span>
<span id="cb145-10"><a href="sec53.html#cb145-10" tabindex="-1"></a>    <span class="fu">print</span>(p)</span>
<span id="cb145-11"><a href="sec53.html#cb145-11" tabindex="-1"></a>}</span>
<span id="cb145-12"><a href="sec53.html#cb145-12" tabindex="-1"></a><span class="fu">plot_filtering_estimates</span>(df)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-23-2.svg" width="672" /></p>
<p>Algorithm <a href="#algPF">5</a> performs resampling at every time step. However, it is common to perform resampling only when the effective sample size of the particles (<span class="math inline">\(ESS = (\sum_{s=1}^S (w_t^{*(s)})^{2})^{-1}\)</span>) falls below a specific threshold, such as 50% of the initial number of particles. Note that when <span class="math inline">\(w_t^{*(s)} = 1/S\)</span>, the effective sample size is <span class="math inline">\(S\)</span>, the total number of particles. Additionally, we should use <span class="math inline">\(\left\{\boldsymbol{\theta}_{1:t}^{(s)}, w_t^{*(s)}\right\}\)</span> to estimate the posterior distribution, as it results in lower Monte Carlo error compared to calculations based on <span class="math inline">\(\left\{\boldsymbol{\theta}_{1:t}^{r(s)}, 1/S\right\}\)</span> <span class="citation">(<a href="#ref-cappe2007overview">Olivier Cappé, Godsill, and Moulines 2007</a>)</span>. Finally, an estimate of the marginal likelihood can be obtained using<br />
<span class="math display">\[
\hat{p}(y_t) = \frac{1}{S}\sum_{s=1}^S w_t^{(s)}.
\]</span></p>
<p><em>Particle filtering</em> offers several advantages, such as being quick and easy to implement, its modularity—allowing one to simply adjust the expressions for the importance distribution and weights when changing the problem—and its suitability for parallel algorithms. Moreover, it enables straightforward sequential inference for very complex models.</p>
<p>However, there are also disadvantages. The resampling step introduces extra Monte Carlo variability. Using the state transition (prior) density as the importance distribution often leads to poor performance, manifested in a lack of robustness with respect to the observed sequence. For instance, performance deteriorates when outliers occur in the data or when the variance of the observation noise is small. Furthermore, the procedure is not well suited for sampling from <span class="math inline">\(\pi(\boldsymbol{\theta}_{0:t} \mid y_{1:t})\)</span> because most particles originate from the same ancestor.</p>
<p>Alternative resampling approaches, such as residual resampling <span class="citation">(<a href="#ref-Liu1995">Liu and Chen 1995</a>)</span> and systematic resampling <span class="citation">(<a href="#ref-Carpenter1999">J. Carpenter, Clifford, and Fearnhead 1999</a>)</span>, preserve unbiasedness while reducing variance. Additionally, auxiliary particle filtering <span class="citation">(<a href="#ref-Cappe2007">O. Cappé, Godsill, and Moulines 2007</a>)</span> can help decrease Monte Carlo variability.</p>
<p>Lastly, estimating fixed parameters such as <span class="math inline">\(\sigma_w^2\)</span>, <span class="math inline">\(\sigma_{\mu}^2\)</span>, and <span class="math inline">\(\phi\)</span> in the dynamic linear model poses a challenge. Various methods exist to address this issue; see <span class="citation">N. Kantas et al. (<a href="#ref-Kantas2009">2009</a>)</span>, <span class="citation">Nikolas Kantas et al. (<a href="#ref-kantas2015particle">2015</a>)</span> for a comprehensive review and <span class="citation">C. Andrieu, Doucet, and Holenstein (<a href="#ref-Andrieu2010">2010</a>)</span> for a seminal work in <em>particle MCMC</em> methods.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Andrieu2010" class="csl-entry">
Andrieu, C., A. Doucet, and R. Holenstein. 2010. <span>“Particle Markov Chain Monte Carlo Methods.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 72 (3): 269–342.
</div>
<div id="ref-Cappe2007" class="csl-entry">
Cappé, O., S. J. Godsill, and E. Moulines. 2007. <span>“An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.”</span> <em>Proceedings of the IEEE</em> 95 (5): 899–924.
</div>
<div id="ref-cappe2007overview" class="csl-entry">
Cappé, Olivier, Simon J Godsill, and Eric Moulines. 2007. <span>“An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo.”</span> <em>Proceedings of the IEEE</em> 95 (5): 899–924.
</div>
<div id="ref-Carpenter1999" class="csl-entry">
Carpenter, J., P. Clifford, and P. Fearnhead. 1999. <span>“Improved Particle Filter for Nonlinear Problems.”</span> <em>IEE Proceedings-Radar, Sonar and Navigation</em> 146 (1): 2–7.
</div>
<div id="ref-doucet2001introduction" class="csl-entry">
Doucet, Arnaud, Nando De Freitas, and Neil Gordon. 2001. <span>“An Introduction to Sequential Monte Carlo Methods.”</span> <em>Sequential Monte Carlo Methods in Practice</em>, 3–14.
</div>
<div id="ref-doucet2009tutorial" class="csl-entry">
Doucet, Arnaud, Adam M Johansen, et al. 2009. <span>“A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later.”</span> <em>Handbook of Nonlinear Filtering</em> 12 (656-704): 3.
</div>
<div id="ref-Gordon1993" class="csl-entry">
Gordon, N. J., D. J. Salmond, and A. F. Smith. 1993. <span>“Novel Approach to Nonlinear/Non-Gaussian Bayesian State Estimation.”</span> <em>IEE Proceedings F (Radar and Signal Processing)</em> 140 (2): 107–13.
</div>
<div id="ref-handschin1969monte" class="csl-entry">
Handschin, Johannes Edmund, and David Q Mayne. 1969. <span>“Monte Carlo Techniques to Estimate the Conditional Expectation in Multi-Stage Non-Linear Filtering.”</span> <em>International Journal of Control</em> 9 (5): 547–59.
</div>
<div id="ref-Kantas2009" class="csl-entry">
Kantas, N., A. Doucet, S. S. Singh, and J. M. Maciejowski. 2009. <span>“An Overview of Sequential Monte Carlo Methods for Parameter Estimation in General State–Space Models.”</span> <em>IFAC Proceedings Volumes</em> 42 (10): 774–85.
</div>
<div id="ref-kantas2015particle" class="csl-entry">
Kantas, Nikolas, Arnaud Doucet, Sumeetpal S Singh, Jan Maciejowski, Nicolas Chopin, et al. 2015. <span>“On Particle Methods for Parameter Estimation in State-Space Models.”</span> <em>Statistical Science</em> 30 (3): 328–51.
</div>
<div id="ref-kong1994sequential" class="csl-entry">
Kong, Augustine, Jun S Liu, and Wing Hung Wong. 1994. <span>“Sequential Imputations and Bayesian Missing Data Problems.”</span> <em>Journal of the American Statistical Association</em> 89 (425): 278–88.
</div>
<div id="ref-Liu1995" class="csl-entry">
Liu, J. S., and R. Chen. 1995. <span>“Blind Deconvolution via Sequential Imputations.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 567–76.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="24">
<li id="fn24"><p><span class="math inline">\(\boldsymbol{\theta}_{0}\)</span> comes from the given distribution <span class="math inline">\(\pi(\boldsymbol{\theta}_{0})\)</span>.<a href="sec53.html#fnref24" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec52.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec54.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/05-Simulation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
