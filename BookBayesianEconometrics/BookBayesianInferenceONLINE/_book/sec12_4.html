<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.4 Gaussian processes | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="12.4 Gaussian processes | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.4 Gaussian processes | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12_3.html"/>
<link rel="next" href="sec12_5.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12_4" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Gaussian processes<a href="sec12_4.html#sec12_4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A Gaussian Process (GP) is an infinite collection of random variables, any finite subset of which follows a joint Gaussian distribution. A GP is fully specified by its mean function and covariance function, that is,</p>
<p><span class="math display">\[
f(\mathbf{x}) \sim \text{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}&#39;)),
\]</span></p>
<p>where <span class="math inline">\(m(\mathbf{x}) = \mathbb{E}[f(\mathbf{x})]\)</span> and <span class="math inline">\(k(\mathbf{x}, \mathbf{x}&#39;) = \mathbb{E}[(f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x}&#39;) - m(\mathbf{x}&#39;))]\)</span>.<br />
It is common to assume <span class="math inline">\(m(\mathbf{x}) = 0\)</span> to simplify calculations, although this is not required.</p>
<p>Perhaps the most commonly used covariance function in Gaussian Processes (GPs) is the <em>squared exponential</em> kernel (or <em>radial basis function</em>) <span class="citation">(<a href="#ref-jacobi2024posterior">Jacobi et al. 2024</a>)</span>, defined as</p>
<p><span class="math display">\[
k(\mathbf{x}, \mathbf{x}&#39;) = \sigma_f^2 \exp\left(-\frac{1}{2l^2} \|\mathbf{x} - \mathbf{x}&#39;\|^2\right),
\]</span></p>
<p>where <span class="math inline">\(\sigma_f^2\)</span> is the signal variance, which controls the vertical variation (amplitude) of the function, <span class="math inline">\(l\)</span> is the length-scale parameter, which determines how quickly the function varies with features distance, and <span class="math inline">\(\|\mathbf{x} - \mathbf{x}&#39;\|^2\)</span> is the squared Euclidean distance between the feature vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}&#39;\)</span>.</p>
<p>The squared exponential kernel implies that the function is infinitely differentiable, leading to very smooth function draws. While this smoothness may be desirable in some applications, it can be too restrictive in others. Alternative kernels like the Matérn class allow for more flexibility by controlling the degree of differentiability <span class="citation">(<a href="#ref-rasmussen2006gaussian">Rasmussen and Williams 2006</a>)</span>.</p>
<p>A GP can be interpreted as a prior distribution over a space of functions. The starting point in working with GPs is the specification of this prior before any data are observed. The following code illustrates five sample paths drawn from a GP with a squared exponential kernel, assuming a signal variance <span class="math inline">\(\sigma_f^2 = 1\)</span> and a length-scale <span class="math inline">\(l = 0.2\)</span>, evaluated over a grid of input values <span class="math inline">\(x \in [0,1]\)</span>. A small <em>jitter term</em> is added to the covariance matrix to ensure numerical stability during simulation. The following figure displays the five realizations drawn from the Gaussian Process.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="sec12_4.html#cb40-1" tabindex="-1"></a><span class="do">####### Gaussian Process #######</span></span>
<span id="cb40-2"><a href="sec12_4.html#cb40-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb40-3"><a href="sec12_4.html#cb40-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb40-4"><a href="sec12_4.html#cb40-4" tabindex="-1"></a></span>
<span id="cb40-5"><a href="sec12_4.html#cb40-5" tabindex="-1"></a><span class="fu">library</span>(ggplot2); <span class="fu">library</span>(dplyr)</span>
<span id="cb40-6"><a href="sec12_4.html#cb40-6" tabindex="-1"></a><span class="fu">library</span>(tidyr); <span class="fu">library</span>(MASS)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="sec12_4.html#cb43-1" tabindex="-1"></a><span class="co"># Simulation setup</span></span>
<span id="cb43-2"><a href="sec12_4.html#cb43-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb43-3"><a href="sec12_4.html#cb43-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> n)</span>
<span id="cb43-4"><a href="sec12_4.html#cb43-4" tabindex="-1"></a>sigma_f <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb43-5"><a href="sec12_4.html#cb43-5" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb43-6"><a href="sec12_4.html#cb43-6" tabindex="-1"></a>sigma_n <span class="ot">&lt;-</span> <span class="fl">1e-8</span></span>
<span id="cb43-7"><a href="sec12_4.html#cb43-7" tabindex="-1"></a></span>
<span id="cb43-8"><a href="sec12_4.html#cb43-8" tabindex="-1"></a><span class="co"># Squared Exponential Kernel function</span></span>
<span id="cb43-9"><a href="sec12_4.html#cb43-9" tabindex="-1"></a>SE_kernel <span class="ot">&lt;-</span> <span class="cf">function</span>(x1, x2, sigma_f, l) {</span>
<span id="cb43-10"><a href="sec12_4.html#cb43-10" tabindex="-1"></a>    <span class="fu">outer</span>(x1, x2, <span class="cf">function</span>(a, b) sigma_f<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> (a <span class="sc">-</span> b)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> l<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb43-11"><a href="sec12_4.html#cb43-11" tabindex="-1"></a>}</span>
<span id="cb43-12"><a href="sec12_4.html#cb43-12" tabindex="-1"></a></span>
<span id="cb43-13"><a href="sec12_4.html#cb43-13" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">SE_kernel</span>(x, x, sigma_f, l) <span class="sc">+</span> <span class="fu">diag</span>(sigma_n, n)</span>
<span id="cb43-14"><a href="sec12_4.html#cb43-14" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">5</span>, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, n), <span class="at">Sigma =</span> K)</span>
<span id="cb43-15"><a href="sec12_4.html#cb43-15" tabindex="-1"></a></span>
<span id="cb43-16"><a href="sec12_4.html#cb43-16" tabindex="-1"></a><span class="co"># Transpose and rename columns to f1, f2, ..., f5</span></span>
<span id="cb43-17"><a href="sec12_4.html#cb43-17" tabindex="-1"></a>samples_t <span class="ot">&lt;-</span> <span class="fu">t</span>(samples)</span>
<span id="cb43-18"><a href="sec12_4.html#cb43-18" tabindex="-1"></a><span class="fu">colnames</span>(samples_t) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;f&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb43-19"><a href="sec12_4.html#cb43-19" tabindex="-1"></a></span>
<span id="cb43-20"><a href="sec12_4.html#cb43-20" tabindex="-1"></a><span class="co"># Convert to tidy data frame</span></span>
<span id="cb43-21"><a href="sec12_4.html#cb43-21" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, samples_t) <span class="sc">|&gt;</span></span>
<span id="cb43-22"><a href="sec12_4.html#cb43-22" tabindex="-1"></a><span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">-</span>x, <span class="at">names_to =</span> <span class="st">&quot;draw&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>)</span>
<span id="cb43-23"><a href="sec12_4.html#cb43-23" tabindex="-1"></a></span>
<span id="cb43-24"><a href="sec12_4.html#cb43-24" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb43-25"><a href="sec12_4.html#cb43-25" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> value, <span class="at">color =</span> draw)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb43-26"><a href="sec12_4.html#cb43-26" tabindex="-1"></a><span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Simulated Gaussian Process Draws&quot;</span>, <span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;f(x)&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Function&quot;</span> ) <span class="sc">+</span> <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;top&quot;</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-1.svg" width="672" /></p>
<p>Thus, for any finite set of feature points <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\)</span>, the corresponding function values follow a multivariate Gaussian distribution:</p>
<p><span class="math display">\[
\mathbf{f} =
\begin{bmatrix}
    f(\mathbf{x}_1) \\
    f(\mathbf{x}_2) \\
    \vdots \\
    f(\mathbf{x}_N)
\end{bmatrix}
\sim \mathcal{N}(\mathbf{0}, \mathbf{K}(\mathbf{X}, \mathbf{X})),
\]</span></p>
<p>where the <span class="math inline">\((i,j)\)</span>-th entry of the covariance matrix <span class="math inline">\(\mathbf{K}(\mathbf{X}, \mathbf{X})\)</span> is given by <span class="math inline">\(\mathbf{K}_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)\)</span>.</p>
<p>If we are interested in the properties of a function evaluated at a finite set of input points <span class="math inline">\(\{(f_i, x_i)\}_{i=1}^N\)</span>, inference can be performed using only those points, effectively disregarding the uncountably infinite values the function may take elsewhere.</p>
<p>The following code illustrates how to perform inference for a GP given four observed points <span class="math inline">\(\{(f_i, x_i)\}_{i=1}^4\)</span>, assuming that the true underlying process is</p>
<p><span class="math display">\[
f_i = \sin(2\pi x_i).
\]</span></p>
<p>The inference is based on the properties of the conditional Gaussian distribution (see below). The figure shows that the posterior mean (solid blue line) interpolates the observed points (red dots). Moreover, the level of uncertainty (light blue shaded area) increases in regions that are farther from the observed inputs, where the posterior mean tends to deviate more from the true underlying function (dashed green line).</p>
<p>In situations where the input locations can be selected, such as in experimental designs, <em>active learning strategies</em> can be employed to choose the points that minimize predictive uncertainty. This is typically achieved by optimizing an <em>acquisition function</em> that quantifies the expected informativeness of candidate locations <span class="citation">(<a href="#ref-settles2012active">Settles 2012</a>)</span>.</p>
<p>Consequently, GPs play a central role in <em>Bayesian optimization</em>, a stochastic method for finding the maximum of expensive or unknown objective functions. In this approach, a prior is placed over the objective function, which is then updated using observed data to form a posterior distribution over possible functions. This posterior guides the selection of new input points by balancing exploration and exploitation through the acquisition function <span class="citation">(<a href="#ref-brochu2010tutorial">Brochu, Cora, and Freitas 2010</a>)</span>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="sec12_4.html#cb44-1" tabindex="-1"></a><span class="do">####### Gaussian Process #######</span></span>
<span id="cb44-2"><a href="sec12_4.html#cb44-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb44-3"><a href="sec12_4.html#cb44-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2); <span class="fu">library</span>(MASS)</span>
<span id="cb44-4"><a href="sec12_4.html#cb44-4" tabindex="-1"></a><span class="co"># Define the squared exponential kernel</span></span>
<span id="cb44-5"><a href="sec12_4.html#cb44-5" tabindex="-1"></a>SE_kernel <span class="ot">&lt;-</span> <span class="cf">function</span>(x1, x2, sigma_f, l) {</span>
<span id="cb44-6"><a href="sec12_4.html#cb44-6" tabindex="-1"></a>    <span class="fu">outer</span>(x1, x2, <span class="cf">function</span>(a, b) sigma_f<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> (a <span class="sc">-</span> b)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> l<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb44-7"><a href="sec12_4.html#cb44-7" tabindex="-1"></a>}</span>
<span id="cb44-8"><a href="sec12_4.html#cb44-8" tabindex="-1"></a><span class="co"># Define the input space and observed points</span></span>
<span id="cb44-9"><a href="sec12_4.html#cb44-9" tabindex="-1"></a>x_star <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb44-10"><a href="sec12_4.html#cb44-10" tabindex="-1"></a>x0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>)</span>
<span id="cb44-11"><a href="sec12_4.html#cb44-11" tabindex="-1"></a>y0 <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> x0)</span>
<span id="cb44-12"><a href="sec12_4.html#cb44-12" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb44-13"><a href="sec12_4.html#cb44-13" tabindex="-1"></a>sigma_f <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb44-14"><a href="sec12_4.html#cb44-14" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb44-15"><a href="sec12_4.html#cb44-15" tabindex="-1"></a>sigma_n <span class="ot">&lt;-</span> <span class="fl">1e-8</span>  <span class="co"># Jitter term for stability</span></span>
<span id="cb44-16"><a href="sec12_4.html#cb44-16" tabindex="-1"></a><span class="co"># Compute covariance matrices</span></span>
<span id="cb44-17"><a href="sec12_4.html#cb44-17" tabindex="-1"></a>K_x0x0 <span class="ot">&lt;-</span> <span class="fu">SE_kernel</span>(x0, x0, sigma_f, l) <span class="sc">+</span> <span class="fu">diag</span>(sigma_n, <span class="fu">length</span>(x0))</span>
<span id="cb44-18"><a href="sec12_4.html#cb44-18" tabindex="-1"></a>K_xstarx0 <span class="ot">&lt;-</span> <span class="fu">SE_kernel</span>(x_star, x0, sigma_f, l)</span>
<span id="cb44-19"><a href="sec12_4.html#cb44-19" tabindex="-1"></a>K_xstarxstar <span class="ot">&lt;-</span> <span class="fu">SE_kernel</span>(x_star, x_star, sigma_f, l) <span class="sc">+</span> <span class="fu">diag</span>(sigma_n, <span class="fu">length</span>(x_star))</span>
<span id="cb44-20"><a href="sec12_4.html#cb44-20" tabindex="-1"></a><span class="co"># Compute posterior mean and covariance</span></span>
<span id="cb44-21"><a href="sec12_4.html#cb44-21" tabindex="-1"></a>K_inv <span class="ot">&lt;-</span> <span class="fu">solve</span>(K_x0x0)</span>
<span id="cb44-22"><a href="sec12_4.html#cb44-22" tabindex="-1"></a>posterior_mean <span class="ot">&lt;-</span> K_xstarx0 <span class="sc">%*%</span> K_inv <span class="sc">%*%</span> y0</span>
<span id="cb44-23"><a href="sec12_4.html#cb44-23" tabindex="-1"></a>posterior_cov <span class="ot">&lt;-</span> K_xstarxstar <span class="sc">-</span> K_xstarx0 <span class="sc">%*%</span> K_inv <span class="sc">%*%</span> <span class="fu">t</span>(K_xstarx0)</span>
<span id="cb44-24"><a href="sec12_4.html#cb44-24" tabindex="-1"></a><span class="co"># Sample from the posterior</span></span>
<span id="cb44-25"><a href="sec12_4.html#cb44-25" tabindex="-1"></a>sample_draw <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> x_star) </span>
<span id="cb44-26"><a href="sec12_4.html#cb44-26" tabindex="-1"></a><span class="co"># Compute 95% intervals</span></span>
<span id="cb44-27"><a href="sec12_4.html#cb44-27" tabindex="-1"></a>posterior_sd <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(posterior_cov))</span>
<span id="cb44-28"><a href="sec12_4.html#cb44-28" tabindex="-1"></a>lower <span class="ot">&lt;-</span> posterior_mean <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> posterior_sd</span>
<span id="cb44-29"><a href="sec12_4.html#cb44-29" tabindex="-1"></a>upper <span class="ot">&lt;-</span> posterior_mean <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> posterior_sd</span>
<span id="cb44-30"><a href="sec12_4.html#cb44-30" tabindex="-1"></a><span class="co"># Data frame for plotting</span></span>
<span id="cb44-31"><a href="sec12_4.html#cb44-31" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb44-32"><a href="sec12_4.html#cb44-32" tabindex="-1"></a><span class="at">x =</span> x_star,</span>
<span id="cb44-33"><a href="sec12_4.html#cb44-33" tabindex="-1"></a><span class="at">mean =</span> posterior_mean,</span>
<span id="cb44-34"><a href="sec12_4.html#cb44-34" tabindex="-1"></a><span class="at">lower =</span> lower,</span>
<span id="cb44-35"><a href="sec12_4.html#cb44-35" tabindex="-1"></a><span class="at">upper =</span> upper,</span>
<span id="cb44-36"><a href="sec12_4.html#cb44-36" tabindex="-1"></a><span class="at">sample =</span> sample_draw</span>
<span id="cb44-37"><a href="sec12_4.html#cb44-37" tabindex="-1"></a>)</span>
<span id="cb44-38"><a href="sec12_4.html#cb44-38" tabindex="-1"></a>obs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x0, <span class="at">y =</span> y0)</span>
<span id="cb44-39"><a href="sec12_4.html#cb44-39" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb44-40"><a href="sec12_4.html#cb44-40" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper), <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean), <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> sample), <span class="at">color =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">data =</span> obs, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Gaussian Process with Conditioning Points&quot;</span>, <span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;f(x)&quot;</span>, <span class="at">caption =</span> <span class="st">&quot;Blue: Posterior mean | Light blue: 95% interval | Dashed green: Population | Red: Observed points&quot;</span> ) <span class="sc">+</span> <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-5-1.svg" width="672" /></p>
<p>In practice, we have an observed dataset <span class="math inline">\(\{(y_i, \mathbf{x}_i)\}_{i=1}^N\)</span> such that</p>
<p><span class="math display">\[
y_i = f(\mathbf{x}_i) + \mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mu_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)\)</span>. This means that <span class="math inline">\(y_i\)</span> is a noisy observation of <span class="math inline">\(f(\mathbf{x}_i)\)</span>.</p>
<p>Thus, the marginal distribution of the observed outputs is</p>
<p><span class="math display">\[
\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{K}(\mathbf{X}, \mathbf{X}) + \sigma^2 \mathbf{I}_N),
\]</span></p>
<p>where <span class="math inline">\(\mathbf{K}(\mathbf{X}, \mathbf{X})\)</span> is the covariance matrix generated by the GP kernel evaluated at the training inputs.</p>
<p>Note that this implies the log marginal likelihood is given by</p>
<p><span class="math display">\[
\log p(\mathbf{y} \mid \mathbf{X}) = -\frac{1}{2} \mathbf{y}^{\top} (\mathbf{K} + \sigma^2 \mathbf{I}_N)^{-1} \mathbf{y}
- \frac{1}{2} \log \left| \mathbf{K} + \sigma^2 \mathbf{I}_N \right|
- \frac{N}{2} \log 2\pi.
\]</span></p>
<p>We can adopt an empirical Bayes approach to estimate the hyperparameters of the GP prior by maximizing the log marginal likelihood with respect to the kernel parameters (e.g., <span class="math inline">\(\sigma_f^2\)</span>, <span class="math inline">\(l\)</span>) and the noise variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>To make predictions at a new set of features <span class="math inline">\(\mathbf{X}_*\)</span>, we consider the joint distribution:</p>
<p><span class="math display">\[
\begin{bmatrix}
\mathbf{y} \\
\mathbf{f}_*
\end{bmatrix}
\sim \mathcal{N}\left(
\mathbf{0},
\begin{bmatrix}
\mathbf{K}(\mathbf{X}, \mathbf{X}) + \sigma^2 \mathbf{I}_N &amp; \mathbf{K}(\mathbf{X}, \mathbf{X}_*) \\
\mathbf{K}(\mathbf{X}_*, \mathbf{X}) &amp; \mathbf{K}(\mathbf{X}_*, \mathbf{X}_*)
\end{bmatrix}
\right).
\]</span></p>
<p>Using the conditional distribution of a multivariate Gaussian, the <em>posterior predictive distribution</em> <span class="citation">(<a href="#ref-rasmussen2006gaussian">Rasmussen and Williams 2006</a>)</span> is:</p>
<p><span class="math display">\[
\mathbf{f}_* \mid \mathbf{y} \sim \mathcal{N}(\bar{\mathbf{f}}_*, \operatorname{cov}(\mathbf{f}_*)),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\bar{\mathbf{f}}_* &amp;= \mathbb{E}[\mathbf{f}_* \mid \mathbf{y}, \mathbf{X}, \mathbf{X}_*]
= \mathbf{K}(\mathbf{X}_*, \mathbf{X}) [\mathbf{K}(\mathbf{X}, \mathbf{X}) + \sigma^2 \mathbf{I}_N]^{-1} \mathbf{y}, \\
\operatorname{cov}(\mathbf{f}_*) &amp;= \mathbf{K}(\mathbf{X}_*, \mathbf{X}_*) -
\mathbf{K}(\mathbf{X}_*, \mathbf{X}) [\mathbf{K}(\mathbf{X}, \mathbf{X}) + \sigma^2 \mathbf{I}_N]^{-1}
\mathbf{K}(\mathbf{X}, \mathbf{X}_*).
\end{aligned}
\]</span></p>
<p>Therefore, Gaussian Process (GP) regression provides a flexible and efficient nonparametric framework for predicting unobserved responses, with accuracy that improves as more data become available. GPs are widely used due to their favorable computational properties, including the availability of closed-form expressions, and posterior consistency under mild conditions <span class="citation">(<a href="#ref-choi2007posterior">Choi and Schervish 2007</a>; <a href="#ref-stuart2018posterior">Stuart and Teckentrup 2018</a>)</span>. Moreover, predictive performance can be further enhanced by incorporating derivative information, as the derivative of a GP is itself a GP <span class="citation">(<a href="#ref-solak2003derivative">Solak et al. 2003</a>; <a href="#ref-jacobi2024posterior">Jacobi et al. 2024</a>)</span>.</p>
<p>However, a major limitation of GPs is the need to invert an <span class="math inline">\(N \times N\)</span> covariance matrix, which requires <span class="math inline">\(O(N^3)\)</span> computational operations, making them computationally expensive for large datasets. To address this, several scalable methods have been proposed that reduce the computational burden. For instance, <span class="citation">Wilson and Nickisch (<a href="#ref-wilson2015kernel">2015</a>)</span>, <span class="citation">Gardner et al. (<a href="#ref-gardner2018product">2018</a>)</span> and <span class="citation">Pleiss et al. (<a href="#ref-pleiss2018constant">2018</a>)</span> develop algorithms that reduce complexity to <span class="math inline">\(O(N)\)</span>.</p>
<p><strong>Example: Simulation exercise to study GP performance</strong></p>
<p>We simulate the process</p>
<p><span class="math display">\[
f_i = \sin(2\pi x_{i1}) + \cos(2\pi x_{i2}) + \sin(x_{i1} x_{i2}),
\]</span></p>
<p>where <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span> are independently drawn from a uniform distribution on the interval <span class="math inline">\([0, 1]\)</span>, for <span class="math inline">\(i = 1, 2, \dots, 100\)</span>.</p>
<p>We use the <em>DiceKriging</em> package in <strong>R</strong> to estimate and make predictions using a Gaussian Process. This package applies maximum likelihood estimation to infer the length-scale parameters (<span class="math inline">\(l_k\)</span>) and the signal variance (<span class="math inline">\(\sigma_f^2\)</span>). Note that there are two separate length-scale parameters, one for each input variable.</p>
<p>The following code illustrates how to carry out this example, and the following figure displays a 3D plot with the observed points and the posterior mean surface. The package also provides pointwise credible intervals for the predictions.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="sec12_4.html#cb45-1" tabindex="-1"></a><span class="do">####### Gaussian Process #######</span></span>
<span id="cb45-2"><a href="sec12_4.html#cb45-2" tabindex="-1"></a><span class="co"># Load required packages</span></span>
<span id="cb45-3"><a href="sec12_4.html#cb45-3" tabindex="-1"></a><span class="fu">library</span>(DiceKriging)</span>
<span id="cb45-4"><a href="sec12_4.html#cb45-4" tabindex="-1"></a><span class="fu">library</span>(rgl)</span>
<span id="cb45-5"><a href="sec12_4.html#cb45-5" tabindex="-1"></a></span>
<span id="cb45-6"><a href="sec12_4.html#cb45-6" tabindex="-1"></a><span class="co"># Simulate training data</span></span>
<span id="cb45-7"><a href="sec12_4.html#cb45-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb45-8"><a href="sec12_4.html#cb45-8" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb45-9"><a href="sec12_4.html#cb45-9" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_train)</span>
<span id="cb45-10"><a href="sec12_4.html#cb45-10" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_train)</span>
<span id="cb45-11"><a href="sec12_4.html#cb45-11" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> x2)</span>
<span id="cb45-12"><a href="sec12_4.html#cb45-12" tabindex="-1"></a></span>
<span id="cb45-13"><a href="sec12_4.html#cb45-13" tabindex="-1"></a><span class="co"># True function without noise</span></span>
<span id="cb45-14"><a href="sec12_4.html#cb45-14" tabindex="-1"></a>f_train <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> X_train<span class="sc">$</span>x1) <span class="sc">+</span> <span class="fu">cos</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> X_train<span class="sc">$</span>x2) <span class="sc">+</span> <span class="fu">sin</span>(X_train<span class="sc">$</span>x1 <span class="sc">*</span> X_train<span class="sc">$</span>x2)</span>
<span id="cb45-15"><a href="sec12_4.html#cb45-15" tabindex="-1"></a></span>
<span id="cb45-16"><a href="sec12_4.html#cb45-16" tabindex="-1"></a><span class="co"># Fit Gaussian Process</span></span>
<span id="cb45-17"><a href="sec12_4.html#cb45-17" tabindex="-1"></a>fit_km <span class="ot">&lt;-</span> <span class="fu">km</span>(<span class="at">design =</span> X_train, <span class="at">response =</span> f_train, <span class="at">covtype =</span> <span class="st">&quot;gauss&quot;</span>, <span class="at">nugget =</span> <span class="fl">1e-10</span>)</span></code></pre></div>
<pre><code>## 
## optimisation start
## ------------------
## * estimation method   : MLE 
## * optimisation method : BFGS 
## * analytical gradient : used
## * trend model : ~1
## * covariance model : 
##   - type :  gauss 
##   - nugget : 1e-10 
##   - parameters lower bounds :  1e-10 1e-10 
##   - parameters upper bounds :  1.958963 1.986954 
##   - variance bounds :  0.08882249 11.58394 
##   - best initial criterion value(s) :  245.853 
## 
## N = 3, M = 5 machine precision = 2.22045e-16
## At X0, 0 variables are exactly at the bounds
## At iterate     0  f=      -245.85  |proj g|=       1.8198
## At iterate     1  f =      -507.42  |proj g|=        10.555
## At iterate     2  f =      -554.97  |proj g|=        10.565
## At iterate     3  f =      -559.13  |proj g|=        10.573
## At iterate     4  f =      -559.52  |proj g|=        10.566
## At iterate     5  f =      -559.71  |proj g|=        10.562
## At iterate     6  f =      -561.14  |proj g|=        10.514
## At iterate     7  f =       -563.7  |proj g|=          10.4
## At iterate     8  f =      -568.91  |proj g|=        10.123
## At iterate     9  f =      -576.93  |proj g|=         9.622
## At iterate    10  f =      -578.62  |proj g|=        6.2251
## At iterate    11  f =      -588.79  |proj g|=        6.5073
## At iterate    12  f =      -593.65  |proj g|=        6.2906
## At iterate    13  f =      -596.53  |proj g|=         4.235
## At iterate    14  f =      -600.42  |proj g|=        2.3681
## At iterate    15  f =      -601.07  |proj g|=         1.616
## At iterate    16  f =      -603.31  |proj g|=        1.4519
## At iterate    17  f =      -605.28  |proj g|=       0.52334
## At iterate    18  f =      -605.49  |proj g|=        1.4752
## At iterate    19  f =      -605.57  |proj g|=        1.4721
## At iterate    20  f =      -605.57  |proj g|=        0.1007
## At iterate    21  f =      -605.57  |proj g|=      0.013566
## At iterate    22  f =      -605.57  |proj g|=     0.0006761
## At iterate    23  f =      -605.57  |proj g|=     0.0012742
## Bad direction in the line search;
##    refresh the lbfgs memory and restart the iteration.
## At iterate    24  f =      -605.57  |proj g|=    0.00016589
## At iterate    25  f =      -605.57  |proj g|=    0.00016589
## 
## iterations 25
## function evaluations 62
## segments explored during Cauchy searches 28
## BFGS updates skipped 0
## active bounds at final generalized Cauchy point 1
## norm of the final projected gradient 0.000165894
## final function value -605.575
## 
## F = -605.575
## final  value -605.574558 
## converged</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="sec12_4.html#cb47-1" tabindex="-1"></a><span class="co"># Prediction grid</span></span>
<span id="cb47-2"><a href="sec12_4.html#cb47-2" tabindex="-1"></a>grid_points <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb47-3"><a href="sec12_4.html#cb47-3" tabindex="-1"></a>x1_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> grid_points)</span>
<span id="cb47-4"><a href="sec12_4.html#cb47-4" tabindex="-1"></a>x2_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> grid_points)</span>
<span id="cb47-5"><a href="sec12_4.html#cb47-5" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> x1_seq, <span class="at">x2 =</span> x2_seq)</span>
<span id="cb47-6"><a href="sec12_4.html#cb47-6" tabindex="-1"></a></span>
<span id="cb47-7"><a href="sec12_4.html#cb47-7" tabindex="-1"></a><span class="co"># Predict GP surface</span></span>
<span id="cb47-8"><a href="sec12_4.html#cb47-8" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_km, <span class="at">newdata =</span> grid, <span class="at">type =</span> <span class="st">&quot;UK&quot;</span>)</span>
<span id="cb47-9"><a href="sec12_4.html#cb47-9" tabindex="-1"></a>z_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(pred<span class="sc">$</span>mean, <span class="at">nrow =</span> grid_points, <span class="at">ncol =</span> grid_points)</span>
<span id="cb47-10"><a href="sec12_4.html#cb47-10" tabindex="-1"></a></span>
<span id="cb47-11"><a href="sec12_4.html#cb47-11" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb47-12"><a href="sec12_4.html#cb47-12" tabindex="-1"></a><span class="fu">persp3d</span>(<span class="at">x =</span> x1_seq, <span class="at">y =</span> x2_seq, <span class="at">z =</span> z_pred,</span>
<span id="cb47-13"><a href="sec12_4.html#cb47-13" tabindex="-1"></a><span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>,</span>
<span id="cb47-14"><a href="sec12_4.html#cb47-14" tabindex="-1"></a><span class="at">xlab =</span> <span class="st">&quot;x1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;x2&quot;</span>, <span class="at">zlab =</span> <span class="st">&quot;GP Mean&quot;</span>)</span>
<span id="cb47-15"><a href="sec12_4.html#cb47-15" tabindex="-1"></a><span class="fu">points3d</span>(<span class="at">x =</span> X_train<span class="sc">$</span>x1, <span class="at">y =</span> X_train<span class="sc">$</span>x2, <span class="at">z =</span> f_train, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">8</span>)</span>
<span id="cb47-16"><a href="sec12_4.html#cb47-16" tabindex="-1"></a></span>
<span id="cb47-17"><a href="sec12_4.html#cb47-17" tabindex="-1"></a>fit_km<span class="sc">@</span>covariance<span class="sc">@</span>range.val <span class="co"># length-scale</span></span></code></pre></div>
<pre><code>## [1] 0.5196972 0.5150810</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="sec12_4.html#cb49-1" tabindex="-1"></a>fit_km<span class="sc">@</span>covariance<span class="sc">@</span>sd2 <span class="co"># Signal variance</span></span></code></pre></div>
<pre><code>## [1] 11.58394</code></pre>
<p><img src="figures/GP.png" width="600px" height="350px" style="display: block; margin: auto;" /></p>
<p>A limitation of the <em>DiceKriging</em> package is that it is designed for deterministic simulations and, consequently, does not estimate the noise variance. Therefore, in Exercise 7, we ask to simulate the process</p>
<p><span class="math display">\[
f_i = \sin(2\pi x_{i1}) + \cos(2\pi x_{i2}) + \sin(x_{i1} x_{i2}) + \mu_i,
\]</span></p>
<p>where <span class="math inline">\(\mu_i \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, 0.1^2)\)</span>, and to use an empirical Bayes approach to estimate the hyperparameters. These estimated hyperparameters should then be used to perform GP prediction.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-brochu2010tutorial" class="csl-entry">
Brochu, Eric, Vlad M. Cora, and Nando de Freitas. 2010. <span>“A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning.”</span> <em>arXiv Preprint arXiv:1012.2599</em>.
</div>
<div id="ref-choi2007posterior" class="csl-entry">
Choi, Taeryon, and Mark J. Schervish. 2007. <span>“On Posterior Consistency in Nonparametric Regression Problems.”</span> <em>Journal of Multivariate Analysis</em> 98 (10): 1969–87. <a href="https://doi.org/10.1016/j.jmva.2007.01.004">https://doi.org/10.1016/j.jmva.2007.01.004</a>.
</div>
<div id="ref-gardner2018product" class="csl-entry">
Gardner, Jacob R., Geoff Pleiss, David Bindel Wu, Kilian Q. Weinberger, and Andrew Gordon Wilson. 2018. <span>“Product Kernel Interpolation for Scalable Gaussian Processes.”</span> In <em>Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 84:1407–16. PMLR.
</div>
<div id="ref-jacobi2024posterior" class="csl-entry">
Jacobi, Liana, Chun Fung Kwok, Andrés Ramı́rez-Hassan, and Nhung Nghiem. 2024. <span>“Posterior Manifolds over Prior Parameter Regions: Beyond Pointwise Sensitivity Assessments for Posterior Statistics from MCMC Inference.”</span> <em>Studies in Nonlinear Dynamics &amp; Econometrics</em> 28 (2): 403–34.
</div>
<div id="ref-pleiss2018constant" class="csl-entry">
Pleiss, Geoff, Jacob R. Gardner, Kilian Q. Weinberger, and Andrew Gordon Wilson. 2018. <span>“Constant-Time Predictive Distributions for Gaussian Processes.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning (ICML)</em>, 80:4111–20. PMLR.
</div>
<div id="ref-rasmussen2006gaussian" class="csl-entry">
Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-settles2012active" class="csl-entry">
Settles, Burr. 2012. <em>Active Learning</em>. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers. <a href="https://doi.org/10.2200/S00429ED1V01Y201207AIM018">https://doi.org/10.2200/S00429ED1V01Y201207AIM018</a>.
</div>
<div id="ref-solak2003derivative" class="csl-entry">
Solak, Ercan, Roderick Murray-Smith, W. E. Leithead, D. J. Leith, and C. E. Rasmussen. 2003. <span>“Derivative Observations in Gaussian Process Models of Dynamic Systems.”</span> In <em>Advances in Neural Information Processing Systems</em>, 1033–40. MIT Press.
</div>
<div id="ref-stuart2018posterior" class="csl-entry">
Stuart, Andrew M., and Aretha L. Teckentrup. 2018. <span>“Posterior Consistency for Gaussian Process Approximations of Bayesian Posterior Distributions.”</span> <em>Mathematics of Computation</em> 87 (310): 721–53. <a href="https://doi.org/10.1090/mcom/3244">https://doi.org/10.1090/mcom/3244</a>.
</div>
<div id="ref-wilson2015kernel" class="csl-entry">
Wilson, Andrew G., and Hannes Nickisch. 2015. <span>“Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP).”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning (ICML)</em>, 37:1775–84. PMLR.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12_3.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec12_5.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/13-RecentDev.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
