<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.2 Regularization | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="12.2 Regularization | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.2 Regularization | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12_1.html"/>
<link rel="next" href="sec12_3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12_2" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Regularization<a href="sec12_2.html#sec12_2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this century, the amount of available data continues to grow. This means we have access to more covariates for prediction, and we can also generate additional inputs to enhance the predictive power of our models. As a result, we often encounter <em>wide</em> datasets, where the number of inputs may exceed the number of observations. Even in modest settings, we might have hundreds of inputs, and we can use them to identify which ones contribute to accurate predictions. However, we generally avoid using all inputs at once due to the risk of overfitting. Thus, we require a class of input selection or <em>regularization</em>.</p>
<p>In the standard linear regression setting,</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{1}_N \beta_0 + \mathbf{W}\boldsymbol{\beta} + \boldsymbol{\mu},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{1}_N\)</span> is an <span class="math inline">\(N\)</span>-dimensional vector of ones, <span class="math inline">\(\mathbf{W}\)</span> is the <span class="math inline">\(N \times K\)</span> design matrix of inputs, and <span class="math inline">\(\boldsymbol{\mu} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_N)\)</span>, there has been extensive development of techniques aimed at regularization within the Frequentist inferential framework. These include methods such as Ridge regression <span class="citation">(<a href="#ref-hoerl1970ridge">Hoerl and Kennard 1970</a>)</span>; discrete subset selection techniques like best subset selection <span class="citation">(<a href="#ref-furnival1974regressions">Furnival and Wilson 1974</a>)</span>, forward selection, and backward stepwise selection <span class="citation">(<a href="#ref-hastie2009elements">Hastie, Tibshirani, and Friedman 2009</a>)</span>; as well as continuous subset selection approaches such as the LASSO <span class="citation">(<a href="#ref-tibshirani1996regression">Tibshirani 1996</a>)</span>, the elastic net <span class="citation">(<a href="#ref-zou2005regularization">Zou and Hastie 2005</a>)</span>, and OSCAR <span class="citation">(<a href="#ref-bondell2008simultaneous">Bondell and Reich 2008</a>)</span>.</p>
<p>It is important to note, however, that Ridge regression does not perform variable selection; rather, it shrinks coefficient estimates toward zero without setting them exactly to zero.</p>
<p>Ridge regression and the LASSO can be viewed as special cases of a more general class of estimators known as <em>Bridge regression</em> <span class="citation">(<a href="#ref-fu1998penalized">Fu 1998</a>)</span>, which also admits a Bayesian interpretation. Consider the following penalized least squares criterion in the linear regression setting:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}^{\text{Bridge}} = \arg\min_{\boldsymbol{\beta}} \left\{ \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{k=1}^K \tilde{w}_{ik} \beta_k \right)^2 + \lambda \sum_{k=1}^K |\beta_k|^q \right\}, \quad q \geq 0,
\]</span></p>
<p>where <span class="math inline">\(\tilde{w}_{ik}\)</span> denotes the standardized inputs. Standardizing inputs is important in variable selection problems to avoid issues caused by differences in scale; otherwise, variables with larger magnitudes will be penalized less and disproportionately influence the regularization path.</p>
<p>Interpreting <span class="math inline">\(|\beta_k|^q\)</span> as proportional to the negative log-prior density of <span class="math inline">\(\beta_k\)</span>, the penalty shapes the contours of the prior distribution on the parameters. Specifically:</p>
<ul>
<li><span class="math inline">\(q = 0\)</span> corresponds to best subset selection, where the penalty counts the number of nonzero coefficients.</li>
<li><span class="math inline">\(q = 1\)</span> yields the LASSO, which corresponds to a Laplace (double-exponential) prior.</li>
<li><span class="math inline">\(q = 2\)</span> yields ridge regression, which corresponds to a Gaussian prior.</li>
</ul>
<p>In this light, best subset selection, the LASSO, and ridge regression can be viewed as maximum a posteriori (MAP) estimators under different priors centered at zero <span class="citation">(<a href="#ref-Park2008">Park and Casella 2008</a>)</span>. However, they are not Bayes estimators in the strict sense, since Bayes estimators are typically defined as the posterior <em>mean</em>. While ridge regression coincides with the posterior mean under a Gaussian prior <span class="citation">(<a href="#ref-Ishwaran2005">Ishwaran and Rao 2005</a>)</span>, the LASSO and best subset selection yield posterior modes.</p>
<p>This distinction is important because the Bayesian framework naturally incorporates regularization through the use of proper priors, which helps mitigate overfitting. Specifically, when proper shrinkage priors are used, the posterior balances data likelihood and prior information, thus controlling model complexity.</p>
<p>Furthermore, empirical Bayes methods, where the marginal likelihood is optimized, or cross-validation can be used to estimate the scale parameter of the prior covariance matrix for the regression coefficients. This scale parameter, in turn, determines the strength of regularization in ridge regression.</p>
<p>Note that regularization introduces bias into the parameter estimates because it constrains the model, shrinking the location parameters toward zero. However, it substantially reduces variance, as the estimates are prevented from varying excessively across samples. As a result, the mean squared error (MSE) of the estimates, which equals the sum of the squared bias and the variance, is often lower for regularization methods compared to ordinary least squares (OLS), which remains unbiased under the classical assumptions. This trade-off is particularly important when the goal is to identify causal effects, where unbiasedness may be preferred over predictive accuracy (see Chapter <a href="Chap13.html#Chap13">13</a>).</p>
<div id="sec12_21" class="section level3 hasAnchor" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Bayesian LASSO<a href="sec12_2.html#sec12_21" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given the popularity of the LASSO as a variable selection technique, we present its Bayesian formulation in this subsection <span class="citation">(<a href="#ref-Park2008">Park and Casella 2008</a>)</span>. The Gibbs sampler for the Bayesian LASSO exploits the representation of the Laplace distribution as a scale mixture of normals. This leads to the following hierarchical representation of the model:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} \mid \beta_0, \boldsymbol{\beta}, \sigma^2, \mathbf{W} &amp;\sim \mathcal{N}(\mathbf{1}_N \beta_0 + \mathbf{W} \boldsymbol{\beta}, \sigma^2 \mathbf{I}_N), \\
\boldsymbol{\beta} \mid \sigma^2, \tau_1^2, \dots, \tau_K^2 &amp;\sim \mathcal{N}(\mathbf{0}_K, \sigma^2 \mathbf{D}_{\tau}), \\
\tau_1^2, \dots, \tau_K^2 &amp;\sim \prod_{k=1}^K \frac{\lambda^2}{2} \exp\left\{ -\frac{\lambda^2}{2} \tau_k^2 \right\}, \\
\sigma^2 &amp;\sim \frac{1}{\sigma^2}, \\
\beta_0 &amp;\sim c,
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{D}_{\tau} = \operatorname{diag}(\tau_1^2, \dots, \tau_K^2)\)</span> and <span class="math inline">\(c\)</span> is a constant.</p>
<p>After integrating out <span class="math inline">\(\tau_1^2, \dots, \tau_K^2\)</span>, the conditional prior of <span class="math inline">\(\boldsymbol{\beta} \mid \sigma^2\)</span> is:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\beta} \mid \sigma^2) = \prod_{k=1}^K \frac{\lambda}{2 \sqrt{\sigma^2}} \exp\left\{ -\frac{\lambda}{\sqrt{\sigma^2}} |\beta_k| \right\},
\]</span></p>
<p>which implies that the log-prior is proportional to <span class="math inline">\(\lambda \sum_{k=1}^K |\beta_k|\)</span>, matching the penalty term in the LASSO optimization problem.</p>
<p>The conditional posterior distributions for the Gibbs sampler are <span class="citation">(<a href="#ref-Park2008">Park and Casella 2008</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\beta} \mid \sigma^2, \tau_1^2, \dots, \tau_K^2, \tilde{\mathbf{W}}, \tilde{\mathbf{y}} &amp;\sim \mathcal{N}(\boldsymbol{\beta}_n, \sigma^2 \mathbf{B}_n), \\
\sigma^2 \mid \boldsymbol{\beta}, \tau_1^2, \dots, \tau_K^2, \tilde{\mathbf{W}}, \tilde{\mathbf{y}} &amp;\sim \text{Inverse-Gamma}(\alpha_n/2, \delta_n/2), \\
1/\tau_k^2 \mid \boldsymbol{\beta}, \sigma^2 &amp;\sim \text{Inverse-Gaussian}(\mu_{kn}, \lambda_n), \\
\beta_0 \mid \sigma^2, \tilde{\mathbf{W}}, \tilde{\mathbf{y}} &amp;\sim \mathcal{N}(\bar{y}, \sigma^2 / N),
\end{aligned}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\beta}_n &amp;= \mathbf{B}_n \tilde{\mathbf{W}}^{\top} \tilde{\mathbf{y}}, \\
\mathbf{B}_n &amp;= \left( \tilde{\mathbf{W}}^{\top} \tilde{\mathbf{W}} + \mathbf{D}_{\tau}^{-1} \right)^{-1}, \\
\alpha_n &amp;= (N - 1) + K, \\
\delta_n &amp;= (\tilde{\mathbf{y}} - \tilde{\mathbf{W}} \boldsymbol{\beta})^{\top} (\tilde{\mathbf{y}} - \tilde{\mathbf{W}} \boldsymbol{\beta}) + \boldsymbol{\beta}^{\top} \mathbf{D}_{\tau}^{-1} \boldsymbol{\beta}, \\
\mu_{kn} &amp;= \sqrt{ \frac{ \lambda^2 \sigma^2 }{ \beta_k^2 } }, \\
\lambda_n &amp;= \lambda^2,
\end{aligned}
\]</span></p>
<p>and <span class="math inline">\(\tilde{\mathbf{W}}\)</span> is the matrix of standardized inputs, and <span class="math inline">\(\tilde{\mathbf{y}}\)</span> is the centered response vector.</p>
<p>Note that the posterior distribution of <span class="math inline">\(\boldsymbol{\tau}\)</span> depends on the data through <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>, which is a typical feature of hierarchical models. In this formulation, we can interpret <span class="math inline">\(\tau_k\)</span> as local shrinkage parameters, while <span class="math inline">\(\lambda\)</span> acts as a global shrinkage parameter. Higher values of <span class="math inline">\(\tau_k\)</span> and <span class="math inline">\(\lambda\)</span> imply stronger shrinkage toward zero. <span class="citation">Park and Casella (<a href="#ref-Park2008">2008</a>)</span> propose two approaches for specifying the global shrinkage parameter: empirical Bayes estimation or a fully Bayesian hierarchical specification, where <span class="math inline">\(\lambda^2\)</span> is assigned a Gamma prior.</p>
<p>We should acknowledge that the Bayesian LASSO is more computationally expensive than the Frequentist LASSO. However, it provides credible intervals for the parameters automatically. In contrast, obtaining standard errors in the Frequentist LASSO is more challenging, particularly for parameters estimated to be exactly zero <span class="citation">(<a href="#ref-kyung2010penalized">Kyung et al. 2010</a>)</span>.</p>
<p><strong>Example: Simulation exercise to study the Bayesian LASSO performance</strong></p>
<p>We simulate the process<br />
<span class="math display">\[\begin{equation*}
y_i = \beta_0 + \sum_{k=1}^{10} \beta_k w_{ik} + \mu_i,
\end{equation*}\]</span>
where <span class="math inline">\(\beta_k \sim \mathcal{U}(-3, 3)\)</span>, <span class="math inline">\(\mu_i \sim \mathcal{N}(0, 1)\)</span>, and <span class="math inline">\(w_{ik} \sim \mathcal{N}(0, 1)\)</span>, for <span class="math inline">\(i = 1, 2, \dots, 500\)</span>.</p>
<p>Additionally, we generate 90 extra potential inputs from a standard normal distribution, which are included in the model specification. Our goal is to assess whether the Bayesian LASSO can successfully identify the truly relevant inputs.</p>
<p>We use the <em>bayesreg</em> package in <strong>R</strong> to perform the Bayesian LASSO, using 5,000 MCMC draws and 1,000 burn-in iterations. The following code illustrates the simulation exercise and compares the posterior means with the true population values.</p>
<p>The summary of the fit and the plot comparing the population parameters with the posterior means show that the Bayesian LASSO is able to identify the variables that are relevant in the data generating process.</p>
<p>In <strong>Exercise 1</strong>, we propose programming the Gibbs sampler from scratch, assuming a hierarchical structure for the global shrinkage parameter, and comparing the results with those obtained using the <em>monomvn</em> package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="sec12_2.html#cb1-1" tabindex="-1"></a><span class="do">####### Bayesian LASSSO #######</span></span>
<span id="cb1-2"><a href="sec12_2.html#cb1-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb1-3"><a href="sec12_2.html#cb1-3" tabindex="-1"></a><span class="fu">library</span>(bayesreg)</span></code></pre></div>
<pre><code>## Loading required package: pgdraw</code></pre>
<pre><code>## Loading required package: doParallel</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="sec12_2.html#cb7-1" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb7-2"><a href="sec12_2.html#cb7-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span>  <span class="co"># sample size</span></span>
<span id="cb7-3"><a href="sec12_2.html#cb7-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># number of predictors</span></span>
<span id="cb7-4"><a href="sec12_2.html#cb7-4" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">10</span>   <span class="co"># number of non-zero coefficients</span></span>
<span id="cb7-5"><a href="sec12_2.html#cb7-5" tabindex="-1"></a><span class="co"># Generate design matrix</span></span>
<span id="cb7-6"><a href="sec12_2.html#cb7-6" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">nrow =</span> n, <span class="at">ncol =</span> p)</span>
<span id="cb7-7"><a href="sec12_2.html#cb7-7" tabindex="-1"></a><span class="co"># True beta: first s coefficients are non-zero, rest are zero</span></span>
<span id="cb7-8"><a href="sec12_2.html#cb7-8" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">runif</span>(s, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="fu">rep</span>(<span class="dv">0</span>, p <span class="sc">-</span> s))</span>
<span id="cb7-9"><a href="sec12_2.html#cb7-9" tabindex="-1"></a><span class="co"># Generate response with some noise</span></span>
<span id="cb7-10"><a href="sec12_2.html#cb7-10" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb7-11"><a href="sec12_2.html#cb7-11" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> sigma)</span>
<span id="cb7-12"><a href="sec12_2.html#cb7-12" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,y)</span>
<span id="cb7-13"><a href="sec12_2.html#cb7-13" tabindex="-1"></a><span class="do">### Using bayesreg </span><span class="al">###</span></span>
<span id="cb7-14"><a href="sec12_2.html#cb7-14" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb7-15"><a href="sec12_2.html#cb7-15" tabindex="-1"></a>fit <span class="ot">&lt;-</span> bayesreg<span class="sc">::</span><span class="fu">bayesreg</span>(y <span class="sc">~</span> X, <span class="at">data =</span> df, <span class="at">model =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">prior =</span> <span class="st">&quot;lasso&quot;</span>, </span>
<span id="cb7-16"><a href="sec12_2.html#cb7-16" tabindex="-1"></a><span class="at">n.samples =</span> <span class="dv">5000</span>, <span class="at">burnin =</span> <span class="dv">1000</span>)</span>
<span id="cb7-17"><a href="sec12_2.html#cb7-17" tabindex="-1"></a><span class="co"># Check summary</span></span>
<span id="cb7-18"><a href="sec12_2.html#cb7-18" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## ==========================================================================================
## |                   Bayesian Penalised Regression Estimation ver. 1.3                    |
## |                     (c) Enes Makalic, Daniel F Schmidt. 2016-2024                      |
## ==========================================================================================
## Bayesian Gaussian lasso regression                              Number of obs   =      500
##                                                                 Number of vars  =      100
## MCMC Samples   =   5000                                         std(Error)      =   1.0281
## MCMC Burnin    =   1000                                         R-squared       =   0.9759
## MCMC Thinning  =      5                                         WAIC            =   766.81
## 
## -------------+-----------------------------------------------------------------------------
##    Parameter |  mean(Coef)  std(Coef)    [95% Cred. Interval]      tStat    Rank        ESS
## -------------+-----------------------------------------------------------------------------
##           X1 |    -0.06817    0.04738     -0.16303    0.02104     -1.439      19 *     5000
##           X2 |    -2.20887    0.05215     -2.30587   -2.10124    -42.353       3 **    5000
##           X3 |    -2.63625    0.05170     -2.73261   -2.52913    -50.995       1 **    4218
##           X4 |    -0.73970    0.04869     -0.83761   -0.64510    -15.192       9 **    5000
##           X5 |     1.81043    0.04979      1.71040    1.90798     36.358       6 **    5000
##           X6 |    -2.11685    0.05023     -2.21480   -2.01762    -42.145       3 **    4534
##           X7 |    -2.09659    0.05131     -2.19625   -1.99562    -40.860       5 **    5000
##           X8 |     0.88085    0.04761      0.78624    0.97390     18.503       8 **    5000
##           X9 |     1.26812    0.05218      1.16656    1.37026     24.304       7 **    5000
##          X10 |    -2.55506    0.05066     -2.65210   -2.45179    -50.431       2 **    4982
##          X11 |    -0.03005    0.04526     -0.12313    0.05465     -0.664      52       5000
##          X12 |    -0.02395    0.05086     -0.12836    0.07321     -0.471      59       4835
##          X13 |    -0.06456    0.05073     -0.16226    0.03284     -1.273      23 *     5000
##          X14 |    -0.02819    0.04583     -0.12295    0.05894     -0.615      52       5000
##          X15 |    -0.04640    0.04900     -0.14495    0.04763     -0.947      32       4794
##          X16 |    -0.08144    0.04876     -0.17715    0.01185     -1.670      15 *     5000
##          X17 |    -0.02076    0.04686     -0.11459    0.06874     -0.443      64       5000
##          X18 |    -0.01186    0.04763     -0.10964    0.07913     -0.249      81       4479
##          X19 |     0.06337    0.04851     -0.03093    0.16048      1.307      22 *     5000
##          X20 |     0.01940    0.04638     -0.07243    0.11002      0.418      64       4973
##          X21 |     0.03018    0.04731     -0.06190    0.12425      0.638      48       4995
##          X22 |    -0.02096    0.04773     -0.11545    0.07091     -0.439      64       4764
##          X23 |     0.03284    0.04894     -0.06031    0.13105      0.671      46       5000
##          X24 |    -0.00284    0.04265     -0.08657    0.08169     -0.067      90       5000
##          X25 |    -0.00356    0.04212     -0.08918    0.07835     -0.085      90       5000
##          X26 |    -0.01291    0.04671     -0.10804    0.08041     -0.276      90       5000
##          X27 |    -0.05861    0.04658     -0.14918    0.03122     -1.258      25 *     4947
##          X28 |     0.02099    0.04705     -0.06666    0.12109      0.446      64       5000
##          X29 |     0.08353    0.05120     -0.01646    0.18564      1.631      15 *     4958
##          X30 |     0.04336    0.04826     -0.04713    0.14057      0.899      38       5000
##          X31 |     0.02803    0.04491     -0.06307    0.11692      0.624      52       5000
##          X32 |    -0.07986    0.04755     -0.17303    0.01085     -1.679      14 *     5000
##          X33 |    -0.02006    0.04602     -0.11316    0.07251     -0.436      64       4990
##          X34 |    -0.01956    0.04823     -0.11827    0.07506     -0.406      64       4829
##          X35 |    -0.01985    0.04607     -0.11221    0.06957     -0.431      64       4973
##          X36 |     0.00130    0.04566     -0.09089    0.09236      0.028      90       5000
##          X37 |    -0.04883    0.04557     -0.14136    0.03949     -1.072      31       5000
##          X38 |    -0.00758    0.04700     -0.10294    0.08499     -0.161      90       5000
##          X39 |     0.01470    0.04744     -0.07977    0.11075      0.310      81       4971
##          X40 |    -0.02029    0.04728     -0.11678    0.07023     -0.429      64       4860
##          X41 |    -0.00985    0.04494     -0.09769    0.08019     -0.219      90       5000
##          X42 |     0.03593    0.04601     -0.05055    0.12921      0.781      43       5000
##          X43 |    -0.05555    0.04732     -0.15159    0.03439     -1.174      25 *     5000
##          X44 |    -0.01799    0.04688     -0.11399    0.07362     -0.384      64       5000
##          X45 |     0.07269    0.04790     -0.01757    0.16772      1.517      18 *     4656
##          X46 |    -0.03822    0.04678     -0.13122    0.05253     -0.817      38       5000
##          X47 |    -0.05041    0.05042     -0.14972    0.04268     -1.000      34       5000
##          X48 |     0.02306    0.04782     -0.07026    0.11844      0.482      64       5000
##          X49 |    -0.01171    0.04855     -0.11082    0.08213     -0.241      64       5000
##          X50 |    -0.02057    0.04313     -0.10874    0.06252     -0.477      59       4714
##          X51 |    -0.05357    0.04561     -0.14391    0.03225     -1.174      27 *     5000
##          X52 |    -0.00239    0.04669     -0.09704    0.08869     -0.051      90       5000
##          X53 |    -0.04892    0.05008     -0.14854    0.04452     -0.977      34       5000
##          X54 |    -0.04487    0.04977     -0.14844    0.05028     -0.902      42       4796
##          X55 |     0.04078    0.04720     -0.05286    0.13503      0.864      36       5000
##          X56 |     0.03038    0.04632     -0.05871    0.12155      0.656      48       5000
##          X57 |     0.01753    0.04582     -0.07114    0.11062      0.383      81       4747
##          X58 |     0.00791    0.04553     -0.08319    0.09785      0.174      90       5000
##          X59 |    -0.05221    0.04660     -0.14673    0.03529     -1.120      28 *     5000
##          X60 |    -0.00133    0.04536     -0.09042    0.08836     -0.029      90       4960
##          X61 |     0.15153    0.04837      0.05694    0.24679      3.133      10 **    5000
##          X62 |     0.03080    0.04683     -0.05819    0.12679      0.658      52       4793
##          X63 |    -0.08342    0.05065     -0.18534    0.01134     -1.647      15 *     5000
##          X64 |    -0.00507    0.04823     -0.09911    0.09092     -0.105      81       5000
##          X65 |    -0.03442    0.04491     -0.12329    0.05354     -0.766      43       5000
##          X66 |    -0.02356    0.04579     -0.11821    0.06331     -0.515      64       4918
##          X67 |     0.01851    0.04551     -0.06848    0.11438      0.407      64       4852
##          X68 |    -0.03064    0.04832     -0.12963    0.06019     -0.634      52       4725
##          X69 |    -0.05074    0.04596     -0.14269    0.03521     -1.104      30       5000
##          X70 |    -0.06563    0.04677     -0.15797    0.02363     -1.403      20 *     4899
##          X71 |    -0.04128    0.04636     -0.13535    0.04579     -0.890      36       4850
##          X72 |    -0.00586    0.04760     -0.09932    0.09098     -0.123      90       5000
##          X73 |    -0.03947    0.04941     -0.13999    0.05518     -0.799      43       5000
##          X74 |     0.02677    0.04559     -0.06232    0.12093      0.587      59       4924
##          X75 |     0.06559    0.04788     -0.02514    0.16033      1.370      20 *     5000
##          X76 |    -0.01302    0.04700     -0.11028    0.08162     -0.277      64       4872
##          X77 |     0.04182    0.04755     -0.04899    0.13738      0.879      38       5000
##          X78 |    -0.02909    0.04887     -0.12819    0.06337     -0.595      52       5000
##          X79 |    -0.01863    0.04560     -0.11060    0.07254     -0.409      81       5000
##          X80 |    -0.05837    0.04836     -0.15699    0.03295     -1.207      23 *     5000
##          X81 |     0.03344    0.04812     -0.05853    0.13239      0.695      48       4481
##          X82 |    -0.02578    0.04606     -0.12062    0.06040     -0.560      59       4962
##          X83 |    -0.04759    0.04736     -0.14112    0.04143     -1.005      32       5000
##          X84 |    -0.09467    0.05195     -0.19778    0.00499     -1.822      12 *     5000
##          X85 |    -0.04005    0.04692     -0.13386    0.05031     -0.854      38       5000
##          X86 |    -0.05430    0.04808     -0.14887    0.03581     -1.129      28       5000
##          X87 |    -0.00655    0.04634     -0.10184    0.08368     -0.141      81       4915
##          X88 |     0.02032    0.04830     -0.07421    0.11716      0.421      64       4933
##          X89 |    -0.08416    0.04748     -0.18043    0.00513     -1.773      13 *     4720
##          X90 |     0.01809    0.04745     -0.07507    0.11206      0.381      64       4547
##          X91 |    -0.00287    0.04568     -0.09490    0.08683     -0.063      90       4778
##          X92 |     0.02651    0.04689     -0.06211    0.12300      0.565      59       5000
##          X93 |    -0.10025    0.05378     -0.21039    0.00161     -1.864      11 *     5000
##          X94 |    -0.02224    0.04338     -0.10913    0.06163     -0.513      64       5000
##          X95 |     0.00419    0.04629     -0.08873    0.09698      0.090      81       4894
##          X96 |     0.01950    0.04574     -0.06851    0.11168      0.426      81       4977
##          X97 |    -0.03581    0.04760     -0.13190    0.05676     -0.752      46       4987
##          X98 |     0.02645    0.04694     -0.06455    0.12217      0.563      52       5000
##          X99 |    -0.03477    0.04715     -0.12801    0.05612     -0.738      48       5000
##         X100 |    -0.01155    0.04503     -0.09926    0.07859     -0.256      81       5000
##  (Intercept) |     0.06137    0.05085     -0.03809    0.16165          .       .          .
## -------------+-----------------------------------------------------------------------------</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="sec12_2.html#cb9-1" tabindex="-1"></a><span class="co"># Extract posterior means of beta</span></span>
<span id="cb9-2"><a href="sec12_2.html#cb9-2" tabindex="-1"></a>beta_post_mean <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(fit<span class="sc">$</span>beta)</span>
<span id="cb9-3"><a href="sec12_2.html#cb9-3" tabindex="-1"></a><span class="co"># Compare true vs estimated</span></span>
<span id="cb9-4"><a href="sec12_2.html#cb9-4" tabindex="-1"></a><span class="fu">plot</span>(beta_true, beta_post_mean, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>,</span>
<span id="cb9-5"><a href="sec12_2.html#cb9-5" tabindex="-1"></a><span class="at">xlab =</span> <span class="st">&quot;True beta&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Posterior mean of beta&quot;</span>,</span>
<span id="cb9-6"><a href="sec12_2.html#cb9-6" tabindex="-1"></a><span class="at">main =</span> <span class="st">&quot;Bayesian LASSO Shrinkage&quot;</span>)</span>
<span id="cb9-7"><a href="sec12_2.html#cb9-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-1-1.svg" width="672" /></p>
</div>
<div id="sec12_22" class="section level3 hasAnchor" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Stochastic search variable selection<a href="sec12_2.html#sec12_22" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another well-known Bayesian strategy for variable selection in the presence of a large set of regressors (inputs) is <em>stochastic search variable selection</em> (SSVS) <span class="citation">(<a href="#ref-george1993variable">E. I. George and McCulloch 1993</a>; <a href="#ref-George1997">E. George and McCulloch 1997</a>)</span>. SSVS is a particular case of the broader class of <em>spike-and-slab</em> priors, in which the prior distribution for the location parameters is specified as a hierarchical mixture that captures the uncertainty inherent in variable selection problems <span class="citation">(<a href="#ref-Ishwaran2005">Ishwaran and Rao 2005</a>)</span>.</p>
<p>The hierarchical structure of the model is given by:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} \mid \beta_0, \boldsymbol{\beta}, \sigma^2, \mathbf{W} &amp;\sim \mathcal{N}(\mathbf{1}_N \beta_0 + \mathbf{W} \boldsymbol{\beta}, \sigma^2 \mathbf{I}_N), \\
\boldsymbol{\beta} \mid \sigma^2, \boldsymbol{\gamma} &amp;\sim \mathcal{N}(\mathbf{0}_K, \sigma^2 \mathbf{D}_\gamma \mathbf{R} \mathbf{D}_\gamma), \\
\sigma^2 &amp;\sim \text{Inverse-Gamma}\left(\frac{v}{2}, \frac{v \lambda_\gamma}{2}\right), \\
\gamma_k &amp;\sim \text{Bernoulli}(p_k),
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(p_k\)</span> is the prior inclusion probability of regressor <span class="math inline">\(w_k\)</span>, that is, <span class="math inline">\(P(\gamma_k = 1) = 1 - P(\gamma_k = 0) = p_k\)</span>, <span class="math inline">\(\mathbf{R}\)</span> is a correlation matrix, and <span class="math inline">\(\mathbf{D}_\gamma\)</span> is a diagonal matrix whose <span class="math inline">\((k,k)\)</span>-th element is defined as:</p>
<p><span class="math display">\[
(\mathbf{D}_\gamma)_{kk} =
\begin{Bmatrix}
v_{0k}, &amp; \text{if } \gamma_k = 0, \\
v_{1k}, &amp; \text{if } \gamma_k = 1
\end{Bmatrix}.
\]</span></p>
<p>This formulation implies that:</p>
<p><span class="math display">\[
\beta_k \sim (1 - \gamma_k) \, \mathcal{N}(0, v_{0k}) + \gamma_k \, \mathcal{N}(0, v_{1k}),
\]</span></p>
<p>where <span class="math inline">\(v_{0k}\)</span> and <span class="math inline">\(v_{1k}\)</span> are chosen such that <span class="math inline">\(v_{0k}\)</span> is small and <span class="math inline">\(v_{1k}\)</span> is large. Therefore, when the data favors <span class="math inline">\(\gamma_k = 0\)</span>, the corresponding <span class="math inline">\(\beta_k\)</span> is shrunk toward zero, effectively excluding input <span class="math inline">\(k\)</span> from the model. In this sense, <span class="math inline">\(\mathcal{N}(0, v_{0k})\)</span> is a spike prior concentrated at zero, while <span class="math inline">\(\mathcal{N}(0, v_{1k})\)</span> is a diffuse slab prior.</p>
<p>A critical aspect of SSVS is the choice of the hyperparameters <span class="math inline">\(v_{0k}\)</span> and <span class="math inline">\(v_{1k}\)</span>, as they determine the amount of shrinkage applied to the regression coefficients (see <span class="citation">E. I. George and McCulloch (<a href="#ref-george1993variable">1993</a>)</span> and <span class="citation">E. George and McCulloch (<a href="#ref-George1997">1997</a>)</span> for details).</p>
<p>The assumption <span class="math inline">\(\gamma_k \sim \text{Bernoulli}(p_k)\)</span> implies that the prior on the inclusion indicators is given by:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\gamma}) = \prod_{k=1}^K p_k^{\gamma_k} (1 - p_k)^{1 - \gamma_k}.
\]</span></p>
<p>This means that the inclusion of input <span class="math inline">\(k\)</span> is independent of the inclusion of any other input <span class="math inline">\(j \neq k\)</span>. A common choice is the uniform prior <span class="math inline">\(\pi(\boldsymbol{\gamma}) = 2^{-K}\)</span>, which corresponds to setting <span class="math inline">\(p_k = 1/2\)</span>, giving each regressor an equal chance of being included <span class="citation">(<a href="#ref-Ishwaran2005">Ishwaran and Rao 2005</a>)</span>.</p>
<p>A practical choice for the correlation matrix is to set <span class="math inline">\(\mathbf{R} \propto (\tilde{\mathbf{W}}^{\top} \tilde{\mathbf{W}})^{-1}\)</span> <span class="citation">(<a href="#ref-george1993variable">E. I. George and McCulloch 1993</a>)</span>. Regarding the hyperparameters <span class="math inline">\(v\)</span> and <span class="math inline">\(\lambda_\gamma\)</span>, it is helpful to interpret <span class="math inline">\(\lambda_\gamma\)</span> as a prior estimate of <span class="math inline">\(\sigma^2\)</span>, and <span class="math inline">\(v\)</span> as the prior sample size associated with this estimate. In the absence of prior information, <span class="citation">E. I. George and McCulloch (<a href="#ref-george1997approaches">1997</a>)</span> recommend setting <span class="math inline">\(\lambda_\gamma\)</span> equal to the least squares estimate of the variance from the <em>saturated model</em>, that is, the model including all regressors, and <span class="math inline">\(v\)</span> to a small number, for instance, 0.01.</p>
<p>The conditional posterior distributions for the Gibbs sampler are <span class="citation">(<a href="#ref-george1993variable">E. I. George and McCulloch 1993</a>)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\beta} \mid \sigma^2, \gamma_1, \dots, \gamma_K, \tilde{\mathbf{W}}, \tilde{\mathbf{y}} &amp;\sim N(\boldsymbol{\beta}_n, \mathbf{B}_n), \\
\sigma^2 \mid \boldsymbol{\beta}, \gamma_1, \dots, \gamma_K, \tilde{\mathbf{W}}, \tilde{\mathbf{y}} &amp;\sim \text{Inverse-Gamma}(\alpha_n/2, \delta_n/2), \\
\gamma_k \mid \boldsymbol{\beta}, \sigma^2 &amp;\sim \text{Bernoulli}(p_{kn}),
\end{aligned}
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{\beta}_n &amp;= \sigma^{-2} \mathbf{B}_n \tilde{\mathbf{W}}^{\top} \tilde{\mathbf{y}}, \\
\mathbf{B}_n &amp;= \left(\sigma^{-2} \tilde{\mathbf{W}}^{\top} \tilde{\mathbf{W}} + \mathbf{D}_{\gamma}^{-1}\mathbf{R}^{-1}\mathbf{D}_{\gamma}^{-1} \right)^{-1}, \\
\alpha_n &amp;= N + v, \\
\delta_n &amp;= (\tilde{\mathbf{y}} - \tilde{\mathbf{W}} \boldsymbol{\beta})^{\top} (\tilde{\mathbf{y}} - \tilde{\mathbf{W}} \boldsymbol{\beta}) + v\lambda_{\gamma}, \\
p_{kn} &amp;= \frac{\pi(\boldsymbol{\beta}\mid \boldsymbol{\gamma}_{-k},\gamma_k=1)\times p_k}{\pi(\boldsymbol{\beta}\mid \boldsymbol{\gamma}_{-k},\gamma_k=1)\times p_k+\pi(\boldsymbol{\beta}\mid \boldsymbol{\gamma}_{-k},\gamma_k=0)\times (1-p_k)},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\tilde{\mathbf{W}}\)</span> is the matrix of standardized inputs, <span class="math inline">\(\tilde{\mathbf{y}}\)</span> is the centered response vector, <span class="math inline">\(\boldsymbol{\gamma}_{-k}\)</span> denotes the vector composed of <span class="math inline">\(\gamma_1, \dots, \gamma_K\)</span> excluding <span class="math inline">\(\gamma_k\)</span>, and <span class="math inline">\(\pi(\boldsymbol{\beta} \mid \boldsymbol{\gamma}_{-k}, \gamma_k = \delta)\)</span> is the posterior density of <span class="math inline">\(\boldsymbol{\beta}\)</span> evaluated at <span class="math inline">\(\boldsymbol{\gamma}_{-k}\)</span> and <span class="math inline">\(\gamma_k = \delta\)</span>, where <span class="math inline">\(\delta \in \{0,1\}\)</span>.</p>
<p>In general, it is wise to consider the inclusion of regressors jointly due to potential correlations among them; that is, the marginal frequency of <span class="math inline">\(\gamma_k = 1\)</span> should be interpreted with caution. SSVS is more effective at identifying a good set of potential models rather than selecting a single best model.</p>
<p><strong>Example: Simulation exercise to study SSVS performance</strong></p>
<p>Let’s use the simulation setting from the previous example to evaluate the performance of SSVS in uncovering the data-generating process. In particular, we use the <em>BoomSpikeSlab</em> package to implement this example.</p>
<p>The analysis is performed using 5,000 posterior draws and the default prior. However, the package allows the user to modify the default prior via the <em>SpikeSlabPrior</em> function.</p>
<p>The results show that the posterior inclusion probabilities for regressors 2 through 10 are 100%, and the model with the highest posterior probability (94%) includes all of these nine variables. However, the true data-generating process, which also includes regressor 1, receives a posterior model probability of 0%. This is because the population coefficient of this regressor is essentially zero. The plot comparing the posterior means with the true population parameters indicates good performance of SSVS. In general, Bayesian methods for variable selection perform well, and the choice of the most suitable method largely depends on the prior specification <span class="citation">(<a href="#ref-ohara2009bayesian">O’Hara and Sillanpää 2009</a>)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="sec12_2.html#cb10-1" tabindex="-1"></a><span class="do">####### Stochastic search variable selection #######</span></span>
<span id="cb10-2"><a href="sec12_2.html#cb10-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb10-3"><a href="sec12_2.html#cb10-3" tabindex="-1"></a><span class="fu">library</span>(BoomSpikeSlab)</span></code></pre></div>
<pre><code>## Loading required package: Boom</code></pre>
<pre><code>## 
## Attaching package: &#39;Boom&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     rWishart</code></pre>
<pre><code>## 
## Attaching package: &#39;BoomSpikeSlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     knots</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="sec12_2.html#cb16-1" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="sec12_2.html#cb20-1" tabindex="-1"></a><span class="fu">library</span>(tibble)</span>
<span id="cb20-2"><a href="sec12_2.html#cb20-2" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb20-3"><a href="sec12_2.html#cb20-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span>  <span class="co"># sample size</span></span>
<span id="cb20-4"><a href="sec12_2.html#cb20-4" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># number of predictors</span></span>
<span id="cb20-5"><a href="sec12_2.html#cb20-5" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">10</span>   <span class="co"># number of non-zero coefficients</span></span>
<span id="cb20-6"><a href="sec12_2.html#cb20-6" tabindex="-1"></a><span class="co"># Generate design matrix</span></span>
<span id="cb20-7"><a href="sec12_2.html#cb20-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> k), <span class="at">nrow =</span> n, <span class="at">ncol =</span> k)</span>
<span id="cb20-8"><a href="sec12_2.html#cb20-8" tabindex="-1"></a><span class="co"># True beta: first s coefficients are non-zero, rest are zero</span></span>
<span id="cb20-9"><a href="sec12_2.html#cb20-9" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">runif</span>(s, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="fu">rep</span>(<span class="dv">0</span>, k <span class="sc">-</span> s))</span>
<span id="cb20-10"><a href="sec12_2.html#cb20-10" tabindex="-1"></a><span class="co"># Generate response with some noise</span></span>
<span id="cb20-11"><a href="sec12_2.html#cb20-11" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb20-12"><a href="sec12_2.html#cb20-12" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> sigma)</span>
<span id="cb20-13"><a href="sec12_2.html#cb20-13" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X,y)</span>
<span id="cb20-14"><a href="sec12_2.html#cb20-14" tabindex="-1"></a><span class="do">### Using BoomSpikeSlab </span><span class="al">###</span></span>
<span id="cb20-15"><a href="sec12_2.html#cb20-15" tabindex="-1"></a><span class="co">#Scale regressors</span></span>
<span id="cb20-16"><a href="sec12_2.html#cb20-16" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">scale</span>(X); yh <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">mean</span>(y)</span>
<span id="cb20-17"><a href="sec12_2.html#cb20-17" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">SpikeSlabPrior</span>(W, yh, </span>
<span id="cb20-18"><a href="sec12_2.html#cb20-18" tabindex="-1"></a><span class="at">expected.model.size =</span> <span class="fu">ncol</span>(W)<span class="sc">/</span><span class="dv">2</span>, <span class="co"># expect 50 nonzero predictors</span></span>
<span id="cb20-19"><a href="sec12_2.html#cb20-19" tabindex="-1"></a><span class="at">prior.df =</span> .<span class="dv">01</span>, <span class="co"># weaker prior than the default</span></span>
<span id="cb20-20"><a href="sec12_2.html#cb20-20" tabindex="-1"></a><span class="at">prior.information.weight =</span> .<span class="dv">01</span>,</span>
<span id="cb20-21"><a href="sec12_2.html#cb20-21" tabindex="-1"></a><span class="at">diagonal.shrinkage =</span> <span class="dv">0</span>) <span class="co"># shrink to zero</span></span>
<span id="cb20-22"><a href="sec12_2.html#cb20-22" tabindex="-1"></a>niter <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb20-23"><a href="sec12_2.html#cb20-23" tabindex="-1"></a><span class="do">######Estimate model########</span></span>
<span id="cb20-24"><a href="sec12_2.html#cb20-24" tabindex="-1"></a>SSBoomNew <span class="ot">&lt;-</span> <span class="fu">lm.spike</span>(yh <span class="sc">~</span> W <span class="sc">-</span> <span class="dv">1</span>, <span class="at">niter =</span> niter, <span class="at">prior =</span> prior)</span></code></pre></div>
<pre><code>## =-=-=-=-= Iteration 0 Sat Jul 12 09:15:39 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 500 Sat Jul 12 09:15:39 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 1000 Sat Jul 12 09:15:40 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 1500 Sat Jul 12 09:15:40 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 2000 Sat Jul 12 09:15:41 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 2500 Sat Jul 12 09:15:41 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 3000 Sat Jul 12 09:15:42 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 3500 Sat Jul 12 09:15:42 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 4000 Sat Jul 12 09:15:43 2025
##  =-=-=-=-=
## =-=-=-=-= Iteration 4500 Sat Jul 12 09:15:43 2025
##  =-=-=-=-=</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="sec12_2.html#cb22-1" tabindex="-1"></a>Models <span class="ot">&lt;-</span> SSBoomNew<span class="sc">$</span>beta <span class="sc">!=</span> <span class="dv">0</span></span>
<span id="cb22-2"><a href="sec12_2.html#cb22-2" tabindex="-1"></a>PIP <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(SSBoomNew<span class="sc">$</span>beta <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb22-3"><a href="sec12_2.html#cb22-3" tabindex="-1"></a><span class="co"># Convert the logical matrix to a data frame and then to a tibble</span></span>
<span id="cb22-4"><a href="sec12_2.html#cb22-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(Models); df_tbl <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(df)</span>
<span id="cb22-5"><a href="sec12_2.html#cb22-5" tabindex="-1"></a><span class="co"># Count identical rows</span></span>
<span id="cb22-6"><a href="sec12_2.html#cb22-6" tabindex="-1"></a>row_counts <span class="ot">&lt;-</span> df_tbl <span class="sc">%&gt;%</span> <span class="fu">count</span>(<span class="fu">across</span>(<span class="fu">everything</span>()), <span class="at">name =</span> <span class="st">&quot;frequency&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">desc</span>(frequency))</span>
<span id="cb22-7"><a href="sec12_2.html#cb22-7" tabindex="-1"></a><span class="fu">sum</span>(row_counts[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,<span class="dv">101</span>])</span></code></pre></div>
<pre><code>## [1] 3948</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="sec12_2.html#cb24-1" tabindex="-1"></a><span class="co"># Ensure your vector and matrix are logical</span></span>
<span id="cb24-2"><a href="sec12_2.html#cb24-2" tabindex="-1"></a>trueModel <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">90</span>)) <span class="sc">==</span> <span class="dv">1</span>  <span class="co"># convert to logical if needed</span></span>
<span id="cb24-3"><a href="sec12_2.html#cb24-3" tabindex="-1"></a><span class="co"># Assume your matrix is named &#39;mat&#39;</span></span>
<span id="cb24-4"><a href="sec12_2.html#cb24-4" tabindex="-1"></a>matching_rows <span class="ot">&lt;-</span> <span class="fu">apply</span>(row_counts[,<span class="sc">-</span><span class="dv">101</span>], <span class="dv">1</span>, <span class="cf">function</span>(row) <span class="fu">all</span>(row <span class="sc">==</span> trueModel))</span>
<span id="cb24-5"><a href="sec12_2.html#cb24-5" tabindex="-1"></a><span class="co"># Get indices (row numbers) where the match is TRUE</span></span>
<span id="cb24-6"><a href="sec12_2.html#cb24-6" tabindex="-1"></a>row_counts[<span class="fu">which</span>(matching_rows), <span class="dv">101</span>]</span></code></pre></div>
<pre><code>## # A tibble: 1 × 1
##   frequency
##       &lt;int&gt;
## 1         3</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="sec12_2.html#cb26-1" tabindex="-1"></a><span class="co"># Coefficients</span></span>
<span id="cb26-2"><a href="sec12_2.html#cb26-2" tabindex="-1"></a>SummarySS <span class="ot">&lt;-</span> <span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(SSBoomNew<span class="sc">$</span>beta))</span>
<span id="cb26-3"><a href="sec12_2.html#cb26-3" tabindex="-1"></a><span class="co"># Extract posterior means of beta</span></span>
<span id="cb26-4"><a href="sec12_2.html#cb26-4" tabindex="-1"></a>beta_post_mean <span class="ot">&lt;-</span> SummarySS<span class="sc">$</span>statistics[, <span class="dv">1</span>]</span>
<span id="cb26-5"><a href="sec12_2.html#cb26-5" tabindex="-1"></a><span class="co"># Compare true vs estimated</span></span>
<span id="cb26-6"><a href="sec12_2.html#cb26-6" tabindex="-1"></a><span class="fu">plot</span>(beta_true, beta_post_mean, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>,</span>
<span id="cb26-7"><a href="sec12_2.html#cb26-7" tabindex="-1"></a><span class="at">xlab =</span> <span class="st">&quot;True beta&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Posterior mean of beta&quot;</span>,</span>
<span id="cb26-8"><a href="sec12_2.html#cb26-8" tabindex="-1"></a><span class="at">main =</span> <span class="st">&quot;SSVS Shrinkage&quot;</span>)</span>
<span id="cb26-9"><a href="sec12_2.html#cb26-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-2-1.svg" width="672" /></p>
<p>The examples and exercises presented thus far have considered scenarios in which the number of inputs is smaller than the number of observations (<span class="math inline">\(K &lt; N\)</span>). In Exercise 4, we challenge the Bayesian LASSO and SSVS in a setting where the number of inputs exceeds the sample size (<span class="math inline">\(K &gt; N\)</span>). As you will observe in that experiment, both the Bayesian LASSO and SSVS perform well. However, the Bayesian LASSO requires more time to produce results compared to SSVS in this exercise. <span class="citation">Ročková and George (<a href="#ref-rockova2018spike">2018</a>)</span> propose a connection between the LASSO and spike-and-slab priors for variable selection in linear models, offering oracle properties and optimal posterior concentration even in high-dimensional settings where <span class="math inline">\(K &gt; N\)</span>.</p>
<p>In addition, there are other Bayesian methods for regularization, such as the spike-and-slab approach proposed by <span class="citation">Ishwaran and Rao (<a href="#ref-Ishwaran2005">2005</a>)</span> and non-local priors introduced by <span class="citation">Johnson and Rossell (<a href="#ref-johnson2012bayesian">2012</a>)</span>, which can be implemented using the <strong>R</strong> packages <code>spikeslab</code> and <code>mombf</code>, respectively.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bondell2008simultaneous" class="csl-entry">
Bondell, Howard D., and Brian J. Reich. 2008. <span>“Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR.”</span> <em>Biometrics</em> 64 (1): 115–23. <a href="https://doi.org/10.1111/j.1541-0420.2007.00843.x">https://doi.org/10.1111/j.1541-0420.2007.00843.x</a>.
</div>
<div id="ref-fu1998penalized" class="csl-entry">
Fu, Wenjiang J. 1998. <span>“Penalized Regression: The Bridge Versus the Lasso.”</span> <em>Journal of Computational and Graphical Statistics</em> 7 (3): 397–416. <a href="https://doi.org/10.1080/10618600.1998.10474784">https://doi.org/10.1080/10618600.1998.10474784</a>.
</div>
<div id="ref-furnival1974regressions" class="csl-entry">
Furnival, George M., and R. W. Wilson. 1974. <span>“Regressions by Leaps and Bounds.”</span> <em>Technometrics</em> 16 (4): 499–511. <a href="https://doi.org/10.1080/00401706.1974.10489152">https://doi.org/10.1080/00401706.1974.10489152</a>.
</div>
<div id="ref-george1993variable" class="csl-entry">
George, Edward I, and Robert E McCulloch. 1993. <span>“Variable Selection via Gibbs Sampling.”</span> <em>Journal of the American Statistical Association</em> 88 (423): 881–89.
</div>
<div id="ref-george1997approaches" class="csl-entry">
———. 1997. <span>“Approaches for <span>B</span>ayesian Variable Selection.”</span> <em>Statistica Sinica</em> 7: 339–73.
</div>
<div id="ref-George1997" class="csl-entry">
George, E., and R. McCulloch. 1997. <span>“Approaches for <span>B</span>ayesian Variable Selection.”</span> <em>Statistica Sinica</em> 7: 339–73.
</div>
<div id="ref-hastie2009elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York: Springer. <a href="https://link.springer.com/book/10.1007/978-0-387-84858-7">https://link.springer.com/book/10.1007/978-0-387-84858-7</a>.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E., and Robert W. Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67. <a href="https://doi.org/10.1080/00401706.1970.10488634">https://doi.org/10.1080/00401706.1970.10488634</a>.
</div>
<div id="ref-Ishwaran2005" class="csl-entry">
Ishwaran, H., and J. S. Rao. 2005. <span>“Spike and Slab Variable Selection: Frequentist and <span>B</span>ayesian Strategies.”</span> <em>The Annals of Statistics</em> 33 (2): 730–73.
</div>
<div id="ref-johnson2012bayesian" class="csl-entry">
Johnson, Valen E, and David Rossell. 2012. <span>“Bayesian Model Selection in High-Dimensional Settings.”</span> <em>Journal of the American Statistical Association</em> 107 (498): 649–60.
</div>
<div id="ref-kyung2010penalized" class="csl-entry">
Kyung, Minjung, Jeff Gill, Malay Ghosh, and George Casella. 2010. <span>“Penalized Regression, Standard Errors, and Bayesian Lassos.”</span> <em>Bayesian Analysis</em> 5 (2): 369–411. <a href="https://doi.org/10.1214/10-BA607">https://doi.org/10.1214/10-BA607</a>.
</div>
<div id="ref-ohara2009bayesian" class="csl-entry">
O’Hara, Robert B., and Mikko J. Sillanpää. 2009. <span>“A Review of Bayesian Variable Selection Methods: What, How and Which.”</span> <em>Bayesian Analysis</em> 4 (1): 85–118. <a href="https://doi.org/10.1214/09-BA403">https://doi.org/10.1214/09-BA403</a>.
</div>
<div id="ref-Park2008" class="csl-entry">
Park, T., and G. Casella. 2008. <span>“The <span>B</span>ayesian Lasso.”</span> <em>Journal of the American Statistical Association</em> 103 (482): 681–86.
</div>
<div id="ref-rockova2018spike" class="csl-entry">
Ročková, Veronika, and Edward I. George. 2018. <span>“The Spike-and-Slab LASSO.”</span> <em>Journal of the American Statistical Association</em> 113 (521): 431–44. <a href="https://doi.org/10.1080/01621459.2016.1260469">https://doi.org/10.1080/01621459.2016.1260469</a>.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88. <a href="https://www.jstor.org/stable/2346178">https://www.jstor.org/stable/2346178</a>.
</div>
<div id="ref-zou2005regularization" class="csl-entry">
Zou, Hui, and Trevor Hastie. 2005. <span>“Regularization and Variable Selection via the Elastic Net.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2): 301–20. <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">https://doi.org/10.1111/j.1467-9868.2005.00503.x</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12_1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec12_3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/13-RecentDev.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
