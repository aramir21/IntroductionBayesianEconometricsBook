<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12.html"/>
<link rel="next" href="summary-chapter-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapGUI.html"><a href="chapGUI.html"><i class="fa fa-check"></i><b>2</b> Graphical user interface</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec13" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Bayesian reports: Decision theory under uncertainty<a href="sec13.html#sec13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Bayesian framework allows reporting the full posterior distributions. However, some situations demand to report a specific value of the posterior distribution (point estimate), an informative interval (set), point or interval predictions and/or selecting a specific model. Decision theory offers an elegant framework to make a decision regarding what are the optimal posterior values to report <span class="citation">(<a href="#ref-berger2013statistical">Berger 2013</a>)</span>.</p>
<p>The point of departure is a <em>loss function</em>, which is a non-negative real value function whose arguments are the unknown <em>state of nature</em> (<span class="math inline">\(\mathbf{\Theta}\)</span>), and a set of <em>actions</em> to be made (<span class="math inline">\(\mathcal{A}\)</span>), that is,
<span class="math display">\[\begin{equation}
L(\mathbf{\theta}, a):\mathbf{\Theta}\times \mathcal{A}\rightarrow R^+.
\end{equation}\]</span></p>
<p>This function is a mathematical expression of the loss of making mistakes. In particular, selecting action <span class="math inline">\(a\in\mathcal{A}\)</span> when <span class="math inline">\(\mathbf{\theta}\in\mathbf{\Theta}\)</span> is the true. In our case, the unknown state of nature can be parameters, functions of them, future or unknown realizations, models, etc.</p>
<p>From a Bayesian perspective, we should choose the action (<span class="math inline">\(a^*(\mathbf{y})\)</span>) that minimizes the posterior expected loss, which is the <em>posterior risk function</em> (<span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}]\)</span>),</p>
<p><span class="math display">\[\begin{equation}
  a^*(\mathbf{y})=\underset{a \in \mathcal{A}}{\mathrm{argmin}} \  \mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}],
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}]= \int_{\mathbf{\Theta}} L(\mathbf{\theta}, a)\pi(\mathbf{\theta}|\mathbf{y})d\mathbf{\theta}\)</span>.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>Different loss functions imply different optimal decisions. We illustrate this assuming <span class="math inline">\(\theta \in \mathcal{R}\)</span>.</p>
<ul>
<li>The quadratic loss function, <span class="math inline">\(L({\theta},a)=[{\theta}-a]^2\)</span>, gives as optimal decision the posterior mean, <span class="math inline">\(a^*(\mathbf{y})=\mathbb{E}[{\theta}|\mathbf{y}]\)</span>, that is</li>
</ul>
<p><span class="math display">\[\begin{equation}
  \mathbb{E}[{\theta}|\mathbf{y}] = \underset{a \in \mathcal{A}}{\mathrm{argmin}} \  \int_{{\Theta}} [{\theta}-a]^2\pi({\theta}|\mathbf{y})d{\theta}.
\end{equation}\]</span></p>
<p>To get this results, let us use the first condition order, differentiate the risk function with respect to <span class="math inline">\(a\)</span>, interchange differential and integral order, and set this equal to zero, <span class="math inline">\(-2\int_{{\Theta}} [{\theta}-a^*]\pi({\theta}|\mathbf{y})d{\theta}=0\)</span> implies that <span class="math inline">\(a^*\int_{{\Theta}} \pi({\theta}|\mathbf{y})d{\theta}=a^*(\mathbf{y})=\int_{{\Theta}} {\theta}\pi({\theta}|\mathbf{y})d{\theta}=\mathbb{E}[{\theta}|\mathbf{y}]\)</span>, that is, the posterior mean is the Bayesian optimal action. This means that we should report the posterior mean as a point estimate of <span class="math inline">\(\theta\)</span> when facing the quadratic loss function.</p>
<ul>
<li><p>The generalized quadratic loss function, <span class="math inline">\(L({\theta},a)=w({\theta})[{\theta}-a]^2\)</span>, where <span class="math inline">\(w({\theta})&gt;0\)</span> is a weighting function, gives as optimal decision rule the weighted mean. We should follow same steps as the previous result to get <span class="math inline">\(a^*(\mathbf{y})=\frac{\mathbb{E}[w({\theta})\times{\theta}|\mathbf{y}]}{\mathbb{E}[w({\theta})|\mathbf{y}]}\)</span>. Observe that the weighted average is driven by the weighted function <span class="math inline">\(w({\theta})\)</span>.</p></li>
<li><p>The absolute error loss function, <span class="math inline">\(L({\theta},a)=|{\theta}-a|\)</span>, gives as optimal action the posterior median (exercise 5).</p></li>
<li><p>The generalized absolute error function,</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
L(\theta,a)=\begin{Bmatrix} K_0(\theta-a), \theta-a\geq 0\\
K_1(a-\theta), \theta-a &lt; 0 \end{Bmatrix}, K_0, K_1 &gt;0,
\end{equation}\]</span></p>
<p>implies the following risk function,</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}[L(\theta, a)|\mathbf{y}]&amp;=\int_{-\infty}^a K_1(a-\theta)\pi(\theta|\mathbf{y})d\theta + \int_a^{\infty} K_0(\theta-a)\pi(\theta|\mathbf{y})d\theta.
\end{align}\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(a\)</span>, interchanging differentials and integrals, and equating to zero,</p>
<p><span class="math display">\[\begin{align}
  K_1\int_{-\infty}^{a^*} \pi(\theta|\mathbf{y})d\theta-K_0\int_{a^*}^{\infty} \pi(\theta|\mathbf{y})d\theta&amp;=0,
\end{align}\]</span></p>
<p>then, <span class="math inline">\(\int_{-\infty}^{a^*} \pi(\theta|\mathbf{y})d\theta=\frac{K_0}{K_0+K_1}\)</span>, that is, any <span class="math inline">\(K_0/(K_0+K_1)\)</span>-percentile of <span class="math inline">\(\pi(\theta|\mathbf{y})\)</span> is an optimal Bayesian estimate of <span class="math inline">\(\theta\)</span>.</p>
<p>We can also use decision theory under uncertainty in hypothesis testing. In particular, testing <span class="math inline">\(H_0:\theta\in\Theta_0\)</span> versus <span class="math inline">\(H_1:\theta\in\Theta_1\)</span>, <span class="math inline">\(\Theta=\Theta_0 \cup \Theta_1\)</span> and <span class="math inline">\(\emptyset=\Theta_0 \cap \Theta_1\)</span>, there are two actions of interest, <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span>, where <span class="math inline">\(a_j\)</span> denotes no rejecting <span class="math inline">\(H_j\)</span>, <span class="math inline">\(j=\left\{0,1\right\}\)</span>.</p>
<p>Given the <span class="math inline">\(0-K_j\)</span> loss function,</p>
<p><span class="math display">\[\begin{equation}
L(\theta,a_j)=\begin{Bmatrix} 0, &amp; \theta\in\Theta_j\\
K_j, &amp; \theta\in\Theta_j, j\neq i \end{Bmatrix}.
\end{equation}\]</span></p>
<p>where there is no loss if the right decision is made, for instance, no rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(\theta\in\Theta_0\)</span>, and the loss is <span class="math inline">\(K_j\)</span> when an error is made, for instance, type I error, rejecting the null hypothesis (<span class="math inline">\(H_0\)</span>) when it is true (<span class="math inline">\(\theta\in\Theta_0\)</span>), implies a loss equal to <span class="math inline">\(K_1\)</span> due to picking <span class="math inline">\(a_1\)</span>, no rejecting <span class="math inline">\(H_1\)</span>.</p>
<p>The posterior expected loss associated with decision <span class="math inline">\(a_j\)</span>, that is, no rejecting <span class="math inline">\(H_j\)</span>, is <span class="math inline">\(\mathbb{E}[L(\theta,a_j)|\mathbf{y}]=0\times P(\Theta_j|\mathbf{y}) + K_jP(\Theta_i|\mathbf{y})=K_jP(\Theta_i|\mathbf{y})\)</span>, <span class="math inline">\(j\neq i\)</span>. Therefore, the Bayes optimal decision is the one that gives the smallest posterior expected loss, that is, the null hypothesis is rejected (<span class="math inline">\(a_1\)</span> is not rejected), when <span class="math inline">\(K_0P(\Theta_1|\mathbf{y}) &gt; K_1P(\Theta_0|\mathbf{y})\)</span>. Given our framework <span class="math inline">\((\Theta=\Theta_0 \cup \Theta_1, \emptyset=\Theta_0 \cap \Theta_1)\)</span>, then <span class="math inline">\(P(\Theta_0|\mathbf{y})=1-P(\Theta_1|\mathbf{y})\)</span>, and as a consequence, <span class="math inline">\(P(\Theta_1|\mathbf{y})&gt;\frac{K_1}{K_1+K_0}\)</span>, that is, the rejection region of the Bayesian test is <span class="math inline">\(R=\left\{\mathbf{y}:P(\Theta_1|\mathbf{y})&gt;\frac{K_1}{K_1+K_0}\right\}\)</span>.</p>
<p>Decision theory also helps to construct interval (region) estimates. Let <span class="math inline">\(\Theta_{C(\mathbf{y})}\subset \Theta\)</span> a <em>credible set</em> for <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(L(\theta,\Theta_{C(\mathbf{y})})=1-\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}\)</span>, where</p>
<p><span class="math display">\[\begin{equation}
\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}=\begin{Bmatrix}1, &amp; \theta\in \Theta_{C(\mathbf{y})}\\  
0, &amp; \theta\notin \Theta_{C(\mathbf{y})}
\end{Bmatrix}.
\end{equation}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\begin{equation}
L(\theta,\Theta_{C(\mathbf{y})})=\begin{Bmatrix}0, &amp; \theta\in \Theta_{C(\mathbf{y})}\\  
1, &amp; \theta\notin \Theta_{C(\mathbf{y})}
\end{Bmatrix}.
\end{equation}\]</span></p>
<p>where the 0–1 loss function is equal to zero if <span class="math inline">\(\theta\in \Theta_{C(\mathbf{y})}\)</span>, and one if <span class="math inline">\(\theta\notin \Theta_{C(\mathbf{y})}\)</span>. Then, the risk function is <span class="math inline">\(1-P(\theta\in \Theta_{C(\mathbf{y})})\)</span>.</p>
<p>Given a <em>measure of credibility</em> (<span class="math inline">\(\alpha(\mathbf{y})\)</span>) that defines the level of trust that <span class="math inline">\(\theta\in \Theta_{C(\mathbf{y})}\)</span>; then, we can measure the accuracy of the report by <span class="math inline">\(L(\theta, \alpha(\mathbf{y}))=[\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}-\alpha(\mathbf{y})]^2\)</span>. This loss function could be used to suggest a choice of the report <span class="math inline">\(\alpha(\mathbf{y})\)</span>. Given that this is a quadratic loss function, the optimal action is the posterior mean, that is <span class="math inline">\(\mathbb{E}[\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}|\mathbf{y}]=P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})\)</span>. This probability can be calculated given the posterior distribution, that is, <span class="math inline">\(P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})=\int_{\Theta_{C(\mathbf{y})}}\pi(\theta|\mathbf{y})d\theta\)</span>. This is a measure of the belief that <span class="math inline">\(\theta\in \Theta_{C(\mathbf{y})}\)</span> given the prior beliefs and sample information.</p>
<p>The set <span class="math inline">\(\Theta_{C(\mathbf{y})}\in\Theta\)</span> is a <span class="math inline">\(100(1-\alpha)\%\)</span> credible set with respect to <span class="math inline">\(\pi(\theta|\mathbf{y})\)</span> if <span class="math inline">\(P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})=\int_{\Theta_{C(\mathbf{y})}}\pi(\theta|\mathbf{y})=1-\alpha\)</span>.</p>
<p>Two alternatives to report credible sets are the <em>symmetric credible set</em> and the <em>highest posterior density set</em> (HPD). The former is based on <span class="math inline">\(\frac{\alpha}{2}\)</span>% and <span class="math inline">\((1-\frac{\alpha}{2})\)</span>% percentiles of the posterior distribution, and the latter is a <span class="math inline">\(100(1-\alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span> with the property that it has the smallest distance compared to any other <span class="math inline">\(100(1-\alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span> based on the posterior distribution. That is, <span class="math inline">\(C(\mathbf{y})=\left\{\theta:\pi(\theta|\mathbf{y})\geq k(\alpha)\right\}\)</span>, where <span class="math inline">\(k(\alpha)\)</span> is the largest number such that <span class="math inline">\(\int_{\theta:\pi(\theta|\mathbf{y})\geq k(\alpha)}\pi(\theta|\mathbf{y})d\theta=1-\alpha\)</span>. The HPDs can be a collection of disjoint intervals when working with multimodal posterior densities. In addition, they have the limitation of not necessary being invariant under transformations.</p>
<p>Decision theory can be used to perform prediction (point, sets or probabilistic). Suppose that there is a loss function <span class="math inline">\(L(Y_0,a)\)</span> involving the prediction of <span class="math inline">\(Y_0\)</span>. Then, <span class="math inline">\(\mathbb{E}_{Y_0}[L(Y_0,a)]=\int_{\mathcal{Y}_0}L(Y_0,a)\pi(Y_0|\mathbf{y})dY_0\)</span>, where <span class="math inline">\(\pi(Y_0|\mathbf{y})\)</span> is the predictive density function. Thus, we make an optimal choice for prediction that minimizes the risk function given a specific loss function.</p>
<p>BMA allows incorporating model uncertainty in a regression framework, sometimes it is desirable to select just one model. A compelling alternative is the model with the highest posterior model probability. This model is the best alternative for prediction in the case of a 0–1 loss function <span class="citation">(<a href="#ref-Clyde2004">Clyde and George 2004</a>)</span>.</p>
<!--
Other approach is to use scoring rules to assess the quality of the predictive (probabilistic) forecasts. This is assigning a numerical score based on the predictive distribution on the event that realizes [@Gneiting2007]. Then, we can use decision theory to define the most relevant scoring rule for the problem at hand, such that we assign a high ordinate to the realized value (*calibration*). In addition, it is possible to add some reward for accuracy in specific parts of the support of the density function (*sharpness*) [@Diks2011].   
-->
<p><strong>Example: Health insurance continues</strong></p>
<p>We show some optimal rules in the health insurance example. In particular, the best point estimates of <span class="math inline">\(\lambda\)</span> given the quadratic, absolute and generalized absolute loss functions. For the latter, we assume that underestimating <span class="math inline">\(\lambda\)</span> is twice as costly as overestimating it, that is, <span class="math inline">\(K_0=2\)</span> and <span class="math inline">\(K_1=1\)</span>.</p>
<p>Taking into account that the posterior distribution of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(G(\alpha_0+\sum_{i=1}^N y_i, \beta_0/(\beta_0N+1))\)</span>, using the hyperparameters from empirical Bayes, we have that <span class="math inline">\(\mathbb{E}[\lambda|\mathbf{y}]=\alpha_n\beta_n=1.2\)</span>, the median is 1.19, and the 2/3-th quantile is 1.26. Those are the optimal point estimates for the quadratic, absolute and generalized absolute loss functions.</p>
<p>In addition, we test the null hypothesis <span class="math inline">\(H_0. \lambda \in [0,1)\)</span> versus <span class="math inline">\(H_1. \lambda \in [1,\infty)\)</span> setting <span class="math inline">\(K_0=K_1=1\)</span> we should reject the null hypothesis due to <span class="math inline">\(P(\lambda \in [0,1))=0.9&gt;K_1/(K_0+K_1)=0.5\)</span>.</p>
<p>We get that the 95% symmetric credible interval is (0.91, 1.53), and the highest posterior density interval is (0.9, 1.51). Finally, the optimal point prediction under a quadratic loss function is 1.2, which is the mean value of the posterior predictive distribution, and the optimal model assuming a 0-1 loss function is the model using the hyperparameters from the empirical Bayes procedure due to the posterior model probability of this model being approximately 1, whereas the posterior model probability of the model using vague hyperparameters is approximately 0.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="sec13.html#cb50-1" tabindex="-1"></a>an <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">+</span> a0EB <span class="co"># Posterior shape parameter</span></span>
<span id="cb50-2"><a href="sec13.html#cb50-2" tabindex="-1"></a>bn <span class="ot">&lt;-</span> b0EB <span class="sc">/</span> (N<span class="sc">*</span>b0EB <span class="sc">+</span> <span class="dv">1</span>) <span class="co"># Posterior scale parameter</span></span>
<span id="cb50-3"><a href="sec13.html#cb50-3" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000000</span> <span class="co"># Number of posterior draws</span></span>
<span id="cb50-4"><a href="sec13.html#cb50-4" tabindex="-1"></a>Draws <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1000000</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Posterior draws</span></span>
<span id="cb50-5"><a href="sec13.html#cb50-5" tabindex="-1"></a><span class="do">###### Point estimation ########</span></span>
<span id="cb50-6"><a href="sec13.html#cb50-6" tabindex="-1"></a>OptQua <span class="ot">&lt;-</span> an<span class="sc">*</span>bn <span class="co"># Mean: Optimal choice quadratic loss function</span></span>
<span id="cb50-7"><a href="sec13.html#cb50-7" tabindex="-1"></a>OptQua</span></code></pre></div>
<pre><code>## [1] 1.200952</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="sec13.html#cb52-1" tabindex="-1"></a>OptAbs <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.5</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Median: Optimal choice absolute loss function</span></span>
<span id="cb52-2"><a href="sec13.html#cb52-2" tabindex="-1"></a>OptAbs</span></code></pre></div>
<pre><code>## [1] 1.194034</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="sec13.html#cb54-1" tabindex="-1"></a><span class="co"># Setting K0 = 2 and K1 = 1, that is, to underestimate lambda is twice as costly as to overestimate it.</span></span>
<span id="cb54-2"><a href="sec13.html#cb54-2" tabindex="-1"></a>K0 <span class="ot">&lt;-</span> <span class="dv">2</span>; K1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb54-3"><a href="sec13.html#cb54-3" tabindex="-1"></a>OptGenAbs <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Draws, K0<span class="sc">/</span>(K0 <span class="sc">+</span> K1)) <span class="co"># Median: Optimal choice generalized absolute loss function</span></span>
<span id="cb54-4"><a href="sec13.html#cb54-4" tabindex="-1"></a>OptGenAbs</span></code></pre></div>
<pre><code>## 66.66667% 
##  1.262986</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="sec13.html#cb56-1" tabindex="-1"></a><span class="do">###### Hypothesis test ########</span></span>
<span id="cb56-2"><a href="sec13.html#cb56-2" tabindex="-1"></a><span class="co"># H0: lambda in [0,1) vs H1: lambda in [1, Inf]</span></span>
<span id="cb56-3"><a href="sec13.html#cb56-3" tabindex="-1"></a>K0 <span class="ot">&lt;-</span> <span class="dv">1</span>; K1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb56-4"><a href="sec13.html#cb56-4" tabindex="-1"></a>ProbH0 <span class="ot">&lt;-</span> <span class="fu">pgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) </span>
<span id="cb56-5"><a href="sec13.html#cb56-5" tabindex="-1"></a>ProbH0 <span class="co"># Posterior  probability H0</span></span></code></pre></div>
<pre><code>## [1] 0.09569011</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="sec13.html#cb58-1" tabindex="-1"></a>ProbH1 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span>ProbH0</span>
<span id="cb58-2"><a href="sec13.html#cb58-2" tabindex="-1"></a>ProbH1 <span class="co"># Posterior  probability H1</span></span></code></pre></div>
<pre><code>## [1] 0.9043099</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="sec13.html#cb60-1" tabindex="-1"></a><span class="co"># we should reject H0 given ProbH1 &gt; K1 / (K0 + K1) </span></span>
<span id="cb60-2"><a href="sec13.html#cb60-2" tabindex="-1"></a></span>
<span id="cb60-3"><a href="sec13.html#cb60-3" tabindex="-1"></a><span class="do">###### Credible intervals ########</span></span>
<span id="cb60-4"><a href="sec13.html#cb60-4" tabindex="-1"></a>LimInf <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.025</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Lower bound</span></span>
<span id="cb60-5"><a href="sec13.html#cb60-5" tabindex="-1"></a>LimInf</span></code></pre></div>
<pre><code>## [1] 0.9114851</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="sec13.html#cb62-1" tabindex="-1"></a>LimSup <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="fl">0.975</span>, <span class="at">shape =</span> an, <span class="at">scale =</span> bn) <span class="co"># Upper bound</span></span>
<span id="cb62-2"><a href="sec13.html#cb62-2" tabindex="-1"></a>LimSup</span></code></pre></div>
<pre><code>## [1] 1.529724</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="sec13.html#cb64-1" tabindex="-1"></a>HDI <span class="ot">&lt;-</span> HDInterval<span class="sc">::</span><span class="fu">hdi</span>(Draws, <span class="at">credMass =</span> <span class="fl">0.95</span>) <span class="co"># Highest posterior density credible interval</span></span>
<span id="cb64-2"><a href="sec13.html#cb64-2" tabindex="-1"></a>HDI</span></code></pre></div>
<pre><code>##     lower     upper 
## 0.8971505 1.5125911 
## attr(,&quot;credMass&quot;)
## [1] 0.95</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="sec13.html#cb66-1" tabindex="-1"></a><span class="do">###### Predictive optimal choices ########</span></span>
<span id="cb66-2"><a href="sec13.html#cb66-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> bn <span class="sc">/</span> (bn <span class="sc">+</span> <span class="dv">1</span>) <span class="co"># Probability negative binomial density</span></span>
<span id="cb66-3"><a href="sec13.html#cb66-3" tabindex="-1"></a>OptPred <span class="ot">&lt;-</span> p<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">*</span>an <span class="co"># Optimal point prediction given a quadratic loss function in prediction</span></span>
<span id="cb66-4"><a href="sec13.html#cb66-4" tabindex="-1"></a>OptPred</span></code></pre></div>
<pre><code>## [1] 1.200952</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="sec13.html#cb68-1" tabindex="-1"></a><span class="co"># Given a 0-1 loss function for prediction, the optimal model is the one using empirical Bayes due to having a posterior model probability approximately equal to 1.</span></span></code></pre></div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berger2013statistical" class="csl-entry">
Berger, James O. 2013. <em>Statistical Decision Theory and Bayesian Analysis</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-Chernozhukov2003" class="csl-entry">
Chernozhukov, V., and H. Hong. 2003. <span>“An <span>MCMC</span> Approach to Classical Estimation.”</span> <em>Journal of Econometrics</em> 115: 293–346.
</div>
<div id="ref-Clyde2004" class="csl-entry">
Clyde, M., and E. George. 2004. <span>“Model Uncertatinty.”</span> <em>Statistical Science</em> 19 (1): 81–94.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p><span class="citation">(<a href="#ref-Chernozhukov2003">Chernozhukov and Hong 2003</a>)</span> propose Laplace type estimators (LTE) based on the <em>quasi-posterior</em>, <span class="math inline">\(p(\mathbf{\theta})=\frac{\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})}{\int_{\mathbf{\Theta}}\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})d\theta}\)</span> where <span class="math inline">\(L_n(\mathbf{\theta})\)</span> is not necessarily a log-likelihood function. The LTE minimizes the <em>quasi-posterior risk</em>.<a href="sec13.html#fnref12" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-chapter-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
