<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.9 Doubly robust Bayesian inferential framework (DRB) | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="13.9 Doubly robust Bayesian inferential framework (DRB) | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.9 Doubly robust Bayesian inferential framework (DRB) | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec13_8.html"/>
<link rel="next" href="sec13_11.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Identification setting</a></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Randomized controlled trial (RCT)</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Conditional independence assumption (CIA)</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Instrumental variables (IV)</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference-in-differences design (DiD)</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="sec13_5.html"><a href="sec13_5.html#sec13_51"><i class="fa fa-check"></i><b>13.5.1</b> Classic DiD: two-group and two-period (<span class="math inline">\(2\times2\)</span>) setup</a></li>
<li class="chapter" data-level="13.5.2" data-path="sec13_5.html"><a href="sec13_5.html#sec13_52"><i class="fa fa-check"></i><b>13.5.2</b> Staggered (SDiD): G-group and T-period (<span class="math inline">\(G\times T\)</span>) setup</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Regression discontinuity design (RD)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="sec13_6.html"><a href="sec13_6.html#sec13_61"><i class="fa fa-check"></i><b>13.6.1</b> Sharp regression discontinuity design (SRD)</a></li>
<li class="chapter" data-level="13.6.2" data-path="sec13_6.html"><a href="sec13_6.html#sec13_62"><i class="fa fa-check"></i><b>13.6.2</b> Fuzzy regression discontinuity design (FRD)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Sample selection</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Bayesian exponentially tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.9" data-path="sec13_10.html"><a href="sec13_10.html"><i class="fa fa-check"></i><b>13.9</b> Doubly robust Bayesian inferential framework (DRB)</a></li>
<li class="chapter" data-level="13.10" data-path="sec13_11.html"><a href="sec13_11.html"><i class="fa fa-check"></i><b>13.10</b> Summary</a></li>
<li class="chapter" data-level="13.11" data-path="sec13_12.html"><a href="sec13_12.html"><i class="fa fa-check"></i><b>13.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec13_10" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> Doubly robust Bayesian inferential framework (DRB)<a href="sec13_10.html#sec13_10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> propose a doubly robust Bayesian (DRB) approach to perform inference on the average treatment effect (ATE) under unconfoundedness. They adjust the prior for the conditional mean outcome function using an estimator of the propensity score, the conditional probability of treatment assignment <span class="citation">(<a href="#ref-rosenbaum1983central">Rosenbaum and Rubin 1983</a>)</span>, and then re-center the posterior distribution of the treatment effect using the propensity score estimator together with a pilot estimator of the outcome regression. The authors show that under unconfoundedness and overlap, the corrected posterior converges in distribution to a normal law centered at a semiparametrically efficient estimator, and that the resulting credible intervals achieve asymptotically exact Frequentist coverage.</p>
<p>Let <span class="math inline">\(Y_i\in\{0,1\}\)</span> be the outcome and <span class="math inline">\(D_i\in\{0,1\}\)</span> the treatment,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> thus</p>
<p><span class="math display">\[
Y_i = D_i\,Y_i(1) + (1-D_i)\,Y_i(0),
\]</span></p>
<p>where <span class="math inline">\(Y_i(1)\)</span> and <span class="math inline">\(Y_i(0)\)</span> are the potential outcomes.</p>
<p>Let <span class="math inline">\(\mathbf{X}_i\)</span> denote pre-treatment covariates with distribution <span class="math inline">\(F_0\)</span> and density <span class="math inline">\(f_0\)</span>, and define the propensity score</p>
<p><span class="math display">\[
\pi_0(\mathbf{x}) := P(D_i=1\mid \mathbf{X}_i=\mathbf{x})
\]</span></p>
<p>and the conditional mean outcome (success probability)</p>
<p><span class="math display">\[
m_0(d,\mathbf{x}) := P(Y_i=1\mid D_i=d,\mathbf{X}_i=\mathbf{x}).
\]</span></p>
<p>For an i.i.d. sample <span class="math inline">\(W_i=(Y_i,D_i,\mathbf{X}_i^{\top})^{\top}\)</span>, the joint density can be written as</p>
<p><span class="math display">\[
p_{f,\pi,m}(y,d,\mathbf{x})
= f(\mathbf{x})\,[\pi(\mathbf{x})]^d\,[1-\pi(\mathbf{x})]^{1-d}\,[m(d,\mathbf{x})]^y\,[1-m(d,\mathbf{x})]^{1-y}.
\]</span></p>
<p>The parameter of interest is the average treatment effect (ATE),</p>
<p><span class="math display">\[
\tau := \mathbb{E}_0\!\left[\,Y_i(1)-Y_i(0)\,\right],
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}_0\)</span> is the expectation with respect to the population distribution.</p>
<p><span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> impose support restrictions via link functions. For the outcome regression and propensity score use logits:</p>
<p><span class="math display">\[
m_\eta(d,\mathbf{x})=\frac{1}{1+\exp\{-\eta^m(d,\mathbf{x})\}}, \qquad
\pi_\eta(\mathbf{x})=\frac{1}{1+\exp\{-\eta^\pi(\mathbf{x})\}},
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\eta^m(d,\mathbf{x})=\log\!\left(\frac{m_\eta(d,\mathbf{x})}{1-m_\eta(d,\mathbf{x})}\right), \qquad
\eta^\pi(\mathbf{x})=\log\!\left(\frac{\pi_\eta(\mathbf{x})}{1-\pi_\eta(\mathbf{x})}\right).
\]</span></p>
<p>For the covariate density, they use a log-density parametrization</p>
<p><span class="math display">\[
f_\eta(\mathbf{x})=\exp\{\eta^f(\mathbf{x})\}, \qquad
\eta^f(\mathbf{x})=\log f_\eta(\mathbf{x}).
\]</span></p>
<p>Given a prior on <span class="math inline">\((\eta^m,\eta^\pi,\eta^f)\)</span>, the induced ATE is</p>
<p><span class="math display">\[
\tau_\eta := \mathbb{E}_\eta\!\left[m_\eta(1,\mathbf{X})-m_\eta(0,\mathbf{X})\right].
\]</span></p>
<p>The basis for inference on <span class="math inline">\(\tau_\eta\)</span> is the <em>efficient influence function</em> (EIF):</p>
<p><span class="math display">\[
\tilde{\tau}_\eta(W)
= \big\{m_\eta(1,\mathbf{X})-m_\eta(0,\mathbf{X})\big\}
+ \gamma_\eta(D,\mathbf{X})\,\big\{Y-m_\eta(D,\mathbf{X})\big\}
- \tau_\eta,
\]</span></p>
<p>where the Riesz representer for the ATE functional is</p>
<p><span class="math display">\[
\gamma_\eta(D,\mathbf{X})
= \frac{D}{\pi_\eta(\mathbf{X})}
- \frac{1-D}{1-\pi_\eta(\mathbf{X})}.
\]</span></p>
<p>An influence function measures the first-order effect of an infinitesimal contamination of the underlying distribution on the estimand. The <em>efficient</em> influence function attains the semiparametric efficiency bound (its variance is the minimal asymptotic variance in the model). At the truth <span class="math inline">\(\eta_0\)</span>, the EIF is mean-zero: <span class="math inline">\(\mathbb{E}_0[\tilde{\tau}_{\eta_0}(W)]=0\)</span>. Moreover, it is <em>doubly robust</em>:</p>
<p><span class="math display">\[
\mathbb{E}_0\!\big[\tilde{\tau}_\eta(W)\big]=0
\quad\text{if either } m_\eta \text{ or } \pi_\eta \text{ is correctly specified.}
\]</span></p>
<p>To conduct Bayesian inference, <span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> place independent priors on the outcome regression and the covariate distribution (equivalently, on <span class="math inline">\(\eta^m\)</span> and <span class="math inline">\(\eta^f\)</span>). For the covariates they use a Dirichlet Process (DP) prior on <span class="math inline">\(F\)</span> (hence on <span class="math inline">\(f\)</span>; see Section @ref(sec11_12)), which, under standard conditions, induces posterior draws that approximate the Bayesian bootstrap for sample supported functionals <span class="citation">(<a href="#ref-Lo1987BayesianBootstrap">Lo 1987</a>)</span> (see Section <a href="sec610.html#sec610">6.10</a>).</p>
<p>For the outcome regression they place a Gaussian Process (GP) prior on <span class="math inline">\(\eta^m\)</span> (see Section @ref(sec12_4)), with a correction term that incorporates a preliminary estimator of the Riesz representer. The propensity score is estimated in a separate first step and is used in the correction/recentering; thus, a prior for <span class="math inline">\(\eta^\pi\)</span> is not required for their procedure.</p>
<p>The following algorithm outlines the doubly robust Bayesian procedure of <span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span>. The pilot estimate of the Riesz representer, <span class="math inline">\(\hat\gamma_\eta\)</span>, is obtained by plug–in, using a propensity score <span class="math inline">\(\hat\pi(\mathbf{x})\)</span> estimated via logistic Lasso.</p>
<p>The pilot estimate of the outcome regression, <span class="math inline">\(\hat m_\eta(d,\mathbf{x})=\Pr(Y=1\mid D=d,\mathbf{X}=\mathbf{x})\)</span>, is taken as the posterior mean from a GP <em>classification</em> model fit with a Laplace approximation.</p>
<p>Concretely, place a GP prior on the latent logit
<span class="math inline">\(\eta^m(d,\mathbf{x})\)</span> with a squared exponential kernel, and map to
success probabilities via the logistic link
<span class="math inline">\(m(d,\mathbf{x})=\operatorname{logit}^{-1}\{\eta^m(d,\mathbf{x})\}\)</span>.
Because the Bernoulli likelihood makes the required integrals over
<span class="math inline">\(\eta^m\)</span> analytically intractable, the posterior is approximated using
Laplace’s method (Expectation Propagation is a common alternative).
See Sections 3.3–3.5 of <span class="citation">Rasmussen and Williams (<a href="#ref-rasmussen2006gaussian">2006</a>)</span> for details.</p>
<p>The <em>adjusted</em> GP prior specifies the latent logit as</p>
<p><span class="math display">\[
\eta^m(d,\mathbf{x}_i)
\;=\;
W^m(d,\mathbf{x}_i)\;+\;\lambda\,\widehat{\gamma}_\eta(d,\mathbf{x}_i),
\qquad
W^m \sim \mathrm{GP}\!\big(0, K\big),\ \ \lambda \sim N(0,\sigma_n^2),
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the base kernel. Following <span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span>, take</p>
<p><span class="math display">\[
\sigma_n \;=\; \frac{\log n}{\sqrt{n}\,\Gamma_n},
\qquad
\Gamma_n \;:=\; \frac{1}{n}\sum_{i=1}^n
\big|\widehat{\gamma}_\eta(d_i,\mathbf{x}_i)\big|.
\]</span></p>
<p>Since <span class="math inline">\(W^m\)</span> and <span class="math inline">\(\lambda\)</span> are Gaussian, this is equivalent to a GP with an augmented kernel,</p>
<p><span class="math display">\[
\eta^m \sim \mathrm{GP}\!\big(0,\, K_c\big),
\quad
K_c\big((d,\mathbf{x}),(d&#39;,\mathbf{x}&#39;)\big)
= K\big((d,\mathbf{x}),(d&#39;,\mathbf{x}&#39;)\big)
+ \sigma_n^2\,\widehat{\gamma}_\eta(d,\mathbf{x})\,
\widehat{\gamma}_\eta(d&#39;,\mathbf{x}&#39;),
\]</span></p>
<p>which, for the vector of latent logits evaluated at the training inputs, corresponds to the covariance matrix
<span class="math inline">\(\mathbf{K} + \sigma_n^2\,\widehat{\boldsymbol{\gamma}}\,
\widehat{\boldsymbol{\gamma}}^{\top}\)</span>.</p>
<figcaption><b>Algorithm: Double Robust Bayesian</b></figcaption>
<figure id="alg:DRBayes">
<pre>
Have initial estimates of γ<sub>η</sub> based on logistic Lasso
Trim the propensity score using standard values (e.g., 10%) or the optimal trimming rule of Crump et al. (2009)
Estimate the Riesz representer using the estimate of the propensity score
Calculate the prior correction term 
      σₙ = log(n) / (√n Γₙ),   Γₙ = (1/n) Σ |γ̂η(dᵢ, xᵢ)|,   n = effective sample size (after trimming)
Calculate the prior adjustment σₙ × γ<sub>η</sub>
Calculate m̂(d,x) = 1 / (1 + exp{-ηᵐ(d,x)}) as the posterior mean of a Gaussian Process (GP) based on d and x (unadjusted mean)
Set an initial GP prior for the adjusted mean 
      mᶜη(d,x) = 1 / (1 + exp{-ηᵐ}),   ηᵐ ~ GP(0, Kc)
      Kc((d,x),(d',x')) = K((d,x),(d',x')) + σₙ² γ<sub>η</sub>(d,x)γ<sub>η</sub>(d',x')
For s = 1,...,S do
     a. Generate the s-th draw of the adjusted GP posterior mᶜˢη(d,x)
     b. Draw Bayesian bootstrap weights Mˢₙᵢ = eˢᵢ / Σ eˢⱼ, with eˢᵢ ~ Exp(1), i=1,...,n
     c. Calculate the corrected posterior draw for the ATE 
           τ̃ˢη = τˢη - b̂ˢη
           where τˢη = Σ Mˢₙᵢ ( mᶜˢη(1,xᵢ) - mᶜˢη(0,xᵢ) )
                 b̂ˢη = (1/n) Σ τ[ mˢη(wᵢ) - m̂(wᵢ) ]
                 τ[m] = m(1,x) - m(0,x) + γ̂(d,x)(y - m(d,x))
End for
</pre>
</figure>
<p>This algorithm produces posterior draws of the ATE.<br />
The <span class="math inline">\((1-\alpha)\%\)</span> credible interval is given by the empirical quantiles at levels <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span>.</p>
<p>It is worth noting that <span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> do not perform sample splitting, whereas <span class="citation">Chernozhukov et al. (<a href="#ref-chernozhukov2018double">2018</a>)</span> recommend it in the context of <em>Double/Debiased Machine Learning for treatment and structural parameters</em>. While sample splitting (or cross-fitting) helps reduce overfitting, its role in <span class="citation">Chernozhukov et al. (<a href="#ref-chernozhukov2018double">2018</a>)</span> is more fundamental: it ensures that the nuisance functions are estimated on data independent of the observations used for inference, which is critical for their theoretical guarantees of <span class="math inline">\(\sqrt{n}\)</span>-consistency and valid asymptotic inference. Nevertheless, <span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> report that they did not find evidence of overfitting in their setting and verified that their results are robust to the use of sample splitting.<br />
Finally, observe that the doubly robust approach does not propagate the uncertainty due to the estimation of the propensity score, which is treated as a plug-in. This omission is justified because the efficient influence function (EIF) is <em>Neyman orthogonal</em>: estimation error in the propensity score enters only at second order and is therefore asymptotically negligible for uncertainty quantification.</p>
<p><strong>Example: Doubly robust Bayesian inference</strong></p>
<p>Let us simulate the process<br />
<span class="math display">\[
\eta^{\pi}_i = -0.3 + 0.8X_{i1} - 0.5X_{i2} + 0.7X_{i3},
\quad
\eta^{m}_i = -0.2 + 0.6D_i + 0.5X_{i1} - 0.4X_{i2} + 0.3X_{i3},
\]</span>
where <span class="math inline">\(X_{1}\sim N(0,1)\)</span>, <span class="math inline">\(X_{2}\sim \mathrm{Bernoulli}(0.5)\)</span>, and <span class="math inline">\(X_{3}\sim U(-1,1)\)</span>, and the sample size is 2,000.</p>
<p>The population ATE from this simulation is approximately <span class="math inline">\(0.138\)</span>. The posterior mean and 95% credible interval of the ATE using the DRB are <span class="math inline">\(0.155\)</span> and <span class="math inline">\((0.107,\,0.206)\)</span>, respectively.</p>
<p>The following code shows how to implement the DRB inferential framework in this setting.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="sec13_10.html#cb11-1" tabindex="-1"></a><span class="do">######### Doubly Robust Bayesian: Simulation #########</span></span>
<span id="cb11-2"><a href="sec13_10.html#cb11-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">123</span>); <span class="fu">library</span>(glmnet)</span>
<span id="cb11-3"><a href="sec13_10.html#cb11-3" tabindex="-1"></a><span class="do">## simulate covariates</span></span>
<span id="cb11-4"><a href="sec13_10.html#cb11-4" tabindex="-1"></a>n  <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb11-5"><a href="sec13_10.html#cb11-5" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)                 <span class="co"># continuous</span></span>
<span id="cb11-6"><a href="sec13_10.html#cb11-6" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fl">0.5</span>)        <span class="co"># binary</span></span>
<span id="cb11-7"><a href="sec13_10.html#cb11-7" tabindex="-1"></a>X3 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)          <span class="co"># continuous</span></span>
<span id="cb11-8"><a href="sec13_10.html#cb11-8" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, X2, X3)</span>
<span id="cb11-9"><a href="sec13_10.html#cb11-9" tabindex="-1"></a><span class="co"># True propensity and outcome</span></span>
<span id="cb11-10"><a href="sec13_10.html#cb11-10" tabindex="-1"></a>logit <span class="ot">&lt;-</span> <span class="cf">function</span>(t) <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>t))</span>
<span id="cb11-11"><a href="sec13_10.html#cb11-11" tabindex="-1"></a>true_pi  <span class="ot">&lt;-</span> <span class="cf">function</span>(X){ <span class="fu">logit</span>(<span class="sc">-</span><span class="fl">0.3</span> <span class="sc">+</span> <span class="fl">0.8</span><span class="sc">*</span>X[,<span class="dv">1</span>] <span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>X[,<span class="dv">2</span>] <span class="sc">+</span> <span class="fl">0.7</span><span class="sc">*</span>X[,<span class="dv">3</span>]) }</span>
<span id="cb11-12"><a href="sec13_10.html#cb11-12" tabindex="-1"></a>true_eta <span class="ot">&lt;-</span> <span class="cf">function</span>(d,X){ <span class="sc">-</span><span class="fl">0.2</span> <span class="sc">+</span> <span class="fl">0.6</span><span class="sc">*</span>d <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>X[,<span class="dv">1</span>] <span class="sc">-</span><span class="fl">0.4</span><span class="sc">*</span>X[,<span class="dv">2</span>] <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>X[,<span class="dv">3</span>] }</span>
<span id="cb11-13"><a href="sec13_10.html#cb11-13" tabindex="-1"></a>true_p   <span class="ot">&lt;-</span> <span class="cf">function</span>(d,X){ <span class="fu">logit</span>(<span class="fu">true_eta</span>(d,X)) }</span>
<span id="cb11-14"><a href="sec13_10.html#cb11-14" tabindex="-1"></a>ATE  <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">true_p</span>(<span class="dv">1</span>,Z) <span class="sc">-</span> <span class="fu">true_p</span>(<span class="dv">0</span>,Z)) <span class="co"># Population ATE</span></span>
<span id="cb11-15"><a href="sec13_10.html#cb11-15" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb11-16"><a href="sec13_10.html#cb11-16" tabindex="-1"></a>pi <span class="ot">&lt;-</span> <span class="fu">true_pi</span>(Z); D  <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="dv">1</span>,pi)</span>
<span id="cb11-17"><a href="sec13_10.html#cb11-17" tabindex="-1"></a>P  <span class="ot">&lt;-</span> <span class="fu">true_p</span>(D,Z); Y  <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="dv">1</span>,P)</span>
<span id="cb11-18"><a href="sec13_10.html#cb11-18" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y, D, X1, <span class="at">X2 =</span> <span class="fu">factor</span>(X2), X3)</span>
<span id="cb11-19"><a href="sec13_10.html#cb11-19" tabindex="-1"></a><span class="do">######### Doubly Robust Bayesian: Implementation #########</span></span>
<span id="cb11-20"><a href="sec13_10.html#cb11-20" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">dim</span>(Z)[<span class="dv">2</span>]; covars <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>,<span class="st">&quot;X2&quot;</span>,<span class="st">&quot;X3&quot;</span>)</span>
<span id="cb11-21"><a href="sec13_10.html#cb11-21" tabindex="-1"></a>Z.df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(Z) <span class="co"># covariates as a dataframe</span></span>
<span id="cb11-22"><a href="sec13_10.html#cb11-22" tabindex="-1"></a><span class="co"># Propensity score model: lasso</span></span>
<span id="cb11-23"><a href="sec13_10.html#cb11-23" tabindex="-1"></a>cvfit <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(Z, D, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">nfolds =</span> <span class="dv">10</span>) <span class="co"># lasso,min</span></span>
<span id="cb11-24"><a href="sec13_10.html#cb11-24" tabindex="-1"></a>ps.coef <span class="ot">&lt;-</span> <span class="fu">coef</span>(cvfit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)[<span class="dv">0</span><span class="sc">:</span>p<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb11-25"><a href="sec13_10.html#cb11-25" tabindex="-1"></a>ps_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(cvfit, <span class="at">newx =</span> Z, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>,<span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb11-26"><a href="sec13_10.html#cb11-26" tabindex="-1"></a>ps_hat <span class="ot">&lt;-</span> <span class="fu">pmin</span>(<span class="fu">pmax</span>(ps_hat, <span class="fl">1e-6</span>), <span class="dv">1</span> <span class="sc">-</span> <span class="fl">1e-6</span>)</span>
<span id="cb11-27"><a href="sec13_10.html#cb11-27" tabindex="-1"></a><span class="co"># Riesz representer</span></span>
<span id="cb11-28"><a href="sec13_10.html#cb11-28" tabindex="-1"></a>riesz_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(d, ps) d<span class="sc">/</span>ps <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> d)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> ps)</span>
<span id="cb11-29"><a href="sec13_10.html#cb11-29" tabindex="-1"></a><span class="co"># GP model</span></span>
<span id="cb11-30"><a href="sec13_10.html#cb11-30" tabindex="-1"></a>make_gp <span class="ot">&lt;-</span> <span class="cf">function</span>(X_train, y_bin, <span class="at">use_correction =</span> <span class="cn">FALSE</span>, <span class="at">gamma_scaled =</span> <span class="cn">NULL</span>,</span>
<span id="cb11-31"><a href="sec13_10.html#cb11-31" tabindex="-1"></a><span class="at">approx_method =</span> gplite<span class="sc">::</span><span class="fu">approx_laplace</span>(),</span>
<span id="cb11-32"><a href="sec13_10.html#cb11-32" tabindex="-1"></a><span class="at">init_lscale =</span> <span class="fl">0.3</span>) {</span>
<span id="cb11-33"><a href="sec13_10.html#cb11-33" tabindex="-1"></a>    <span class="fu">stopifnot</span>(<span class="fu">length</span>(y_bin) <span class="sc">==</span> <span class="fu">nrow</span>(X_train))</span>
<span id="cb11-34"><a href="sec13_10.html#cb11-34" tabindex="-1"></a>    <span class="co"># Initializing the covariance function</span></span>
<span id="cb11-35"><a href="sec13_10.html#cb11-35" tabindex="-1"></a>    cf_se <span class="ot">&lt;-</span> gplite<span class="sc">::</span><span class="fu">cf_sexp</span>(<span class="at">vars =</span> <span class="fu">colnames</span>(X_train), <span class="at">lscale =</span> init_lscale, <span class="at">magn =</span> <span class="dv">1</span>, <span class="at">normalize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-36"><a href="sec13_10.html#cb11-36" tabindex="-1"></a>    cfs <span class="ot">&lt;-</span> <span class="fu">list</span>(cf_se)</span>
<span id="cb11-37"><a href="sec13_10.html#cb11-37" tabindex="-1"></a>    <span class="cf">if</span> (use_correction) {</span>
<span id="cb11-38"><a href="sec13_10.html#cb11-38" tabindex="-1"></a>        <span class="fu">stopifnot</span>(<span class="sc">!</span><span class="fu">is.null</span>(gamma_scaled))</span>
<span id="cb11-39"><a href="sec13_10.html#cb11-39" tabindex="-1"></a>        <span class="co"># Add linear kernel on the last column (gamma_scaled)</span></span>
<span id="cb11-40"><a href="sec13_10.html#cb11-40" tabindex="-1"></a>        cf_lin <span class="ot">&lt;-</span> gplite<span class="sc">::</span><span class="fu">cf_lin</span>(<span class="at">vars =</span> <span class="st">&quot;gamma&quot;</span>, <span class="at">magn =</span> <span class="dv">1</span>, <span class="at">normalize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb11-41"><a href="sec13_10.html#cb11-41" tabindex="-1"></a>        cfs <span class="ot">&lt;-</span> <span class="fu">list</span>(cf_se, cf_lin)</span>
<span id="cb11-42"><a href="sec13_10.html#cb11-42" tabindex="-1"></a>    }</span>
<span id="cb11-43"><a href="sec13_10.html#cb11-43" tabindex="-1"></a>    <span class="co"># Initializes a GP model</span></span>
<span id="cb11-44"><a href="sec13_10.html#cb11-44" tabindex="-1"></a>    gp <span class="ot">&lt;-</span> gplite<span class="sc">::</span><span class="fu">gp_init</span>(<span class="at">cfs =</span> cfs, <span class="at">lik =</span> gplite<span class="sc">::</span><span class="fu">lik_bernoulli</span>(), <span class="at">approx =</span> approx_method)</span>
<span id="cb11-45"><a href="sec13_10.html#cb11-45" tabindex="-1"></a>    <span class="co"># Optimizing hyperparameters</span></span>
<span id="cb11-46"><a href="sec13_10.html#cb11-46" tabindex="-1"></a>    gp <span class="ot">&lt;-</span> gplite<span class="sc">::</span><span class="fu">gp_optim</span>(gp, <span class="at">x =</span> X_train, <span class="at">y =</span> y_bin, <span class="at">maxiter =</span> <span class="dv">1000</span>, <span class="at">restarts =</span> <span class="dv">3</span>, <span class="at">tol =</span> <span class="fl">1e-05</span>)</span>
<span id="cb11-47"><a href="sec13_10.html#cb11-47" tabindex="-1"></a>    <span class="fu">return</span>(gp)</span>
<span id="cb11-48"><a href="sec13_10.html#cb11-48" tabindex="-1"></a>}</span>
<span id="cb11-49"><a href="sec13_10.html#cb11-49" tabindex="-1"></a>posterior_draws_prob <span class="ot">&lt;-</span> <span class="cf">function</span>(gp, X_test, <span class="at">ndraws =</span> <span class="dv">5000</span>) {</span>
<span id="cb11-50"><a href="sec13_10.html#cb11-50" tabindex="-1"></a>    out <span class="ot">&lt;-</span> gplite<span class="sc">::</span><span class="fu">gp_draw</span>(gp, <span class="at">xnew =</span> X_test, <span class="at">draws =</span> ndraws, <span class="at">transform =</span> <span class="cn">TRUE</span>, <span class="at">target =</span> <span class="cn">FALSE</span>, <span class="at">jitter =</span> <span class="fl">1e-6</span>)</span>
<span id="cb11-51"><a href="sec13_10.html#cb11-51" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">t</span>(out))</span>
<span id="cb11-52"><a href="sec13_10.html#cb11-52" tabindex="-1"></a>}</span>
<span id="cb11-53"><a href="sec13_10.html#cb11-53" tabindex="-1"></a><span class="co"># Main function</span></span>
<span id="cb11-54"><a href="sec13_10.html#cb11-54" tabindex="-1"></a>run_trim <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">t_trim =</span> <span class="fl">0.10</span>, <span class="at">N_post =</span> <span class="dv">5000</span>, <span class="at">h_r =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">cor_size =</span> <span class="dv">1</span>) {</span>
<span id="cb11-55"><a href="sec13_10.html#cb11-55" tabindex="-1"></a>    <span class="co"># Trim by estimated PS</span></span>
<span id="cb11-56"><a href="sec13_10.html#cb11-56" tabindex="-1"></a>    keep <span class="ot">&lt;-</span> <span class="fu">which</span>(ps_hat <span class="sc">&gt;=</span> t_trim <span class="sc">&amp;</span> ps_hat <span class="sc">&lt;=</span> (<span class="dv">1</span> <span class="sc">-</span> t_trim))</span>
<span id="cb11-57"><a href="sec13_10.html#cb11-57" tabindex="-1"></a>    n_eff <span class="ot">&lt;-</span> <span class="fu">length</span>(keep)</span>
<span id="cb11-58"><a href="sec13_10.html#cb11-58" tabindex="-1"></a>    <span class="cf">if</span> (n_eff <span class="sc">&lt;</span> <span class="dv">50</span>) <span class="fu">stop</span>(<span class="st">&quot;Too few observations after trimming.&quot;</span>)</span>
<span id="cb11-59"><a href="sec13_10.html#cb11-59" tabindex="-1"></a>    Yk  <span class="ot">&lt;-</span> Y[keep]</span>
<span id="cb11-60"><a href="sec13_10.html#cb11-60" tabindex="-1"></a>    Dk  <span class="ot">&lt;-</span> D[keep]</span>
<span id="cb11-61"><a href="sec13_10.html#cb11-61" tabindex="-1"></a>    Zk  <span class="ot">&lt;-</span> Z[keep, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb11-62"><a href="sec13_10.html#cb11-62" tabindex="-1"></a>    psk <span class="ot">&lt;-</span> ps_hat[keep]</span>
<span id="cb11-63"><a href="sec13_10.html#cb11-63" tabindex="-1"></a>    <span class="co"># Riesz representer and sigma_n choice</span></span>
<span id="cb11-64"><a href="sec13_10.html#cb11-64" tabindex="-1"></a>    gamma_k <span class="ot">&lt;-</span> <span class="fu">riesz_fun</span>(Dk, psk)</span>
<span id="cb11-65"><a href="sec13_10.html#cb11-65" tabindex="-1"></a>    sigma_n <span class="ot">&lt;-</span> cor_size <span class="sc">*</span> (<span class="fu">log</span>(n_eff) <span class="sc">/</span> ( (n_eff)<span class="sc">^</span>(h_r) <span class="sc">*</span> <span class="fu">mean</span>(<span class="fu">abs</span>(gamma_k)) ))</span>
<span id="cb11-66"><a href="sec13_10.html#cb11-66" tabindex="-1"></a>    gamma_scaled <span class="ot">&lt;-</span> sigma_n <span class="sc">*</span> gamma_k</span>
<span id="cb11-67"><a href="sec13_10.html#cb11-67" tabindex="-1"></a>    <span class="co"># Training inputs for GP: [Z, D] and, if corrected, an extra column &#39;gamma&#39;</span></span>
<span id="cb11-68"><a href="sec13_10.html#cb11-68" tabindex="-1"></a>    X_train_nc <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Zk)</span>
<span id="cb11-69"><a href="sec13_10.html#cb11-69" tabindex="-1"></a>    X_train_nc<span class="sc">$</span>D <span class="ot">&lt;-</span> Dk</span>
<span id="cb11-70"><a href="sec13_10.html#cb11-70" tabindex="-1"></a>    <span class="fu">colnames</span>(X_train_nc) <span class="ot">&lt;-</span> <span class="fu">c</span>(covars, <span class="st">&quot;D&quot;</span>)</span>
<span id="cb11-71"><a href="sec13_10.html#cb11-71" tabindex="-1"></a>    X_train_c <span class="ot">&lt;-</span> X_train_nc</span>
<span id="cb11-72"><a href="sec13_10.html#cb11-72" tabindex="-1"></a>    X_train_c<span class="sc">$</span>gamma <span class="ot">&lt;-</span> gamma_scaled</span>
<span id="cb11-73"><a href="sec13_10.html#cb11-73" tabindex="-1"></a>    <span class="co"># Test inputs: for each i, (Z_i, d=0) and (Z_i, d=1)</span></span>
<span id="cb11-74"><a href="sec13_10.html#cb11-74" tabindex="-1"></a>    X_t0 <span class="ot">&lt;-</span> X_train_nc; X_t0<span class="sc">$</span>D <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb11-75"><a href="sec13_10.html#cb11-75" tabindex="-1"></a>    X_t1 <span class="ot">&lt;-</span> X_train_nc; X_t1<span class="sc">$</span>D <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb11-76"><a href="sec13_10.html#cb11-76" tabindex="-1"></a>    X_t0c <span class="ot">&lt;-</span> X_t0; X_t1c <span class="ot">&lt;-</span> X_t1</span>
<span id="cb11-77"><a href="sec13_10.html#cb11-77" tabindex="-1"></a>    X_t0c<span class="sc">$</span>gamma <span class="ot">&lt;-</span> sigma_n <span class="sc">*</span> <span class="fu">riesz_fun</span>(<span class="dv">0</span>, psk)   </span>
<span id="cb11-78"><a href="sec13_10.html#cb11-78" tabindex="-1"></a>    X_t1c<span class="sc">$</span>gamma <span class="ot">&lt;-</span> sigma_n <span class="sc">*</span> <span class="fu">riesz_fun</span>(<span class="dv">1</span>, psk) </span>
<span id="cb11-79"><a href="sec13_10.html#cb11-79" tabindex="-1"></a>    <span class="co"># GP WITHOUT prior correction #</span></span>
<span id="cb11-80"><a href="sec13_10.html#cb11-80" tabindex="-1"></a>    gp_nc <span class="ot">&lt;-</span> <span class="fu">make_gp</span>(<span class="at">X_train =</span> X_train_nc, <span class="at">y_bin =</span> Yk, <span class="at">use_correction =</span> <span class="cn">FALSE</span>)</span>
<span id="cb11-81"><a href="sec13_10.html#cb11-81" tabindex="-1"></a>    <span class="co"># joint draws for [m(Z,0); m(Z,1)]</span></span>
<span id="cb11-82"><a href="sec13_10.html#cb11-82" tabindex="-1"></a>    M_nc_0 <span class="ot">&lt;-</span> <span class="fu">posterior_draws_prob</span>(gp_nc, X_t0, <span class="at">ndraws =</span> N_post)  <span class="co"># N_post x n_eff</span></span>
<span id="cb11-83"><a href="sec13_10.html#cb11-83" tabindex="-1"></a>    M_nc_1 <span class="ot">&lt;-</span> <span class="fu">posterior_draws_prob</span>(gp_nc, X_t1, <span class="at">ndraws =</span> N_post)</span>
<span id="cb11-84"><a href="sec13_10.html#cb11-84" tabindex="-1"></a>    <span class="co"># observed arm probabilities</span></span>
<span id="cb11-85"><a href="sec13_10.html#cb11-85" tabindex="-1"></a>    M_nc_obs <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">matrix</span>(Dk, <span class="at">nrow =</span> N_post, <span class="at">ncol =</span> n_eff, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">==</span> <span class="dv">1</span>, M_nc_1, M_nc_0)</span>
<span id="cb11-86"><a href="sec13_10.html#cb11-86" tabindex="-1"></a>    <span class="co"># GP WITH prior correction #</span></span>
<span id="cb11-87"><a href="sec13_10.html#cb11-87" tabindex="-1"></a>    gp_c <span class="ot">&lt;-</span> <span class="fu">make_gp</span>(<span class="at">X_train =</span> X_train_c, <span class="at">y_bin =</span> Yk, <span class="at">use_correction =</span> <span class="cn">TRUE</span>, <span class="at">gamma_scaled =</span> gamma_scaled)</span>
<span id="cb11-88"><a href="sec13_10.html#cb11-88" tabindex="-1"></a>    M_c_0 <span class="ot">&lt;-</span> <span class="fu">posterior_draws_prob</span>(gp_c, X_t0c, <span class="at">ndraws =</span> N_post)</span>
<span id="cb11-89"><a href="sec13_10.html#cb11-89" tabindex="-1"></a>    M_c_1 <span class="ot">&lt;-</span> <span class="fu">posterior_draws_prob</span>(gp_c, X_t1c, <span class="at">ndraws =</span> N_post)</span>
<span id="cb11-90"><a href="sec13_10.html#cb11-90" tabindex="-1"></a>    M_c_obs <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">matrix</span>(Dk, <span class="at">nrow =</span> N_post, <span class="at">ncol =</span> n_eff, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">==</span> <span class="dv">1</span>, M_c_1, M_c_0)</span>
<span id="cb11-91"><a href="sec13_10.html#cb11-91" tabindex="-1"></a>    <span class="co"># Bayesian bootstrap weights #</span></span>
<span id="cb11-92"><a href="sec13_10.html#cb11-92" tabindex="-1"></a>    W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rexp</span>(N_post <span class="sc">*</span> n_eff, <span class="at">rate =</span> <span class="dv">1</span>), <span class="at">nrow =</span> N_post)</span>
<span id="cb11-93"><a href="sec13_10.html#cb11-93" tabindex="-1"></a>    W <span class="ot">&lt;-</span> W <span class="sc">/</span> <span class="fu">rowSums</span>(W)</span>
<span id="cb11-94"><a href="sec13_10.html#cb11-94" tabindex="-1"></a>    <span class="co"># ATEs</span></span>
<span id="cb11-95"><a href="sec13_10.html#cb11-95" tabindex="-1"></a>    <span class="co"># (1) Unadjusted Bayes</span></span>
<span id="cb11-96"><a href="sec13_10.html#cb11-96" tabindex="-1"></a>    Ate_nc_draws <span class="ot">&lt;-</span> <span class="fu">rowSums</span>((M_nc_1 <span class="sc">-</span> M_nc_0) <span class="sc">*</span> W)</span>
<span id="cb11-97"><a href="sec13_10.html#cb11-97" tabindex="-1"></a>    <span class="co"># (2) Prior-adjusted Bayes</span></span>
<span id="cb11-98"><a href="sec13_10.html#cb11-98" tabindex="-1"></a>    Ate_c_draws  <span class="ot">&lt;-</span> <span class="fu">rowSums</span>((M_c_1 <span class="sc">-</span> M_c_0) <span class="sc">*</span> W)</span>
<span id="cb11-99"><a href="sec13_10.html#cb11-99" tabindex="-1"></a>    <span class="co"># (3) DR Bayes (posterior recentering)</span></span>
<span id="cb11-100"><a href="sec13_10.html#cb11-100" tabindex="-1"></a>    <span class="co"># Pre-centering using UNcorrected GP posterior means</span></span>
<span id="cb11-101"><a href="sec13_10.html#cb11-101" tabindex="-1"></a>    mu_nc_diff <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(M_nc_1 <span class="sc">-</span> M_nc_0)                </span>
<span id="cb11-102"><a href="sec13_10.html#cb11-102" tabindex="-1"></a>    mu_nc_obs  <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(M_nc_obs)                       </span>
<span id="cb11-103"><a href="sec13_10.html#cb11-103" tabindex="-1"></a>    ATE_dr_pre <span class="ot">&lt;-</span> <span class="fu">mean</span>(mu_nc_diff <span class="sc">+</span> gamma_k <span class="sc">*</span> (Yk <span class="sc">-</span> mu_nc_obs))</span>
<span id="cb11-104"><a href="sec13_10.html#cb11-104" tabindex="-1"></a>    <span class="co"># Recenter draws using corrected GP draws</span></span>
<span id="cb11-105"><a href="sec13_10.html#cb11-105" tabindex="-1"></a>    DR_rec_1 <span class="ot">&lt;-</span> (<span class="fu">matrix</span>(gamma_k, <span class="at">nrow =</span> N_post, <span class="at">ncol =</span> n_eff, <span class="at">byrow =</span> <span class="cn">TRUE</span>)) <span class="sc">*</span> (<span class="fu">matrix</span>(Yk, <span class="at">nrow =</span> N_post, <span class="at">ncol =</span> n_eff, <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">-</span> M_c_obs)</span>
<span id="cb11-106"><a href="sec13_10.html#cb11-106" tabindex="-1"></a>    <span class="co"># The first component is \tau_{\eta}^s in Algorithm 1</span></span>
<span id="cb11-107"><a href="sec13_10.html#cb11-107" tabindex="-1"></a>    <span class="co"># The second component is \hat{m} a scalar. This has positive sign because</span></span>
<span id="cb11-108"><a href="sec13_10.html#cb11-108" tabindex="-1"></a>    <span class="co"># minus x minus, the bias is with minus, and then, this component enters with minus in the bias term</span></span>
<span id="cb11-109"><a href="sec13_10.html#cb11-109" tabindex="-1"></a>    <span class="co"># The third component is \gamma_k x (y-m(d,x)), m(d,x) is with prior correction</span></span>
<span id="cb11-110"><a href="sec13_10.html#cb11-110" tabindex="-1"></a>    <span class="co"># The fourth component is m(1,x)-m(0,x) also with correction in the prior</span></span>
<span id="cb11-111"><a href="sec13_10.html#cb11-111" tabindex="-1"></a>    Ate_drb_draws <span class="ot">&lt;-</span> <span class="fu">rowSums</span>((M_c_1 <span class="sc">-</span> M_c_0) <span class="sc">*</span> W) <span class="sc">+</span> ATE_dr_pre <span class="sc">-</span> <span class="fu">rowSums</span>(DR_rec_1) <span class="sc">/</span> n_eff <span class="sc">-</span> <span class="fu">rowSums</span>(M_c_1 <span class="sc">-</span> M_c_0) <span class="sc">/</span> n_eff</span>
<span id="cb11-112"><a href="sec13_10.html#cb11-112" tabindex="-1"></a>    </span>
<span id="cb11-113"><a href="sec13_10.html#cb11-113" tabindex="-1"></a>    qfun <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb11-114"><a href="sec13_10.html#cb11-114" tabindex="-1"></a>        qs <span class="ot">&lt;-</span> <span class="fu">quantile</span>(x, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb11-115"><a href="sec13_10.html#cb11-115" tabindex="-1"></a>        <span class="fu">c</span>(<span class="at">mean   =</span> <span class="fu">mean</span>(x),     <span class="at">median =</span> <span class="fu">median</span>(x), <span class="at">lo =</span> qs[<span class="dv">1</span>], <span class="at">hi =</span> qs[<span class="dv">2</span>], <span class="at">len =</span> <span class="fu">diff</span>(qs)</span>
<span id="cb11-116"><a href="sec13_10.html#cb11-116" tabindex="-1"></a>        )</span>
<span id="cb11-117"><a href="sec13_10.html#cb11-117" tabindex="-1"></a>    }   </span>
<span id="cb11-118"><a href="sec13_10.html#cb11-118" tabindex="-1"></a>    vals <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb11-119"><a href="sec13_10.html#cb11-119" tabindex="-1"></a>    <span class="at">Bayes      =</span> <span class="fu">qfun</span>(Ate_nc_draws),</span>
<span id="cb11-120"><a href="sec13_10.html#cb11-120" tabindex="-1"></a>    <span class="st">`</span><span class="at">PA Bayes</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">qfun</span>(Ate_c_draws),</span>
<span id="cb11-121"><a href="sec13_10.html#cb11-121" tabindex="-1"></a>    <span class="st">`</span><span class="at">DR Bayes</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">qfun</span>(Ate_drb_draws)</span>
<span id="cb11-122"><a href="sec13_10.html#cb11-122" tabindex="-1"></a>    )</span>
<span id="cb11-123"><a href="sec13_10.html#cb11-123" tabindex="-1"></a>    out_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">do.call</span>(rbind, vals), <span class="at">check.names =</span> <span class="cn">FALSE</span>)</span>
<span id="cb11-124"><a href="sec13_10.html#cb11-124" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">t_trim =</span> t_trim, <span class="at">results =</span> out_df)</span>
<span id="cb11-125"><a href="sec13_10.html#cb11-125" tabindex="-1"></a>}</span>
<span id="cb11-126"><a href="sec13_10.html#cb11-126" tabindex="-1"></a>N_post <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb11-127"><a href="sec13_10.html#cb11-127" tabindex="-1"></a>h_r    <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span></span>
<span id="cb11-128"><a href="sec13_10.html#cb11-128" tabindex="-1"></a>cor_size <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb11-129"><a href="sec13_10.html#cb11-129" tabindex="-1"></a>trim <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb11-130"><a href="sec13_10.html#cb11-130" tabindex="-1"></a>res_list <span class="ot">&lt;-</span> <span class="fu">run_trim</span>(<span class="at">t_trim =</span> trim, <span class="at">N_post =</span> N_post, <span class="at">h_r =</span> h_r, <span class="at">cor_size =</span> cor_size)</span>
<span id="cb11-131"><a href="sec13_10.html#cb11-131" tabindex="-1"></a><span class="fu">print</span>(res_list<span class="sc">$</span>results)</span></code></pre></div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-breunig2025double" class="csl-entry">
Breunig, Christoph, Ruixuan Liu, and Zhengfei Yu. 2025. <span>“Double Robust Bayesian Inference on Average Treatment Effects.”</span> <em>Econometrica</em> 93 (2): 539–68. <a href="https://doi.org/10.3982/ECTA21442">https://doi.org/10.3982/ECTA21442</a>.
</div>
<div id="ref-chernozhukov2018double" class="csl-entry">
Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. <span>“Double/Debiased Machine Learning for Treatment and Structural Parameters.”</span> <em>The Econometrics Journal</em> 21: C1–68. <a href="https://doi.org/10.1111/ectj.12097">https://doi.org/10.1111/ectj.12097</a>.
</div>
<div id="ref-Lo1987BayesianBootstrap" class="csl-entry">
Lo, Albert Y. 1987. <span>“A Large Sample Study of the <span>Bayesian</span> Bootstrap.”</span> <em>The Annals of Statistics</em> 15 (1): 360–75. <a href="https://doi.org/10.1214/aos/1176350271">https://doi.org/10.1214/aos/1176350271</a>.
</div>
<div id="ref-rasmussen2006gaussian" class="csl-entry">
Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-rosenbaum1983central" class="csl-entry">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“The Central Role of the Propensity Score in Observational Studies for Causal Effects.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1093/biomet/70.1.41">https://doi.org/10.1093/biomet/70.1.41</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p><span class="citation">Breunig, Liu, and Yu (<a href="#ref-breunig2025double">2025</a>)</span> extend their framework to continuous, counting and multinomial outcomes.<a href="sec13_10.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec13_8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec13_11.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/10-Diagnostics.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
