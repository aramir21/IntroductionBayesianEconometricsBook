<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14.2 Optimization approaches | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="14.2 Optimization approaches | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14.2 Optimization approaches | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-04-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec14_1.html"/>
<link rel="next" href="summary-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec14_2" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Optimization approaches<a href="sec14_2.html#sec14_2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional MCMC and importance sampling (IS) algorithms require pointwise evaluation of the likelihood function, which entails a massive number of operations when applied to very large datasets. Unfortunately, these algorithms are not designed to be <em>scalable</em>, at least in their standard form (see Chapter <a href="Chap12.html#Chap12">12</a> for alternatives). Moreover, when the parameter space is large, they also lack <em>scalability</em> with respect to the number of parameters. Therefore, approximation methods should be considered even when the likelihood function has an analytical expression.</p>
<p><em>Optimization approaches</em> are designed to scale efficiently with high-dimensional parameter spaces and large datasets. The key idea is to replace simulation with optimization. In this section, we introduce the most common optimization approaches within the Bayesian inferential framework.</p>
<div id="sec14_21" class="section level3 hasAnchor" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> Integrated nested Laplace approximations<a href="sec14_2.html#sec14_21" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Integrated Nested Laplace Approximations</em> (INLA) is a deterministic approach for Bayesian inference in latent Gaussian models (LGMs) <span class="citation">(<a href="#ref-rue2009approximate">Rue, Martino, and Chopin 2009</a>)</span>. In particular, INLA approximates the marginal posterior distributions using a combination of Laplace approximations, low-dimensional deterministic integration, and optimization steps in sparse covariance settings <span class="citation">(<a href="#ref-rue2017bayesian">Rue et al. 2017</a>)</span>. The advantages of INLA compared to MCMC are that it is fast and does not suffer from poor mixing.</p>
<p>The point of departure is a structured additive regression model, where the response variable <span class="math inline">\(y_i\)</span> belongs to the exponential family such that the mean <span class="math inline">\(\mu_i\)</span> is linked to a linear predictor <span class="math inline">\(\eta_i\)</span> through the link function <span class="math inline">\(g(\cdot)\)</span>, that is, <span class="math inline">\(\eta_i = g(\mu_i)\)</span>, where</p>
<p><span class="math display">\[\begin{equation*}
\mu_i = \alpha + \boldsymbol{\beta}^{\top}\boldsymbol{x}_{i} + \sum_{j=1}^{J}f^{(j)}(u_{ji}) + \epsilon_{i},
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the general intercept, <span class="math inline">\(f^{(j)}\)</span> are unknown functions of the covariates <span class="math inline">\(\boldsymbol{u}_i\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span> is a <span class="math inline">\(K\)</span>-dimensional vector of linear effects associated with regressors <span class="math inline">\(\boldsymbol{x}_i\)</span>, and <span class="math inline">\(\epsilon_{i}\)</span> is the unstructured error.</p>
<p>Note that latent Gaussian models encompass a wide range of relevant empirical models depending on the specific elements and structure involved in <span class="math inline">\(f^{(j)}\)</span>, such as generalized linear models with unobserved heterogeneity, spatial and/or temporal dependence, and semi-parametric models.</p>
<p>Latent Gaussian models assign a Gaussian prior to <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(f^{(j)}\)</span>, and <span class="math inline">\(\epsilon_i\)</span>. Let <span class="math inline">\(\boldsymbol{z}\)</span> denote the vector of all latent Gaussian variables <span class="math inline">\(\{\alpha, \boldsymbol{\beta}, f^{(j)}, \eta_i\}\)</span>, where the dimension is potentially <span class="math inline">\(P = 1 + K + J + N\)</span> (although this is not always the case), and let <span class="math inline">\(\boldsymbol{\theta}\)</span> be the <span class="math inline">\(m\)</span>-dimensional vector of hyperparameters. Then, the density <span class="math inline">\(\pi(\boldsymbol{z} \mid \boldsymbol{\theta}_1)\)</span> is Gaussian with mean zero and precision matrix (i.e., the inverse of the covariance matrix) <span class="math inline">\(\boldsymbol{Q}(\boldsymbol{\theta}_1)\)</span>.</p>
<p>The distribution of <span class="math inline">\(\boldsymbol{y}\)</span> is <span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{z}, \boldsymbol{\theta}_2)\)</span> such that <span class="math inline">\(y_i\)</span> are conditionally independent given <span class="math inline">\(z_i, \boldsymbol{\theta}_2\)</span>, and <span class="math inline">\(\boldsymbol{\theta} = [\boldsymbol{\theta}_1^{\top} \ \boldsymbol{\theta}_2^{\top}]^{\top}\)</span> for <span class="math inline">\(i = 1,2,\dots,N\)</span>. Thus, the posterior distribution is</p>
<p><span class="math display" id="eq:1ch14">\[\begin{align}
\pi(\boldsymbol{z}, \boldsymbol{\theta} \mid \boldsymbol{y}) &amp;\propto \pi(\boldsymbol{\theta}) \times \pi(\boldsymbol{z} \mid \boldsymbol{\theta}_1) \times \prod_{i=1}^{N} p(y_i \mid \boldsymbol{z}, \boldsymbol{\theta}_2)   \tag{14.1} \\
&amp;\propto \pi(\boldsymbol{\theta}) \, |\boldsymbol{Q}(\boldsymbol{\theta}_1)|^{1/2} \exp\left\{-\frac{1}{2} \boldsymbol{z}^{\top} \boldsymbol{Q}(\boldsymbol{\theta}_1) \boldsymbol{z} + \sum_{i=1}^N \log p(y_i \mid z_i, \boldsymbol{\theta}_2) \right\}.\notag
\end{align}\]</span></p>
<p>Most models in INLA assume a conditional independence structure within the high-dimensional latent Gaussian field; that is, <span class="math inline">\(\boldsymbol{z}\)</span> is a Gaussian Markov random field (GMRF) with a sparse precision matrix <span class="math inline">\(\boldsymbol{Q}(\boldsymbol{\theta}_1)\)</span>. A second key assumption is that the dimension of <span class="math inline">\(\boldsymbol{\theta}\)</span> is small, for instance, <span class="math inline">\(m &lt; 15\)</span>. These assumptions are essential for enabling fast approximate inference.</p>
<p>The main aim in INLA is to approximate the marginal posterior distributions <span class="math inline">\(\pi(z_i \mid \boldsymbol{y})\)</span> and <span class="math inline">\(\pi(\theta_l \mid \boldsymbol{y})\)</span>, for <span class="math inline">\(l = 1, 2, \dots, m\)</span>.</p>
<p>The posterior distribution of <span class="math inline">\(\boldsymbol{\theta}\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\pi(\boldsymbol{\theta} \mid \boldsymbol{y})
&amp;= \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{p(\boldsymbol{y})} \\
&amp;= \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{p(\boldsymbol{y})} \times \frac{\pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})}{\pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})} \\
&amp;= \frac{p(\boldsymbol{z}, \boldsymbol{\theta}, \boldsymbol{y})}{p(\boldsymbol{y}) \pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})} \\
&amp;\propto \frac{p(\boldsymbol{z}, \boldsymbol{\theta}, \boldsymbol{y})}{\pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})} \\
&amp;\propto \frac{p(\boldsymbol{y} \mid \boldsymbol{z}, \boldsymbol{\theta}) \pi(\boldsymbol{z} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})}{\pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})}.
\end{align*}\]</span></p>
<p>The numerator in the previous expression is easy to calculate (see Equation <a href="sec14_2.html#eq:1ch14">(14.1)</a>), but the denominator is generally not available in closed form and is difficult to compute. Thus, INLA approximates it at specific values <span class="math inline">\(\boldsymbol{\theta}_g\)</span> as follows:</p>
<p><span class="math display">\[\begin{align*}
\pi_a(\boldsymbol{\theta}_g \mid \boldsymbol{y})
&amp;\propto \frac{p(\boldsymbol{y} \mid \boldsymbol{z}, \boldsymbol{\theta}_g) \pi(\boldsymbol{z} \mid \boldsymbol{\theta}_g) \pi(\boldsymbol{\theta}_g)}{\pi_{a,G}(\boldsymbol{z} \mid \boldsymbol{\theta}_g, \boldsymbol{y})},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\pi_{a,G}(z_i \mid \boldsymbol{\theta}_g, \boldsymbol{y})\)</span> is a Gaussian approximation that matches the mode and covariance matrix of the full posterior <span class="math inline">\(\pi(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})\)</span>.</p>
<p>The approximation error of <span class="math inline">\(\pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> is <span class="math inline">\(O(N^{-1})\)</span> under standard conditions, meaning that the error, when multiplied by <span class="math inline">\(N\)</span>, remains bounded <span class="citation">(<a href="#ref-Tierney1986">Tierney and Kadane 1986</a>; <a href="#ref-rue2009approximate">Rue, Martino, and Chopin 2009</a>)</span>. However, in many applications using INLA, the dimension of the latent Gaussian variables, <span class="math inline">\(P\)</span>, increases with the sample size. In such cases, the error rate becomes <span class="math inline">\(O(P/N)\)</span>. When <span class="math inline">\(P/N \rightarrow 1\)</span>, which occurs in many models, the approximation error becomes <span class="math inline">\(O(1)\)</span>: bounded, but potentially large.</p>
<p>Therefore, it is important to check the effective number of parameters, as the asymptotic error of <span class="math inline">\(\pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> depends on the dimension of <span class="math inline">\(\boldsymbol{z}\)</span>. According to <span class="citation">Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span>, in most of their applications the effective number of parameters is small relative to the sample size in regions near the mode of <span class="math inline">\(\pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>.</p>
<p>The marginal posterior <span class="math inline">\(\pi(z_i \mid \boldsymbol{\theta}, \boldsymbol{y})\)</span> is more challenging to compute due to the potentially high dimension of <span class="math inline">\(\boldsymbol{z}\)</span>. It may seem intuitive to use the Gaussian approximation <span class="math inline">\(\pi_{a,G}(\boldsymbol{z} \mid \boldsymbol{\theta}, \boldsymbol{y})\)</span>; however, this is often not sufficiently accurate due to its lack of skewness. An alternative is to use the following expression</p>
<p><span class="math display">\[\begin{align*}
    \pi(z_i\mid \boldsymbol{\theta},\boldsymbol{y})&amp;=\frac{p(z_i,\boldsymbol{\theta},\boldsymbol{y})}{p(\boldsymbol{\theta},\boldsymbol{y})}\\
    &amp;=\frac{p(z_i,\boldsymbol{\theta},\boldsymbol{y})}{p(\boldsymbol{\theta},\boldsymbol{y})}\times \frac{\pi(\boldsymbol{z}\mid\boldsymbol{\theta},\boldsymbol{y})}{\pi(\boldsymbol{z}\mid\boldsymbol{\theta},\boldsymbol{y})}\\
    &amp;=\frac{\pi(\boldsymbol{z}\mid\boldsymbol{\theta},\boldsymbol{y})}{p(\boldsymbol{z},\boldsymbol{\theta},\boldsymbol{y})}\times p(z_i,\boldsymbol{\theta},\boldsymbol{y})\\
    &amp;=\frac{\pi(\boldsymbol{z}\mid\boldsymbol{\theta},\boldsymbol{y})}{\pi(\boldsymbol{z}_{-i}\mid z_i,\boldsymbol{\theta},\boldsymbol{y})}\\
    &amp;\propto \frac{p(\boldsymbol{y}\mid \boldsymbol{z},\boldsymbol{\theta})\pi(\boldsymbol{z}\mid\boldsymbol{\theta})\pi(\boldsymbol{\theta})}{\pi(\boldsymbol{z}_{-i}\mid z_i,\boldsymbol{\theta},\boldsymbol{y})},
\end{align*}\]</span>
where the second-to-last equality follows from the identity <span class="math inline">\(\pi(\boldsymbol{z}_{-i} \mid z_i, \boldsymbol{\theta}, \boldsymbol{y}) = \frac{p(\boldsymbol{z}, \boldsymbol{\theta}, \boldsymbol{y})}{p(z_i, \boldsymbol{\theta}, \boldsymbol{y})}\)</span>, <span class="math inline">\(-i\)</span> denotes all elements of <span class="math inline">\(\boldsymbol{z}\)</span> except the <span class="math inline">\(i\)</span>-th, and approximating the denominator in the last expression using a simplified Laplace approximation, which corrects the Gaussian approximation for location and skewness via a Taylor series expansion about the mode of the Laplace approximation . This follows the spirit of the approximations developed by for posterior moments and marginal densities, and is similar to those used in Chapter <a href="Chap10.html#Chap10">10</a> when performing Bayesian model averaging (BMA) using the BIC information criterion. The discussion about the asymptotic error of the approximation <span class="math inline">\(\pi_{a}(z_i \mid \boldsymbol{\theta}, \boldsymbol{y})\)</span> follows the same arguments as those for <span class="math inline">\(\pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>.</p>
<p>We can obtain the marginal posterior distributions integrating with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span>,
<span class="math display">\[\begin{align*}
    \pi(z_i\mid \boldsymbol{y})&amp;=\int_{{\Theta}} \pi(z_i\mid \boldsymbol{\theta},\boldsymbol{y})\pi(\boldsymbol{\theta}\mid\boldsymbol{y})d\boldsymbol{\theta}\\
    \pi(\theta_l\mid\boldsymbol{y})&amp;=\int_{{\Theta}} \pi(\boldsymbol{\theta}\mid \boldsymbol{y})d\boldsymbol{\theta}_{-l}.
\end{align*}\]</span>
This integrals are solve numerically using a smart grid around the mode of <span class="math inline">\(\pi_a(\boldsymbol{\theta}\mid\boldsymbol{y})\)</span>. In particular,
<span class="math display">\[\begin{align*}
    \pi_a(z_i\mid \boldsymbol{y})&amp;=\sum_{g=1}^G \pi_{a}(z_i\mid \boldsymbol{\theta}_g,\boldsymbol{y})\pi_a(\boldsymbol{\theta}_g\mid \boldsymbol{y})\Delta_g,
\end{align*}\]</span>
where <span class="math inline">\(\pi_{a}(z_i\mid \boldsymbol{\theta}_g,\boldsymbol{y})\)</span> is the approximation of <span class="math inline">\(\pi(z_i\mid \boldsymbol{\theta},\boldsymbol{y})\)</span> evaluated at <span class="math inline">\(\boldsymbol{\theta}_g\)</span>. Then, we have the sum over the values of <span class="math inline">\(\boldsymbol{\theta}\)</span> with area weights <span class="math inline">\(\Delta_g\)</span>. If all support points are equidistant, then <span class="math inline">\(\Delta_g = 1\)</span>.</p>
<p>The following Algorithm presents the INLA algorithm. Note that stages 2 and 3 correspond to the nested Laplace approximations, whereas stage 4 involves the integration step.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Integrated Nested Laplace Approximations </strong></p>
<ol style="list-style-type: decimal">
<li><p>Obtain the mode of <span class="math inline">\(\pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>, that is, <span class="math inline">\(\boldsymbol{\theta}^*\)</span>, by maximizing <span class="math inline">\(\log \pi_a(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\pi_a(\boldsymbol{\theta}_g \mid \boldsymbol{y})\)</span> for a set of high-density points <span class="math inline">\(\boldsymbol{\theta}_g\)</span>, <span class="math inline">\(g = 1, 2, \dots, G\)</span>.</p></li>
<li><p>Compute the approximation <span class="math inline">\(\pi_a(z_i \mid \boldsymbol{\theta}_g, \boldsymbol{y})\)</span> for <span class="math inline">\(g = 1, 2, \dots, G\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\pi_a(z_i \mid \boldsymbol{y})\)</span> and <span class="math inline">\(\pi_a(\theta_l \mid \boldsymbol{y})\)</span> using numerical integration.</p></li>
</ol>
</div>
</div>
<p>Thus, INLA can be used in latent Gaussian models (LGMs) that satisfy the following conditions <span class="citation">(<a href="#ref-rue2017bayesian">Rue et al. 2017</a>; <a href="#ref-martino2019integrated">Martino and Riebler 2019</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>There is conditional independence, that is, <span class="math inline">\(y_i \perp y_s \mid \eta_i, \boldsymbol{\theta}\)</span>, such that<br />
<span class="math display">\[
p(\boldsymbol{y} \mid \boldsymbol{z}, \boldsymbol{\theta}) = \prod_{i=1}^{N} p(y_i \mid \eta_i, \boldsymbol{\theta}).
\]</span></li>
<li>The dimension of <span class="math inline">\(\boldsymbol{\theta}\)</span> is small, typically less than 15.</li>
<li><span class="math inline">\(\boldsymbol{z}\)</span> is a Gaussian Markov random field (GMRF) with a sparse precision matrix.</li>
<li>The linear predictor depends linearly on the smooth unknown functions of covariates.</li>
<li>Inference is focused on the marginal posterior distributions <span class="math inline">\(\pi(z_i \mid \boldsymbol{y})\)</span> and <span class="math inline">\(\pi(\theta_l \mid \boldsymbol{y})\)</span>.</li>
</ol>
<p><span class="citation">Rue, Martino, and Chopin (<a href="#ref-rue2009approximate">2009</a>)</span> also discusses how to approximate the marginal likelihood and compute the deviance information criterion (DIC) <span class="citation">(<a href="#ref-spiegelhalter2002bayesian">Spiegelhalter et al. 2002</a>)</span> for model selection, as well as how to perform predictive analysis.</p>
<p>The starting point for INLA is the class of latent Gaussian models (LGMs); therefore, <em>discrete latent classes are not supported</em>. Additionally, since INLA relies on local approximations around the mode, it may struggle with <em>multimodal posteriors</em>, as there is no global exploration of the parameter space.</p>
<p>Implementing INLA from scratch is complex <span class="citation">(<a href="#ref-martino2019integrated">Martino and Riebler 2019</a>)</span>; applications are therefore generally limited to the models available in the <em>INLA</em> package in <strong>R</strong>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> However, new packages have been developed for specialized models, and recent approaches combine INLA with MCMC (see Table 2 in <span class="citation">Martino and Riebler (<a href="#ref-martino2019integrated">2019</a>)</span>).</p>
<p><strong>Example: Poisson model with unobserved heterogeneity</strong></p>
<p>Let’s simulate the model <span class="math inline">\(Y_i \sim \text{Poisson}(\lambda_i)\)</span>, where <span class="math inline">\(\lambda_i = \exp\left\{1 + x_i + \epsilon_i\right\}\)</span>, with<br />
<span class="math inline">\(\epsilon_i \sim {N}(0, 0.5^2)\)</span> and <span class="math inline">\(x_i \sim {N}(0, 1^2)\)</span> for <span class="math inline">\(i = 1, 2, \dots, 10,\!000\)</span>.<br />
Note that <span class="math inline">\(\epsilon_i\)</span> represents unobserved heterogeneity.</p>
<p>The following code demonstrates how to perform inference using the <em>INLA</em> package.<br />
Keep in mind that INLA specifies Gaussian priors in terms of <strong>precision</strong> (the inverse of the variance).</p>
<p>We present results using the three available approximation strategies in INLA: Simplified Laplace (default), Gaussian, and full Laplace.<br />
In this example, the results are practically identical across methods (see the Figures), and all 95% credible intervals contain the true population parameters.<br />
Despite the large sample size, INLA performs inference in a matter of seconds.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="sec14_2.html#cb3-1" tabindex="-1"></a><span class="co"># Install Rtools according to your R version</span></span>
<span id="cb3-2"><a href="sec14_2.html#cb3-2" tabindex="-1"></a><span class="co"># Check Rtolls is properly installed</span></span>
<span id="cb3-3"><a href="sec14_2.html#cb3-3" tabindex="-1"></a><span class="co"># Install INLA</span></span>
<span id="cb3-4"><a href="sec14_2.html#cb3-4" tabindex="-1"></a><span class="co"># install.packages(&quot;INLA&quot;,repos=c(getOption(&quot;repos&quot;),INLA=&quot;https://inla.r-inla-download.org/R/stable&quot;), dep=TRUE)</span></span>
<span id="cb3-5"><a href="sec14_2.html#cb3-5" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>); <span class="fu">library</span>(INLA)</span>
<span id="cb3-6"><a href="sec14_2.html#cb3-6" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span>; x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="dv">1</span>); u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb3-7"><a href="sec14_2.html#cb3-7" tabindex="-1"></a>intercept <span class="ot">&lt;-</span> <span class="dv">1</span>; beta <span class="ot">&lt;-</span> <span class="dv">1</span>; id <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb3-8"><a href="sec14_2.html#cb3-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n, <span class="at">lambda =</span> <span class="fu">exp</span>(intercept <span class="sc">+</span> beta <span class="sc">*</span> x <span class="sc">+</span> u))</span>
<span id="cb3-9"><a href="sec14_2.html#cb3-9" tabindex="-1"></a>my.data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x, id)</span>
<span id="cb3-10"><a href="sec14_2.html#cb3-10" tabindex="-1"></a>formula <span class="ot">&lt;-</span> y <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> x <span class="sc">+</span> <span class="fu">f</span>(id, <span class="at">model=</span><span class="st">&quot;iid&quot;</span>)</span>
<span id="cb3-11"><a href="sec14_2.html#cb3-11" tabindex="-1"></a>inla.sla <span class="ot">&lt;-</span> <span class="fu">inla</span>(formula, <span class="at">data =</span> my.data, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">control.compute=</span><span class="fu">list</span>(<span class="at">return.marginals.predictor=</span><span class="cn">TRUE</span>))</span>
<span id="cb3-12"><a href="sec14_2.html#cb3-12" tabindex="-1"></a>inla.ga <span class="ot">&lt;-</span> <span class="fu">inla</span>(formula, <span class="at">data =</span> my.data, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">control.inla =</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">int.strategy =</span> <span class="st">&quot;eb&quot;</span>), <span class="at">control.compute=</span><span class="fu">list</span>(<span class="at">return.marginals.predictor=</span><span class="cn">TRUE</span>))</span>
<span id="cb3-13"><a href="sec14_2.html#cb3-13" tabindex="-1"></a>inla.la <span class="ot">&lt;-</span> <span class="fu">inla</span>(formula, <span class="at">data =</span> my.data, <span class="at">family =</span> <span class="st">&quot;poisson&quot;</span>, <span class="at">control.inla =</span> <span class="fu">list</span>(<span class="at">strategy =</span> <span class="st">&quot;laplace&quot;</span>, <span class="at">int.strategy =</span> <span class="st">&quot;grid&quot;</span>, <span class="at">dz=</span><span class="fl">0.1</span>, <span class="at">diff.logdens=</span><span class="dv">20</span>),</span>
<span id="cb3-14"><a href="sec14_2.html#cb3-14" tabindex="-1"></a><span class="at">control.compute=</span><span class="fu">list</span>(<span class="at">return.marginals.predictor=</span><span class="cn">TRUE</span>))</span>
<span id="cb3-15"><a href="sec14_2.html#cb3-15" tabindex="-1"></a><span class="fu">summary</span>(inla.sla)</span>
<span id="cb3-16"><a href="sec14_2.html#cb3-16" tabindex="-1"></a>marg_sla <span class="ot">&lt;-</span> inla.sla<span class="sc">$</span>marginals.fixed<span class="sc">$</span>x</span>
<span id="cb3-17"><a href="sec14_2.html#cb3-17" tabindex="-1"></a>marg_ga  <span class="ot">&lt;-</span> inla.ga<span class="sc">$</span>marginals.fixed<span class="sc">$</span>x</span>
<span id="cb3-18"><a href="sec14_2.html#cb3-18" tabindex="-1"></a>marg_la  <span class="ot">&lt;-</span> inla.la<span class="sc">$</span>marginals.fixed<span class="sc">$</span>x</span>
<span id="cb3-19"><a href="sec14_2.html#cb3-19" tabindex="-1"></a><span class="fu">plot</span>(marg_sla, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(beta[x]), <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Posterior of slope under different INLA strategies&quot;</span>)</span>
<span id="cb3-20"><a href="sec14_2.html#cb3-20" tabindex="-1"></a><span class="fu">lines</span>(marg_ga, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-21"><a href="sec14_2.html#cb3-21" tabindex="-1"></a><span class="fu">lines</span>(marg_la, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb3-22"><a href="sec14_2.html#cb3-22" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lty =</span> <span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-23"><a href="sec14_2.html#cb3-23" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Simplified Laplace&quot;</span>, <span class="st">&quot;Gaussian&quot;</span>, <span class="st">&quot;Full Laplace&quot;</span>, <span class="st">&quot;True = 1&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb3-24"><a href="sec14_2.html#cb3-24" tabindex="-1"></a><span class="co"># Summary beta sla</span></span>
<span id="cb3-25"><a href="sec14_2.html#cb3-25" tabindex="-1"></a><span class="fu">inla.qmarginal</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>), marg_sla) <span class="co"># 95% credible interval</span></span>
<span id="cb3-26"><a href="sec14_2.html#cb3-26" tabindex="-1"></a><span class="co"># Variance</span></span>
<span id="cb3-27"><a href="sec14_2.html#cb3-27" tabindex="-1"></a>marg.prec.sla <span class="ot">&lt;-</span> inla.sla<span class="sc">$</span>marginals.hyperpar[[<span class="st">&quot;Precision for id&quot;</span>]]</span>
<span id="cb3-28"><a href="sec14_2.html#cb3-28" tabindex="-1"></a>marg.prec.ga  <span class="ot">&lt;-</span> inla.ga<span class="sc">$</span>marginals.hyperpar[[<span class="st">&quot;Precision for id&quot;</span>]]</span>
<span id="cb3-29"><a href="sec14_2.html#cb3-29" tabindex="-1"></a>marg.prec.la  <span class="ot">&lt;-</span> inla.la<span class="sc">$</span>marginals.hyperpar[[<span class="st">&quot;Precision for id&quot;</span>]]</span>
<span id="cb3-30"><a href="sec14_2.html#cb3-30" tabindex="-1"></a>marg.var.sla <span class="ot">&lt;-</span> <span class="fu">inla.tmarginal</span>(<span class="cf">function</span>(x) <span class="dv">1</span><span class="sc">/</span>x, marg.prec.sla)</span>
<span id="cb3-31"><a href="sec14_2.html#cb3-31" tabindex="-1"></a>marg.var.ga  <span class="ot">&lt;-</span> <span class="fu">inla.tmarginal</span>(<span class="cf">function</span>(x) <span class="dv">1</span><span class="sc">/</span>x, marg.prec.ga)</span>
<span id="cb3-32"><a href="sec14_2.html#cb3-32" tabindex="-1"></a>marg.var.la  <span class="ot">&lt;-</span> <span class="fu">inla.tmarginal</span>(<span class="cf">function</span>(x) <span class="dv">1</span><span class="sc">/</span>x, marg.prec.la)</span>
<span id="cb3-33"><a href="sec14_2.html#cb3-33" tabindex="-1"></a><span class="co"># Base plot</span></span>
<span id="cb3-34"><a href="sec14_2.html#cb3-34" tabindex="-1"></a><span class="fu">plot</span>(marg.var.sla, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(sigma<span class="sc">^</span><span class="dv">2</span>), <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Posterior of Random Effect Variance&quot;</span>)</span>
<span id="cb3-35"><a href="sec14_2.html#cb3-35" tabindex="-1"></a><span class="fu">lines</span>(marg.var.ga, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-36"><a href="sec14_2.html#cb3-36" tabindex="-1"></a><span class="fu">lines</span>(marg.var.la, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb3-37"><a href="sec14_2.html#cb3-37" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.25</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lty =</span> <span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-38"><a href="sec14_2.html#cb3-38" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Simplified Laplace&quot;</span>, <span class="st">&quot;Gaussian&quot;</span>, <span class="st">&quot;Full Laplace&quot;</span>, <span class="st">&quot;True Variance (0.25)&quot;</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb3-39"><a href="sec14_2.html#cb3-39" tabindex="-1"></a><span class="co"># Summary variance sla</span></span>
<span id="cb3-40"><a href="sec14_2.html#cb3-40" tabindex="-1"></a><span class="fu">inla.qmarginal</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>), marg.var.sla) <span class="co"># 95% credible interval</span></span></code></pre></div>
<p><strong>Example: Spatial econometrics model</strong></p>
<p>The starting point of spatial econometrics is the <em>contiguity</em> or <em>adjacency</em> matrix <span class="math inline">\(W\)</span>, which defines which spatial polygons (regions) are considered neighbors. This is an <span class="math inline">\(N \times N\)</span> matrix, where each row and column corresponds to a spatial polygon, and a non-zero element indicates that two polygons are neighbors. By construction, the main diagonal is zero, meaning no polygon is a neighbor to itself. Given its structure, the contiguity matrix is sparse.</p>
<p>There are various ways to define contiguity between spatial units. Two common criteria are the <em>queen</em> and <em>rook</em> criteria, inspired by chess. Under the queen criterion, two units are neighbors if they share any part of a boundary or a point. In contrast, under the rook criterion, two units are neighbors only if they share a common edge, touching at a corner is not sufficient.</p>
<p>However, users may define contiguity in other ways depending on context. For example, contiguity could be based on travel time between centroids or proximity between main towns. Typically, the matrix <span class="math inline">\(W\)</span> is binary: a 1 indicates two regions are neighbors, and a 0 otherwise. Often, the matrix is row-standardized to aid in analyzing spatial stationarity.</p>
<p><span class="citation">LeSage and Pace (<a href="#ref-lesage2009introduction">2009</a>)</span> and <span class="citation">Bivand, Gómez-Rubio, and Rue (<a href="#ref-bivand2015spatial">2015</a>)</span> describe the most widely used models in spatial econometrics: the spatial error model (SEM), spatial autoregressive model (SAR), and spatial Durbin model (SDM), which can be combined into the general nesting spatial (GNS) model <span class="citation">(<a href="#ref-elhorst2014spatial">Elhorst et al. 2014</a>; <a href="#ref-Ramirez2017">Ramírez Hassan 2017</a>)</span>.</p>
<p>In particular, the SEM is defined as:</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{y} &amp;= \boldsymbol{X\beta} + \boldsymbol{\mu}, \\
\boldsymbol{\mu} &amp;= \lambda \boldsymbol{W\mu} + \boldsymbol{\epsilon},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\epsilon} \sim {N}(\boldsymbol{0}_N, \sigma^2 \boldsymbol{I}_N)\)</span>. Then,</p>
<p><span class="math display">\[
\boldsymbol{\mu} = (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} \boldsymbol{\epsilon},
\]</span></p>
<p>which implies</p>
<p><span class="math display">\[
\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{e},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{e} \sim {N}(\boldsymbol{0}, \sigma^2 (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} (\boldsymbol{I}_N - \rho \boldsymbol{W}^{\top})^{-1}).
\]</span></p>
<p>In this model, <span class="math inline">\(\lambda\)</span> controls the degree of spatial dependence.</p>
<p>The SAR model is given by:</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{y} &amp;= \rho \boldsymbol{W y} + \boldsymbol{X\beta} + \boldsymbol{\mu}, \\
               &amp;= (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} \boldsymbol{X\beta} + \boldsymbol{\epsilon},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\epsilon} = (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} \boldsymbol{\mu}\)</span>.</p>
<p>The SDM model is:</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{y} &amp;= \rho \boldsymbol{W y} + \boldsymbol{X\beta} + \boldsymbol{W X \delta} + \boldsymbol{\mu}, \\
               &amp;= (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} (\boldsymbol{X\beta} + \boldsymbol{W X \delta}) + \boldsymbol{\epsilon},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\epsilon} = (\boldsymbol{I}_N - \rho \boldsymbol{W})^{-1} \boldsymbol{\mu}\)</span>.</p>
<p>The degree of spatial dependence in the SAR and SDM models is determined by the parameter <span class="math inline">\(\rho\)</span>. Note that the SDM model is similar to the SAR model, except that it also includes spatial lags of the regressors as additional covariates.</p>
<p><span class="citation">Ord (<a href="#ref-ord75">1975</a>)</span> and <span class="citation">Anselin (<a href="#ref-anselin82">1982</a>)</span> show that a necessary condition for weak stationarity in spatial autoregressive processes with a row-standardized contiguity matrix is that the spatial autocorrelation coefficient must lie between <span class="math inline">\(1/\omega_{\min}\)</span> and 1, where <span class="math inline">\(\omega_{\min}\)</span> is the smallest (most negative) eigenvalue of the contiguity matrix.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>Note that these three models, conditional on the spatial parameter, are standard linear regressions, which can be easily estimated using <em>INLA</em>. <span class="citation">Bivand, Gómez-Rubio, and Rue (<a href="#ref-bivand2015spatial">2015</a>)</span> use the <em>INLABMA</em> package to estimate these models conditional on different values of the spatial parameters, and then perform Bayesian Model Averaging (BMA) using the resulting estimates. In particular, it is necessary to define a grid over the spatial parameters, perform Bayesian inference using INLA for each value in the grid, and then aggregate the posterior results using BMA.</p>
<p>We now perform Bayesian inference on a spatial econometric model using the <em>INLA</em> and <em>INLABMA</em> packages, based on the dataset provided by <span class="citation">Ramı́rez Hassan and Montoya Blandón (<a href="#ref-ramirez2019welfare">2019</a>)</span>, who conducted a Bayesian analysis of electricity demand in the department of Antioquia (Colombia), accounting for spatial dependence between municipalities using a Conditional Autoregressive (CAR) spatial model. In particular, we conduct inference using the following specification:</p>
<p><span class="math display">\[\begin{align*}
\log(\text{Electricity}_i) &amp;= \beta_1 + \beta_2 \log(\text{Elect. price}_i) + \beta_3 \log(\text{Income}_i) \\
&amp;\quad + \beta_4 \log(\text{Subs. price}_i) + \mu_i,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu} = \lambda \boldsymbol{W} \boldsymbol{\mu} + \epsilon\)</span>.<br />
<em>Electricity</em> is the average per capita annual consumption of electricity by individuals living in households of stratum one in each municipality of Antioquia. <em>Elect. price</em> is the average price of electricity (per kWh), <em>Income</em> is average per capita annual income, and <em>Subs. price</em> is the average price of an electricity substitute (see <span class="citation">Ramı́rez Hassan and Montoya Blandón (<a href="#ref-ramirez2019welfare">2019</a>)</span> for details).</p>
<p>The following code illustrates how to carry out a spatial econometric analysis. First, we download the files needed to plot the maps and construct the contiguity matrix. These files are available in the folder <em>DataApp/Antioquia</em> of our GitHub repository: <strong><a href="https://github.com/besmarter/BSTApp" class="uri">https://github.com/besmarter/BSTApp</a></strong>. The initial part of the code demonstrates how to download the GitHub repository and extract the necessary files for mapping.</p>
<p>The first Figure displays the average electricity consumption per municipality. We observe the presence of spatial clusters, particularly in the northwestern region, which corresponds to a low-altitude area along the Caribbean coast.</p>
<p>The second part of the code constructs the contiguity matrix using the queen criterion. The second Figure illustrates the spatial links between municipalities. Following this, we estimate a standard Ordinary Least Squares (OLS) regression and conduct spatial autocorrelation tests on the residuals. The null hypothesis of both the global and local Moran’s I tests is the absence of spatial autocorrelation in the OLS residuals. We reject the null hypothesis.</p>
<p>Finally, we perform Bayesian inference over a predefined grid of values for the spatial coefficient and apply Bayesian Model Averaging (BMA) using the <em>INLABMA</em> package. The third Figure shows the BMA posterior distribution of the spatial coefficient in the SEM, which indicates the presence of spatial dependence. The fourth Figure displays the posterior density of the own-price elasticity of electricity demand.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="sec14_2.html#cb4-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb4-2"><a href="sec14_2.html#cb4-2" tabindex="-1"></a><span class="fu">library</span>(sf); <span class="fu">library</span>(tmap); <span class="fu">library</span>(classInt)</span>
<span id="cb4-3"><a href="sec14_2.html#cb4-3" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer); <span class="fu">library</span>(spdep); <span class="fu">library</span>(parallel)</span>
<span id="cb4-4"><a href="sec14_2.html#cb4-4" tabindex="-1"></a><span class="fu">library</span>(INLABMA); <span class="fu">library</span>(sf); <span class="fu">library</span>(INLA)</span>
<span id="cb4-5"><a href="sec14_2.html#cb4-5" tabindex="-1"></a>zip_url <span class="ot">&lt;-</span> <span class="st">&quot;https://github.com/BEsmarter-consultancy/BSTApp/archive/refs/heads/master.zip&quot;</span></span>
<span id="cb4-6"><a href="sec14_2.html#cb4-6" tabindex="-1"></a>temp_zip <span class="ot">&lt;-</span> <span class="fu">tempfile</span>(<span class="at">fileext =</span> <span class="st">&quot;.zip&quot;</span>)</span>
<span id="cb4-7"><a href="sec14_2.html#cb4-7" tabindex="-1"></a><span class="fu">download.file</span>(zip_url, temp_zip, <span class="at">mode =</span> <span class="st">&quot;wb&quot;</span>)</span>
<span id="cb4-8"><a href="sec14_2.html#cb4-8" tabindex="-1"></a>temp_dir <span class="ot">&lt;-</span> <span class="fu">tempdir</span>()</span>
<span id="cb4-9"><a href="sec14_2.html#cb4-9" tabindex="-1"></a><span class="fu">unzip</span>(temp_zip, <span class="at">exdir =</span> temp_dir)</span>
<span id="cb4-10"><a href="sec14_2.html#cb4-10" tabindex="-1"></a>antioquia_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(temp_dir, <span class="st">&quot;BSTApp-master&quot;</span>, <span class="st">&quot;DataApp&quot;</span>, <span class="st">&quot;Antioquia&quot;</span>)</span>
<span id="cb4-11"><a href="sec14_2.html#cb4-11" tabindex="-1"></a><span class="fu">list.files</span>(antioquia_path)</span>
<span id="cb4-12"><a href="sec14_2.html#cb4-12" tabindex="-1"></a>antioquia_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(temp_dir, <span class="st">&quot;BSTApp-master&quot;</span>, <span class="st">&quot;DataApp&quot;</span>, <span class="st">&quot;Antioquia&quot;</span>)</span>
<span id="cb4-13"><a href="sec14_2.html#cb4-13" tabindex="-1"></a>shp_file <span class="ot">&lt;-</span> <span class="fu">file.path</span>(antioquia_path, <span class="st">&quot;Antioquia.shp&quot;</span>)</span>
<span id="cb4-14"><a href="sec14_2.html#cb4-14" tabindex="-1"></a>antioquia <span class="ot">&lt;-</span> <span class="fu">st_read</span>(shp_file)</span>
<span id="cb4-15"><a href="sec14_2.html#cb4-15" tabindex="-1"></a><span class="fu">print</span>(antioquia)</span>
<span id="cb4-16"><a href="sec14_2.html#cb4-16" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">st_geometry</span>(antioquia), <span class="at">main =</span> <span class="st">&quot;Map of Antioquia&quot;</span>)</span>
<span id="cb4-17"><a href="sec14_2.html#cb4-17" tabindex="-1"></a>interval <span class="ot">&lt;-</span> <span class="fu">classIntervals</span>(antioquia<span class="sc">$</span>CONS_OLD, <span class="dv">5</span>, <span class="at">style =</span> <span class="st">&quot;quantile&quot;</span>)</span>
<span id="cb4-18"><a href="sec14_2.html#cb4-18" tabindex="-1"></a>antioquia<span class="sc">$</span>cons_class <span class="ot">&lt;-</span> <span class="fu">cut</span>(antioquia<span class="sc">$</span>CONS_OLD,<span class="at">breaks =</span> interval<span class="sc">$</span>brks,<span class="at">include.lowest =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-19"><a href="sec14_2.html#cb4-19" tabindex="-1"></a>plotcolors <span class="ot">&lt;-</span> <span class="fu">brewer.pal</span>(<span class="dv">5</span>, <span class="st">&quot;Reds&quot;</span>)</span>
<span id="cb4-20"><a href="sec14_2.html#cb4-20" tabindex="-1"></a><span class="fu">tmap_mode</span>(<span class="st">&quot;plot&quot;</span>)</span>
<span id="cb4-21"><a href="sec14_2.html#cb4-21" tabindex="-1"></a><span class="fu">tm_shape</span>(antioquia) <span class="sc">+</span></span>
<span id="cb4-22"><a href="sec14_2.html#cb4-22" tabindex="-1"></a><span class="fu">tm_fill</span>(<span class="at">fill =</span> <span class="st">&quot;cons_class&quot;</span>, <span class="at">fill.scale =</span> <span class="fu">tm_scale</span>(<span class="at">values =</span> plotcolors), <span class="at">fill.legend =</span> <span class="fu">tm_legend</span>(<span class="at">title =</span> <span class="st">&quot;Electricity consumption&quot;</span>)) <span class="sc">+</span> <span class="fu">tm_borders</span>(<span class="at">col =</span> <span class="st">&quot;grey90&quot;</span>) <span class="sc">+</span> <span class="fu">tm_compass</span>(<span class="at">type =</span> <span class="st">&quot;8star&quot;</span>, <span class="at">position =</span> <span class="fu">c</span>(<span class="st">&quot;right&quot;</span>, <span class="st">&quot;top&quot;</span>)) <span class="sc">+</span> <span class="fu">tm_layout</span>(<span class="at">legend.outside =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-23"><a href="sec14_2.html#cb4-23" tabindex="-1"></a>nb_object <span class="ot">&lt;-</span> <span class="fu">poly2nb</span>(antioquia, <span class="at">queen =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-24"><a href="sec14_2.html#cb4-24" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">st_centroid</span>(<span class="fu">st_geometry</span>(antioquia))</span>
<span id="cb4-25"><a href="sec14_2.html#cb4-25" tabindex="-1"></a>coords <span class="ot">&lt;-</span> <span class="fu">st_coordinates</span>(centroids)</span>
<span id="cb4-26"><a href="sec14_2.html#cb4-26" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">st_geometry</span>(antioquia), <span class="at">border =</span> <span class="st">&quot;grey&quot;</span>)</span>
<span id="cb4-27"><a href="sec14_2.html#cb4-27" tabindex="-1"></a><span class="fu">plot</span>(nb_object, coords, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb4-28"><a href="sec14_2.html#cb4-28" tabindex="-1"></a><span class="fu">attach</span>(antioquia)</span>
<span id="cb4-29"><a href="sec14_2.html#cb4-29" tabindex="-1"></a>fform <span class="ot">&lt;-</span> L_CONS_OLD <span class="sc">~</span> L_P_OLD <span class="sc">+</span> L_ING_US <span class="sc">+</span> L_P_SUST </span>
<span id="cb4-30"><a href="sec14_2.html#cb4-30" tabindex="-1"></a>RegOLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(fform)</span>
<span id="cb4-31"><a href="sec14_2.html#cb4-31" tabindex="-1"></a><span class="fu">summary</span>(RegOLS)</span>
<span id="cb4-32"><a href="sec14_2.html#cb4-32" tabindex="-1"></a>res <span class="ot">&lt;-</span> RegOLS<span class="sc">$</span>residuals</span>
<span id="cb4-33"><a href="sec14_2.html#cb4-33" tabindex="-1"></a>NBList <span class="ot">&lt;-</span> <span class="fu">nb2listw</span>(nb_object, <span class="at">style =</span> <span class="st">&quot;B&quot;</span>)</span>
<span id="cb4-34"><a href="sec14_2.html#cb4-34" tabindex="-1"></a>moran_mc<span class="ot">&lt;-</span> <span class="fu">moran.mc</span>(res, <span class="at">listw =</span> NBList, <span class="dv">10000</span>)</span>
<span id="cb4-35"><a href="sec14_2.html#cb4-35" tabindex="-1"></a>LM<span class="ot">&lt;-</span><span class="fu">localmoran</span>(<span class="fu">as.vector</span>(res), NBList)</span>
<span id="cb4-36"><a href="sec14_2.html#cb4-36" tabindex="-1"></a><span class="fu">sum</span>(LM[,<span class="dv">5</span>]<span class="sc">&lt;</span><span class="fl">0.05</span>)</span>
<span id="cb4-37"><a href="sec14_2.html#cb4-37" tabindex="-1"></a><span class="co"># Bayesian estimation</span></span>
<span id="cb4-38"><a href="sec14_2.html#cb4-38" tabindex="-1"></a>zero.variance <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">prec =</span> <span class="fu">list</span>(<span class="at">initial =</span> <span class="dv">25</span>, <span class="at">fixed =</span> <span class="cn">TRUE</span>))</span>
<span id="cb4-39"><a href="sec14_2.html#cb4-39" tabindex="-1"></a>ant.mat <span class="ot">&lt;-</span> <span class="fu">nb2mat</span>(nb_object)</span>
<span id="cb4-40"><a href="sec14_2.html#cb4-40" tabindex="-1"></a>bmsp <span class="ot">&lt;-</span> <span class="fu">as</span>(ant.mat, <span class="st">&quot;CsparseMatrix&quot;</span>)</span>
<span id="cb4-41"><a href="sec14_2.html#cb4-41" tabindex="-1"></a>antioquia<span class="sc">$</span>idx <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(antioquia)</span>
<span id="cb4-42"><a href="sec14_2.html#cb4-42" tabindex="-1"></a>rrho1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.5</span>, <span class="fl">0.95</span>, <span class="at">len =</span> <span class="dv">10</span>)</span>
<span id="cb4-43"><a href="sec14_2.html#cb4-43" tabindex="-1"></a>semmodels <span class="ot">&lt;-</span> <span class="fu">mclapply</span>(rrho1, <span class="cf">function</span>(rho) {</span>
<span id="cb4-44"><a href="sec14_2.html#cb4-44" tabindex="-1"></a>    <span class="fu">sem.inla</span>(fform, <span class="at">d =</span> <span class="fu">as.data.frame</span>(antioquia), <span class="at">W =</span> bmsp, <span class="at">rho =</span> rho,</span>
<span id="cb4-45"><a href="sec14_2.html#cb4-45" tabindex="-1"></a>    <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">impacts =</span> <span class="cn">FALSE</span>,</span>
<span id="cb4-46"><a href="sec14_2.html#cb4-46" tabindex="-1"></a>    <span class="at">control.family =</span> <span class="fu">list</span>(<span class="at">hyper =</span> zero.variance),</span>
<span id="cb4-47"><a href="sec14_2.html#cb4-47" tabindex="-1"></a>    <span class="at">control.predictor =</span> <span class="fu">list</span>(<span class="at">compute =</span> <span class="cn">TRUE</span>),</span>
<span id="cb4-48"><a href="sec14_2.html#cb4-48" tabindex="-1"></a>    <span class="at">control.compute =</span> <span class="fu">list</span>(<span class="at">dic =</span> <span class="cn">TRUE</span>, <span class="at">cpo =</span> <span class="cn">TRUE</span>),</span>
<span id="cb4-49"><a href="sec14_2.html#cb4-49" tabindex="-1"></a>    <span class="at">control.inla =</span> <span class="fu">list</span>(<span class="at">print.joint.hyper =</span> <span class="cn">TRUE</span>))</span>
<span id="cb4-50"><a href="sec14_2.html#cb4-50" tabindex="-1"></a>})</span>
<span id="cb4-51"><a href="sec14_2.html#cb4-51" tabindex="-1"></a>bmasem <span class="ot">&lt;-</span> <span class="fu">INLABMA</span>(semmodels, rrho1, <span class="dv">0</span>, <span class="at">impacts =</span> <span class="cn">FALSE</span>)</span>
<span id="cb4-52"><a href="sec14_2.html#cb4-52" tabindex="-1"></a><span class="co">#Display results</span></span>
<span id="cb4-53"><a href="sec14_2.html#cb4-53" tabindex="-1"></a><span class="fu">plot</span>(bmasem<span class="sc">$</span>rho<span class="sc">$</span>marginal, <span class="at">type=</span><span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Spatial error model: Posterior spatial coefficient&quot;</span>)</span>
<span id="cb4-54"><a href="sec14_2.html#cb4-54" tabindex="-1"></a>bmasem[[<span class="st">&quot;rho&quot;</span>]][[<span class="st">&quot;quantiles&quot;</span>]]</span>
<span id="cb4-55"><a href="sec14_2.html#cb4-55" tabindex="-1"></a>marg_sla <span class="ot">&lt;-</span> bmasem[[<span class="st">&quot;marginals.fixed&quot;</span>]][[<span class="st">&quot;L_P_OLD&quot;</span>]]</span>
<span id="cb4-56"><a href="sec14_2.html#cb4-56" tabindex="-1"></a><span class="fu">plot</span>(marg_sla, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(beta[x]), <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Spatial error model: Posterior price elasticity&quot;</span>)</span>
<span id="cb4-57"><a href="sec14_2.html#cb4-57" tabindex="-1"></a>bmasem[[<span class="st">&quot;summary.fixed&quot;</span>]]</span></code></pre></div>
</div>
<div id="sec14_22" class="section level3 hasAnchor" number="14.2.2">
<h3><span class="header-section-number">14.2.2</span> Variational Bayes<a href="sec14_2.html#sec14_22" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Variational Bayes</em> (VB) is a method from machine learning <span class="citation">(<a href="#ref-jordan1999introduction">Jordan et al. 1999</a>; <a href="#ref-wainwright2008graphical">Wainwright, Jordan, et al. 2008</a>)</span> that replaces <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> with an approximation obtained through optimization using the calculus of variations, hence the name <em>variational</em> Bayes. This approach is useful when the posterior distribution is complex (e.g., multimodal) or when the parameter space is high-dimensional, making MCMC or IS algorithms computationally expensive.</p>
<p>The goal in VB is to approximate the posterior distribution using a distribution <span class="math inline">\(q(\boldsymbol{\theta})\)</span> from a variational family <span class="math inline">\(\mathcal{Q}\)</span>, a class of distributions that is computationally convenient yet flexible enough to closely approximate the true posterior <span class="citation">(<a href="#ref-blei2017variational">Blei, Kucukelbir, and McAuliffe 2017</a>)</span>. The distribution <span class="math inline">\(q\)</span> is called the <em>variational approximation</em> to the posterior, and a particular <span class="math inline">\(q\)</span> in <span class="math inline">\(\mathcal{Q}\)</span> is defined by a specific set of <em>variational parameters</em>. Typically, this approximation is obtained by minimizing the Kullback–Leibler (KL) divergence between <span class="math inline">\(q(\boldsymbol{\theta})\)</span> and <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span>.
<span class="math display" id="eq:2ch14">\[\begin{align}
    q^*(\boldsymbol{\theta}):=\underset{q \in \mathcal{Q}}{argmin} \  \text{KL}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta} \mid \mathbf{y})),
    \tag{14.2}
\end{align}\]</span><br />
where <span class="math inline">\(\text{KL}(q(\boldsymbol{\theta}) \| \pi(\boldsymbol{\theta} \mid \mathbf{y})) = \mathbb{E}_q\left[\log\left(\frac{q(\boldsymbol{\theta})}{\pi(\boldsymbol{\theta} \mid \mathbf{y})}\right)\right]\)</span>.</p>
<p>Note that in relatively complex models, the optimization in Equation (<a href="sec14_2.html#eq:2ch14">(14.2)</a> is not computable because it depends on the marginal likelihood <span class="math inline">\(p(\boldsymbol{y})\)</span>, which is typically unknown due to the intractability of the integral involved. However, there is a solution to this problem. Let’s see:
<span class="math display">\[\begin{align*}
    \log(p(\boldsymbol{y}))&amp;=\log\left(\int_{\boldsymbol{\Theta}}p(\boldsymbol{y}\mid \boldsymbol{\theta})\pi(\boldsymbol{\theta})d\boldsymbol{\theta}\right)\\
    &amp;=\log\left(\int_{\boldsymbol{\Theta}}p(\boldsymbol{y}, \boldsymbol{\theta})d\boldsymbol{\theta}\right)\\
    &amp;=\log\left(\int_{\boldsymbol{\Theta}}\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{q(\boldsymbol{\theta})}q(\boldsymbol{\theta})d\boldsymbol{\theta}\right)\\
    &amp;=\log \mathbb{E}_q\left(\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{q(\boldsymbol{\theta})}\right)\\
    &amp;\geq \mathbb{E}_q\log\left(\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{q(\boldsymbol{\theta})}\right)\\
    &amp;=\mathbb{E}_q\log(p(\boldsymbol{y}, \boldsymbol{\theta}))-\mathbb{E}_q\log(q(\boldsymbol{\theta}))\\
    &amp;=\text{ELBO}(q(\boldsymbol{\theta})),
\end{align*}\]</span>
where the inequality follows from Jensen’s inequality, since <span class="math inline">\(\log(\cdot)\)</span> is concave. The last term is the (ELBO), which serves as a lower bound for the marginal likelihood. Note that the gap between the marginal likelihood and the ELBO is given by
<span class="math display">\[\begin{align*}
    \log(p(\boldsymbol{y})) - \text{ELBO}(q(\boldsymbol{\theta})) &amp; = \log(p(\boldsymbol{y})) - \mathbb{E}_q\log(p(\boldsymbol{y}, \boldsymbol{\theta}))+\mathbb{E}_q\log(q(\boldsymbol{\theta}))\\
    &amp;=\mathbb{E}_q\left(\log(q(\boldsymbol{\theta}))-\log\left(\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{p(\boldsymbol{y})}\right)\right)\\
    &amp;=\mathbb{E}_q\left(\log(q(\boldsymbol{\theta}))-\log\left(\frac{p(\boldsymbol{y}\mid \boldsymbol{\theta})\pi( \boldsymbol{\theta})}{p(\boldsymbol{y})}\right)\right)\\
    &amp;=\mathbb{E}_q\left(\log(q(\boldsymbol{\theta}))-\log(\pi(\boldsymbol{\theta}\mid \boldsymbol{y}))\right)\\
    &amp;=\text{KL}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta} \mid \mathbf{y})).   
\end{align*}\]</span>
Then,
<span class="math display">\[\begin{align*}
    \log(p(\boldsymbol{y})) = \text{KL}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta} \mid \mathbf{y})) + \text{ELBO}(q(\boldsymbol{\theta})),  
\end{align*}\]</span>
which implies that maximizing the ELBO with respect to <span class="math inline">\(q(\boldsymbol{\theta})\)</span> is equivalent to minimizing the KL divergence, since <span class="math inline">\(\log(p(\boldsymbol{y}))\)</span> does not depend on <span class="math inline">\(q(\boldsymbol{\theta})\)</span>. This avoids the need to compute the marginal likelihood and, consequently, makes the variational problem easier to solve. In addition, it provides a lower bound for the marginal likelihood, which can potentially be used for model selection. Thus, solving problem <a href="sec14_2.html#eq:2ch14">(14.2)</a> is equivalent to solving
<span class="math display" id="eq:2ch14">\[\begin{align}
    q^*(\boldsymbol{\theta}):=\underset{q \in \mathcal{Q}}{argmax} \  \text{ELBO}(q(\boldsymbol{\theta})).
    \tag{14.2}
\end{align}\]</span></p>
<p>Note that the ELBO can be also expressed as
<span class="math display">\[\begin{align*}
    \text{ELBO}(q(\boldsymbol{\theta}))&amp;=\mathbb{E}_q\log(p(\boldsymbol{y}, \boldsymbol{\theta}))-\mathbb{E}_q\log(q(\boldsymbol{\theta}))\\
    &amp;=\mathbb{E}_q\log(p(\boldsymbol{y}\mid \boldsymbol{\theta}))+\mathbb{E}_q\log(\pi(\boldsymbol{\theta}))-\mathbb{E}_q\log(q(\boldsymbol{\theta}))\\
    &amp;=\mathbb{E}_q\log(p(\boldsymbol{y}\mid \boldsymbol{\theta}))-\text{KL}(q(\boldsymbol{\theta})||\pi(\boldsymbol{\theta})).
\end{align*}\]</span>
This means that when maximizing the ELBO, we seek the distribution <span class="math inline">\(q(\boldsymbol{\theta})\)</span> that both maximizes the likelihood and minimizes the KL divergence between the variational distribution and the prior. In other words, we aim to strike a balance between the prior and the likelihood, which aligns with the core principle of Bayesian inference deriving the posterior distribution.</p>
<p>The most common approach for specifying <span class="math inline">\(q(\boldsymbol{\theta})\)</span> is to assume independence across blocks of <span class="math inline">\(\boldsymbol{\theta}\)</span>, i.e., <span class="math inline">\(q(\boldsymbol{\theta}) = \prod_{l=1}^K q_l(\boldsymbol{\theta}_l)\)</span>. This is known as the <em>mean-field variational family</em>, a term that originates from statistical physics. Each <span class="math inline">\(q_l(\boldsymbol{\theta}_l)\)</span> is parameterized by a set of <em>variational parameters</em>, and optimization is performed with respect to these parameters. Note that the mean-field approximation does not capture dependencies between parameters, although it can approximate the marginal distributions.</p>
<p>Let us now decompose the ELBO under the mean-field variational family <span class="citation">(<a href="#ref-nguyen2023depth">Nguyen 2023</a>)</span>,
<span class="math display">\[\begin{align*}
        \text{ELBO}(q(\boldsymbol{\theta}))&amp;=\mathbb{E}_q\log(p(\boldsymbol{y}, \boldsymbol{\theta}))-\mathbb{E}_q\log(q(\boldsymbol{\theta}))\\
    &amp;=\int_{\mathbb{R}^K} \log(p(\boldsymbol{y}, \boldsymbol{\theta}))q(\boldsymbol{\theta})d\boldsymbol{\theta}-\int_{\mathbb{R}^K}\log(q(\boldsymbol{\theta}))q(\boldsymbol{\theta})d\boldsymbol{\theta}\\
    &amp;=\int_{\mathbb{R}^K} \log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right)\prod_{l=1}^K q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l-\int_{\mathbb{R}^K}\log\left(\prod_{l=1}^K q_l(\boldsymbol{\theta}_l)\right)\prod_{l=1}^K q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\\
    &amp;=\int_{\mathbb{R}} \underbrace{\left(\int_{\mathbb{R}^{K-1}}\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right)\prod_{l\neq k} q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\right)}_{\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))}q_k(\boldsymbol{\theta}_k)d\boldsymbol{\theta}_k\\
    &amp;-\int_{\mathbb{R}}\log(q_k(\boldsymbol{\theta}_k))q_k(\boldsymbol{\theta}_k)d\boldsymbol{\theta}_k-\sum_{l\neq k}\int_{\mathbb{R}}\log(q_l(\boldsymbol{\theta}_l))q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\\
    &amp;=\int_{\mathbb{R}}\log\left\{\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))\right)\right\}q_k(\boldsymbol{\theta}_k)d\boldsymbol{\theta}_k\\
    &amp;-\int_{\mathbb{R}}\log(q_k(\boldsymbol{\theta}_k))q_k(\boldsymbol{\theta}_k)d\boldsymbol{\theta}_k-\sum_{l\neq k}\int_{\mathbb{R}}\log(q_l(\boldsymbol{\theta}_l))q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\\
    &amp;=\int_{\mathbb{R}}\log\left\{\frac{\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))\right)}{q_k(\boldsymbol{\theta}_k)}\right\}q_k(\boldsymbol{\theta}_k)d\boldsymbol{\theta}_k\\
    &amp;-\sum_{l\neq k}\int_{\mathbb{R}}\log(q_l(\boldsymbol{\theta}_l))q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\\
    &amp;=-\text{KL}(q_k(\boldsymbol{\theta}_k)||\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))\right))-\sum_{l\neq k}\int_{\mathbb{R}}\log(q_l(\boldsymbol{\theta}_l))q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l\\
    &amp;\leq -\sum_{l\neq k}\int_{\mathbb{R}}\log(q_l(\boldsymbol{\theta}_l))q_l(\boldsymbol{\theta}_l)d\boldsymbol{\theta}_l,
\end{align*}\]</span>
where <span class="math inline">\(\mathbb{E}_{-k}\)</span> denotes expectation with respect to the distribution <span class="math inline">\(\prod_{l\neq k}q(\boldsymbol{\theta}_l)\)</span>.</p>
<p>Note that we maximize the ELBO when <span class="math inline">\(\text{KL}(q_k(\boldsymbol{\theta}_k)||\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))\right))\)</span> equals 0, that is, when
<span class="math display" id="eq:3ch14">\[\begin{align}
    q_k^*(\boldsymbol{\theta}_k)&amp;\propto\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta})\right))\right)\nonumber\\
    &amp;=\exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{y}, \boldsymbol{\theta}_{-k},\boldsymbol{\theta}_k)\right))\right)\nonumber\\
    &amp;=\exp\left(\mathbb{E}_{-k}(\log\left(p({\boldsymbol{\theta}}_{k}\mid\boldsymbol{y},\boldsymbol\theta_{-k})p(\boldsymbol{y},\boldsymbol\theta_{-k})\right))\right)\nonumber\\
    &amp;\propto \exp\left(\mathbb{E}_{-k}(\log\left(p(\boldsymbol{\theta}_{k}\mid\boldsymbol{y},\boldsymbol\theta_{-k})\right))\right), k=1,2,\dots,K.
    \tag{14.3}
\end{align}\]</span></p>
<p>Note the circular dependency inherent in <span class="math inline">\(q_k^*(\boldsymbol{\theta}_k)\)</span>: it depends on <span class="math inline">\(q_{-k}^*(\boldsymbol{\theta}_{-k})\)</span>. Therefore, this situation must be addressed algorithmically. One of the most common algorithms used to solve the optimization problem in <a href="sec14_2.html#eq:2ch14">(14.2)</a> using <a href="sec14_2.html#eq:3ch14">(14.3)</a> is the <em>coordinate ascent variational inference</em> (CAVI) algorithm <span class="citation">(<a href="#ref-bishop2006pattern">Bishop and Nasrabadi 2006</a>)</span>. The algorithm starts from an initial solution and iteratively cycles through each <span class="math inline">\(q_k^*(\boldsymbol{\theta}_k)\)</span>, for <span class="math inline">\(k = 1, 2, \dots, K\)</span>, updating each component in turn. The following Algorithm outlines the basic structure of the CAVI algorithm.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Variational Bayes – Coordinate Ascent Variational Inference</strong></p>
<ol style="list-style-type: decimal">
<li>Initialize the variational factors <span class="math inline">\(q_l^*(\boldsymbol{\theta}_l),\ l = 1, 2, \dots, K\)</span></li>
<li><strong>While</strong> ELBO <span class="math inline">\(&gt; \epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is small:
<ul>
<li><strong>For</strong> <span class="math inline">\(l = 1, 2, \dots, L\)</span>:
<ul>
<li>Update<br />
<span class="math display">\[
q_l^*(\boldsymbol{\theta}_l) \propto \exp\left(\mathbb{E}_{-l}\left[\log\left(p(\theta_{l} \mid \boldsymbol{y}, \boldsymbol{\theta}_{-l})\right)\right]\right)
\]</span></li>
</ul></li>
<li>Compute<br />
<span class="math display">\[
\text{ELBO}(q) = \mathbb{E}_q[\log(p(\boldsymbol{y}, \boldsymbol{\theta}))] - \mathbb{E}_q[\log(q(\boldsymbol{\theta}))]
\]</span></li>
</ul></li>
<li>Return <span class="math inline">\(q(\boldsymbol{\theta})\)</span></li>
</ol>
</div>
</div>
<p>We should note that the CAVI algorithm is sensitive to the initial variational factors because it guarantees convergence to a local maximum, which may depend heavily on the initialization point. This issue can be mitigated by using multiple starting points and selecting the solution with the highest ELBO <span class="citation">(<a href="#ref-blei2006variational">Blei and Jordan 2006</a>)</span>. In addition, it is well known that VB tends to overestimate the precision of the posterior distribution, although it does not necessarily suffer in terms of accuracy <span class="citation">(<a href="#ref-blei2017variational">Blei, Kucukelbir, and McAuliffe 2017</a>)</span>.</p>
<p>An important feature of the method is that it is deterministic rather than stochastic; that is, it does not require approximating the posterior distribution via sampling from simpler distributions. This often makes VB faster than MCMC methods, which are inherently stochastic. Instead, VB solves a deterministic optimization problem to find the <em>best variational distribution</em> within a chosen family, typically from the exponential family, by optimizing the variational parameters to minimize the KL divergence from the posterior distribution. Once the <em>variational parameters</em> are obtained, we can sample from the <em>variational distribution</em> to conduct estimation, hypothesis testing, prediction, and other tasks.</p>
<p>A limitation of the CAVI algorithm is that it requires evaluation at each data point, making it non-scalable in large data settings. In such situations, we can use <em>stochastic variational inference</em> (SVI) <span class="citation">(<a href="#ref-hoffman2013stochastic">Hoffman et al. 2013</a>)</span>, an algorithm that optimizes the ELBO using natural gradients combined with stochastic optimization. SVI is particularly effective when each complete conditional belongs to the exponential family (see Section <a href="sec41.html#sec41">3.1</a>), which includes most of the models discussed in this book. It is especially useful for conditionally conjugate models that include <em>local</em> latent variables (<span class="math inline">\(\boldsymbol{z}_i\)</span>) associated with specific data points, and <em>global</em> parameters (<span class="math inline">\(\boldsymbol{\phi}\)</span>) shared across the entire dataset. That is, we define <span class="math inline">\(\boldsymbol\theta = [\boldsymbol{z}^{\top} \ \boldsymbol{\phi}^{\top}]^{\top}\)</span>. An example is the probit model using data augmentation <span class="citation">(<a href="#ref-Tanner1987">Tanner and Wong 1987</a>)</span>.</p>
<p>Given the global-local exchangeable structure, the joint distribution can be expressed as <span class="math inline">\(p(\boldsymbol{\phi}, \boldsymbol{z}, \boldsymbol{y}) = \pi(\boldsymbol{\phi} \mid \boldsymbol{\alpha}) \prod_{i=1}^{N} p(\boldsymbol{z}_i, \boldsymbol{y}_i \mid \boldsymbol{\phi})\)</span>, where <span class="math inline">\(\boldsymbol{\alpha} = [\boldsymbol{\alpha}_1^{\top} \ \alpha_2]^{\top}\)</span> represents the hyperparameters of the prior distribution of the global parameters <span class="math inline">\(\boldsymbol{\phi}\)</span>.</p>
<p>Assuming that the joint distribution <span class="math inline">\(p(\boldsymbol{z}_i, \boldsymbol{y}_i \mid \boldsymbol{\phi})\)</span> is in the canonical form of the exponential family, and that each prior distribution is conjugate, the complete conditional distribution of the global parameters is also in the exponential family. The posterior parameters are given by <span class="math inline">\(\boldsymbol{\alpha}_n = [\boldsymbol{\alpha}_1^{\top} + \sum_{i=1}^N t(\boldsymbol{z}_i, \boldsymbol{y}_i)^{\top} \ \alpha_2 + N]^{\top}\)</span>, where <span class="math inline">\(t(\boldsymbol{z}_i, \boldsymbol{y}_i)\)</span> is a sufficient statistic of <span class="math inline">\(\boldsymbol{z}_i\)</span> and <span class="math inline">\(\boldsymbol{y}_i\)</span> (see Section <a href="sec42.html#sec42">3.2</a>).</p>
<p>Additionally, the structure of the model implies that <span class="math inline">\(\boldsymbol{z}_i\)</span> is independent of <span class="math inline">\(\boldsymbol{z}_{-i}\)</span> and <span class="math inline">\(\boldsymbol{y}_{-i}\)</span> given <span class="math inline">\(\boldsymbol{y}_i\)</span> and <span class="math inline">\(\boldsymbol{\phi}\)</span>. Thus, if <span class="math inline">\(p(\boldsymbol{z}_i \mid \boldsymbol{y}_i, \boldsymbol{\phi})\)</span> is in the canonical form of the exponential family, then the local variational update is given by <span class="math inline">\(\boldsymbol{\psi}_i = \mathbb{E}_{\boldsymbol{\xi}}[\boldsymbol{\eta}(\boldsymbol{\phi}, \boldsymbol{y}_i)]\)</span>, where the parameter set <span class="math inline">\(\boldsymbol{\eta}(\boldsymbol{\phi}, \boldsymbol{y}_i)\)</span> is a function of the conditional set, <span class="math inline">\(\boldsymbol{\xi}\)</span>, and <span class="math inline">\(\boldsymbol{\psi}_i\)</span> are the variational parameters of the variational approximations <span class="math inline">\(q(\boldsymbol{\phi} \mid \boldsymbol{\xi})\)</span> and <span class="math inline">\(q(\boldsymbol{z}_i \mid \boldsymbol{\psi}_i)\)</span> for the posterior distributions of <span class="math inline">\(\boldsymbol{\phi}\)</span> and <span class="math inline">\(\boldsymbol{z}_i\)</span>, respectively. The global variational update is given by <span class="math inline">\(\boldsymbol{\xi} = [\boldsymbol{\alpha}_1^{\top} + \sum_{i=1}^N \mathbb{E}_{\boldsymbol{\psi}_i} t(\boldsymbol{z}_i, \boldsymbol{y}_i)^{\top} \ \alpha_2 + N]^{\top}\)</span>.</p>
<p>SVI focuses on the global parameters, updating them using the ELBO natural gradients with respect to <span class="math inline">\(\boldsymbol{\xi}\)</span>, where natural gradients are the usual gradients premultiplied by the inverse covariance matrix of the sufficient statistic. These gradients are easily calculated in exponential families. In particular, the updates are given by
<span class="math display">\[\begin{align*}
    \boldsymbol{\xi}_t=\boldsymbol{\xi}_{t-1}+\epsilon_t g(\boldsymbol{\xi}_{t-1}),
\end{align*}\]</span>
where <span class="math inline">\(\epsilon_t\)</span> is the step size, and <span class="math inline">\(g(\boldsymbol{\xi}_t)=\mathbb{E}_{\boldsymbol{\psi}_i}[\boldsymbol{\alpha}_n-\boldsymbol{\xi}_t]\)</span> is the natural gradient of the global variational parameters <span class="citation">(<a href="#ref-blei2017variational">Blei, Kucukelbir, and McAuliffe 2017</a>)</span>. Consequently,
<span class="math display">\[\begin{align*}
    \boldsymbol{\xi}_t=(1-\epsilon_t)\boldsymbol{\xi}_{t-1}+\epsilon_t \mathbb{E}_{\boldsymbol{\psi}}[\boldsymbol{\alpha}_n].
\end{align*}\]</span></p>
<p>However, calculating this update requires using the entire dataset, which is computationally burdensome. Therefore, we should use stochastic optimization, which follows noisy but cheap-to-compute unbiased gradients to optimize the function. The key idea is to construct the natural gradient using only one random draw from the dataset, and then scale it (by multiplying it with the sample size) to approximate the sample information:
<span class="math display">\[\begin{align*}
    \hat{\boldsymbol{\xi}}&amp;=\boldsymbol{\alpha}+N(\mathbb{E}_{\boldsymbol{\psi}_i^*}[t(\boldsymbol{z}_i,\boldsymbol{y}_i)]^{\top},1)^{\top}\\
    \boldsymbol{\xi}_t&amp;=(1-\epsilon_t)\boldsymbol{\xi}_{t-1}+\epsilon_t\hat{\boldsymbol{\xi}},
\end{align*}\]</span>
where <span class="math inline">\((\boldsymbol{z}_i, \boldsymbol{y}_i)\)</span> is a random draw from the sample and its corresponding latent variable. Finally, the step size schedule to update the global parameters are given by
<span class="math display">\[\begin{align*}
    \epsilon_t&amp;=t^{-\kappa}, \ 0.5 &lt; \kappa \leq 1.
\end{align*}\]</span>
This step size schedule satisfies the Robbins and Monro conditions necessary for stochastic optimization <span class="citation">(<a href="#ref-robbins1951stochastic">Robbins and Monro 1951</a>)</span>.</p>
<p>The following Algorithm shows the stochastic variational inference algorithm.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Variational Bayes – Stochastic Variational Inference</strong></p>
<ol style="list-style-type: decimal">
<li>Initialize the variational global parameter <span class="math inline">\(\boldsymbol{\phi}_0\)</span><br />
</li>
<li>Set the step size schedule <span class="math inline">\(\epsilon_t = t^{-\kappa},\ 0.5 &lt; \kappa \leq 1\)</span><br />
</li>
<li><strong>While</strong> TRUE:
<ul>
<li>Randomly select a data point <span class="math inline">\(\boldsymbol{y}_i \sim U(1, 2, \dots, N)\)</span></li>
<li>Optimize its local variational parameters:<br />
<span class="math display">\[
\boldsymbol{\psi}_i^* = \mathbb{E}_{\boldsymbol{\xi}_{t-1}}[\boldsymbol{\eta}(\boldsymbol{\phi}, y_i)]
\]</span></li>
<li>Compute the coordinate updates assuming <span class="math inline">\(\boldsymbol{y}_i\)</span> was repeated <span class="math inline">\(N\)</span> times:<br />
<span class="math display">\[
\hat{\boldsymbol{\xi}} = \boldsymbol{\alpha} + N \cdot
\left(
  \mathbb{E}_{\boldsymbol{\psi}_i^*}[t(\boldsymbol{z}_i, \boldsymbol{y}_i)]^\top, 1
\right)^\top
\]</span></li>
<li>Update the global variational parameters:<br />
<span class="math display">\[
\boldsymbol{\xi}_t = (1 - \epsilon_t) \boldsymbol{\xi}_{t-1} + \epsilon_t \hat{\boldsymbol{\xi}}
\]</span></li>
</ul></li>
<li>Return <span class="math inline">\(q_{\boldsymbol{\xi}}(\boldsymbol{\phi})\)</span></li>
</ol>
</div>
</div>
<p><span class="citation">Zhang and Gao (<a href="#ref-zhang2020convergence">2020</a>)</span> analyzes the asymptotic properties of the VB posterior by decomposing the convergence rates of VB into the convergence rate of the true posterior and the approximation error induced by the variational family. These authors show that the VB posterior concentrates entirely in a neighborhood of the true posterior distribution. In addition, if the loss function is convex, there exists a point estimator that converges at the same rate. <span class="citation">Zhang and Gao (<a href="#ref-zhang2020convergence">2020</a>)</span> also shows that, for specific cases such as sparse linear models, the concentration rate of the VB posterior is faster than the concentration rate of the exact posterior.</p>
<p>Variational Bayes (VB) shares some similarities with Gibbs sampling. In Gibbs sampling, we iteratively sample from the conditional posterior distributions, whereas in VB we iteratively update the variational parameters of the variational family. The former is stochastic, while the latter is deterministic, and consequently faster in complex models or large datasets. VB also bears resemblance to Expectation Propagation (EP): both are deterministic algorithms that approximate the posterior distribution by minimizing the Kullback-Leibler (KL) divergence. However, VB minimizes <span class="math inline">\(\text{KL}(q(\boldsymbol{\theta}) \| \pi(\boldsymbol{\theta} \mid \boldsymbol{y}))\)</span>, whereas EP minimizes <span class="math inline">\(\text{KL}(\pi(\boldsymbol{\theta} \mid \boldsymbol{y}) \| q(\boldsymbol{\theta}))\)</span>. As a result, VB tends to approximate the mode of the posterior by maximizing the ELBO, while EP focuses on matching the mean and variance through moment matching <span class="citation">(<a href="#ref-bishop2006pattern">Bishop and Nasrabadi 2006</a>; <a href="#ref-gelman2021bayesian">Gelman et al. 2021</a>)</span>. Although EP often provides better uncertainty quantification, it tends to be less stable and does not scale well to large datasets.</p>
<p>VB also shares some conceptual features with the Expectation-Maximization (EM) algorithm. In both methods, there is an initial step involving the computation of expectations — used in VB to derive the variational distributions — and an iterative step that maximizes a target function (the ELBO in VB, the expected complete-data log-likelihood in EM). However, EM yields point estimates, whereas VB yields full posterior approximations.</p>
<p><strong>Example: Linear regression</strong></p>
<p>Let’s perform variational Bayes inference in the linear regression model with conjugate family. In particular,<br />
<span class="math display">\[
\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\mu},
\]</span><br />
where <span class="math inline">\(\boldsymbol{\mu} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I})\)</span>. This implies that<br />
<span class="math display">\[
\boldsymbol{y} \sim N(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}).
\]</span></p>
<p>The conjugate priors for the parameters are:<br />
<span class="math display">\[\begin{align*}
\boldsymbol{\beta}\mid \sigma^2 &amp; \sim N(\boldsymbol{\beta}_0, \sigma^2 \boldsymbol{B}_0),\\
\sigma^2 &amp; \sim IG(\alpha_0/2, \delta_0/2).
\end{align*}\]</span></p>
<p>Then, the posterior distributions are<br />
<span class="math display">\[
\boldsymbol{\beta} \mid \sigma^2, \boldsymbol{y}, \boldsymbol{X} \sim N(\boldsymbol{\beta}_n, \sigma^2 \boldsymbol{B}_n), \quad
\sigma^2 \mid \boldsymbol{y}, \boldsymbol{X} \sim IG(\alpha_n/2, \delta_n/2),
\]</span><br />
where<br />
- <span class="math inline">\(\boldsymbol{B}_n = (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top} \boldsymbol{X})^{-1}\)</span><br />
- <span class="math inline">\(\boldsymbol{\beta}_n = \boldsymbol{B}_n(\boldsymbol{B}_0^{-1} \boldsymbol{\beta}_0 + \boldsymbol{X}^{\top} \boldsymbol{X} \hat{\boldsymbol{\beta}})\)</span>,<br />
- <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the MLE,<br />
- <span class="math inline">\(\alpha_n = \alpha_0 + N\)</span>,<br />
- <span class="math inline">\(\delta_n = \delta_0 + \boldsymbol{y}^{\top} \boldsymbol{y} + \boldsymbol{\beta}_0^{\top} \boldsymbol{B}_0^{-1} \boldsymbol{\beta}_0 - \boldsymbol{\beta}_n^{\top} \boldsymbol{B}_n^{-1} \boldsymbol{\beta}_n\)</span> (see Section <a href="sec43.html#sec43">3.3</a>).</p>
<p>Let’s use the mean-field variational family <span class="math inline">\(q(\boldsymbol{\beta},\sigma^2)=q(\boldsymbol{\beta})q(\sigma^2)\approx \pi(\boldsymbol{\beta},\sigma^2\mid\boldsymbol{y},\boldsymbol{X})\)</span>. Then,
<span class="math display">\[\begin{align*}
    \log q^*(\boldsymbol{\beta})&amp;\propto\mathbb{E}_{\sigma^2}[\log p(\boldsymbol{y},\boldsymbol{\beta},\sigma^2\mid\boldsymbol{X})]\\
    &amp;=\mathbb{E}_{\sigma^2}[\log p(\boldsymbol{y}\mid\boldsymbol{\beta},\sigma^2,\boldsymbol{X}) +\log \pi(\boldsymbol{\beta}\mid\sigma^2)+\log \pi(\sigma^2)]\\
    &amp;=\mathbb{E}_{\sigma^2}\left(-\frac{N}{2}\log\sigma^2-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^{\top}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})-\frac{K}{2}\log\sigma^2\right.\\
    &amp;\left.-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\top}\boldsymbol{B}_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)\right)+c_1\\
    &amp;=-0.5\mathbb{E}_{\sigma^2}\left(\frac{1}{\sigma^2}\right)[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)]+c_2.\\
\end{align*}\]</span>
The last equality follows the same steps to obtain the posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are arbitrary constants. This expression implies that <span class="math inline">\(q^*(\boldsymbol{\beta})\)</span> is <span class="math inline">\(N\left(\boldsymbol{\beta}_n,\left(\mathbb{E}_{\sigma^2}\left(\frac{1}{\sigma^2}\right)\right)^{-1}\boldsymbol{B}_n\right)\)</span>.</p>
<p>In addition,
<span class="math display">\[\begin{align*}
    \log q^*(\sigma^2)&amp;\propto\mathbb{E}_{\boldsymbol{\beta}}[\log p(\boldsymbol{y},\boldsymbol{\beta},\sigma^2\mid\boldsymbol{X})]\\
    &amp;=\mathbb{E}_{\boldsymbol{\beta}}[\log p(\boldsymbol{y}\mid\boldsymbol{\beta},\sigma^2,\boldsymbol{X}) +\log \pi(\boldsymbol{\beta}\mid\sigma^2)+\log \pi(\sigma^2)]\\
    &amp;=\mathbb{E}_{\boldsymbol{\beta}}\left(-\frac{N}{2}\log\sigma^2-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^{\top}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})-\frac{K}{2}\log\sigma^2\right.\\
    &amp;\left.-\frac{1}{2\sigma^2}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\top}\boldsymbol{B}_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)-(\alpha_0/2+1)\log \sigma^2 -\frac{\delta_0}{2\sigma^2}\right)+c_1\\
    &amp;=-\mathbb{E}_{\boldsymbol{\beta}}\left[\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^{\top}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})+(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\top}\boldsymbol{B}_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)+\delta_0\right]\\
    &amp;-\left(\frac{N+K+\alpha_0}{2}+1\right)\log\sigma^2+c_1.
\end{align*}\]</span>
This means that <span class="math inline">\(q^*(\sigma^2)\)</span> is <span class="math inline">\(IG(\alpha_n/2,\delta_n/2)\)</span>, where <span class="math inline">\(\alpha_n=N+K+\alpha_0\)</span>, and
<span class="math display">\[\begin{align*}
    \delta_n&amp;=\mathbb{E}_{\boldsymbol{\beta}}\left[(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^{\top}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})+(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\top}\boldsymbol{B}_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)+\delta_0\right]\\
    &amp;=\mathbb{E}_{\boldsymbol{\beta}}\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)\right]-\boldsymbol{\beta}_n^{\top}\boldsymbol{B}_n^{-1}\boldsymbol{\beta}_n+\boldsymbol{y}^{\top}\boldsymbol{y}+\boldsymbol{\beta}_0^{\top}\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0+\delta_0.
\end{align*}\]</span>
This result implies that <span class="math inline">\(\mathbb{E}_{\sigma^2}\left(\frac{1}{\sigma^2}\right) = \alpha_n / \delta_n\)</span>, since the inverse of a gamma-distributed random variable (in the rate parametrization) follows an inverse-gamma distribution. Therefore, <span class="math inline">\(\operatorname{Var}(\boldsymbol{\beta}) = \frac{\delta_n}{\alpha_n} \boldsymbol{B}_n\)</span>.
Note that
<span class="math display">\[\begin{align*}
    \mathbb{E}_{\boldsymbol{\beta}}\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)\right]&amp;=tr\left\{\mathbb{E}_{\boldsymbol{\beta}}\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)\right]\right\}\\
    &amp;=\mathbb{E}_{\boldsymbol{\beta}}\left\{tr\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_n)\right]\right\}\\
    &amp;=\mathbb{E}_{\boldsymbol{\beta}}\left\{tr\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\boldsymbol{B}_n^{-1}\right]\right\}\\
    &amp;=tr\left\{\mathbb{E}_{\boldsymbol{\beta}}\left[(\boldsymbol{\beta}-\boldsymbol{\beta}_n)(\boldsymbol{\beta}-\boldsymbol{\beta}_n)^{\top}\right]\boldsymbol{B}_n^{-1}\right\}\\
    &amp;=tr\left\{Var(\boldsymbol{\beta})\boldsymbol{B}_n^{-1}\right\},\\
\end{align*}\]</span>
where we use that the trace of a scalar is the scalar itself, that expectation and trace can be interchanged since both are linear operators, and that the trace operator is invariant under cyclic permutations.
Then, <span class="math display">\[\begin{align*}
    \delta_n&amp;=tr\left\{Var(\boldsymbol{\beta})\boldsymbol{B}_n^{-1}\right\}-\boldsymbol{\beta}_n^{\top}\boldsymbol{B}_n^{-1}\boldsymbol{\beta}_n+\boldsymbol{y}^{\top}\boldsymbol{y}+\boldsymbol{\beta}_0^{\top}\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0+\delta_0\\
    &amp;=\left(\frac{\alpha_0+N+K}{\alpha_0+K}\right)\left(-\boldsymbol{\beta}_n^{\top}\boldsymbol{B}_n^{-1}\boldsymbol{\beta}_n+\boldsymbol{y}^{\top}\boldsymbol{y}+\boldsymbol{\beta}_0^{\top}\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0+\delta_0\right),
\end{align*}\]</span>
where the last equality follows from <span class="math inline">\(tr\left\{Var(\boldsymbol{\beta})\boldsymbol{B}_n^{-1}\right\}=\delta_n/\alpha_n tr\left\{\boldsymbol{I}_K\right\}\)</span>.</p>
<p>In addition (Exercise 3),
<span class="math display">\[\begin{align*}
    \text{ELBO}(q(\boldsymbol{\beta},\sigma^2))&amp;=\mathbb{E}_{\boldsymbol{\beta},\sigma^2}[\log p(\boldsymbol{y},\boldsymbol{\beta},\sigma^2\mid\boldsymbol{X})]-\mathbb{E}_{\boldsymbol{\beta},\sigma^2}[\log q(\boldsymbol{\beta},\sigma^2)]\\
    &amp;=-\frac{N}{2}\log(2\pi)+\frac{\alpha_0}{2}\log(\delta_0/2)-\frac{\alpha_n}{2}\log(\delta_n/2)-0.5\log|\boldsymbol{B}_0|\\
    &amp;+0.5\log|\boldsymbol{B}_n|-\log\Gamma(\alpha_0/2)+\log\Gamma(\alpha_n/2)\\
    &amp;-K/2\log(\alpha_n/\delta_n)+0.5tr\left\{Var(\boldsymbol{\beta})\boldsymbol{B}_n^{-1}\right\}.
\end{align*}\]</span>
Note that the first two lines of the ELBO share the same structure as the log marginal likelihood <span class="math inline">\(p(\boldsymbol{y})\)</span> in Section <a href="sec43.html#sec43">3.3</a>.</p>
<p>The following code presents a simulation setting with a sample size of 500 and two regressors drawn from standard normal distributions. The population parameters are set to 1, and we use non-informative priors with 10,000 posterior draws.</p>
<p>First, we perform VB inference using the <em>LaplacesDemon</em> package, and then implement it from scratch. Although we have analytical solutions in this setting, we apply our own CAVI algorithm to assess its performance. We also compare the results with those from the Gibbs sampler, the marginal likelihood, and the ELBO.</p>
<p>In general, all calculations seem to perform well: the 95% credible intervals contain the population parameters, and the posterior means are very close to them.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="sec14_2.html#cb5-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb5-2"><a href="sec14_2.html#cb5-2" tabindex="-1"></a><span class="fu">library</span>(LaplacesDemon)</span>
<span id="cb5-3"><a href="sec14_2.html#cb5-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">500</span>; K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb5-4"><a href="sec14_2.html#cb5-4" tabindex="-1"></a>sig2 <span class="ot">&lt;-</span> <span class="dv">1</span>; B <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, K <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb5-5"><a href="sec14_2.html#cb5-5" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N<span class="sc">*</span>K), N, K))</span>
<span id="cb5-6"><a href="sec14_2.html#cb5-6" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, sig2<span class="sc">^</span><span class="fl">0.5</span>)</span>
<span id="cb5-7"><a href="sec14_2.html#cb5-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>B <span class="sc">+</span> e</span>
<span id="cb5-8"><a href="sec14_2.html#cb5-8" tabindex="-1"></a><span class="do">######################### Data List Preparation #########################</span></span>
<span id="cb5-9"><a href="sec14_2.html#cb5-9" tabindex="-1"></a>mon.names <span class="ot">&lt;-</span> <span class="st">&quot;mu[1]&quot;</span></span>
<span id="cb5-10"><a href="sec14_2.html#cb5-10" tabindex="-1"></a>parm.names <span class="ot">&lt;-</span> <span class="fu">as.parm.names</span>(<span class="fu">list</span>(<span class="at">beta=</span><span class="fu">rep</span>(<span class="dv">0</span>,K <span class="sc">+</span> <span class="dv">1</span>), <span class="at">sigma2=</span><span class="dv">0</span>))</span>
<span id="cb5-11"><a href="sec14_2.html#cb5-11" tabindex="-1"></a>pos.beta <span class="ot">&lt;-</span> <span class="fu">grep</span>(<span class="st">&quot;beta&quot;</span>, parm.names)</span>
<span id="cb5-12"><a href="sec14_2.html#cb5-12" tabindex="-1"></a>pos.sigma2 <span class="ot">&lt;-</span> <span class="fu">grep</span>(<span class="st">&quot;sigma2&quot;</span>, parm.names)</span>
<span id="cb5-13"><a href="sec14_2.html#cb5-13" tabindex="-1"></a>PGF <span class="ot">&lt;-</span> <span class="cf">function</span>(Data) {</span>
<span id="cb5-14"><a href="sec14_2.html#cb5-14" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(Data<span class="sc">$</span>K)</span>
<span id="cb5-15"><a href="sec14_2.html#cb5-15" tabindex="-1"></a>    sigma2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb5-16"><a href="sec14_2.html#cb5-16" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">c</span>(beta, sigma2))</span>
<span id="cb5-17"><a href="sec14_2.html#cb5-17" tabindex="-1"></a>}</span>
<span id="cb5-18"><a href="sec14_2.html#cb5-18" tabindex="-1"></a>MyData <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">K=</span>K, <span class="at">PGF=</span>PGF, <span class="at">X=</span>X, <span class="at">mon.names=</span>mon.names,</span>
<span id="cb5-19"><a href="sec14_2.html#cb5-19" tabindex="-1"></a><span class="at">parm.names=</span>parm.names, <span class="at">pos.beta=</span>pos.beta, <span class="at">pos.sigma2=</span>pos.sigma2, <span class="at">y=</span>y)</span>
<span id="cb5-20"><a href="sec14_2.html#cb5-20" tabindex="-1"></a><span class="do">########################## Model Specification ##########################</span></span>
<span id="cb5-21"><a href="sec14_2.html#cb5-21" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="dv">0</span>; B0 <span class="ot">&lt;-</span> <span class="dv">1000</span>; a0 <span class="ot">&lt;-</span> <span class="fl">0.01</span>; d0 <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb5-22"><a href="sec14_2.html#cb5-22" tabindex="-1"></a>Model <span class="ot">&lt;-</span> <span class="cf">function</span>(parm, Data)</span>
<span id="cb5-23"><a href="sec14_2.html#cb5-23" tabindex="-1"></a>{</span>
<span id="cb5-24"><a href="sec14_2.html#cb5-24" tabindex="-1"></a>    <span class="do">### Parameters</span></span>
<span id="cb5-25"><a href="sec14_2.html#cb5-25" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> parm[Data<span class="sc">$</span>pos.beta]</span>
<span id="cb5-26"><a href="sec14_2.html#cb5-26" tabindex="-1"></a>    sigma2 <span class="ot">&lt;-</span> <span class="fu">interval</span>(parm[Data<span class="sc">$</span>pos.sigma2], <span class="fl">1e-100</span>, <span class="cn">Inf</span>)</span>
<span id="cb5-27"><a href="sec14_2.html#cb5-27" tabindex="-1"></a>    parm[Data<span class="sc">$</span>pos.sigma2] <span class="ot">&lt;-</span> sigma2</span>
<span id="cb5-28"><a href="sec14_2.html#cb5-28" tabindex="-1"></a>    <span class="do">### Log-Prior</span></span>
<span id="cb5-29"><a href="sec14_2.html#cb5-29" tabindex="-1"></a>    beta.prior <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dnormv</span>(beta, b0, B0, <span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb5-30"><a href="sec14_2.html#cb5-30" tabindex="-1"></a>    sigma2.prior <span class="ot">&lt;-</span> <span class="fu">dinvgamma</span>(sigma2, a0<span class="sc">/</span><span class="dv">2</span>, d0<span class="sc">/</span><span class="dv">2</span>, <span class="at">log=</span><span class="cn">TRUE</span>)</span>
<span id="cb5-31"><a href="sec14_2.html#cb5-31" tabindex="-1"></a>    <span class="do">### Log-Likelihood</span></span>
<span id="cb5-32"><a href="sec14_2.html#cb5-32" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> <span class="fu">tcrossprod</span>(Data<span class="sc">$</span>X, <span class="fu">t</span>(beta))</span>
<span id="cb5-33"><a href="sec14_2.html#cb5-33" tabindex="-1"></a>    LL <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dnorm</span>(Data<span class="sc">$</span>y, mu, sigma2<span class="sc">^</span><span class="fl">0.5</span>, <span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb5-34"><a href="sec14_2.html#cb5-34" tabindex="-1"></a>    <span class="do">### Log-Posterior</span></span>
<span id="cb5-35"><a href="sec14_2.html#cb5-35" tabindex="-1"></a>    LP <span class="ot">&lt;-</span> LL <span class="sc">+</span> beta.prior <span class="sc">+</span> sigma2.prior</span>
<span id="cb5-36"><a href="sec14_2.html#cb5-36" tabindex="-1"></a>    Modelout <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">LP=</span>LP, <span class="at">Dev=</span><span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>LL, <span class="at">Monitor=</span>mu[<span class="dv">1</span>],</span>
<span id="cb5-37"><a href="sec14_2.html#cb5-37" tabindex="-1"></a>    <span class="at">yhat=</span><span class="fu">rnorm</span>(<span class="fu">length</span>(mu), mu, sigma2<span class="sc">^</span><span class="fl">0.5</span>), <span class="at">parm=</span>parm)</span>
<span id="cb5-38"><a href="sec14_2.html#cb5-38" tabindex="-1"></a>    <span class="fu">return</span>(Modelout)</span>
<span id="cb5-39"><a href="sec14_2.html#cb5-39" tabindex="-1"></a>}</span>
<span id="cb5-40"><a href="sec14_2.html#cb5-40" tabindex="-1"></a>Initial.Values <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,K<span class="sc">+</span><span class="dv">2</span>); S <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb5-41"><a href="sec14_2.html#cb5-41" tabindex="-1"></a>Fit <span class="ot">&lt;-</span> <span class="fu">VariationalBayes</span>(Model, Initial.Values, <span class="at">Data=</span>MyData, <span class="at">Covar=</span><span class="cn">NULL</span>, </span>
<span id="cb5-42"><a href="sec14_2.html#cb5-42" tabindex="-1"></a><span class="at">Iterations=</span>S, <span class="at">Method=</span><span class="st">&quot;Salimans2&quot;</span>, <span class="at">Stop.Tolerance=</span><span class="fl">1e-2</span>, <span class="at">CPUs=</span><span class="dv">1</span>)</span>
<span id="cb5-43"><a href="sec14_2.html#cb5-43" tabindex="-1"></a><span class="fu">print</span>(Fit)</span>
<span id="cb5-44"><a href="sec14_2.html#cb5-44" tabindex="-1"></a><span class="fu">PosteriorChecks</span>(Fit)</span>
<span id="cb5-45"><a href="sec14_2.html#cb5-45" tabindex="-1"></a><span class="fu">caterpillar.plot</span>(Fit, <span class="at">Parms=</span><span class="st">&quot;beta&quot;</span>)</span>
<span id="cb5-46"><a href="sec14_2.html#cb5-46" tabindex="-1"></a><span class="fu">plot</span>(Fit, MyData, <span class="at">PDF=</span><span class="cn">FALSE</span>)</span>
<span id="cb5-47"><a href="sec14_2.html#cb5-47" tabindex="-1"></a>Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(Fit, Model, MyData, <span class="at">CPUs=</span><span class="dv">1</span>)</span>
<span id="cb5-48"><a href="sec14_2.html#cb5-48" tabindex="-1"></a><span class="fu">summary</span>(Pred, <span class="at">Discrep=</span><span class="st">&quot;Chi-Square&quot;</span>)</span>
<span id="cb5-49"><a href="sec14_2.html#cb5-49" tabindex="-1"></a><span class="do">####### MCMC #######</span></span>
<span id="cb5-50"><a href="sec14_2.html#cb5-50" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb5-51"><a href="sec14_2.html#cb5-51" tabindex="-1"></a>b0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(b0, K<span class="sc">+</span><span class="dv">1</span>); B0 <span class="ot">&lt;-</span> B0<span class="sc">*</span><span class="fu">diag</span>(K<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb5-52"><a href="sec14_2.html#cb5-52" tabindex="-1"></a>bhat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>y</span>
<span id="cb5-53"><a href="sec14_2.html#cb5-53" tabindex="-1"></a>Bn <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(Matrix<span class="sc">::</span><span class="fu">forceSymmetric</span>(<span class="fu">solve</span>(<span class="fu">solve</span>(B0) <span class="sc">+</span> <span class="fu">t</span>(X)<span class="sc">%*%</span>X)))</span>
<span id="cb5-54"><a href="sec14_2.html#cb5-54" tabindex="-1"></a>bn <span class="ot">&lt;-</span> Bn<span class="sc">%*%</span>(<span class="fu">solve</span>(B0)<span class="sc">%*%</span>b0 <span class="sc">+</span> <span class="fu">t</span>(X)<span class="sc">%*%</span>X<span class="sc">%*%</span>bhat)</span>
<span id="cb5-55"><a href="sec14_2.html#cb5-55" tabindex="-1"></a>dn <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(d0 <span class="sc">+</span> <span class="fu">t</span>(y)<span class="sc">%*%</span>y<span class="sc">+</span><span class="fu">t</span>(b0)<span class="sc">%*%</span><span class="fu">solve</span>(B0)<span class="sc">%*%</span>b0<span class="sc">-</span><span class="fu">t</span>(bn)<span class="sc">%*%</span><span class="fu">solve</span>(Bn)<span class="sc">%*%</span>bn)</span>
<span id="cb5-56"><a href="sec14_2.html#cb5-56" tabindex="-1"></a>an <span class="ot">&lt;-</span> a0 <span class="sc">+</span> N</span>
<span id="cb5-57"><a href="sec14_2.html#cb5-57" tabindex="-1"></a><span class="co"># Posterior draws</span></span>
<span id="cb5-58"><a href="sec14_2.html#cb5-58" tabindex="-1"></a>sig2 <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">rinvgamma</span>(S,an<span class="sc">/</span><span class="dv">2</span>,dn<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb5-59"><a href="sec14_2.html#cb5-59" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(sig2))</span>
<span id="cb5-60"><a href="sec14_2.html#cb5-60" tabindex="-1"></a>Betas <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="cf">function</span>(s){MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="dv">1</span>, bn, sig2[s]<span class="sc">*</span>Bn)}))</span>
<span id="cb5-61"><a href="sec14_2.html#cb5-61" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(Betas))</span>
<span id="cb5-62"><a href="sec14_2.html#cb5-62" tabindex="-1"></a><span class="do">####### VB from scratch #######</span></span>
<span id="cb5-63"><a href="sec14_2.html#cb5-63" tabindex="-1"></a>dnVB <span class="ot">&lt;-</span> ((a0<span class="sc">+</span>N<span class="sc">+</span>K)<span class="sc">/</span>(a0<span class="sc">+</span>N))<span class="sc">*</span>dn; anVB <span class="ot">&lt;-</span> a0 <span class="sc">+</span> N <span class="sc">+</span> K</span>
<span id="cb5-64"><a href="sec14_2.html#cb5-64" tabindex="-1"></a>BnVB <span class="ot">&lt;-</span> Bn; bnVB <span class="ot">&lt;-</span> bn</span>
<span id="cb5-65"><a href="sec14_2.html#cb5-65" tabindex="-1"></a>sig2VB <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">rinvgamma</span>(S,anVB<span class="sc">/</span><span class="dv">2</span>,dnVB<span class="sc">/</span><span class="dv">2</span>) </span>
<span id="cb5-66"><a href="sec14_2.html#cb5-66" tabindex="-1"></a>BetasVB <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(S, <span class="at">mu =</span> bnVB, <span class="at">Sigma =</span> (dn<span class="sc">/</span>an)<span class="sc">*</span>BnVB)</span>
<span id="cb5-67"><a href="sec14_2.html#cb5-67" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(sig2VB)); <span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(BetasVB))</span>
<span id="cb5-68"><a href="sec14_2.html#cb5-68" tabindex="-1"></a>ELBO <span class="ot">&lt;-</span> <span class="sc">-</span>N<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">+</span> a0<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(d0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> anVB<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(dnVB<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(B0)) <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(BnVB)) <span class="sc">-</span> <span class="fu">lgamma</span>(a0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">lgamma</span>(anVB<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> K<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(anVB<span class="sc">/</span>dnVB) <span class="sc">+</span> K<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb5-69"><a href="sec14_2.html#cb5-69" tabindex="-1"></a>LogMarLik <span class="ot">&lt;-</span> <span class="sc">-</span>N<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">+</span> a0<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(d0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> an<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(dn<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(B0)) <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(Bn)) <span class="sc">-</span> <span class="fu">lgamma</span>(a0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">lgamma</span>(an<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb5-70"><a href="sec14_2.html#cb5-70" tabindex="-1"></a>ELBO; LogMarLik; ELBO <span class="sc">&lt;</span> LogMarLik </span>
<span id="cb5-71"><a href="sec14_2.html#cb5-71" tabindex="-1"></a><span class="co"># CAVI</span></span>
<span id="cb5-72"><a href="sec14_2.html#cb5-72" tabindex="-1"></a>ELBOfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(d,B){</span>
<span id="cb5-73"><a href="sec14_2.html#cb5-73" tabindex="-1"></a>    ELBOi <span class="ot">&lt;-</span> <span class="sc">-</span>N<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">+</span> a0<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(d0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> anVB<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(d<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(B0)) <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(<span class="fu">det</span>(B)) <span class="sc">-</span> <span class="fu">lgamma</span>(a0<span class="sc">/</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">lgamma</span>(anVB<span class="sc">/</span><span class="dv">2</span>) <span class="sc">-</span> K<span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>(anVB<span class="sc">/</span>d) <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>(anVB<span class="sc">/</span>d)<span class="sc">*</span><span class="fu">sum</span>(<span class="fu">diag</span>(B<span class="sc">%*%</span><span class="fu">solve</span>(Bn)))</span>
<span id="cb5-74"><a href="sec14_2.html#cb5-74" tabindex="-1"></a>    <span class="fu">return</span>(ELBOi)</span>
<span id="cb5-75"><a href="sec14_2.html#cb5-75" tabindex="-1"></a>}</span>
<span id="cb5-76"><a href="sec14_2.html#cb5-76" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="dv">100</span>; B <span class="ot">&lt;-</span> <span class="fu">diag</span>(K) </span>
<span id="cb5-77"><a href="sec14_2.html#cb5-77" tabindex="-1"></a>Esig2inv <span class="ot">&lt;-</span> anVB<span class="sc">/</span>d; ELBOs <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="sc">-</span><span class="cn">Inf</span>, S); epsilon <span class="ot">&lt;-</span> <span class="fl">1e-5</span></span>
<span id="cb5-78"><a href="sec14_2.html#cb5-78" tabindex="-1"></a><span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>S){</span>
<span id="cb5-79"><a href="sec14_2.html#cb5-79" tabindex="-1"></a>    B <span class="ot">&lt;-</span> Esig2inv<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>Bn</span>
<span id="cb5-80"><a href="sec14_2.html#cb5-80" tabindex="-1"></a>    EbQb <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(B<span class="sc">%*%</span><span class="fu">solve</span>(Bn))) <span class="sc">-</span> <span class="fu">t</span>(bn)<span class="sc">%*%</span><span class="fu">solve</span>(Bn)<span class="sc">%*%</span>bn <span class="sc">+</span> <span class="fu">t</span>(y)<span class="sc">%*%</span>y <span class="sc">+</span> <span class="fu">t</span>(b0)<span class="sc">%*%</span><span class="fu">solve</span>(B0)<span class="sc">%*%</span>b0</span>
<span id="cb5-81"><a href="sec14_2.html#cb5-81" tabindex="-1"></a>    d <span class="ot">&lt;-</span> EbQb <span class="sc">+</span> d0</span>
<span id="cb5-82"><a href="sec14_2.html#cb5-82" tabindex="-1"></a>    Esig2inv <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(anVB<span class="sc">/</span>d)</span>
<span id="cb5-83"><a href="sec14_2.html#cb5-83" tabindex="-1"></a>    ELBOs[s] <span class="ot">&lt;-</span> <span class="fu">ELBOfunc</span>(<span class="at">d =</span> d, <span class="at">B =</span> B)</span>
<span id="cb5-84"><a href="sec14_2.html#cb5-84" tabindex="-1"></a>    <span class="cf">if</span> (ELBOs[s] <span class="sc">&lt;</span> ELBOs[s <span class="sc">-</span> <span class="dv">1</span>]) { <span class="fu">message</span>(<span class="st">&quot;Lower bound decreases!</span><span class="sc">\n</span><span class="st">&quot;</span>)}</span>
<span id="cb5-85"><a href="sec14_2.html#cb5-85" tabindex="-1"></a>    <span class="co"># Check for convergence</span></span>
<span id="cb5-86"><a href="sec14_2.html#cb5-86" tabindex="-1"></a>    <span class="cf">if</span> (ELBOs[s] <span class="sc">-</span> ELBOs[s <span class="sc">-</span> <span class="dv">1</span>] <span class="sc">&lt;</span> epsilon) { <span class="cf">break</span> }</span>
<span id="cb5-87"><a href="sec14_2.html#cb5-87" tabindex="-1"></a>    <span class="co"># Check if VB converged in the given maximum iterations</span></span>
<span id="cb5-88"><a href="sec14_2.html#cb5-88" tabindex="-1"></a>    <span class="cf">if</span> (s <span class="sc">==</span> S) {<span class="fu">warning</span>(<span class="st">&quot;VB did not converge!</span><span class="sc">\n</span><span class="st">&quot;</span>)}</span>
<span id="cb5-89"><a href="sec14_2.html#cb5-89" tabindex="-1"></a>}</span>
<span id="cb5-90"><a href="sec14_2.html#cb5-90" tabindex="-1"></a>sig2VBscratch <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">rinvgamma</span>(S,anVB<span class="sc">/</span><span class="dv">2</span>,d<span class="sc">/</span><span class="dv">2</span>) </span>
<span id="cb5-91"><a href="sec14_2.html#cb5-91" tabindex="-1"></a>BetasVBscratch <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(S, <span class="at">mu =</span> bnVB, <span class="at">Sigma =</span> B)</span>
<span id="cb5-92"><a href="sec14_2.html#cb5-92" tabindex="-1"></a><span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(sig2VBscratch)); <span class="fu">summary</span>(coda<span class="sc">::</span><span class="fu">mcmc</span>(BetasVBscratch))</span></code></pre></div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-anselin82" class="csl-entry">
Anselin, Luc. 1982. <span>“A Note of Small Sample Properties of Estimators in a First Order Spatial Autoregressive Model.”</span> <em>Environment and Planning</em> 14: 1023–30.
</div>
<div id="ref-bishop2006pattern" class="csl-entry">
Bishop, Christopher M, and Nasser M Nasrabadi. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
</div>
<div id="ref-bivand2015spatial" class="csl-entry">
Bivand, Roger, Virgilio Gómez-Rubio, and Håvard Rue. 2015. <span>“Spatial Data Analysis with r-INLA with Some Extensions.”</span> <em>Journal of Statistical Software</em> 63: 1–31.
</div>
<div id="ref-blei2006variational" class="csl-entry">
Blei, David M, and Michael I Jordan. 2006. <span>“Variational Inference for Dirichlet Process Mixtures.”</span> <em>Bayesian Analysis</em> 1 (1): 121–43.
</div>
<div id="ref-blei2017variational" class="csl-entry">
Blei, David M, Alp Kucukelbir, and Jon D McAuliffe. 2017. <span>“Variational Inference: A Review for Statisticians.”</span> <em>Journal of the American Statistical Association</em> 112 (518): 859–77.
</div>
<div id="ref-elhorst2014spatial" class="csl-entry">
Elhorst, J Paul et al. 2014. <em>Spatial Econometrics: From Cross-Sectional Data to Spatial Panels</em>. Vol. 479. Springer.
</div>
<div id="ref-gelman2021bayesian" class="csl-entry">
Gelman, Andrew, John B Carlin, Hal S Stern, David Dunson, Aki Vehtari, and Donald B Rubin. 2021. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.
</div>
<div id="ref-haining90" class="csl-entry">
Haining, Robert. 1990. <em>Spatial Data Analysis in the Social and Environmental Sciences</em>. First. United of Kingdom: Cambridge University Press.
</div>
<div id="ref-hoffman2013stochastic" class="csl-entry">
Hoffman, Matthew D, David M Blei, Chong Wang, and John Paisley. 2013. <span>“Stochastic Variational Inference.”</span> <em>The Journal of Machine Learning Research</em> 14 (1): 1303–47.
</div>
<div id="ref-jordan1999introduction" class="csl-entry">
Jordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. 1999. <span>“An Introduction to Variational Methods for Graphical Models.”</span> <em>Machine Learning</em> 37: 183–233.
</div>
<div id="ref-lesage2009introduction" class="csl-entry">
LeSage, James, and Robert Kelley Pace. 2009. <em>Introduction to Spatial Econometrics</em>. Chapman; Hall/CRC.
</div>
<div id="ref-martino2019integrated" class="csl-entry">
Martino, Sara, and Andrea Riebler. 2019. <span>“Integrated Nested Laplace Approximations (INLA).”</span> <em>arXiv Preprint arXiv:1907.01248</em>.
</div>
<div id="ref-nguyen2023depth" class="csl-entry">
Nguyen, Duy. 2023. <span>“An in Depth Introduction to Variational Bayes Note.”</span> <em>Available at SSRN 4541076</em>.
</div>
<div id="ref-ord75" class="csl-entry">
Ord, K. 1975. <span>“Estimation Methods for Models of Spatial Interaction.”</span> <em>Journal of the American Statistical Association</em> 70 (349): 120–26.
</div>
<div id="ref-Ramirez2017" class="csl-entry">
Ramírez Hassan, A. 2017. <span>“The Interplay Between the <span>B</span>ayesian and Frequentist Approaches: A General Nesting Spatial Panel Data Model.”</span> <em>Spatial Economic Analysis</em> 12 (1): 92–112.
</div>
<div id="ref-ramirez2019welfare" class="csl-entry">
Ramı́rez Hassan, Andrés, and Santiago Montoya Blandón. 2019. <span>“Welfare Gains of the Poor: An Endogenous Bayesian Approach with Spatial Random Effects.”</span> <em>Econometric Reviews</em> 38 (3): 301–18.
</div>
<div id="ref-robbins1951stochastic" class="csl-entry">
Robbins, Herbert, and Sutton Monro. 1951. <span>“A Stochastic Approximation Method.”</span> <em>The Annals of Mathematical Statistics</em>, 400–407.
</div>
<div id="ref-rue2009approximate" class="csl-entry">
Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. <span>“Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 71 (2): 319–92.
</div>
<div id="ref-rue2017bayesian" class="csl-entry">
Rue, Håvard, Andrea Riebler, Sigrunn H Sørbye, Janine B Illian, Daniel P Simpson, and Finn K Lindgren. 2017. <span>“Bayesian Computing with INLA: A Review.”</span> <em>Annual Review of Statistics and Its Application</em> 4 (1): 395–421.
</div>
<div id="ref-spiegelhalter2002bayesian" class="csl-entry">
Spiegelhalter, David J, Nicola G Best, Bradley P Carlin, and Angelika Van Der Linde. 2002. <span>“Bayesian Measures of Model Complexity and Fit.”</span> <em>Journal of the Royal Statistical Society: Series b (Statistical Methodology)</em> 64 (4): 583–639.
</div>
<div id="ref-Tanner1987" class="csl-entry">
Tanner, M. A., and W. H. Wong. 1987. <span>“The Calculation of Posterior Distributions by Data Augmentation.”</span> <em>Journal of the American Statistical Association</em> 82 (398): 528–40.
</div>
<div id="ref-Tierney1986" class="csl-entry">
Tierney, Luke, and Joseph B Kadane. 1986. <span>“Accurate Approximations for Posterior Moments and Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 81 (393): 82–86.
</div>
<div id="ref-wainwright2008graphical" class="csl-entry">
Wainwright, Martin J, Michael I Jordan, et al. 2008. <span>“Graphical Models, Exponential Families, and Variational Inference.”</span> <em>Foundations and Trends<span></span> in Machine Learning</em> 1 (1–2): 1–305.
</div>
<div id="ref-zhang2020convergence" class="csl-entry">
Zhang, Fengshuo, and Chao Gao. 2020. <span>“Convergence Rates of Variational Posterior Distributions.”</span> <em>The Annals of Statistics</em> 48 (4): 2180–2207.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Visit <a href="https://www.r-inla.org/" class="uri">https://www.r-inla.org/</a> for documentation.<a href="sec14_2.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This is a necessary condition to ensure weak stationarity but is not sufficient due to edge and corner effects <span class="citation">Haining (<a href="#ref-haining90">1990</a>)</span>.<a href="sec14_2.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec14_1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/14-ApproximationMethods.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
