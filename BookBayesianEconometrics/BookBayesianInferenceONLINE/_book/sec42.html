<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.2 Conjugate prior to exponential family | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="3.2 Conjugate prior to exponential family | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.2 Conjugate prior to exponential family | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec41.html"/>
<link rel="next" href="sec43.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Dirichlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Non-parametric generalized additive models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximation methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Bayesian synthetic likelihood</a></li>
<li class="chapter" data-level="14.3" data-path="sec14_3.html"><a href="sec14_3.html"><i class="fa fa-check"></i><b>14.3</b> Expectation propagation</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.5" data-path="sec14_5.html"><a href="sec14_5.html"><i class="fa fa-check"></i><b>14.5</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec42" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Conjugate prior to exponential family<a href="sec42.html#sec42" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Theorem 4.2.1</strong></p>
<p>The prior distribution <span class="math inline">\(\pi(\boldsymbol{\theta})\propto C(\boldsymbol{\theta})^{b_0}\exp\left\{\eta(\boldsymbol{\theta})^{\top}\boldsymbol{a}_0\right\}\)</span> is conjugate to the exponential family (equation <a href="sec41.html#eq:414">(3.4)</a>).</p>
<p><strong>Proof</strong></p>
<p><span class="math display">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \boldsymbol{y})&amp; \propto C(\boldsymbol{\theta})^{b_0}\exp\left\{\eta(\boldsymbol{\theta})^{\top}\boldsymbol{a}_0\right\} \times h(\boldsymbol{y}) C(\boldsymbol{\theta})^N\exp\left\{\eta(\boldsymbol{\theta})^{\top}\boldsymbol{T}(\boldsymbol{y})\right\}\nonumber\\
    &amp; \propto C(\boldsymbol{\theta})^{N+b_0} \exp\left\{\eta(\boldsymbol{\theta})^{\top}(\boldsymbol{T}(\boldsymbol{y})+\boldsymbol{a}_0)\right\}.\nonumber
\end{align}\]</span></p>
<p>Observe that the posterior is in the exponential family, <span class="math inline">\(\pi(\boldsymbol{\theta}\mid \boldsymbol{y})\propto C(\boldsymbol{\theta})^{\beta_n} \exp\left\{\eta(\boldsymbol{\theta})^{\top}\boldsymbol{\alpha}_n\right\}\)</span>, <span class="math inline">\(\beta_n=N+b_0\)</span> and <span class="math inline">\(\boldsymbol{\alpha}_n=\boldsymbol{T}(\boldsymbol{y})+\boldsymbol{a}_0\)</span>.</p>
<p><strong>Remarks</strong></p>
<p>We observe, by comparing the prior and the likelihood, that <span class="math inline">\(b_0\)</span> plays the role of a hypothetical sample size, and <span class="math inline">\(\boldsymbol{a}_0\)</span> plays the role of hypothetical sufficient statistics. This perspective aids the elicitation process, that is, integrating non-sample information into the prior distribution.</p>
<p>We established this result in the <em>standard form</em> of the exponential family. We can also establish it in the <em>canonical form</em> of the exponential family. Observe that, given <span class="math inline">\(\boldsymbol{\eta} = \boldsymbol{\eta}(\boldsymbol{\theta})\)</span>, another way to derive a prior for <span class="math inline">\(\boldsymbol{\eta}\)</span> is to use the change of variable theorem, given a bijective function.</p>
<p>In the case where there is a regular conjugate prior, <span class="citation">Diaconis, Ylvisaker, et al. (<a href="#ref-diaconis1979conjugate">1979</a>)</span> show that the posterior expectation of the sufficient statistics is a weighted average between the prior expectation and the likelihood estimate.</p>
<div id="sec421" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Examples: Theorem 4.2.1<a href="sec42.html#sec421" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Likelihood functions from discrete distributions</strong></li>
</ol>
<p><em>The Poisson-gamma model</em></p>
<p>Given a random sample <span class="math inline">\(\boldsymbol{Y}=[Y_1 \ Y_2 \ \dots \ Y_N]^{\top}\)</span> from a Poisson distribution then a conjugate prior density for <span class="math inline">\(\lambda\)</span> has the form
<span class="math display">\[\begin{align}
    \pi(\lambda)&amp;\propto \left(\exp(-\lambda)\right)^{b_0} \exp\left\{a_0\log(\lambda)\right\}\nonumber\\
    &amp; = \exp(-\lambda b_0) \lambda^{a_0}\nonumber\\
    &amp; = \exp(-\lambda \beta_0) \lambda^{\alpha_0-1}.\nonumber
\end{align}\]</span>
This is the kernel of a gamma density in the <em>rate parametrization</em>, <span class="math inline">\(G(\alpha_0, \beta_0)\)</span>, where <span class="math inline">\(\alpha_0 = a_0 + 1\)</span> and <span class="math inline">\(\beta_0 = b_0\)</span>.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> Thus, a prior conjugate distribution for the Poisson likelihood is a gamma distribution.</p>
<p>Since <span class="math inline">\(\sum_{i=1}^N Y_i\)</span> is a sufficient statistic for the Poisson distribution, we can interpret <span class="math inline">\(a_0\)</span> as the number of occurrences in <span class="math inline">\(b_0\)</span> experiments.</p>
<p>Observe that
<span class="math display">\[\begin{align}
    \pi(\lambda\mid \boldsymbol{y})&amp;\propto \exp(-\lambda \beta_0) \lambda^{\alpha_0-1} \times \exp(-N\lambda)\lambda^{\sum_{i=1}^Ny_i}\nonumber\\
    &amp;= \exp(-\lambda(N+\beta_0)) \lambda^{\sum_{i=1}^Ny_i+\alpha_0-1}.\nonumber
\end{align}\]</span>
As expected, this is the kernel of a gamma distribution, which means <span class="math inline">\(\lambda\mid \boldsymbol{y}\sim G(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\sum_{i=1}^Ny_i+\alpha_0\)</span> and <span class="math inline">\(\beta_n=N+\beta_0\)</span>.</p>
<p>Observe that <span class="math inline">\(\alpha_0/\beta_0\)</span> is the prior mean, and <span class="math inline">\(\alpha_0/\beta_0^2\)</span> is the prior variance. Then, <span class="math inline">\(\alpha_0\rightarrow 0\)</span> and <span class="math inline">\(\beta_0\rightarrow 0\)</span> imply a non-informative prior such that the posterior mean converges to the maximum likelihood estimate <span class="math inline">\(\bar{y}=\frac{\sum_{i=1}^N y_i}{N}\)</span>,
<span class="math display">\[\begin{align}
    \mathbb{E}\left[\lambda\mid \boldsymbol{y}\right]&amp;=\frac{\alpha_n}{\beta_n}\nonumber\\
    &amp;=\frac{\sum_{i=1}^Ny_i+\alpha_0}{N+\beta_0}\nonumber\\
    &amp;=\frac{N\bar{y}}{N+\beta_0}+\frac{\alpha_0}{N+\beta_0}.\nonumber
\end{align}\]</span>
The posterior mean is a weighted average of the sample and prior information. This is a general result for regular conjugate priors <span class="citation">(<a href="#ref-diaconis1979conjugate">Diaconis, Ylvisaker, et al. 1979</a>)</span>. Note that <span class="math inline">\(\mathbb{E}[\lambda \mid \boldsymbol{y}] = \bar{y}, \quad \lim_{N \to \infty}\)</span>.</p>
<p>Additionally, <span class="math inline">\(\alpha_0 \to 0\)</span> and <span class="math inline">\(\beta_0 \to 0\)</span> corresponds to <span class="math inline">\(\pi(\lambda) \propto \frac{1}{\lambda}\)</span>, which is an improper prior. Improper priors may have undesirable consequences for Bayes factors (hypothesis testing); see below for a discussion of this in the linear regression framework. In this example, we can obtain analytical solutions for the marginal likelihood and the predictive distribution (see the health insurance example and Exercise 3 in Chapter <a href="Chap1.html#Chap1">1</a>).</p>
<p><em>The Bernoulli-beta model</em></p>
<p>Given a random sample <span class="math inline">\(\boldsymbol{Y}=[Y_1 \ Y_2 \ \dots \ Y_N]^{\top}\)</span> from a Bernoulli distribution then a conjugate prior density for <span class="math inline">\(\theta\)</span> has the form
<span class="math display">\[\begin{align}
    \pi(\theta)&amp;\propto (1-\theta)^{b_0} \exp\left\{a_0\log\left(\frac{\theta}{1-\theta}\right)\right\}\nonumber\\
    &amp; = (1-\theta)^{b_0-a_0}\theta^{a_0}\nonumber\\
    &amp; = \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}.\nonumber
\end{align}\]</span>
This is the kernel of a beta density, <span class="math inline">\(B(\alpha_0, \beta_0)\)</span>, where <span class="math inline">\(\alpha_0 = a_0 + 1\)</span> and <span class="math inline">\(\beta_0 = b_0 - a_0 + 1\)</span>. A prior conjugate distribution for the Bernoulli likelihood is a beta distribution. Given that <span class="math inline">\(b_0\)</span> is the hypothetical sample size and <span class="math inline">\(a_0\)</span> is the hypothetical sufficient statistic (the number of successes), <span class="math inline">\(b_0 - a_0\)</span> represents the number of failures. This implies that <span class="math inline">\(\alpha_0\)</span> is the number of prior successes plus one, and <span class="math inline">\(\beta_0\)</span> is the number of prior failures plus one.</p>
<p>Since the mode of a beta-distributed random variable is given by <span class="math inline">\(\frac{\alpha_0 - 1}{\alpha_0 + \beta_0 - 2} = \frac{a_0}{b_0}\)</span>, we can interpret this as the prior probability of success. Setting <span class="math inline">\(\alpha_0 = 1\)</span> and <span class="math inline">\(\beta_0 = 1\)</span>, which corresponds to a uniform distribution on the interval [0, 1], represents a setting with 0 successes (and 0 failures) in 0 experiments.</p>
<p>Observe that
<span class="math display">\[\begin{align}
    \pi(\theta\mid \boldsymbol{y})&amp;\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^Ny_i}\nonumber\\
    &amp;= \theta^{\alpha_0+\sum_{i=1}^N y_i-1}(1-\theta)^{\beta_0+N-\sum_{i=1}^Ny_i-1}.\nonumber
\end{align}\]</span>
The posterior distribution is beta, <span class="math inline">\(\theta\mid \boldsymbol{y}\sim B(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\alpha_0+\sum_{i=1}^N y_i\)</span> and <span class="math inline">\(\beta_n=\beta_0+N-\sum_{i=1}^Ny_i\)</span>, where the posterior mean <span class="math inline">\(\mathbb{E}[\theta\mid \boldsymbol{y}]=\frac{\alpha_n}{\alpha_n+\beta_n}=\frac{\alpha_0+N\bar{y}}{\alpha_0+\beta_0+N}=\frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+N}\frac{\alpha_0}{\alpha_0+\beta_0}+\frac{N}{\alpha_0+\beta_0+N}\bar{y}\)</span>. The posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>El marginal likelihood in this setting is
<span class="math display">\[\begin{align}
    p(\boldsymbol{y})=&amp;\int_{0}^1 \frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0,\beta_0)}\times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}d\theta\nonumber\\
    =&amp; \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)},\nonumber
\end{align}\]</span>
where <span class="math inline">\(B(\cdot ,\cdot)\)</span> is the beta function.</p>
<p>In addition, the predictive density is
<span class="math display">\[\begin{align}
    p(y_0\mid \boldsymbol{y})&amp;=\int_0^1 \theta^{y_0}(1-\theta)^{1-y_0}\times \frac{\theta^{\alpha_n-1}(1-\theta)^{\beta_n-1}}{B(\alpha_n,\beta_n)}d\theta\nonumber\\
    &amp;=\frac{B(\alpha_n+y_0,\beta_n+1-y_0)}{B(\alpha_n,\beta_n)}\nonumber\\
    &amp;=\frac{\Gamma(\alpha_n+\beta_n)\Gamma(\alpha_n+y_0)\Gamma(\beta_n+1-y_0)}{\Gamma(\alpha_n+\beta_n+1)\Gamma(\alpha_n)\Gamma(\beta_n)}\nonumber\\
    &amp;=\begin{Bmatrix}
        \frac{\alpha_n}{\alpha_n+\beta_n}, &amp; y_0=1\\
        \frac{\beta_n}{\alpha_n+\beta_n}, &amp; y_0=0\\
    \end{Bmatrix}.\nonumber
\end{align}\]</span></p>
<p>This is a Bernoulli distribution with probability of success equal to <span class="math inline">\(\frac{\alpha_n}{\alpha_n+\beta_n}\)</span>.</p>
<p><em>The multinomial-Dirichlet model</em></p>
<p>Given a random sample <span class="math inline">\(\boldsymbol{Y}=[Y_1 \ Y_2 \ \dots \ Y_N]^{\top}\)</span> from a multinomial distribution then a conjugate prior density for <span class="math inline">\(\boldsymbol{\theta}=\left[\theta_1 \ \theta_2 \ \dots \ \theta_m\right]\)</span> has the form
<span class="math display">\[\begin{align}
    \pi(\boldsymbol{\theta})&amp;\propto \theta_m^{b_0} \exp\left\{\boldsymbol{\eta}(\boldsymbol{\theta})^{\top}\boldsymbol{a}_0\right\}\nonumber\\
    &amp; = \prod_{l=1}^{m-1}\theta_l^{a_{0l}}\theta_m^{b_0-\sum_{l=1}^{m-1}a_{0l}}\nonumber\\
    &amp; = \prod_{l=1}^{m}\theta_l^{\alpha_{0l}-1},\nonumber
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\eta}(\boldsymbol{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right) \ \dots \ \log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]\)</span>, <span class="math inline">\(\boldsymbol{a}_0=\left[a_{01} \ \dots \ a_{0m-1}\right]^{\top}\)</span>, <span class="math inline">\(\boldsymbol{\alpha}_0=\left[\alpha_{01} \ \alpha_{02} \ \dots \ \alpha_{0m}\right]\)</span>, <span class="math inline">\(\alpha_{0l}=a_{0l}+1\)</span>, <span class="math inline">\(l=1,2,\dots,m-1\)</span> and <span class="math inline">\(\alpha_{0m}=b_0-\sum_{l=1}^{m-1} a_{0l}+1\)</span>.</p>
<p>This is the kernel of a Dirichlet distribution, that is, the prior distribution is <span class="math inline">\(D(\boldsymbol{\alpha}_0)\)</span>.</p>
<p>Observe that <span class="math inline">\(a_{0l}\)</span> is the hypothetical number of times outcome <span class="math inline">\(l\)</span> is observed over the hypothetical <span class="math inline">\(b_0\)</span> trials. Setting <span class="math inline">\(\alpha_{0l} = 1\)</span>, which corresponds to a uniform distribution over the open standard simplex, implicitly sets <span class="math inline">\(a_{0l} = 0\)</span>, meaning that there are 0 occurrences of category <span class="math inline">\(l\)</span> in <span class="math inline">\(b_0 = 0\)</span> experiments.</p>
<p>The posterior distribution of the multinomial-Dirichlet model is given by
<span class="math display">\[\begin{align}
    \pi(\boldsymbol{\theta}\mid \boldsymbol{y})&amp;\propto \prod_{l=1}^m \theta_l^{\alpha_{0l}-1}\times\prod_{l=1}^m \theta_l^{\sum_{i=1}^{N} y_{il}}\nonumber\\
    &amp;=\prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^{N} y_{il}-1}\nonumber.
\end{align}\]</span>
This is the kernel of a Dirichlet distribution <span class="math inline">\(D(\boldsymbol{\alpha}_n)\)</span>, <span class="math inline">\(\boldsymbol{\alpha}_n=\left[\alpha_{n1} \ \alpha_{n2} \ \dots \ \alpha_{nm}\right]\)</span>, <span class="math inline">\(\alpha_{nl}=\alpha_{0l}+\sum_{i=1}^{N}y_{il}\)</span>, <span class="math inline">\(l=1,2,\dots,m\)</span>. Observe that
<span class="math display">\[\begin{align}
    \mathbb{E}[\theta_{j}\mid \boldsymbol{y}]&amp;=\frac{\alpha_{nj}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\nonumber\\
    &amp;=\frac{\sum_{l=1}^m \alpha_{0l}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\alpha_{0j}}{\sum_{l=1}^m \alpha_{0l}}\nonumber\\
    &amp;+\frac{\sum_{l=1}^m\sum_{i=1}^N y_{il}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\sum_{i=1}^N y_{ij}}{\sum_{l=1}^m\sum_{i=1}^N y_{il}}.\nonumber
\end{align}\]</span>
We have again that the posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>The marginal likelihood is
<span class="math display">\[\begin{align}
    p(\boldsymbol{y})&amp;=\int_{\boldsymbol{\Theta}}\frac{\prod_{l=1}^m \theta_l^{\alpha_{0l}-1}}{B(\boldsymbol{\alpha}_0)}\times \prod_{i=1}^N\frac{n!}{\prod_{l=1}^m y_{il}}\prod_{l=1}^m \theta_{l}^{y_{il}}d\boldsymbol{\theta}\nonumber\\
    &amp;=\frac{N\times n!}{B(\boldsymbol{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\int_{\boldsymbol{\Theta}} \prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^N y_{il}-1} d\boldsymbol{\theta}\nonumber\\
    &amp;=\frac{N\times n!}{B(\boldsymbol{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}B(\boldsymbol{\alpha}_n)\nonumber\\
    &amp;=\frac{N\times n! \Gamma\left(\sum_{l=1}^m\nonumber \alpha_{0l}\right)}{\Gamma\left(\sum_{l=1}^m \alpha_{0l}+N\times n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}\right)}{\Gamma\left(\alpha_{0l}\right)\prod_{i=1}^N y_{il}!},\nonumber
\end{align}\]</span>
where <span class="math inline">\(B(\boldsymbol{\alpha})=\frac{\prod_{l=1}^m\Gamma(\alpha_l)}{\Gamma\left(\sum_{l=1}^m \alpha_l\right)}\)</span>.</p>
<p>Following similar steps we get the predictive density
<span class="math display">\[\begin{align}
    p(y_0\mid \boldsymbol{y})&amp;=\frac{ n! \Gamma\left(\sum_{l=1}^m \alpha_{nl}\right)}{\Gamma\left(\sum_{l=1}^m \alpha_{nl}+ n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}+y_{0l}\right)}{\Gamma\left(\alpha_{nl}\right) y_{0l}!}.\nonumber
\end{align}\]</span>
This is a Dirichlet-multinomial distribution with parameters <span class="math inline">\(\boldsymbol{\alpha}_n\)</span>.</p>
<p><strong>Example: English premier league, Liverpool vs Manchester city</strong></p>
<p>Let’s consider an example using data from the English Premier League. In particular, we want to calculate the probability that, in the next five matches between Liverpool and Manchester City, Liverpool wins two games and Manchester City wins three. This calculation is based on historical data from the last five matches where Liverpool played at home between January 14th, 2018, and April 10th, 2022. In those matches, Liverpool secured two wins, there were two draws, and Manchester City won one match.
<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<p>We use two strategies to estimate the hyperparameters. First, we estimate the hyperparameters of the Dirichlet distribution using betting odds from bookmakers at 19:05 on October 6th, 2022 (Colombia time). We obtained data from 24 bookmakers (see file <em>DataOddsLIVvsMAN.csv</em>)<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>, and we transform these odds into probabilities using a simple standardization approach. Then, we apply maximum likelihood estimation to estimate the hyperparameters.</p>
<p>Second, we use empirical Bayes, where we estimate the hyperparameters by optimizing the marginal likelihood.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="sec42.html#cb69-1" tabindex="-1"></a><span class="co"># Multinomial-Dirichlet example: Liverpool vs Manchester city</span></span>
<span id="cb69-2"><a href="sec42.html#cb69-2" tabindex="-1"></a>Data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/DataOddsLIVvsMAN.csv&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb69-3"><a href="sec42.html#cb69-3" tabindex="-1"></a><span class="fu">attach</span>(Data)</span>
<span id="cb69-4"><a href="sec42.html#cb69-4" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="sec42.html#cb73-1" tabindex="-1"></a>Probs <span class="ot">&lt;-</span> Data <span class="sc">%&gt;%</span></span>
<span id="cb73-2"><a href="sec42.html#cb73-2" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">pns1 =</span> <span class="dv">1</span><span class="sc">/</span>home, <span class="at">pns2 =</span> <span class="dv">1</span><span class="sc">/</span>draw, <span class="at">pns3 =</span> <span class="dv">1</span><span class="sc">/</span>away)<span class="sc">%&gt;%</span> </span>
<span id="cb73-3"><a href="sec42.html#cb73-3" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">SumInvOdds =</span> pns1 <span class="sc">+</span> pns2 <span class="sc">+</span> pns3) <span class="sc">%&gt;%</span> </span>
<span id="cb73-4"><a href="sec42.html#cb73-4" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">p1 =</span> pns1<span class="sc">/</span>SumInvOdds, <span class="at">p2 =</span> pns2<span class="sc">/</span>SumInvOdds, <span class="at">p3 =</span> pns3<span class="sc">/</span>SumInvOdds) <span class="sc">%&gt;%</span> </span>
<span id="cb73-5"><a href="sec42.html#cb73-5" tabindex="-1"></a>    <span class="fu">select</span>(p1, p2, p3)</span>
<span id="cb73-6"><a href="sec42.html#cb73-6" tabindex="-1"></a><span class="co"># We get probabilities using simple standardization. There are more technical approaches to do this. See for instance Shin (1993) and Strumbelj (2014). </span></span>
<span id="cb73-7"><a href="sec42.html#cb73-7" tabindex="-1"></a>DirMLE <span class="ot">&lt;-</span> sirt<span class="sc">::</span><span class="fu">dirichlet.mle</span>(Probs)</span>
<span id="cb73-8"><a href="sec42.html#cb73-8" tabindex="-1"></a><span class="co"># Use maximum likelihood to estimate parameters of the</span></span>
<span id="cb73-9"><a href="sec42.html#cb73-9" tabindex="-1"></a><span class="co"># Dirichlet distribution</span></span>
<span id="cb73-10"><a href="sec42.html#cb73-10" tabindex="-1"></a>alpha0odds <span class="ot">&lt;-</span> DirMLE<span class="sc">$</span>alpha</span>
<span id="cb73-11"><a href="sec42.html#cb73-11" tabindex="-1"></a>alpha0odds</span></code></pre></div>
<pre><code>##       p1       p2       p3 
## 1599.122 1342.703 2483.129</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="sec42.html#cb75-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>) </span>
<span id="cb75-2"><a href="sec42.html#cb75-2" tabindex="-1"></a><span class="co"># Historical records last five mathces</span></span>
<span id="cb75-3"><a href="sec42.html#cb75-3" tabindex="-1"></a><span class="co"># Liverpool wins (2), draws (2) and Manchester</span></span>
<span id="cb75-4"><a href="sec42.html#cb75-4" tabindex="-1"></a><span class="co"># city wins (1)</span></span>
<span id="cb75-5"><a href="sec42.html#cb75-5" tabindex="-1"></a></span>
<span id="cb75-6"><a href="sec42.html#cb75-6" tabindex="-1"></a><span class="co"># Marginal likelihood</span></span>
<span id="cb75-7"><a href="sec42.html#cb75-7" tabindex="-1"></a>MarLik <span class="ot">&lt;-</span> <span class="cf">function</span>(a0){</span>
<span id="cb75-8"><a href="sec42.html#cb75-8" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb75-9"><a href="sec42.html#cb75-9" tabindex="-1"></a>    Res1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y), </span>
<span id="cb75-10"><a href="sec42.html#cb75-10" tabindex="-1"></a>    <span class="cf">function</span>(l){<span class="fu">lgamma</span>(a0[l]<span class="sc">+</span>y[l])<span class="sc">-</span><span class="fu">lgamma</span>(a0[l])}))</span>
<span id="cb75-11"><a href="sec42.html#cb75-11" tabindex="-1"></a>    Res <span class="ot">&lt;-</span> <span class="fu">lgamma</span>(<span class="fu">sum</span>(a0))<span class="sc">-</span><span class="fu">lgamma</span>(<span class="fu">sum</span>(a0)<span class="sc">+</span>n)<span class="sc">+</span>Res1</span>
<span id="cb75-12"><a href="sec42.html#cb75-12" tabindex="-1"></a>    <span class="fu">return</span>(<span class="sc">-</span>Res)</span>
<span id="cb75-13"><a href="sec42.html#cb75-13" tabindex="-1"></a>}</span>
<span id="cb75-14"><a href="sec42.html#cb75-14" tabindex="-1"></a>EmpBay <span class="ot">&lt;-</span> <span class="fu">optim</span>(alpha0odds, MarLik, <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>)</span>
<span id="cb75-15"><a href="sec42.html#cb75-15" tabindex="-1"></a>alpha0EB <span class="ot">&lt;-</span> EmpBay<span class="sc">$</span>par</span>
<span id="cb75-16"><a href="sec42.html#cb75-16" tabindex="-1"></a>alpha0EB</span></code></pre></div>
<pre><code>##       p1       p2       p3 
## 2362.622 2660.153 1279.510</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="sec42.html#cb77-1" tabindex="-1"></a><span class="co"># Bayes factor empirical Bayes vs betting odds. </span></span>
<span id="cb77-2"><a href="sec42.html#cb77-2" tabindex="-1"></a><span class="co"># This is greather than 1 by construction</span></span>
<span id="cb77-3"><a href="sec42.html#cb77-3" tabindex="-1"></a>BF <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">MarLik</span>(alpha0EB))<span class="sc">/</span><span class="fu">exp</span>(<span class="sc">-</span><span class="fu">MarLik</span>(alpha0odds))</span>
<span id="cb77-4"><a href="sec42.html#cb77-4" tabindex="-1"></a>BF</span></code></pre></div>
<pre><code>## [1] 2.085819</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="sec42.html#cb79-1" tabindex="-1"></a><span class="co"># Posterior distribution based on empirical Bayes</span></span>
<span id="cb79-2"><a href="sec42.html#cb79-2" tabindex="-1"></a>alphan <span class="ot">&lt;-</span> alpha0EB <span class="sc">+</span> y </span>
<span id="cb79-3"><a href="sec42.html#cb79-3" tabindex="-1"></a><span class="co"># Posterior parameters </span></span>
<span id="cb79-4"><a href="sec42.html#cb79-4" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb79-5"><a href="sec42.html#cb79-5" tabindex="-1"></a><span class="co"># Simulation draws from the Dirichlet distribution </span></span>
<span id="cb79-6"><a href="sec42.html#cb79-6" tabindex="-1"></a>thetas <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">rdirichlet</span>(S, alphan)</span>
<span id="cb79-7"><a href="sec42.html#cb79-7" tabindex="-1"></a><span class="fu">colnames</span>(thetas) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Liverpool&quot;</span>,<span class="st">&quot;Draw&quot;</span>,<span class="st">&quot;Manchester&quot;</span>)</span>
<span id="cb79-8"><a href="sec42.html#cb79-8" tabindex="-1"></a><span class="co"># Predictive distribution based on simulations</span></span>
<span id="cb79-9"><a href="sec42.html#cb79-9" tabindex="-1"></a>y0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>) </span>
<span id="cb79-10"><a href="sec42.html#cb79-10" tabindex="-1"></a><span class="co"># Liverpool two wins and Manchester city three wins in next five matches</span></span>
<span id="cb79-11"><a href="sec42.html#cb79-11" tabindex="-1"></a>Pred <span class="ot">&lt;-</span> <span class="fu">apply</span>(thetas, <span class="dv">1</span>, <span class="cf">function</span>(p) {<span class="fu">rmultinom</span>(<span class="dv">1</span>, <span class="at">size =</span> <span class="fu">sum</span>(y0), <span class="at">prob =</span> p)})</span>
<span id="cb79-12"><a href="sec42.html#cb79-12" tabindex="-1"></a>ProY0 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>S,<span class="cf">function</span>(s){<span class="fu">sum</span>(Pred[,s]<span class="sc">==</span>y0)<span class="sc">==</span><span class="dv">3</span>}))<span class="sc">/</span>S</span>
<span id="cb79-13"><a href="sec42.html#cb79-13" tabindex="-1"></a>ProY0 <span class="co"># Probability of y0</span></span></code></pre></div>
<pre><code>## [1] 0.01224</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="sec42.html#cb81-1" tabindex="-1"></a><span class="co"># Predictive distribution using analytical expression</span></span>
<span id="cb81-2"><a href="sec42.html#cb81-2" tabindex="-1"></a>PredY0 <span class="ot">&lt;-</span> <span class="cf">function</span>(y0){</span>
<span id="cb81-3"><a href="sec42.html#cb81-3" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">sum</span>(y0)</span>
<span id="cb81-4"><a href="sec42.html#cb81-4" tabindex="-1"></a>    Res1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y), <span class="cf">function</span>(l){<span class="fu">lgamma</span>(alphan[l]<span class="sc">+</span>y0[l]) <span class="sc">-</span> <span class="fu">lgamma</span>(alphan[l])<span class="sc">-</span><span class="fu">lfactorial</span>(y0[l])}))</span>
<span id="cb81-5"><a href="sec42.html#cb81-5" tabindex="-1"></a>    Res <span class="ot">&lt;-</span> <span class="fu">lfactorial</span>(n) <span class="sc">+</span> <span class="fu">lgamma</span>(<span class="fu">sum</span>(alphan)) <span class="sc">-</span> <span class="fu">lgamma</span>(<span class="fu">sum</span>(alphan)<span class="sc">+</span>n) <span class="sc">+</span> Res1</span>
<span id="cb81-6"><a href="sec42.html#cb81-6" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">exp</span>(Res))</span>
<span id="cb81-7"><a href="sec42.html#cb81-7" tabindex="-1"></a>}</span>
<span id="cb81-8"><a href="sec42.html#cb81-8" tabindex="-1"></a><span class="fu">PredY0</span>(y0)         </span></code></pre></div>
<pre><code>## [1] 0.01177531</code></pre>
<p>We observe that the Bayes factor provides evidence in favor of the hyperparameters estimated via empirical Bayes, as these hyperparameters are specifically chosen to maximize the marginal likelihood.</p>
<p>Using the hyperparameters obtained from empirical Bayes, we calculate that the probability of Liverpool winning two out of the next five games, while Manchester City wins three, is 1.2%. The result obtained from the predictive distribution via simulations is similar to the probability derived using the exact predictive distribution.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Likelihood functions from continuous distributions</strong></li>
</ol>
<p><em>The normal-normal/inverse-gamma model</em></p>
<p>Given a random sample <span class="math inline">\(\boldsymbol{Y}=[Y_1 \ Y_2 \ \dots \ Y_N]^{\top}\)</span> from a normal distribution, then the conjugate prior density has the form
<span class="math display">\[\begin{align}
    \pi(\mu,\sigma^2)&amp;\propto \exp\left\{b_0\left(-\frac{\mu^2}{2\sigma^2}-\frac{\log \sigma^2}{2}\right)\right\}\exp\left\{a_{01}\frac{\mu}{\sigma^2}-a_{02}\frac{1}{\sigma^2}\right\}\nonumber\\
    &amp;=\exp\left\{b_0\left(-\frac{\mu^2}{2\sigma^2}-\frac{\log \sigma^2}{2}\right)\right\}\exp\left\{a_{01}\frac{\mu}{\sigma^2}-a_{02}\frac{1}{\sigma^2}\right\}\nonumber\\
    &amp;\times \exp\left\{-\frac{a_{01}^2}{2\sigma^2b_0}\right\}\exp\left\{\frac{a_{01}^2}{2\sigma^2b_0}\right\}\nonumber\\
    &amp;=\exp\left\{-\frac{b_0}{2\sigma^2}\left(\mu-\frac{a_{01}}{b_0}\right)^2\right\}\left(\frac{1}{\sigma^2}\right)^{\frac{b_0+1-1}{2}}\nonumber\\
    &amp;\times \exp\left\{\frac{1}{\sigma^2}\frac{-2b_0a_{02}+a_{01}^2}{2b_0}\right\}\nonumber\\
    &amp;=\underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{1}{2}}\exp\left\{-\frac{b_0}{2\sigma^2}\left(\mu-\frac{a_{01}}{b_0}\right)^2\right\}}_{1}\nonumber\\
    &amp;\times\underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{b_0-1}{2}}\exp\left\{-\frac{1}{\sigma^2}\frac{2b_0a_{02}-a_{01}^2}{2b_0}\right\}}_{2}.\nonumber
\end{align}\]</span>
The first part is the kernel of a normal density with mean <span class="math inline">\(\mu_0 = \frac{a_{01}}{\beta_0}\)</span> and variance <span class="math inline">\(\frac{\sigma^2}{\beta_0}\)</span>, where <span class="math inline">\(\beta_0 = b_0\)</span>. That is, <span class="math inline">\(\mu \mid \sigma^2 \sim N\left(\mu_0, \frac{\sigma^2}{\beta_0}\right)\)</span>. The second part is the kernel of an inverse gamma density with shape parameter <span class="math inline">\(\frac{\alpha_0}{2} = \frac{\beta_0 - 3}{2}\)</span> and scale parameter <span class="math inline">\(\frac{\delta_0}{2} = \frac{2\beta_0 a_{02} - a_{01}^2}{2\beta_0}\)</span>, so <span class="math inline">\(\sigma^2 \sim IG\left(\frac{\alpha_0}{2}, \frac{\delta_0}{2}\right)\)</span>.</p>
<p>Observe that <span class="math inline">\(b_0 = \beta_0\)</span> represents the hypothetical sample size, and <span class="math inline">\(a_{01}\)</span> is the hypothetical sum of prior observations. Therefore, it makes sense that <span class="math inline">\(\frac{a_{01}}{\beta_0}\)</span> and <span class="math inline">\(\frac{\sigma^2}{\beta_0}\)</span> represent the prior mean and variance, respectively.</p>
<p>Therefore, the posterior distribution is also a normal-inverse gamma distribution,
<span class="math display">\[\begin{align}
    \pi(\mu,\sigma^2\mid \boldsymbol{y})&amp;\propto \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{\beta_0}{2\sigma^2}(\mu-\mu_0)^2\right\}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\nonumber\\
    &amp;\times(\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\nonumber\\
    &amp; = \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}\left(\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N (y_i-\bar{y})^2+N(\mu-\bar{y})^2+\delta_0\right)\right\}\nonumber\\
    &amp; \times\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1} + \frac{(\beta_0\mu_0+N\bar{y})^2}{\beta_0+N} - \frac{(\beta_0\mu_0+N\bar{y})^2}{\beta_0+N}\nonumber\\
    &amp; = \underbrace{\left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}\left((\beta_0+N)\left(\mu-\left(\frac{\beta_0\mu_0+N\bar{y}}{\beta_0+N}\right)\right)^2\right)\right\}}_{1}\nonumber\\
    &amp; \times \underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^N (y_i-\bar{y})^2+\delta_0+\frac{\beta_0N}{\beta_0+N}(\bar{y}-\mu_0)^2\right)\right\}}_{2}.\nonumber
\end{align}\]</span></p>
<p>The first term is the kernel of a normal density, <span class="math inline">\(\mu\mid \sigma^2,\boldsymbol{y}\sim N \left(\mu_n, \sigma_n^2\right)\)</span>, where <span class="math inline">\(\mu_n=\frac{\beta_0\mu_0+N\bar{y}}{\beta_0+N}\)</span> and <span class="math inline">\(\sigma_n^2=\frac{\sigma^2}{\beta_n}\)</span>, <span class="math inline">\(\beta_n=\beta_0+N\)</span>. The second term is the kernel of an inverse gamma density, <span class="math inline">\(\sigma^2\mid \boldsymbol{y}\sim IG(\alpha_n/2,\delta_n/2)\)</span> where <span class="math inline">\(\alpha_n=\alpha_0+N\)</span> and <span class="math inline">\(\delta_n=\sum_{i=1}^N (y_i-\bar{y})^2+\delta_0+\frac{\beta_0N}{\beta_0+N}(\bar{y}-\mu_0)^2\)</span>. Observe that the posterior mean is a weighted average between prior and sample information. The weights depends on the sample sizes (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(N\)</span>).</p>
<p>The marginal posterior for <span class="math inline">\(\sigma^2\)</span> is inverse gamma with shape and scale parameters <span class="math inline">\(\alpha_n/2\)</span> and <span class="math inline">\(\delta_n/2\)</span>, respectively. The marginal posterior of <span class="math inline">\(\mu\)</span> is
<span class="math display">\[\begin{align}
    \pi(\mu\mid \boldsymbol{y})&amp;\propto \int_{0}^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+1}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\beta_n(\mu-\mu_n)^2+\delta_n)\right\}\right\}d\sigma^2\nonumber\\
    &amp;=\frac{\Gamma\left(\frac{\alpha_n+1}{2}\right)}{\left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{\frac{\alpha_n+1}{2}}}\nonumber\\
    &amp;\propto \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}\left(\frac{\delta_n}{\delta_n}\right)^{-\frac{\alpha_n+1}{2}}\nonumber\\
    &amp;\propto \left[\frac{\alpha_n\beta_n(\mu-\mu_n)^2}{\alpha_n\delta_n}+1\right]^{-\frac{\alpha_n+1}{2}},\nonumber
\end{align}\]</span>
The second line follows from having the kernel of an inverse gamma density with parameters <span class="math inline">\(\frac{\alpha_n + 1}{2}\)</span> and <span class="math inline">\(\frac{1}{2} \left( \beta_n (\mu - \mu_n)^2 + \delta_n \right)\)</span>.</p>
<p>This corresponds to the kernel of a Student’s <span class="math inline">\(t\)</span>-distribution:
<span class="math display">\[
\mu \mid \boldsymbol{y} \sim t\left(\mu_n, \frac{\delta_n}{\beta_n \alpha_n}, \alpha_n\right),
\]</span>
where <span class="math inline">\(\mathbb{E}[\mu \mid \boldsymbol{y}] = \mu_n\)</span> and
<span class="math display">\[
\text{Var}[\mu \mid \boldsymbol{y}] = \frac{\alpha_n}{\alpha_n - 2} \left( \frac{\delta_n}{\beta_n \alpha_n} \right) = \frac{\delta_n}{(\alpha_n - 2) \beta_n}, \quad \alpha_n &gt; 2.
\]</span>
Observe that the marginal posterior distribution for <span class="math inline">\(\mu\)</span> has heavier tails than the conditional posterior distribution due to the incorporation of uncertainty regarding <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The marginal likelihood is
<span class="math display">\[\begin{align}
    p(\boldsymbol{y})&amp;=\int_{-\infty}^{\infty}\int_{0}^{\infty}\left\{ (2\pi\sigma^2/\beta_0)^{-1/2}\exp\left\{-\frac{1}{2\sigma^2/\beta_0}(\mu-\mu_0)^2\right\}\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\right.\nonumber\\
    &amp;\times\left.\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}(2\pi\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(y_i-\mu)^2\right\}\right\}d\sigma^2d\mu\nonumber\\
    &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\int_{-\infty}^{\infty}\int_{0}^{\infty}\left\{\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N+1}{2}+1}\right.\nonumber\\
    &amp;\times\left.\exp\left\{-\frac{1}{2\sigma^2}(\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N (y_i-\mu)^2+\delta_0)\right\}\right\}d\sigma^2d\mu\nonumber\\
    &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{N+1+\alpha_0}{2}\right)\nonumber\\
    &amp;\times \int_{-\infty}^{\infty} \left[\frac{\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N(y_i-\mu)^2+\delta_0}{2}\right]^{-\frac{\alpha_0+N+1}{2}}d\mu\nonumber\\
    &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{N+1+\alpha_0}{2}\right)\nonumber\\
    &amp;\times \int_{-\infty}^{\infty} \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n/2}{\delta_n/2}\right)^{-\frac{\alpha_n+1}{2}}\nonumber\\
    &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{\alpha_n+1}{2}\right)\left(\frac{\delta_n}{2}\right)^{-\frac{\alpha_n+1}{2}}\frac{\left(\frac{\delta_n\pi}{\beta_n}\right)^{1/2}\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_n+1}{2}\right)}\nonumber\\
    &amp;=\frac{\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_0}{2}\right)}\frac{(\delta_0/2)^{\alpha_0/2}}{(\delta_n/2)^{\alpha_n/2}}\left(\frac{\beta_0}{\beta_n}\right)^{1/2}(\pi)^{-N/2},\nonumber
\end{align}\]</span></p>
<p>where we take into account that <span class="math inline">\(\int_{-\infty}^{\infty} \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n/2}{\delta_n/2}\right)^{-\frac{\alpha_n+1}{2}}=\int_{-\infty}^{\infty} \left[\frac{\beta_n\alpha_n(\mu-\mu_n)^2}{\delta_n\alpha_n}+1\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n}{2}\right)^{-\frac{\alpha_n+1}{2}}\)</span>. The term in the integral is the kernel of a Student’s t density, this means that the integral is equal to <span class="math inline">\(\frac{\left(\frac{\delta_n\pi}{\beta_n}\right)^{1/2}\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_n+1}{2}\right)}\)</span>.</p>
<p>The predictive density is
<span class="math display">\[\begin{align}
    \pi(y_0\mid \boldsymbol{y})&amp;\propto\int_{-\infty}^{\infty}\int_0^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}(y_0-\mu)^2\right\}\left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{\beta_n}{2\sigma^2}(\mu-\mu_n)^2\right\}\right.\nonumber\\
    &amp;\times \left.\left(\frac{1}{\sigma^2}\right)^{\alpha_n/2+1}\exp\left\{-\frac{\delta_n}{2\sigma^2}\right\}\right\}d\sigma^2d\mu\nonumber\\
    &amp;=\int_{-\infty}^{\infty}\int_0^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+2}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}((y_0-\mu)^2+\beta_n(\mu-\mu_n)^2+\delta_n)\right\}\right\}d\sigma^2d\mu\nonumber\\
    &amp;\propto\int_{-\infty}^{\infty}\left[\beta_n(\mu-\mu_n)^2+(y_0-\mu)^2+\delta_n\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\nonumber\\
    &amp;=\int_{-\infty}^{\infty}\left[(\beta_n+1)\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2+\frac{\beta_n(y_0-\mu_n)^2}{\beta_n+1}+\delta_n\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\nonumber\\
    &amp;=\int_{-\infty}^{\infty}\left[1+\frac{(\beta_n+1)^2\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2}{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\nonumber\\
    &amp;\times\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{\beta_n+1}\right)^{-\left(\frac{\alpha_n}{2}+1\right)}\nonumber\\
    &amp;\propto\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{(\beta_n+1)^2(\alpha_n+1)}\right)^{\frac{1}{2}}\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{\beta_n+1}\right)^{-\left(\frac{\alpha_n}{2}+1\right)}\nonumber\\
    &amp;\propto (\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n)^{\left(\frac{\alpha_n+1}{2}\right)}\nonumber\\
    &amp;\propto\left[1+\frac{\beta_n\alpha_n}{(\beta_n+1)\delta_n\alpha_n}(y_0-\mu_n)^2\right]^{-\left(\frac{\alpha_n+1}{2}\right)},\nonumber
\end{align}\]</span></p>
<p>where we have that <span class="math inline">\(\left[1+\frac{(\beta_n+1)^2\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2}{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}\right]^{-\left(\frac{\alpha_n}{2}+1\right)}\)</span> is the kernel of a Student’s t density with degrees of freedom <span class="math inline">\(\alpha_n+1\)</span> and scale <span class="math inline">\(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{(\beta_n+1)^2(\alpha_n+1)}\)</span>.</p>
<p>The last expression is the kernel of a Student’s t density, that is, <span class="math inline">\(Y_0\mid \boldsymbol{y}\sim t\left(\mu_n,\frac{(\beta_n+1)\delta_n}{\beta_n\alpha_n},\alpha_n\right)\)</span>.</p>
<p><em>The multivariate normal-normal/inverse-Wishart model</em></p>
<p>We show in subsection <a href="sec41.html#sec41">3.1</a> that the multivariate normal distribution is in the exponential family where
<span class="math display">\[\begin{equation*}
C(\boldsymbol{\mu},\boldsymbol{\Sigma})=\exp\left\{-\frac{1}{2}\left(tr\left(\boldsymbol{\mu}\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\right)+\log(|\Sigma|)\right)\right\},
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
\eta(\boldsymbol{\mu},\boldsymbol{\Sigma})^{\top}=\left[\left(vec\left(\boldsymbol{\Sigma}^{-1}\right)\right)^{\top} \ \ \left(vec\left(\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\right)\right)^{\top}\right],
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
T(\boldsymbol{y})=\left[-\frac{1}{2}\left(vec\left(\boldsymbol{S}\right)^{\top}+N vec\left(\hat{\boldsymbol{\mu}}\hat{\boldsymbol{\mu}}^{\top}\right)^{\top}\right) \ \ -N\hat{\boldsymbol{\mu}}^{\top}\right]^{\top}
\end{equation*}\]</span> and
<span class="math display">\[\begin{equation*}
h(\boldsymbol{y})=(2\pi)^{-pN/2}.
\end{equation*}\]</span></p>
<p>Then, its conjugate prior distribution should have the form
<span class="math display">\[\begin{align}
    \pi(\boldsymbol{\mu},\boldsymbol{\Sigma})&amp;\propto \exp\left\{-\frac{b_0}{2}\left(tr\left(\boldsymbol{\mu}\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\right)+\log(|\Sigma|)\right)\right\}\nonumber\\
    &amp;\times \exp\left\{\boldsymbol{a}_{01}^{\top} vec\left(\boldsymbol{\Sigma}^{-1}\right)+\boldsymbol{a}_{02}^{\top}vec\left(\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\right)\right\}\nonumber\\
    &amp;=|\Sigma|^{-b_0/2}\exp\left\{-\frac{b_0}{2}\left(tr\left(\boldsymbol{\mu}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}\right)\right)+tr\left(\boldsymbol{a}_{02}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}\right)\right\}\nonumber\\
    &amp;\times \exp\left\{\boldsymbol{a}_{01}^{\top} vec\left(\boldsymbol{\Sigma}^{-1}\right)+\frac{\boldsymbol{a}_{02}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{a}_{02}}{2b_0}-\frac{\boldsymbol{a}_{02}^{\top}\boldsymbol{\Sigma}^{-1}\boldsymbol{a}_{02}}{2b_0}\right\}\nonumber\\
    &amp;=|\Sigma|^{-b_0/2}\exp\left\{-\frac{b_0}{2}\left(\boldsymbol{\mu}-\frac{\boldsymbol{a}_{02}}{b_0}\right)^{\top}\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}-\frac{\boldsymbol{a}_{02}}{b_0}\right)\right\}\nonumber\\
    &amp;\times \exp\left\{-\frac{1}{2}tr\left(\left(\boldsymbol{A}_{01}-\frac{\boldsymbol{a}_{02}\boldsymbol{a}_{02}^{\top}}{b_0}\right)\boldsymbol{\Sigma}^{-1}\right)\right\}\nonumber\\
    &amp;=\underbrace{|\Sigma|^{-1/2}\exp\left\{-\frac{b_0}{2}\left(\boldsymbol{\mu}-\frac{\boldsymbol{a}_{02}}{b_0}\right)^{\top}\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}-\frac{\boldsymbol{a}_{02}}{b_0}\right)\right\}}_1\nonumber\\
    &amp;\times \underbrace{|\Sigma|^{-(\alpha_0+p+1)/2}\exp\left\{-\frac{1}{2}tr\left(\left(\boldsymbol{A}_{01}-\frac{\boldsymbol{a}_{02}\boldsymbol{a}_{02}^{\top}}{b_0}\right)\boldsymbol{\Sigma}^{-1}\right)\right\}}_2,\nonumber
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(b_0\)</span> represents the hypothetical sample size, and <span class="math inline">\(\boldsymbol{a}_{01}\)</span> and <span class="math inline">\(\boldsymbol{a}_{02}\)</span> are <span class="math inline">\(p^2\)</span>-dimensional and <span class="math inline">\(p\)</span>-dimensional vectors of prior sufficient statistics, respectively. Specifically, <span class="math inline">\(\boldsymbol{a}_{01} = -\frac{1}{2} \text{vec}(\boldsymbol{A}_{01})\)</span>, where <span class="math inline">\(\boldsymbol{A}_{01}\)</span> is a <span class="math inline">\(p \times p\)</span> positive semi-definite matrix.</p>
<p>Setting <span class="math inline">\(b_0 = 1 + \alpha_0 + p + 1\)</span>, we observe that the first part of the last expression is the kernel of a multivariate normal density with mean <span class="math inline">\(\boldsymbol{\mu}_0 = \frac{\boldsymbol{a}_{02}}{b_0}\)</span> and covariance <span class="math inline">\(\frac{\boldsymbol{\Sigma}}{b_0}\)</span>, i.e.,
<span class="math display">\[
\boldsymbol{\mu} \mid \boldsymbol{\Sigma} \sim N_p \left( \boldsymbol{\mu}_0, \frac{\boldsymbol{\Sigma}}{b_0} \right),
\]</span>
where <span class="math inline">\(b_0 = \beta_0\)</span>. This choice of hyperparameters is intuitive because <span class="math inline">\(\boldsymbol{a}_{02}\)</span> represents the hypothetical sum of prior observations, and <span class="math inline">\(b_0\)</span> represents the hypothetical prior sample size.</p>
<p>Additionally, the second part of the last expression corresponds to the kernel of an inverse Wishart distribution with scale matrix <span class="math inline">\(\boldsymbol{\Psi}_0 = \left( \boldsymbol{A}_{01} - \frac{\boldsymbol{a}_{02} \boldsymbol{a}_{02}^{\top}}{b_0} \right)\)</span> and <span class="math inline">\(\alpha_0\)</span> degrees of freedom, i.e.,
<span class="math display">\[
\boldsymbol{\Sigma} \sim IW_p (\boldsymbol{\Psi}_0, \alpha_0).
\]</span>
Observe that <span class="math inline">\(\boldsymbol{\Psi}_0\)</span> has the same structure as the first part of the sufficient statistics in <span class="math inline">\(T(\boldsymbol{y})\)</span>, except that it should be understood as arising from prior hypothetical observations.</p>
<p>Therefore, the prior distribution in this setting is normal/inverse-Wishart, and, due to conjugacy, the posterior distribution belongs to the same family.
<span class="math display">\[\begin{align}
    \pi(\boldsymbol{\mu},\boldsymbol{\Sigma}\mid \boldsymbol{y})&amp;\propto
    (2\pi)^{-p N/2}|\boldsymbol{\Sigma}|^{-N/2}\exp\left\{-\frac{1}{2}tr\left[\left(\boldsymbol{S}+N\left(\boldsymbol{\mu}-\hat{\boldsymbol{\mu}}\right)\left(\boldsymbol{\mu}-\hat{\boldsymbol{\mu}}\right)^{\top}\right)\boldsymbol{\Sigma}^{-1}\right]\right\}\nonumber\\
    &amp;\times |\boldsymbol{\Sigma}|^{-1/2}\exp\left\{-\frac{\beta_0}{2}tr\left[(\boldsymbol{\mu}-\boldsymbol{\mu}_0)(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^{\top}\boldsymbol{\Sigma}^{-1}\right]\right\}|\boldsymbol{\Sigma}|^{-(\alpha_0+p+1)/2}\nonumber\\
    &amp;\times\exp\left\{-\frac{1}{2}tr(\boldsymbol{\Psi}_0\boldsymbol{\Sigma}^{-1})\right\}\nonumber.
\end{align}\]</span></p>
<p>Taking into account that</p>
<p><span class="math display">\[\begin{align}
    N\left(\boldsymbol{\mu}-\hat{\boldsymbol{\mu}}\right)\left(\boldsymbol{\mu}-\hat{\boldsymbol{\mu}}\right)^{\top}+\beta_0\left(\boldsymbol{\mu}-\boldsymbol{\mu}_0\right)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_0\right)^{\top}&amp;=(N+\beta_0)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}\nonumber\\
    &amp;+\frac{N\beta_0}{N+\beta_0}\left(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0\right)\left(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0\right)^{\top},\nonumber
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}_n=\frac{N}{N+\beta_0}\hat{\boldsymbol{\mu}}+\frac{\beta_0}{N+\beta_0}\boldsymbol{\mu}_0\)</span> is the posterior mean. We have</p>
<p><span class="math display">\[\begin{align}
    \pi(\boldsymbol{\mu},\boldsymbol{\Sigma}\mid \boldsymbol{y})&amp;\propto |\boldsymbol\Sigma|^{-1/2}\exp\left\{-\frac{N+\beta_0}{2}tr\left[\left(\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}\right)\boldsymbol{\Sigma}^{-1}\right]\right\}\nonumber\\
    &amp;\times |\boldsymbol{\Sigma}|^{-(N+\alpha_0+p+1)/2}\nonumber\\
    &amp;\times\exp\left\{-\frac{1}{2}tr\left[\left(\boldsymbol{\Psi}_0+\boldsymbol{S}+\frac{N\beta_0}{N+\beta_0}(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0)(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0)^{\top}\right)\boldsymbol{\Sigma}^{-1}\right]\right\}.\nonumber
\end{align}\]</span></p>
<p>Then, <span class="math inline">\(\boldsymbol{\mu}\mid \boldsymbol{\Sigma},\boldsymbol{y}\sim N_p\left(\boldsymbol{\mu}_n,\frac{1}{\beta_n}\boldsymbol{\Sigma}\right)\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}\mid \boldsymbol{y}\sim IW\left(\boldsymbol{\Psi}_n,\alpha_n\right)\)</span> where <span class="math inline">\(\beta_n=N+\beta_0\)</span>, <span class="math inline">\(\alpha_n=N+\alpha_0\)</span> and <span class="math inline">\(\boldsymbol{\Psi}_n=\boldsymbol{\Psi}_0+\boldsymbol{S}+\frac{N\beta_0}{N+\beta_0}(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0)(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}_0)^{\top}\)</span>.</p>
<p>The marginal posterior of <span class="math inline">\(\boldsymbol{\mu}\)</span> is given by <span class="math inline">\(\int_{\mathcal{S}} \pi(\boldsymbol{\mu},\boldsymbol{\Sigma})d\boldsymbol{\Sigma}\)</span> where <span class="math inline">\(\mathcal{S}\)</span> is the space of positive semi-definite matrices. Then,</p>
<p><span class="math display">\[\begin{align}
    \pi(\boldsymbol{\mu}\mid \boldsymbol{y})&amp;\propto\int_{\mathcal{S}}\left\{|\boldsymbol{\Sigma}|^{-(\alpha_n+p+2)/2}\right.\nonumber\\
    &amp;\left. \exp\left\{-\frac{1}{2}tr\left[\left(\beta_n\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}+\boldsymbol{\Psi}_n\right)\boldsymbol{\Sigma}^{-1}\right]\right\} \right\}d\boldsymbol{\Sigma}\nonumber\\
    &amp;\propto \big\lvert\left(\beta_n\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}+\boldsymbol{\Psi}_n\right)\big\lvert^{-(\alpha_n+1)/2}\nonumber\\
    &amp;=\left[\big\lvert\boldsymbol{\Psi}_n\big\lvert\times \big\lvert1+\beta_n\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}\boldsymbol{\Psi}_n^{-1}\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\big\lvert\right]^{-(\alpha_n+1)/2}\nonumber\\
    &amp;\propto \left(1+\frac{1}{\alpha_n+1-p}\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)^{\top}\left(\frac{\boldsymbol{\Psi}_n}{(\alpha_n+1-p)\beta_n}\right)^{-1}\left(\boldsymbol{\mu}-\boldsymbol{\mu}_n\right)\right)^{-(\alpha_n+1-p+p)/2},\nonumber
\end{align}\]</span></p>
<p>where the second line uses properties of the inverse Wishart distribution, and the third line uses a particular case of the Sylvester’s determinant theorem.</p>
<p>We observe that the last line is the kernel of a multivariate t distribution, that is, <span class="math inline">\(\boldsymbol{\mu}\mid \boldsymbol{y}\sim t_p(v_n,\boldsymbol{\mu}_n,\boldsymbol{\Sigma}_n)\)</span> where <span class="math inline">\(v_n=\alpha_n+1-p\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_n=\frac{\boldsymbol{\Psi}_n}{(\alpha_n+1-p)\beta_n}\)</span>.</p>
<p>The marginal likelihood is given by
<span class="math display">\[\begin{align}
    p(\boldsymbol{y})=\frac{\Gamma_p\left(\frac{v_n}{2}\right)}{\Gamma_p\left(\frac{\alpha_0}{2}\right)}\frac{|\boldsymbol{\Psi}_0|^{\alpha_0/2}}{|\boldsymbol{\Psi}_n|^{\alpha_n/2}}\left(\frac{\beta_0}{\beta_n}\right)^{p/2}(2\pi)^{-Np/2},\nonumber
\end{align}\]</span></p>
<p>where <span class="math inline">\(\Gamma_p\)</span> is the multivariate gamma function (see Exercise 5).</p>
<p>The posterior predictive distribution is <span class="math inline">\(\boldsymbol{Y}_0\mid \boldsymbol{y}\sim t_p(v_n,\boldsymbol{\mu}_n,(\beta_n+1)\boldsymbol{\Sigma}_n)\)</span> (see Exercise 6).</p>
<p><strong>Example: Tangency portfolio of US tech stocks</strong></p>
<p>The tangency portfolio is the portfolio that maximizes the Sharpe ratio, which is defined as the excess return of a portfolio standardized by its risk.</p>
<p>We aim to find the portfolio weights <span class="math inline">\(\boldsymbol{w}\)</span> that maximize the Sharpe ratio, where <span class="math inline">\(\mu_{i,T+\kappa} = \mathbb{E}\left( R_{i,T+\kappa} - R_{f,T+\kappa} \mid \mathcal{I}_T \right)\)</span>, with <span class="math inline">\(R_{i,T+\kappa}\)</span> and <span class="math inline">\(R_{f,T+\kappa}\)</span> representing the returns of stock <span class="math inline">\(i\)</span> and the risk-free asset, respectively. Here, <span class="math inline">\(\mu_{i,T+\kappa}\)</span> is the expected value of the excess return at period <span class="math inline">\(T+\kappa\)</span>, conditional on information available up to time <span class="math inline">\(T\)</span> (<span class="math inline">\(\mathcal{I}_T\)</span>), and <span class="math inline">\(\boldsymbol{\Sigma}_{T+\kappa}\)</span> is the covariance matrix of the excess returns, which quantifies the risk.
<span class="math display">\[\begin{equation*}
    \text{argmax}_{{\boldsymbol w}\in \mathbb{R}^{p}} \frac{{\boldsymbol w}^{\top}\boldsymbol{\mu}_{T+\kappa}}{\sqrt{{\boldsymbol w}^{\top}{\boldsymbol{\Sigma}}_{T+\kappa} {\boldsymbol w}}}; \hspace{1cm} \text{s.t}\hspace{.5cm} {\boldsymbol w}^{\top}{\boldsymbol{1}}=1,
\end{equation*}\]</span>
where the solution is
<span class="math display">\[\begin{equation*}
    {\boldsymbol w}^*=\frac{{\boldsymbol{\Sigma}}^{-1}_{T+\kappa}\boldsymbol{\mu}_{T+\kappa}}{{\boldsymbol{1}}^{\top}{\boldsymbol \Sigma}^{-1}_{T+\kappa}\boldsymbol{\mu}_{T+\kappa}}.
\end{equation*}\]</span></p>
<p>If we want to find the optimal portfolio for the next period under the assumption that the excess returns follow a multivariate normal distribution –a common assumption in these applications– we can set <span class="math inline">\(\kappa = 1\)</span> and use the predictive distribution of the excess returns. In this case, <span class="math inline">\(\boldsymbol{\mu}_{T+1} = \boldsymbol{\mu}_n\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_{T+1} = \frac{v_n}{v_n - 2} (\beta_n + 1) \boldsymbol{\Sigma}_n\)</span>, based on the previous predictive result.</p>
<p>We apply this framework to ten tech stocks of the US market between January first, 2021, and September ninth, 2022. In particular, we use information from Yahoo Finance for Apple (AAPL), Netflix (NFLX), Amazon (AMZN), Microsoft (MSFT), Google (GOOG), Meta (META), Tesla (TSLA), NVIDIA Corporation (NVDA), Intel (INTC), and PayPal (PYPL).</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="sec42.html#cb83-1" tabindex="-1"></a><span class="fu">library</span>(quantmod)</span></code></pre></div>
<pre><code>## Loading required package: xts</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## 
## ######################### Warning from &#39;xts&#39; package ##########################
## #                                                                             #
## # The dplyr lag() function breaks how base R&#39;s lag() function is supposed to  #
## # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #
## # source() into this session won&#39;t work correctly.                            #
## #                                                                             #
## # Use stats::lag() to make sure you&#39;re not using dplyr::lag(), or you can add #
## # conflictRules(&#39;dplyr&#39;, exclude = &#39;lag&#39;) to your .Rprofile to stop           #
## # dplyr from breaking base R&#39;s lag() function.                                #
## #                                                                             #
## # Code in packages is not affected. It&#39;s protected by R&#39;s namespace mechanism #
## # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #
## #                                                                             #
## ###############################################################################</code></pre>
<pre><code>## 
## Attaching package: &#39;xts&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     first, last</code></pre>
<pre><code>## Loading required package: TTR</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;quantmod&#39;:
##   method            from
##   as.zoo.data.frame zoo</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="sec42.html#cb93-1" tabindex="-1"></a><span class="fu">library</span>(xts)</span>
<span id="cb93-2"><a href="sec42.html#cb93-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb93-3"><a href="sec42.html#cb93-3" tabindex="-1"></a><span class="fu">library</span>(gridExtra) </span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;gridExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="sec42.html#cb96-1" tabindex="-1"></a><span class="co"># grid.arrange</span></span>
<span id="cb96-2"><a href="sec42.html#cb96-2" tabindex="-1"></a><span class="fu">graphics.off</span>()</span>
<span id="cb96-3"><a href="sec42.html#cb96-3" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list=</span><span class="fu">ls</span>())</span>
<span id="cb96-4"><a href="sec42.html#cb96-4" tabindex="-1"></a><span class="co"># Data Range</span></span>
<span id="cb96-5"><a href="sec42.html#cb96-5" tabindex="-1"></a>sdate <span class="ot">&lt;-</span> <span class="fu">as.Date</span>(<span class="st">&quot;2021-01-01&quot;</span>)</span>
<span id="cb96-6"><a href="sec42.html#cb96-6" tabindex="-1"></a>edate <span class="ot">&lt;-</span> <span class="fu">as.Date</span>(<span class="st">&quot;2022-09-30&quot;</span>)</span>
<span id="cb96-7"><a href="sec42.html#cb96-7" tabindex="-1"></a>Date <span class="ot">&lt;-</span> <span class="fu">seq</span>(sdate, edate, <span class="at">by =</span> <span class="st">&quot;day&quot;</span>)</span>
<span id="cb96-8"><a href="sec42.html#cb96-8" tabindex="-1"></a>tickers <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;AAPL&quot;</span>, <span class="st">&quot;NFLX&quot;</span>, <span class="st">&quot;AMZN&quot;</span>, <span class="st">&quot;GOOG&quot;</span>, <span class="st">&quot;INTC&quot;</span>,<span class="st">&quot;META&quot;</span>, <span class="st">&quot;MSFT&quot;</span>, <span class="st">&quot;TSLA&quot;</span>, <span class="st">&quot;NVDA&quot;</span>, <span class="st">&quot;PYPL&quot;</span>)</span>
<span id="cb96-9"><a href="sec42.html#cb96-9" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(tickers)</span>
<span id="cb96-10"><a href="sec42.html#cb96-10" tabindex="-1"></a><span class="co"># AAPL: Apple, NFLX: Netflix, AMZN: Amazon, </span></span>
<span id="cb96-11"><a href="sec42.html#cb96-11" tabindex="-1"></a><span class="co"># MSFT: Microsoft, GOOG: Google, META: Meta,</span></span>
<span id="cb96-12"><a href="sec42.html#cb96-12" tabindex="-1"></a><span class="co"># TSLA: Tesla, NVDA: NVIDIA Corporation</span></span>
<span id="cb96-13"><a href="sec42.html#cb96-13" tabindex="-1"></a><span class="co"># INTC: Intel, PYPL: PayPal </span></span>
<span id="cb96-14"><a href="sec42.html#cb96-14" tabindex="-1"></a>ss_stock <span class="ot">&lt;-</span> <span class="fu">getSymbols</span>(tickers, <span class="at">from=</span>sdate, <span class="at">to=</span>edate, <span class="at">auto.assign =</span> T)</span>
<span id="cb96-15"><a href="sec42.html#cb96-15" tabindex="-1"></a>ss_stock <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map</span>(tickers,<span class="cf">function</span>(x) <span class="fu">Ad</span>(<span class="fu">get</span>(x)))</span>
<span id="cb96-16"><a href="sec42.html#cb96-16" tabindex="-1"></a>ss_stock <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(purrr<span class="sc">::</span><span class="fu">reduce</span>(ss_stock, merge))</span>
<span id="cb96-17"><a href="sec42.html#cb96-17" tabindex="-1"></a><span class="fu">colnames</span>(ss_stock) <span class="ot">&lt;-</span> tickers</span>
<span id="cb96-18"><a href="sec42.html#cb96-18" tabindex="-1"></a><span class="co"># This is to get stock prices</span></span>
<span id="cb96-19"><a href="sec42.html#cb96-19" tabindex="-1"></a>ss_rtn <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">apply</span>(ss_stock, <span class="dv">2</span>, <span class="cf">function</span>(x) {<span class="fu">diff</span>(<span class="fu">log</span>(x), <span class="dv">1</span>)}))</span>
<span id="cb96-20"><a href="sec42.html#cb96-20" tabindex="-1"></a><span class="co"># Daily returns</span></span>
<span id="cb96-21"><a href="sec42.html#cb96-21" tabindex="-1"></a>t10yr <span class="ot">&lt;-</span> <span class="fu">getSymbols</span>(<span class="at">Symbols =</span> <span class="st">&quot;DGS10&quot;</span>, <span class="at">src =</span> <span class="st">&quot;FRED&quot;</span>, <span class="at">from=</span>sdate, <span class="at">to=</span>edate, <span class="at">auto.assign =</span> F)</span>
<span id="cb96-22"><a href="sec42.html#cb96-22" tabindex="-1"></a><span class="co"># To get 10-Year US Treasury yield data from the Federal Reserve Electronic Database (FRED)</span></span>
<span id="cb96-23"><a href="sec42.html#cb96-23" tabindex="-1"></a>t10yrd <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">+</span> t10yr<span class="sc">/</span><span class="dv">100</span>)<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">365</span>)<span class="sc">-</span><span class="dv">1</span> </span>
<span id="cb96-24"><a href="sec42.html#cb96-24" tabindex="-1"></a><span class="co"># Daily returns</span></span>
<span id="cb96-25"><a href="sec42.html#cb96-25" tabindex="-1"></a>t10yrd <span class="ot">&lt;-</span> t10yrd[<span class="fu">row.names</span>(ss_rtn)]</span>
<span id="cb96-26"><a href="sec42.html#cb96-26" tabindex="-1"></a>Exc_rtn <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(ss_rtn) <span class="sc">-</span> <span class="fu">kronecker</span>(<span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, p)), <span class="fu">as.matrix</span>(t10yrd))</span>
<span id="cb96-27"><a href="sec42.html#cb96-27" tabindex="-1"></a><span class="co"># Excesses of return</span></span>
<span id="cb96-28"><a href="sec42.html#cb96-28" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(Exc_rtn)</span>
<span id="cb96-29"><a href="sec42.html#cb96-29" tabindex="-1"></a>df<span class="sc">$</span>Date <span class="ot">&lt;-</span> <span class="fu">as.Date</span>(<span class="fu">rownames</span>(df))</span>
<span id="cb96-30"><a href="sec42.html#cb96-30" tabindex="-1"></a><span class="co">#  Get months</span></span>
<span id="cb96-31"><a href="sec42.html#cb96-31" tabindex="-1"></a>df<span class="sc">$</span>Month <span class="ot">&lt;-</span> <span class="fu">months</span>(df<span class="sc">$</span>Date)</span>
<span id="cb96-32"><a href="sec42.html#cb96-32" tabindex="-1"></a><span class="co">#  Get years</span></span>
<span id="cb96-33"><a href="sec42.html#cb96-33" tabindex="-1"></a>df<span class="sc">$</span>Year <span class="ot">&lt;-</span> <span class="fu">format</span>(df<span class="sc">$</span>Date, <span class="at">format=</span><span class="st">&quot;%y&quot;</span>)</span>
<span id="cb96-34"><a href="sec42.html#cb96-34" tabindex="-1"></a><span class="co">#  Aggregate on months and year and get mean</span></span>
<span id="cb96-35"><a href="sec42.html#cb96-35" tabindex="-1"></a>Data <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>p, <span class="cf">function</span>(i) {</span>
<span id="cb96-36"><a href="sec42.html#cb96-36" tabindex="-1"></a>    <span class="fu">aggregate</span>(df[, i] <span class="sc">~</span> Month <span class="sc">+</span> Year, df, mean)})</span>
<span id="cb96-37"><a href="sec42.html#cb96-37" tabindex="-1"></a>DataExcRtn <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(Data[, <span class="dv">1</span>]<span class="sc">$</span>Month), p)</span>
<span id="cb96-38"><a href="sec42.html#cb96-38" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p){</span>
<span id="cb96-39"><a href="sec42.html#cb96-39" tabindex="-1"></a>    DataExcRtn[, i] <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(Data[, i]<span class="sc">$</span><span class="st">`</span><span class="at">df[, i]</span><span class="st">`</span>)</span>
<span id="cb96-40"><a href="sec42.html#cb96-40" tabindex="-1"></a>}</span>
<span id="cb96-41"><a href="sec42.html#cb96-41" tabindex="-1"></a><span class="fu">colnames</span>(DataExcRtn) <span class="ot">&lt;-</span> tickers</span>
<span id="cb96-42"><a href="sec42.html#cb96-42" tabindex="-1"></a><span class="fu">head</span>(DataExcRtn)</span></code></pre></div>
<pre><code>##              AAPL          NFLX          AMZN         GOOG          INTC
## [1,]  0.003453436 -0.0007978684  0.0053804495 0.0072313890 -0.0051193877
## [2,]  0.001856496  0.0042864089  0.0018802467 0.0032834533  0.0005462708
## [3,]  0.003214835 -0.0029236867 -0.0023355829 0.0006654206  0.0020368917
## [4,]  0.001054383  0.0009737922  0.0003104527 0.0033227708  0.0061459733
## [5,] -0.004406269  0.0006005357 -0.0019272761 0.0054374191  0.0050578243
## [6,]  0.002962127 -0.0010048976 -0.0016201572 0.0035865836 -0.0021341328
##               META         MSFT         TSLA          NVDA          PYPL
## [1,]  0.0046552164 0.0031597866  0.002826751  0.0055412954  0.0036246294
## [2,]  0.0028180326 0.0026818375  0.003066171  0.0062470706  0.0020811144
## [3,]  0.0015960684 0.0007412593 -0.003674775 -0.0048193681  0.0008583936
## [4,] -0.0022658207 0.0034977089  0.004623756 -0.0005564295  0.0005398989
## [5,] -0.0001791074 0.0001820381 -0.008509941  0.0028232820  0.0054109929
## [6,]  0.0011262192 0.0023652517  0.000486675 -0.0012498933 -0.0027156472</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="sec42.html#cb98-1" tabindex="-1"></a><span class="co"># Hyperparameters #</span></span>
<span id="cb98-2"><a href="sec42.html#cb98-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">dim</span>(DataExcRtn)[<span class="dv">1</span>]</span>
<span id="cb98-3"><a href="sec42.html#cb98-3" tabindex="-1"></a>mu0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb98-4"><a href="sec42.html#cb98-4" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb98-5"><a href="sec42.html#cb98-5" tabindex="-1"></a>Psi0 <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb98-6"><a href="sec42.html#cb98-6" tabindex="-1"></a>alpha0 <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="dv">2</span></span>
<span id="cb98-7"><a href="sec42.html#cb98-7" tabindex="-1"></a><span class="co"># Posterior parameters #</span></span>
<span id="cb98-8"><a href="sec42.html#cb98-8" tabindex="-1"></a>alphan <span class="ot">&lt;-</span> N <span class="sc">+</span> alpha0</span>
<span id="cb98-9"><a href="sec42.html#cb98-9" tabindex="-1"></a>vn <span class="ot">&lt;-</span> alphan <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> p</span>
<span id="cb98-10"><a href="sec42.html#cb98-10" tabindex="-1"></a>muhat <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(DataExcRtn)</span>
<span id="cb98-11"><a href="sec42.html#cb98-11" tabindex="-1"></a>mun <span class="ot">&lt;-</span> N<span class="sc">/</span>(N <span class="sc">+</span> beta0) <span class="sc">*</span> muhat <span class="sc">+</span> beta0<span class="sc">/</span>(N <span class="sc">+</span> beta0) <span class="sc">*</span> mu0</span>
<span id="cb98-12"><a href="sec42.html#cb98-12" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">t</span>(DataExcRtn <span class="sc">-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N)<span class="sc">%*%</span><span class="fu">t</span>(muhat))<span class="sc">%*%</span>(DataExcRtn <span class="sc">-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N) <span class="sc">%*%</span><span class="fu">t</span>(muhat)) </span>
<span id="cb98-13"><a href="sec42.html#cb98-13" tabindex="-1"></a>Psin <span class="ot">&lt;-</span> Psi0 <span class="sc">+</span> S <span class="sc">+</span> N<span class="sc">*</span>beta0<span class="sc">/</span>(N <span class="sc">+</span> beta0)<span class="sc">*</span>(muhat <span class="sc">-</span> mu0)<span class="sc">%*%</span><span class="fu">t</span>(muhat <span class="sc">-</span> mu0)</span>
<span id="cb98-14"><a href="sec42.html#cb98-14" tabindex="-1"></a>betan <span class="ot">&lt;-</span> N <span class="sc">+</span> beta0</span>
<span id="cb98-15"><a href="sec42.html#cb98-15" tabindex="-1"></a>Sigman <span class="ot">&lt;-</span> Psin<span class="sc">/</span>((alphan <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> p)<span class="sc">*</span>betan)</span>
<span id="cb98-16"><a href="sec42.html#cb98-16" tabindex="-1"></a>Covarn <span class="ot">&lt;-</span> (Sigman <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> betan)) <span class="sc">*</span> vn <span class="sc">/</span> (vn <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb98-17"><a href="sec42.html#cb98-17" tabindex="-1"></a>Covari <span class="ot">&lt;-</span> <span class="fu">solve</span>(Covarn)</span>
<span id="cb98-18"><a href="sec42.html#cb98-18" tabindex="-1"></a>OptShare <span class="ot">&lt;-</span> <span class="fu">t</span>(Covari<span class="sc">%*%</span>mun<span class="sc">/</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, p))<span class="sc">%*%</span>Covari<span class="sc">%*%</span>mun)))</span>
<span id="cb98-19"><a href="sec42.html#cb98-19" tabindex="-1"></a><span class="fu">colnames</span>(OptShare) <span class="ot">&lt;-</span> tickers</span>
<span id="cb98-20"><a href="sec42.html#cb98-20" tabindex="-1"></a>OptShare</span></code></pre></div>
<pre><code>##             AAPL     NFLX      AMZN        GOOG      INTC      META       MSFT
## [1,] -0.01871449 0.248481 0.1028211 -0.03408626 0.1733602 0.2299969 -0.0222197
##             TSLA       NVDA      PYPL
## [1,] -0.01591862 0.03534303 0.3009368</code></pre>
<p>We find that the optimal tangency portfolio is composed by 24.8%, 10.2%, 17.3%, 23%, 3.5% and 30.1% weights of Netflix, Amazon, Intel, Meta, NVIDIA and PayPal, and -1.9%, -3.4%, -2.2% and -1.6% weights of Apple, Google, Microsoft and Tesla. A negative weight means being short in financial jargon, that is, borrowing a stock to sell it.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-diaconis1979conjugate" class="csl-entry">
Diaconis, Persi, Donald Ylvisaker, et al. 1979. <span>“Conjugate Priors for Exponential Families.”</span> <em>The Annals of Statistics</em> 7 (2): 269–81.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>Another parametrization of the gamma density is the <em>scale parametrization</em>, where <span class="math inline">\(\kappa_0 = 1/\beta_0\)</span>. See the health insurance example in Chapter <a href="Chap1.html#Chap1">1</a>.<a href="sec42.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p><a href="https://www.11v11.com/teams/manchester-city/tab/opposingTeams/opposition/Liverpool/" class="uri">https://www.11v11.com/teams/manchester-city/tab/opposingTeams/opposition/Liverpool/</a>.<a href="sec42.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p><a href="https://www.oddsportal.com/soccer/england/premier-league/liverpool-manchester-city-WrqgEz5S/" class="uri">https://www.oddsportal.com/soccer/england/premier-league/liverpool-manchester-city-WrqgEz5S/</a><a href="sec42.html#fnref20" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec41.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec43.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
