<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.9 General Bayes posteriors | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="13.9 General Bayes posteriors | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.9 General Bayes posteriors | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec13_8.html"/>
<link rel="next" href="sec13_10.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Identification setting</a></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Randomized controlled trial (RCT)</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Conditional independence assumption (CIA)</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Instrumental variables (IV)</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference-in-differences design (DiD)</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="sec13_5.html"><a href="sec13_5.html#sec13_51"><i class="fa fa-check"></i><b>13.5.1</b> Classic DiD: two-group and two-period (<span class="math inline">\(2\times2\)</span>) setup</a></li>
<li class="chapter" data-level="13.5.2" data-path="sec13_5.html"><a href="sec13_5.html#sec13_52"><i class="fa fa-check"></i><b>13.5.2</b> Staggered (SDiD): G-group and T-period (<span class="math inline">\(G\times T\)</span>) setup</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Regression discontinuity design (RD)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="sec13_6.html"><a href="sec13_6.html#sec13_61"><i class="fa fa-check"></i><b>13.6.1</b> Sharp regression discontinuity design (SRD)</a></li>
<li class="chapter" data-level="13.6.2" data-path="sec13_6.html"><a href="sec13_6.html#sec13_62"><i class="fa fa-check"></i><b>13.6.2</b> Fuzzy regression discontinuity design (FRD)</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Sample selection</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Bayesian exponentially tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.9" data-path="sec13_9.html"><a href="sec13_9.html"><i class="fa fa-check"></i><b>13.9</b> General Bayes posteriors</a></li>
<li class="chapter" data-level="13.10" data-path="sec13_10.html"><a href="sec13_10.html"><i class="fa fa-check"></i><b>13.10</b> Doubly robust Bayesian inferential framework (DRB)</a></li>
<li class="chapter" data-level="13.11" data-path="sec13_11.html"><a href="sec13_11.html"><i class="fa fa-check"></i><b>13.11</b> Summary</a></li>
<li class="chapter" data-level="13.12" data-path="sec13_12.html"><a href="sec13_12.html"><i class="fa fa-check"></i><b>13.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec13_9" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> General Bayes posteriors<a href="sec13_9.html#sec13_9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is well known that, under model misspecification, that is, when the assumed likelihood function does not coincide with the true data-generating likelihood, the posterior concentrates its mass near those points in the support of the prior that minimize the Kullback–Leibler divergence with respect to the true model <span class="citation">(<a href="#ref-Kleijn2006">Kleijn and Vaart 2006</a>)</span>. However, this updating process may yield credible sets that fail to achieve the desired coverage, Bayes factors that can be misleading, and predictions that may remain approximately valid but require careful checking. In addition, the misspecified posterior distribution may exhibit suboptimal risk performance. In this setting, a modified posterior directly linked to the risk function of interest, known as the <em>Gibbs posterior</em>, can still perform very well <span class="citation">(<a href="#ref-Jiang2008">Jiang and Tanner 2008</a>)</span>,</p>
<p><span class="math display" id="eq:GibbsPost">\[\begin{equation}
    \hat{\pi}(\boldsymbol{\theta}\mid\mathbf{y})
    =\frac{\exp\left\{-Nw\,R_N(\boldsymbol{\theta},\mathbf{y})\right\}\pi(\boldsymbol{\theta})}
    {\int_{\boldsymbol{\Theta}}\exp\left\{-Nw\,R_N(\boldsymbol{\theta},\mathbf{y})\right\}\pi(\boldsymbol{\theta})\,d\boldsymbol{\theta}},
    \tag{13.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(w&gt;0\)</span> is the <em>learning rate</em> (or the inverse temperature in simulated annealing), which balances the information in the data with that in the prior, <span class="math inline">\(N\)</span> is the sample size, and</p>
<p><span class="math display">\[
R_N(\boldsymbol{\theta},\mathbf{y})=\frac{1}{N}\sum_{i=1}^N l(\boldsymbol{\theta},\mathbf{y}_i)
\]</span></p>
<p>is the empirical risk associated with the loss function <span class="math inline">\(l(\boldsymbol{\theta},\mathbf{y}_i)\)</span>.</p>
<p>Note that Equation <a href="sec13_9.html#eq:GibbsPost">(13.4)</a> reduces to the ordinary Bayesian posterior when the loss is chosen as the negative log-likelihood, <span class="math inline">\(l(\boldsymbol{\theta},y_i)=-\log p(y_i\mid\boldsymbol{\theta})\)</span>, under i.i.d. sampling and with <span class="math inline">\(w=1\)</span>. In this case, we are asserting knowledge of the data-generating process <span class="math inline">\(p(y_i\mid \boldsymbol{\theta})\)</span>. More generally, however, <span class="math inline">\(R_N(\boldsymbol{\theta},\mathbf{y})\)</span> can be based on any loss function that satisfies the required conditions for coherent belief updating: non-negativity, existence of a well-defined expectation, identifiability, and additivity across observations. Importantly, correctness of the parametric model is not required, since</p>
<p><span class="math display">\[
\frac{1}{N}\sum_{i=1}^N l(\boldsymbol{\theta},\mathbf{y}_i)\;\stackrel{p}{\longrightarrow}\;\int_{\mathcal{Y}} l(\boldsymbol{\theta},\mathbf{y})\,p(\mathbf{y}\mid \boldsymbol{\theta})\,d\mathbf{y},
\]</span></p>
<p>by the law of large numbers as <span class="math inline">\(N\to\infty\)</span>.</p>
<p>Thus, the Gibbs posterior provides inference on the parameter values that minimize the chosen risk function, with minimal modeling assumptions. In contrast, the standard Bayesian posterior allows inference on essentially any feature of the data-generating distribution, but at the cost of strong model assumptions <span class="citation">(<a href="#ref-Syring2019">Syring and Martin 2019</a>)</span>.</p>
<p><span class="citation">Bissiri, Holmes, and Walker (<a href="#ref-bissiri2016general">2016</a>)</span> show that Equation <a href="sec13_9.html#eq:GibbsPost">(13.4)</a> is a valid, coherent mechanism to update prior beliefs. In particular, there must exist a mapping <span class="math inline">\(h\)</span> such that</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}\mid \mathbf{y}) = h\!\big(l(\boldsymbol{\theta},\mathbf{y}),\pi(\boldsymbol{\theta})\big),
\]</span></p>
<p>where <span class="math inline">\(h\)</span> satisfies the coherence property</p>
<p><span class="math display">\[
h\!\left[l(\boldsymbol{\theta},y_2),\,h\!\big(l(\boldsymbol{\theta},y_1),\pi(\boldsymbol{\theta})\big)\right]
= h\!\big(l(\boldsymbol{\theta},y_2)+l(\boldsymbol{\theta},y_1),\pi(\boldsymbol{\theta})\big).
\]</span></p>
<p>This ensures that the updated posterior <span class="math inline">\(\pi(\boldsymbol{\theta}\mid y_1,y_2)\)</span> is the same whether we update with <span class="math inline">\((y_1,y_2)\)</span> jointly or sequentially.</p>
<p>Equation <a href="sec13_9.html#eq:GibbsPost">(13.4)</a> is the solution of minimizing the loss function <span class="math inline">\(L(\nu;\pi,\mathbf{y})\)</span> on the space of probability measures on <span class="math inline">\(\theta\)</span>-space,</p>
<p><span class="math display">\[
\hat{\pi}=\arg\min_{\nu} L(\nu;\pi,\mathbf{y}),
\]</span></p>
<p>such that <span class="math inline">\(\hat{\pi}\)</span> is the representation of beliefs about <span class="math inline">\(\boldsymbol{\theta}\)</span> given prior beliefs (<span class="math inline">\(\pi\)</span>) and data (<span class="math inline">\(\mathbf{y}\)</span>). Given that the prior beliefs and the data are two independent pieces of information, it makes sense that the loss function is additive in these two arguments,</p>
<p><span class="math display">\[
L(\nu;\pi,\mathbf{y})=h_1(\nu,\mathbf{y})+w^{-1}h_2(\nu,\pi),
\]</span></p>
<p>where <span class="math inline">\(h_1(\cdot)\)</span> and <span class="math inline">\(h_2(\cdot)\)</span> are loss functions in their arguments.<br />
The coherence requirement implies that</p>
<p><span class="math display">\[
h_2(\nu,\pi)=d_{KL}(\nu,\pi)=\int \log\frac{\nu(d\boldsymbol{\theta})}{\pi(d\boldsymbol{\theta})} \nu(d\boldsymbol{\theta}),
\]</span></p>
<p>that is, <span class="math inline">\(h_2(\nu,\pi)\)</span> must be the Kullback-Leibler divergence <span class="citation">(<a href="#ref-bissiri2016general">Bissiri, Holmes, and Walker 2016</a>)</span>.</p>
<p>On the other hand,</p>
<p><span class="math display">\[
h_1(\nu,\mathbf{y})=\int l(\boldsymbol{\theta},\mathbf{y})\nu(d\boldsymbol{\theta})
\]</span></p>
<p>is the expected loss of the action with respect to the data.</p>
<p>Therefore, the minimizer of</p>
<p><span class="math display">\[
L(\nu;\pi,\mathbf{y})=\int l(\boldsymbol{\theta},\mathbf{y})\nu(d\boldsymbol{\theta})+d_{KL}(\nu,\pi),
\]</span></p>
<p>is given by Equation <a href="sec13_9.html#eq:GibbsPost">(13.4)</a> <span class="citation">(<a href="#ref-Zhang2006KLentropy">Zhang 2006</a>; <a href="#ref-Jiang2008">Jiang and Tanner 2008</a>; <a href="#ref-bissiri2016general">Bissiri, Holmes, and Walker 2016</a>)</span>.</p>
<p>A very important parameter in the Gibbs posterior <span class="citation">(<a href="#ref-Jiang2008">Jiang and Tanner 2008</a>)</span>, or <em>general Bayes posterior</em> <span class="citation">(<a href="#ref-bissiri2016general">Bissiri, Holmes, and Walker 2016</a>)</span>, is the learning rate, as it determines the asymptotic sampling properties of <span class="math inline">\(\hat{\pi}(\boldsymbol{\theta}\mid\mathbf{y})\)</span> used to perform inference on <span class="math inline">\(\boldsymbol{\theta}\)</span>. For instance, <span class="citation">Bissiri, Holmes, and Walker (<a href="#ref-bissiri2016general">2016</a>)</span> propose different strategies to set this parameter, and <span class="citation">Syring and Martin (<a href="#ref-Syring2019">2019</a>)</span> propose a Monte Carlo algorithm that selects the learning rate so that the resulting credible region attains the nominal Frequentist coverage probability.</p>
<p><span class="citation">Chernozhukov and Hong (<a href="#ref-chernozhukov2003mcmc">2003</a>)</span> introduce the <em>Laplace-type estimator</em> (LTE) or <em>quasi-posterior</em> distribution, which can be interpreted as a special case of the <em>general Bayes</em> update of <span class="citation">Bissiri, Holmes, and Walker (<a href="#ref-bissiri2016general">2016</a>)</span>, taking as loss a scaled sample criterion based on the moment conditions (e.g., the GMM quadratic form).</p>
<p>In particular, given the moment conditions
<span class="math display">\[
\mathbb{E}\!\big[\mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})\big]=\mathbf{0}_{d}
\quad \text{if and only if } \boldsymbol{\theta}=\boldsymbol{\theta}_0,
\]</span>
where the expectation is taken with respect to the population distribution,<br />
<span class="math inline">\(\mathbf{w}_{1:N}:=[\mathbf{w}_1 \ \mathbf{w}_2 \ \dots \ \mathbf{w}_N]\)</span> is a random sample from <span class="math inline">\(\mathbf{W}\subset \mathbb{R}^{d_w}\)</span>,<br />
<span class="math inline">\(\mathbf{g}:\mathbb{R}^{d_w}\times\boldsymbol{\Theta}\to\mathbb{R}^{d}\)</span> is a vector of known functions, and<br />
<span class="math inline">\(\boldsymbol{\theta}=[\theta_{1}\ \theta_{2}\ \dots\ \theta_{p}]^{\top}\in\boldsymbol{\Theta}\subset\mathbb{R}^{p}\)</span> with <span class="math inline">\(d\geq p\)</span>, the risk function can be defined as
<span class="math display" id="eq:RiskLTE">\[
R_N(\boldsymbol{\theta},\mathbf{w})=\tfrac{1}{2}\left(\underbrace{\frac{1}{N}\sum_{i=1}^N \mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})}_{\mathbf{g}_N(\boldsymbol{\theta})}\right)^{\top}\mathbf{W}_N\left(\underbrace{\frac{1}{N}\sum_{i=1}^N\mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta})}_{\mathbf{g}_N(\boldsymbol{\theta})}\right)
\]</span> \tag{13.5}</p>
<p>where <span class="math inline">\(\mathbf{W}_N\)</span> is a positive semi-definite weighting matrix such that
<span class="math display">\[
\mathbf{W}_N \;\to\;
\Bigg(\text{Var}\left[\sqrt{N}\left(\tfrac{1}{N}\sum_{i=1}^N \mathbf{g}_i(\mathbf{w}_i,\boldsymbol{\theta}_0)\right)\right]\Bigg)^{-1}
\quad \text{as } N\rightarrow \infty.
\]</span></p>
<p>Then, the <em>quasi-posterior</em> in <span class="citation">Chernozhukov and Hong (<a href="#ref-chernozhukov2003mcmc">2003</a>)</span> is similar to <a href="sec13_9.html#eq:GibbsPost">(13.4)</a> with <span class="math inline">\(w=1\)</span>.</p>
<p>The following algorithm shows the Metropolis–Hastings to perform inference using the general Bayes posterior.</p>
<figcaption><b>Algorithm: General Bayes posterior — Metropolis-Hastings</b></figcaption>
<figure id="alg:GibbsPosterior">
<pre>
Set θ(0) in the support of π̂(θ|y)
For s=1,2,...,S do
  Draw θᶜ from q(θᶜ | θ(s-1))
  Calculate α(θ(s-1), θᶜ) = min( ( q(θ(s-1)|θᶜ) exp{-Nw R<sub>N</sub>(θᶜ,y)} π(θᶜ) ) /
                               ( q(θᶜ|θ(s-1)) exp{-Nw R<sub>N</sub>(θ(s-1),y)} π(θ(s-1)) ), 1 )
  Draw U from U(0,1)
  θ(s) = θᶜ if U ≤ α(θ(s-1), θᶜ)
  θ(s) = θ(s-1) otherwise
End for
</pre>
</figure>
<p>Under suitable regularity conditions, <span class="citation">Chernozhukov and Hong (<a href="#ref-chernozhukov2003mcmc">2003</a>)</span> show that the LTE posterior mean is first-order equivalent to the efficient GMM estimator and that posterior quantiles yield confidence sets with asymptotically correct Frequentist coverage. Relatedly, <span class="citation">Müller (<a href="#ref-Muller2013SandwichBayes">2013</a>)</span> demonstrates that, under model misspecification, replacing the original likelihood with a curvature-adjusted (“sandwich”) log-likelihood can improve Frequentist risk, that is, lower the average loss over repeated samples when making inference on the parameters. Moreover, <span class="citation">Chen, Christensen, and Tamer (<a href="#ref-ChenChristensenTamer2018">2018</a>)</span> develop confidence sets based on quasi-posteriors that achieve exact asymptotic Frequentist coverage for identified sets of parameters in complex nonlinear structural models, regardless of whether the parameters are point identified. Finally, <span class="citation">Andrews and Mikusheva (<a href="#ref-andrews2022weakgmm">2022</a>)</span> study decision rules for weak GMM and provide support for quasi-Bayesian procedures built from the GMM quadratic form, in line with the spirit of the LTE in settings with weak identification.</p>
<p><strong>Example: Instrumental variable quantile regression (IVQR)</strong></p>
<p><span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> propose an instrumental variable model of quantile treatment effects to characterize the heterogeneous impact of treatments across different points of the outcome distribution. Their model requires conditions that restrict the evolution of ranks across treatment states. Using these conditions, they address the endogeneity problem and recover quantile treatment effects using instrumental variables for the entire population, not only for compliers.</p>
<p>Assume a binary treatment variable <span class="math inline">\(D_i\in\{0,1\}\)</span>, with potential outcomes <span class="math inline">\(\{Y_i(0),Y_i(1)\}\)</span>. The estimand of interest is the quantile treatment effect, which summarizes the differences in the quantiles of potential outcomes across treatment states,</p>
<p><span class="math display">\[
q(D_i=1,\mathbf{X}_i=\mathbf{x}_i,\tau)-q(D=0,\mathbf{X}_i=\mathbf{x}_i,\tau),
\]</span></p>
<p>where <span class="math inline">\(q(D_i=d,\mathbf{X}_i=\mathbf{x}_i,\tau)\)</span> denotes the <span class="math inline">\(\tau\)</span>-th quantile treatment response function.</p>
<p>The potential outcomes are related to the quantile treatment response via</p>
<p><span class="math display">\[
Y_i(d)=q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,U(d_i)),
\]</span></p>
<p>where <span class="math inline">\(U(d_i)\sim U(0,1)\)</span> is the <em>rank variable</em>. This variable captures unobserved heterogeneity that explains differences in outcomes given observed characteristics <span class="math inline">\(\mathbf{x}_i\)</span> and treatment <span class="math inline">\(d_i\)</span>.</p>
<p>For instance, consider a retirement savings model, where the potential outcome is net financial assets under different retirement plan statuses <span class="math inline">\(d\)</span>, and <span class="math inline">\(q(D=d,\mathbf{X}=\mathbf{x},\tau)\)</span> is the net financial asset function describing how an individual with retirement status <span class="math inline">\(d\)</span> and “financial ability” <span class="math inline">\(\tau\)</span> is rewarded in the financial market. Because the function depends on <span class="math inline">\(\tau\)</span>, treatment effects are heterogeneous.</p>
<p>The identification conditions for the IVQR model are stated in <span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> as follows:</p>
<p><em>1. Potential outcomes:</em><br />
<span class="math display">\[
Y_i(d)=q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,U(d_i)),
\]</span><br />
where <span class="math inline">\(q(D_i=d_i,\mathbf{X}_i=\mathbf{x}_i,\tau)\)</span> is strictly increasing in <span class="math inline">\(\tau\)</span> and <span class="math inline">\(U(d_i)\sim U(0,1)\)</span>.</p>
<p><em>2. Independence:</em> Conditional on <span class="math inline">\(\mathbf{X}_i=\mathbf{x}_i\)</span>, the rank variables <span class="math inline">\(\{U(d_i)\}\)</span> are independent of the instruments <span class="math inline">\(\mathbf{Z}_i\)</span>.</p>
<p><em>3. Selection:</em> The treatment assignment is given by <span class="math inline">\(D_i=\delta(\mathbf{Z}_i,\mathbf{X}_i,\mathbf{V}_i)\)</span> for some unknown function <span class="math inline">\(\delta\)</span> and unobserved heterogeneity <span class="math inline">\(\mathbf{V}_i\)</span>.</p>
<p><em>4. Rank invariance:</em> Conditional on <span class="math inline">\(\mathbf{X}_i=\mathbf{x}_i,\mathbf{Z}_i=\mathbf{z}_i\)</span>,</p>
<ul>
<li>either <span class="math inline">\(\{U(d_i)\}\)</span> coincide (<span class="math inline">\(U(d_i)=U\)</span> for all <span class="math inline">\(d\)</span>), or<br />
</li>
<li><span class="math inline">\(\{U(d_i)\}\)</span> are identically distributed conditional on <span class="math inline">\(\mathbf{V}_i\)</span>.</li>
</ul>
<p><em>5. Observables:</em> The observed data consist of <span class="math inline">\(Y_i=q(D_i,\mathbf{X}_i,U(D_i))\)</span>, <span class="math inline">\(D_i\)</span>, <span class="math inline">\(\mathbf{X}_i\)</span>, and <span class="math inline">\(\mathbf{Z}_i\)</span>, <span class="math inline">\(i=1,2,\dots,N\)</span>.</p>
<p>The most important identification restriction is <em>rank invariance</em>, which implies that individuals with higher unobserved rank remain higher-ranked regardless of treatment status. This condition accommodates more general selection mechanisms than the monotonicity assumption used in the LATE framework, while being less restrictive than full independence assumptions between instruments (<span class="math inline">\(\mathbf{Z}\)</span>) and unobserved variables in the selection equation (<span class="math inline">\(\mathbf{V}\)</span>) in other common models (see <span class="citation">Chernozhukov and Hansen (<a href="#ref-chernozhukov2004effects">2004</a>)</span>; <span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span> for details).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The main testable implication of the identification restrictions is that</p>
<p><span class="math display">\[
P\!\left(Y_i \leq q(D_i,\mathbf{X}_i,\tau)\mid \mathbf{X}_i,\mathbf{Z}_i\right)
= P\!\left(Y_i &lt; q(D_i,\mathbf{X}_i,\tau)\mid \mathbf{X}_i,\mathbf{Z}_i\right)
= \tau,
\]</span></p>
<p>for all <span class="math inline">\(\tau\)</span> almost surely, and <span class="math inline">\(U(D_i)\sim U(0,1)\)</span> conditional on <span class="math inline">\((\mathbf{X}_i,\mathbf{Z}_i)\)</span>.</p>
<p>This conditional moment restriction implies the unconditional moment conditions that form the basis for estimation and inference in the IVQR model <span class="citation">(<a href="#ref-chernozhukov2003mcmc">Chernozhukov and Hong 2003</a>; <a href="#ref-chernozhukov2004effects">Chernozhukov and Hansen 2004</a>)</span>,</p>
<p><span class="math display">\[
\mathbf{g}_N(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^N (\tau-\mathbf{1}(Y_i\leq \alpha_{\tau}D_i+\mathbf{X}_i^{\top}\boldsymbol{\beta}_{\tau}))
\begin{bmatrix}
\mathbf{X}_i\\
\mathbf{Z}_i
\end{bmatrix}.
\]</span></p>
<p>Thus, the empirical risk function is given by</p>
<p><span class="math display">\[
R_N(\boldsymbol{\theta},\mathbf{w})=\tfrac{1}{2}\mathbf{g}_N(\boldsymbol{\theta})^{\top}\mathbf{W}_N\mathbf{g}_N(\boldsymbol{\theta}),
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{W}_N=\frac{1}{\tau(1-\tau)}\left(\frac{1}{N}\sum_{i=1}^N \mathbf{Z}_i\mathbf{Z}_i^{\top}\right)^{-1}.
\]</span></p>
<p>The following code implements the previous algorithm to perform inference on the quantile treatment effect of participation in the 401(k) retirement program on net financial assets, using eligibility as an instrument (see the 401(k) treatment effects example in Section~@ref(sec13_4) for details of the data).</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="sec13_9.html#cb21-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb21-2"><a href="sec13_9.html#cb21-2" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb21-3"><a href="sec13_9.html#cb21-3" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv&quot;</span>,</span>
<span id="cb21-4"><a href="sec13_9.html#cb21-4" tabindex="-1"></a><span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>, <span class="at">quote =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb21-5"><a href="sec13_9.html#cb21-5" tabindex="-1"></a><span class="co"># Attach variables</span></span>
<span id="cb21-6"><a href="sec13_9.html#cb21-6" tabindex="-1"></a><span class="fu">attach</span>(df)</span>
<span id="cb21-7"><a href="sec13_9.html#cb21-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> net_tfa<span class="sc">/</span><span class="dv">1000</span>    <span class="co"># Outcome: net financial assets</span></span>
<span id="cb21-8"><a href="sec13_9.html#cb21-8" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(p401) <span class="co"># Endogenous regressor: participation</span></span>
<span id="cb21-9"><a href="sec13_9.html#cb21-9" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(age, inc, fsize, educ, marr, twoearn, db, pira, hown))  <span class="co"># Exogenous regressors</span></span>
<span id="cb21-10"><a href="sec13_9.html#cb21-10" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(e401)  <span class="co"># Instrument: eligibility (NO intercept here)</span></span>
<span id="cb21-11"><a href="sec13_9.html#cb21-11" tabindex="-1"></a><span class="fu">library</span>(quantreg)</span>
<span id="cb21-12"><a href="sec13_9.html#cb21-12" tabindex="-1"></a>tau <span class="ot">&lt;-</span> <span class="fl">0.5</span> </span>
<span id="cb21-13"><a href="sec13_9.html#cb21-13" tabindex="-1"></a>QuanReg <span class="ot">&lt;-</span> <span class="fu">rq</span>(y <span class="sc">~</span> p401 <span class="sc">+</span> age <span class="sc">+</span> inc <span class="sc">+</span> fsize <span class="sc">+</span> educ <span class="sc">+</span> marr <span class="sc">+</span> twoearn <span class="sc">+</span> db <span class="sc">+</span> pira <span class="sc">+</span> hown, <span class="at">tau =</span> tau, <span class="at">data =</span> df)</span>
<span id="cb21-14"><a href="sec13_9.html#cb21-14" tabindex="-1"></a><span class="fu">summary</span>(QuanReg)</span>
<span id="cb21-15"><a href="sec13_9.html#cb21-15" tabindex="-1"></a>Reg <span class="ot">&lt;-</span> MCMCpack<span class="sc">::</span><span class="fu">MCMCquantreg</span>(y <span class="sc">~</span> x <span class="sc">+</span> w, <span class="at">data =</span> df, <span class="at">tau =</span> tau)</span>
<span id="cb21-16"><a href="sec13_9.html#cb21-16" tabindex="-1"></a>BayesExo <span class="ot">&lt;-</span> <span class="fu">summary</span>(Reg)</span>
<span id="cb21-17"><a href="sec13_9.html#cb21-17" tabindex="-1"></a>LossFunct <span class="ot">&lt;-</span> <span class="cf">function</span>(par, tau, y, z, x, w){</span>
<span id="cb21-18"><a href="sec13_9.html#cb21-18" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb21-19"><a href="sec13_9.html#cb21-19" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x, w)</span>
<span id="cb21-20"><a href="sec13_9.html#cb21-20" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z, w)</span>
<span id="cb21-21"><a href="sec13_9.html#cb21-21" tabindex="-1"></a>    Ind <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(y <span class="sc">&lt;=</span> X<span class="sc">%*%</span>par) </span>
<span id="cb21-22"><a href="sec13_9.html#cb21-22" tabindex="-1"></a>    gn <span class="ot">&lt;-</span> <span class="fu">colMeans</span>((tau <span class="sc">-</span> Ind) <span class="sc">*</span> Z)</span>
<span id="cb21-23"><a href="sec13_9.html#cb21-23" tabindex="-1"></a>    Wni <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="cf">function</span>(i) {Z[i,] <span class="sc">%*%</span> <span class="fu">t</span>(Z[i,])}) </span>
<span id="cb21-24"><a href="sec13_9.html#cb21-24" tabindex="-1"></a>    Wn <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (tau <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> tau)) <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, Wni)<span class="sc">/</span>n)</span>
<span id="cb21-25"><a href="sec13_9.html#cb21-25" tabindex="-1"></a>    Ln <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fl">0.5</span> <span class="sc">*</span> n <span class="sc">*</span> <span class="fu">t</span>(gn) <span class="sc">%*%</span> Wn <span class="sc">%*%</span> gn</span>
<span id="cb21-26"><a href="sec13_9.html#cb21-26" tabindex="-1"></a>    <span class="fu">return</span>(Ln)</span>
<span id="cb21-27"><a href="sec13_9.html#cb21-27" tabindex="-1"></a>}</span>
<span id="cb21-28"><a href="sec13_9.html#cb21-28" tabindex="-1"></a>par0 <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(Reg)</span>
<span id="cb21-29"><a href="sec13_9.html#cb21-29" tabindex="-1"></a><span class="fu">LossFunct</span>(<span class="at">par =</span> par0, <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb21-30"><a href="sec13_9.html#cb21-30" tabindex="-1"></a><span class="co"># ----- MH using Ln -----</span></span>
<span id="cb21-31"><a href="sec13_9.html#cb21-31" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">length</span>(par0); b0 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, k); B0 <span class="ot">&lt;-</span> <span class="dv">1000</span><span class="sc">*</span><span class="fu">diag</span>(k)</span>
<span id="cb21-32"><a href="sec13_9.html#cb21-32" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">10000</span>; burnin <span class="ot">&lt;-</span> <span class="dv">10000</span>; thin <span class="ot">&lt;-</span> <span class="dv">1</span>; tot <span class="ot">&lt;-</span> S <span class="sc">+</span> burnin</span>
<span id="cb21-33"><a href="sec13_9.html#cb21-33" tabindex="-1"></a>BETA <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, tot, k); accept <span class="ot">&lt;-</span> <span class="fu">logical</span>(tot)</span>
<span id="cb21-34"><a href="sec13_9.html#cb21-34" tabindex="-1"></a>SIGMA <span class="ot">&lt;-</span> <span class="fu">diag</span>(BayesExo[[<span class="st">&quot;statistics&quot;</span>]][,<span class="dv">2</span>]); tune <span class="ot">&lt;-</span> <span class="fl">2.4</span> <span class="sc">/</span> <span class="fu">sqrt</span>(k)</span>
<span id="cb21-35"><a href="sec13_9.html#cb21-35" tabindex="-1"></a>BETA[<span class="dv">1</span>,] <span class="ot">&lt;-</span> par0</span>
<span id="cb21-36"><a href="sec13_9.html#cb21-36" tabindex="-1"></a>LL <span class="ot">&lt;-</span> <span class="fu">LossFunct</span>(<span class="at">par =</span> BETA[<span class="dv">1</span>,], <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb21-37"><a href="sec13_9.html#cb21-37" tabindex="-1"></a>pb <span class="ot">&lt;-</span> <span class="fu">txtProgressBar</span>(<span class="at">min=</span><span class="dv">0</span>, <span class="at">max=</span>tot, <span class="at">style=</span><span class="dv">3</span>)</span>
<span id="cb21-38"><a href="sec13_9.html#cb21-38" tabindex="-1"></a><span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>tot){</span>
<span id="cb21-39"><a href="sec13_9.html#cb21-39" tabindex="-1"></a>    cand <span class="ot">&lt;-</span> BETA[s<span class="dv">-1</span>,] <span class="sc">+</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, k), tune<span class="sc">*</span>SIGMA)</span>
<span id="cb21-40"><a href="sec13_9.html#cb21-40" tabindex="-1"></a>    LLc  <span class="ot">&lt;-</span> <span class="fu">LossFunct</span>(<span class="at">par =</span> cand, <span class="at">tau =</span> tau, <span class="at">y =</span> y, <span class="at">z =</span> z, <span class="at">x =</span> x, <span class="at">w =</span> w)</span>
<span id="cb21-41"><a href="sec13_9.html#cb21-41" tabindex="-1"></a>    priorRat <span class="ot">&lt;-</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(cand, b0, B0, <span class="at">log=</span><span class="cn">TRUE</span>) <span class="sc">-</span></span>
<span id="cb21-42"><a href="sec13_9.html#cb21-42" tabindex="-1"></a>    mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(BETA[s<span class="dv">-1</span>,], b0, B0, <span class="at">log=</span><span class="cn">TRUE</span>)</span>
<span id="cb21-43"><a href="sec13_9.html#cb21-43" tabindex="-1"></a>    loga <span class="ot">&lt;-</span> (LLc <span class="sc">-</span> LL) <span class="sc">+</span> priorRat</span>
<span id="cb21-44"><a href="sec13_9.html#cb21-44" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">is.finite</span>(loga) <span class="sc">&amp;&amp;</span> <span class="fu">log</span>(<span class="fu">runif</span>(<span class="dv">1</span>)) <span class="sc">&lt;=</span> loga) {</span>
<span id="cb21-45"><a href="sec13_9.html#cb21-45" tabindex="-1"></a>        BETA[s,] <span class="ot">&lt;-</span> cand; LL <span class="ot">&lt;-</span> LLc; accept[s] <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb21-46"><a href="sec13_9.html#cb21-46" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb21-47"><a href="sec13_9.html#cb21-47" tabindex="-1"></a>        BETA[s,] <span class="ot">&lt;-</span> BETA[s<span class="dv">-1</span>,]; accept[s] <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb21-48"><a href="sec13_9.html#cb21-48" tabindex="-1"></a>    }</span>
<span id="cb21-49"><a href="sec13_9.html#cb21-49" tabindex="-1"></a>    <span class="cf">if</span> (s <span class="sc">&lt;=</span> burnin <span class="sc">&amp;&amp;</span> s <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {    </span>
<span id="cb21-50"><a href="sec13_9.html#cb21-50" tabindex="-1"></a>        acc <span class="ot">&lt;-</span> <span class="fu">mean</span>(accept[(s<span class="dv">-99</span>)<span class="sc">:</span>s])</span>
<span id="cb21-51"><a href="sec13_9.html#cb21-51" tabindex="-1"></a>        <span class="cf">if</span> (acc <span class="sc">&gt;</span> <span class="fl">0.35</span>) tune <span class="ot">&lt;-</span> tune <span class="sc">*</span> <span class="fl">1.25</span></span>
<span id="cb21-52"><a href="sec13_9.html#cb21-52" tabindex="-1"></a>        <span class="cf">if</span> (acc <span class="sc">&lt;</span> <span class="fl">0.15</span>) tune <span class="ot">&lt;-</span> tune <span class="sc">/</span> <span class="fl">1.25</span></span>
<span id="cb21-53"><a href="sec13_9.html#cb21-53" tabindex="-1"></a>    }</span>
<span id="cb21-54"><a href="sec13_9.html#cb21-54" tabindex="-1"></a>    <span class="fu">setTxtProgressBar</span>(pb, s)</span>
<span id="cb21-55"><a href="sec13_9.html#cb21-55" tabindex="-1"></a>}</span>
<span id="cb21-56"><a href="sec13_9.html#cb21-56" tabindex="-1"></a><span class="fu">close</span>(pb)</span>
<span id="cb21-57"><a href="sec13_9.html#cb21-57" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Acceptance rate:&quot;</span>, <span class="fu">mean</span>(accept), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb21-58"><a href="sec13_9.html#cb21-58" tabindex="-1"></a>keep <span class="ot">&lt;-</span> <span class="fu">seq</span>(burnin, tot, <span class="at">by =</span> thin)</span>
<span id="cb21-59"><a href="sec13_9.html#cb21-59" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Acceptance rate:&quot;</span>, <span class="fu">mean</span>(accept[keep]), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb21-60"><a href="sec13_9.html#cb21-60" tabindex="-1"></a>post <span class="ot">&lt;-</span> BETA[keep, , drop<span class="ot">=</span><span class="cn">FALSE</span>]   <span class="co"># posterior draws in scaled space</span></span>
<span id="cb21-61"><a href="sec13_9.html#cb21-61" tabindex="-1"></a><span class="fu">colnames</span>(post) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Int&quot;</span>, <span class="st">&quot;p401&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;inc&quot;</span>, <span class="st">&quot;fsize&quot;</span>, <span class="st">&quot;educ&quot;</span>, <span class="st">&quot;marr&quot;</span>, <span class="st">&quot;twoearn&quot;</span>, <span class="st">&quot;db&quot;</span>, <span class="st">&quot;pira&quot;</span>, <span class="st">&quot;hown&quot;</span>)</span>
<span id="cb21-62"><a href="sec13_9.html#cb21-62" tabindex="-1"></a><span class="co"># Posterior summaries</span></span>
<span id="cb21-63"><a href="sec13_9.html#cb21-63" tabindex="-1"></a>post_mean <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(post)</span>
<span id="cb21-64"><a href="sec13_9.html#cb21-64" tabindex="-1"></a>post_ci   <span class="ot">&lt;-</span> <span class="fu">apply</span>(post, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb21-65"><a href="sec13_9.html#cb21-65" tabindex="-1"></a><span class="fu">round</span>(post_mean, <span class="dv">3</span>); <span class="fu">round</span>(post_ci, <span class="dv">3</span>)</span></code></pre></div>
<p>Setting <span class="math inline">\(\tau = 0.5\)</span> (the median), the quantile treatment effect is USD 6,719, with a 95% credible interval of (USD 6,612, USD 6,806). For <span class="math inline">\(\tau = 0.9\)</span>, the treatment effect is USD 19,318, with a 95% credible interval of (USD 18,634, USD 20,085). Note that, in the instrumental variable example, the posterior mean of the LATE was USD 8,520, which is higher than the treatment effect at the median (<span class="math inline">\(\tau = 0.5\)</span>). This suggests that the LATE overstates the causal effect of 401(k) participation due to the large impact at higher quantiles of the outcome distribution.</p>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-andrews2022weakgmm" class="csl-entry">
Andrews, Isaiah, and Anna Mikusheva. 2022. <span>“Optimal Decision Rules for Weak <span>GMM</span>.”</span> <em>Econometrica</em> 90 (2): 715–48. <a href="https://doi.org/10.3982/ECTA18678">https://doi.org/10.3982/ECTA18678</a>.
</div>
<div id="ref-bissiri2016general" class="csl-entry">
Bissiri, Pier Giovanni, Chris C Holmes, and Stephen G Walker. 2016. <span>“A General Framework for Updating Belief Distributions.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 78 (5): 1103–30.
</div>
<div id="ref-ChenChristensenTamer2018" class="csl-entry">
Chen, Xiaohong, Timothy M. Christensen, and Elie Tamer. 2018. <span>“Monte Carlo Confidence Sets for Identified Sets.”</span> <em>Econometrica</em> 86 (6): 1965–2018. <a href="https://doi.org/10.3982/ECTA14525">https://doi.org/10.3982/ECTA14525</a>.
</div>
<div id="ref-chernozhukov2004effects" class="csl-entry">
Chernozhukov, Victor, and Christian Hansen. 2004. <span>“The Effects of 401(k) Participation on the Wealth Distribution: An Instrumental Quantile Regression Analysis.”</span> <em>The Review of Economics and Statistics</em> 86 (3): 735–51. <a href="https://doi.org/10.1162/0034653041811749">https://doi.org/10.1162/0034653041811749</a>.
</div>
<div id="ref-Chernozhukov2005" class="csl-entry">
———. 2005. <span>“An IV Model of Quantile Treatment Effects.”</span> <em>Econometrica</em> 73 (1): 245–61. <a href="https://doi.org/10.1111/j.1468-0262.2005.00570.x">https://doi.org/10.1111/j.1468-0262.2005.00570.x</a>.
</div>
<div id="ref-chernozhukov2003mcmc" class="csl-entry">
Chernozhukov, Victor, and Han Hong. 2003. <span>“An MCMC Approach to Classical Estimation.”</span> <em>Journal of Econometrics</em> 115 (2): 293–346.
</div>
<div id="ref-Jiang2008" class="csl-entry">
Jiang, Wenhua, and Martin A. Tanner. 2008. <span>“Gibbs Posterior for Variable Selection in High-Dimensional Classification and Data Mining.”</span> <em>Annals of Statistics</em> 36 (5): 2207–31. <a href="https://doi.org/10.1214/07-AOS547">https://doi.org/10.1214/07-AOS547</a>.
</div>
<div id="ref-Kleijn2006" class="csl-entry">
Kleijn, Bas J. K., and Aad W. van der Vaart. 2006. <span>“Misspecification in Infinite-Dimensional Bayesian Statistics.”</span> <em>Annals of Statistics</em> 34 (2): 837–77. <a href="https://doi.org/10.1214/009053606000000029">https://doi.org/10.1214/009053606000000029</a>.
</div>
<div id="ref-Muller2013SandwichBayes" class="csl-entry">
Müller, Ulrich K. 2013. <span>“Risk of <span>Bayesian</span> Inference in Misspecified Models, and the Sandwich Covariance Matrix.”</span> <em>Econometrica</em> 81 (5): 1805–49. <a href="https://doi.org/10.3982/ECTA9097">https://doi.org/10.3982/ECTA9097</a>.
</div>
<div id="ref-Syring2019" class="csl-entry">
Syring, Nicholas, and Ryan Martin. 2019. <span>“Calibrating General Posterior Credible Regions.”</span> <em>Biometrika</em> 106 (2): 479–86. <a href="https://doi.org/10.1093/biomet/asy068">https://doi.org/10.1093/biomet/asy068</a>.
</div>
<div id="ref-Zhang2006KLentropy" class="csl-entry">
Zhang, Tong. 2006. <span>“<span class="nocase">From <span class="math inline">\(\\varepsilon\)</span>-Entropy to KL-Entropy: Analysis of Minimum Information Complexity Density Estimation</span>.”</span> <em>Annals of Statistics</em> 34 (5): 2180–210. <a href="https://doi.org/10.1214/009053606000000704">https://doi.org/10.1214/009053606000000704</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p><span class="citation">Chernozhukov and Hansen (<a href="#ref-Chernozhukov2005">2005</a>)</span>’s proposal allows for both discrete and continuous <span class="math inline">\(D\)</span> and <span class="math inline">\(\mathbf{Z}\)</span>.<a href="sec13_9.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec13_8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec13_10.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/10-Diagnostics.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
