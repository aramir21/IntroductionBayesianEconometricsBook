<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.5 Tall data problems | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="12.5 Tall data problems | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.5 Tall data problems | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12_4.html"/>
<link rel="next" href="id_13_6.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Bayesian machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross-validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
<li class="chapter" data-level="12.5" data-path="sec12_5.html"><a href="sec12_5.html"><i class="fa fa-check"></i><b>12.5</b> Tall data problems</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="sec12_5.html"><a href="sec12_5.html#sec12_51"><i class="fa fa-check"></i><b>12.5.1</b> Divide-and-conquer methods</a></li>
<li class="chapter" data-level="12.5.2" data-path="sec12_5.html"><a href="sec12_5.html#sec12_52"><i class="fa fa-check"></i><b>12.5.2</b> Subsampling-based algorithms</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="id_13_6.html"><a href="id_13_6.html"><i class="fa fa-check"></i><b>12.6</b> Summary</a></li>
<li class="chapter" data-level="12.7" data-path="id_13_7.html"><a href="id_13_7.html"><i class="fa fa-check"></i><b>12.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12_5" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Tall data problems<a href="sec12_5.html#sec12_5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we explore several methods developed to perform Bayesian inference when the sample size is large, particularly when there is a large number of observational units, commonly referred to as <em>tall datasets</em>.</p>
<p>Bayesian inference in such settings is computationally demanding because each iteration of an MCMC algorithm requires evaluating the likelihood function over all <span class="math inline">\(N\)</span> observations. For large <span class="math inline">\(N\)</span>, this renders standard MCMC methods prohibitively expensive. Recent efforts have focused on developing scalable Monte Carlo algorithms that significantly reduce the computational cost compared to standard approaches. One alternative is to use <em>Variational Bayes</em> (see Chapter <a href="Chap14.html#Chap14">14</a>); however, it can be challenging to implement and may exhibit limitations in uncertainty quantification, particularly for the joint posterior distribution. Another alternative is the <em>Integrated Nested Laplace Approximation</em> (INLA, see Chapter <a href="Chap14.html#Chap14">14</a>); however, its computational cost grows exponentially with the dimension of the parameter space.</p>
<p>In scenarios where observations are assumed to be independent, two main frameworks have been proposed to scale MCMC algorithms: <em>divide-and-conquer approaches</em> and <em>subsampling-based algorithms</em>. Divide-and-conquer methods partition the dataset into disjoint subsets, run MCMC independently on each batch to obtain subposteriors, and then combine them to approximate the full posterior distribution. Subsampling-based algorithms, on the other hand, aim to reduce the number of data points used to evaluate the likelihood at each iteration, often relying on <em>pseudo-marginal</em> MCMC methods <span class="citation">(<a href="#ref-andrieu2009pseudoefficient">Andrieu and Roberts 2009</a>)</span>. The key idea of pseudo-marginal MCMC is to augment the model with latent variables such that the sample average of the likelihood, computed over draws from these latent variables, provides an unbiased estimator of the marginal likelihood. This approach is particularly valuable when the marginal likelihood is not available in closed form. Moreover, the same principles can be adapted to reduce the computational burden of evaluating the log-likelihood. For an excellent review of divide-and-conquer and subsampling-based approaches, see <span class="citation">(<a href="#ref-bardenet2017markov">Bardenet, Doucet, and Holmes 2017</a>)</span>.</p>
<div id="sec12_51" class="section level3 hasAnchor" number="12.5.1">
<h3><span class="header-section-number">12.5.1</span> Divide-and-conquer methods<a href="sec12_5.html#sec12_51" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <em>divide-and-conquer</em> methods, the main idea is to partition the dataset and distribute the subsets across multiple computing machines/cores. An independent MCMC algorithm is then executed on each subset to obtain a corresponding <em>subposterior</em> distribution. The central challenge lies in accurately and efficiently combining these subposteriors into a single approximation of the full posterior distribution.</p>
<p>Several approaches have been proposed to address this issue. For instance, <span class="citation">Huang and Gelman (<a href="#ref-huang2005sampling">2005</a>)</span>, <span class="citation">Steven L. Scott et al. (<a href="#ref-scott2016bayes">2016</a>)</span>, <span class="citation">Rendell et al. (<a href="#ref-rendell2020global">2020</a>)</span> and <span class="citation">Steven L. Scott et al. (<a href="#ref-scott2022bayes">2022</a>)</span> introduce the <em>Consensus Monte Carlo</em> algorithm; <span class="citation">Wang and Dunson (<a href="#ref-wang2013parallelizing">2013</a>)</span> develop a method based on the <em>Weierstrass sampler</em> for parallelizing MCMC; <span class="citation">Minsker (<a href="#ref-minsker2015scalable">2015</a>)</span> propose using the <em>geometric median of posterior distributions</em>; and <span class="citation">Wu and Robert (<a href="#ref-wu2017average">2017</a>)</span> suggest combining <em>rescaled subposteriors</em>.</p>
<p>In divide-and-conquer methods, the dataset is partitioned into <span class="math inline">\(B\)</span> disjoint batches <span class="math inline">\(\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_B\)</span>, and the posterior is rewritten using the identity:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta} \mid \mathbf{y}) \propto \prod_{b=1}^B \pi(\boldsymbol{\theta})^{1/B} p(\mathbf{y}_b \mid \boldsymbol{\theta}),
\]</span></p>
<p>which implies that the full posterior is proportional to the product of appropriately rescaled subposteriors.</p>
<p><em>Consensus Monte Carlo</em> (CMC) operates by running separate Monte Carlo algorithms on each subset in parallel, and then averaging the resulting posterior draws. Specifically, given samples <span class="math inline">\(\boldsymbol{\theta}_b^{(s)}\)</span>, for <span class="math inline">\(b = 1, 2, \dots, B\)</span> and <span class="math inline">\(s = 1, 2, \dots, S\)</span>, obtained independently from each batch <span class="math inline">\(\mathbf{y}_b\)</span>, the <span class="math inline">\(s\)</span>-th draw from the consensus posterior is computed as:</p>
<p><span class="math display">\[
\boldsymbol{\theta}^{(s)} = \left( \sum_{b=1}^B \mathbf{w}_b \right)^{-1} \sum_{b=1}^B \mathbf{w}_b \boldsymbol{\theta}_b^{(s)},
\]</span></p>
<p>where the optimal weight is the inverse covariance matrix of the subposterior, i.e., <span class="math inline">\(\mathbf{w}_b = \operatorname{Var}^{-1}(\boldsymbol{\theta} \mid \mathbf{y}_b)\)</span>. In practice, one may use the marginal variances of each parameter to simplify the computation, which can still yield good performance.</p>
<p>When each subposterior <span class="math inline">\(\pi_b(\boldsymbol{\theta} \mid \mathbf{y}_b)\)</span> is Gaussian, the full posterior <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> is also Gaussian and can be recovered exactly by combining the subposteriors using simple rules based on their means and covariances <span class="citation">(<a href="#ref-scott2016bayes">Steven L. Scott et al. 2016</a>; <a href="#ref-scott2022bayes">Steven L. Scott et al. 2022</a>)</span>. In the non-Gaussian case, standard asymptotic results in Bayesian inference (see Chapter <a href="Chap1.html#Chap1">1</a>) imply that the posterior distributions converge to a Gaussian distribution as the batch size increases.</p>
<p>Alternative merging procedures that are more robust to non-Gaussianity have also been proposed <span class="citation">(<a href="#ref-neiswanger2013asymptotically">Neiswanger, Wang, and Xing 2013</a>; <a href="#ref-minsker2017robust">Minsker et al. 2017</a>)</span>; however, it remains difficult to quantify the approximation error in these approaches. Moreover, this procedure is limited to continuous parameter spaces and may exhibit small-sample bias; that is, when the dataset is divided into small batches, the subposterior distributions may be biased. In such cases, jackknife bias corrections are recommended to reduce the overall approximation error.</p>
<p>In particular, we perform CMC using the following Algorithm <span class="citation">(<a href="#ref-scott2016bayes">Steven L. Scott et al. 2016</a>)</span>. Next, we compute the CMC posterior repeatedly, each time leaving out one of the <span class="math inline">\(B\)</span> subsets. Let <span class="math inline">\(\pi_{-b}(\boldsymbol{\theta} \mid \mathbf{y})\)</span> denote the resulting posterior when subset <span class="math inline">\(b\)</span> is excluded. The average of these leave-one-out posteriors is denoted by:</p>
<p><span class="math display">\[
\bar{\pi}_{-b}(\boldsymbol{\theta} \mid \mathbf{y}) = \frac{1}{B} \sum_{b=1}^B \pi_{-b}(\boldsymbol{\theta} \mid \mathbf{y}).
\]</span></p>
<p>Then, the jackknife bias-corrected posterior is given by:</p>
<p><span class="math display">\[
\pi_{\text{jk}}(\boldsymbol{\theta} \mid \mathbf{y}) = B \cdot \pi_{\text{CMC}}(\boldsymbol{\theta} \mid \mathbf{y}) - (B - 1) \cdot \bar{\pi}_{-b}(\boldsymbol{\theta} \mid \mathbf{y}),
\]</span></p>
<p>where <span class="math inline">\(\pi_{\text{CMC}}(\boldsymbol{\theta} \mid \mathbf{y})\)</span> is the original CMC posterior based on all <span class="math inline">\(B\)</span> subsets.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Consensus Monte Carlo</strong></p>
<ol style="list-style-type: decimal">
<li><p>Divide the dataset into <span class="math inline">\(B\)</span> disjoint batches <span class="math inline">\(\mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_B\)</span></p></li>
<li><p>Run <span class="math inline">\(B\)</span> separate MCMC algorithms to sample <span class="math inline">\(\boldsymbol{\theta}_b^{(s)}\sim \pi(\boldsymbol{\theta}\mid \mathbf{y}_b)\)</span>, <span class="math inline">\(b=1,2,\dots,B\)</span>, and <span class="math inline">\(s=1,2,\dots,S\)</span> using the prior distribution <span class="math inline">\(\pi(\boldsymbol{\theta})^{1/B}\)</span></p></li>
<li><p>Combine the posterior draws using <span class="math inline">\(\boldsymbol{\theta}^{(s)}=\left(\sum_{b=1}^B\mathbf{w}_b\right)^{-1} \sum_{b=1}^B\mathbf{w}_b \boldsymbol{\theta}^{(s)}_b\)</span> using <span class="math inline">\(\mathbf{w}_b = \operatorname{Var}^{-1}(\boldsymbol{\theta} \mid \mathbf{y}_b)\)</span><br />
</p></li>
</ol>
</div>
</div>
<p>The main difficulty is how to effectively merge the subposterior distributions, especially when their supports are not well-aligned. This misalignment can lead to poor scalability with an increasing number of batches. Moreover, most theoretical guarantees for these methods are asymptotic in the size of each batch, which may limit their performance in practice <span class="citation">(<a href="#ref-bardenet2017markov">Bardenet, Doucet, and Holmes 2017</a>)</span>.</p>
</div>
<div id="sec12_52" class="section level3 hasAnchor" number="12.5.2">
<h3><span class="header-section-number">12.5.2</span> Subsampling-based algorithms<a href="sec12_5.html#sec12_52" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative to divide-and-conquer methods is to avoid evaluating the likelihood over all observations, which requires <span class="math inline">\(O(N)\)</span> operations. Instead, the likelihood is approximated using a smaller subset of observations, <span class="math inline">\(n \ll N\)</span>, in order to reduce the computational burden of the algorithm. The starting point is the log-likelihood function for <span class="math inline">\(N\)</span> independent observations:
<span class="math display">\[
\log p(\mathbf{y} \mid \boldsymbol{\theta}) = \sum_{i=1}^N \log p(y_i \mid \boldsymbol{\theta}).
\]</span>
The literature has focused on the log-likelihood because it is a sum over independent contributions, which is analogous to the problem of estimating a population total.</p>
<p>A class of subsampling methods relies on estimating the marginal likelihood via the <em>pseudo-marginal</em> approach. Examples include the confidence sampler <span class="citation">(<a href="#ref-bardenet2014towards">Bardenet, Doucet, and Holmes 2014</a>)</span>, the Firefly Monte Carlo algorithm <span class="citation">(<a href="#ref-Maclaurin2015">Maclaurin and Adams 2015</a>)</span>, whose relationship to subsampling MCMC is formally established in <span class="citation">Bardenet, Doucet, and Holmes (<a href="#ref-bardenet2017markov">2017</a>)</span>, and approaches using data-expanded and parameter-expanded control variates <span class="citation">(<a href="#ref-quiroz2019speeding">Quiroz et al. 2019</a>)</span>.</p>
<p>The intuition behind the pseudo-marginal method is straightforward: introduce a set of auxiliary random variables <span class="math inline">\(\mathbf{z} \sim p(\mathbf{z})\)</span>, such that the marginal likelihood can be written as an expectation with respect to <span class="math inline">\(\mathbf{z}\)</span>:
<span class="math display">\[
\mathbb{E}_{\mathbf{z}}[p(\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z})] = \int_{\mathcal{Z}} p(\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z}) \, p(\mathbf{z}) \, d\mathbf{z} = p(\mathbf{y} \mid \boldsymbol{\theta}).
\]</span>
This implies that
<span class="math display">\[
\hat{p}(\mathbf{y} \mid \boldsymbol{\theta}) = \frac{1}{S} \sum_{s=1}^S p(\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z}^{(s)}),
\]</span>
where <span class="math inline">\(\mathbf{z}^{(s)} \sim p(\mathbf{z})\)</span>, is an unbiased estimator of the marginal likelihood.</p>
<p>As a result, the pseudo-marginal method enables exact simulation-based inference for <span class="math inline">\(p(\mathbf{y} \mid \boldsymbol{\theta})\)</span> in settings where the likelihood cannot be evaluated analytically <span class="citation">(<a href="#ref-andrieu2009pseudoefficient">Andrieu and Roberts 2009</a>)</span>, for instance, in non-linear random effects models (see also approximate methods such as <em>approximate Bayesian computation</em> and <em>Bayesian synthetic likelihood</em> in Chapter <a href="Chap14.html#Chap14">14</a>). <span class="citation">Andrieu and Roberts (<a href="#ref-andrieu2009pseudoefficient">2009</a>)</span> show that replacing the likelihood with an unbiased and positive estimator within the Metropolis–Hastings (MH) algorithm yields samples from the correct posterior distribution.</p>
<p>The pseudo-marginal likelihood approach can also be applied in settings where the sample size is so large that evaluating the full likelihood at each iteration of an MCMC algorithm becomes computationally prohibitive. In such cases, the likelihood can be approximated using a small subset of observations, <span class="math inline">\(n \ll N\)</span>. The choice of the subset size <span class="math inline">\(n\)</span> is particularly important, as it directly affects the variance of the likelihood estimator, which in turn is critical to ensuring an efficient Metropolis–Hastings (MH) algorithm.</p>
<p>In particular, a likelihood estimator with high variance may result in an accepted draw that overestimates the likelihood. As a consequence, subsequent proposals are unlikely to be accepted, causing the algorithm to become stuck and leading to a very low acceptance rate. Therefore, the choice of <span class="math inline">\(n\)</span> determines the computational efficiency of the algorithm: a small <span class="math inline">\(n\)</span> increases the estimator’s variance, which reduces the acceptance rate, whereas a large <span class="math inline">\(n\)</span> increases the number of likelihood evaluations per iteration.</p>
<p><span class="citation">Quiroz et al. (<a href="#ref-quiroz2018subsampling">2018</a>)</span> recommend targeting a likelihood estimator variance between 1 and 3.3 to optimize computational efficiency, as supported by the findings of <span class="citation">Pitt et al. (<a href="#ref-pitt2012some">2012</a>)</span>.</p>
<p>Let <span class="math inline">\(\ell_i(y_i \mid \boldsymbol{\theta}) = \log p(y_i \mid \boldsymbol{\theta})\)</span> denote the contribution of the <span class="math inline">\(i\)</span>-th observation to the log-likelihood, and let <span class="math inline">\(z_1, \dots, z_N\)</span> be latent binary variables such that <span class="math inline">\(z_i = 1\)</span> indicates that <span class="math inline">\(y_i\)</span> is included in a subsample of size <span class="math inline">\(n\)</span>, selected without replacement. Then, an unbiased estimator of the log-likelihood is given by
<span class="math display">\[
\hat{\ell}(\mathbf{y} \mid \boldsymbol{\theta}) = \frac{N}{n} \sum_{i=1}^N \ell_i(y_i \mid \boldsymbol{\theta}) z_i.
\]</span></p>
<p>However, note that what we require is an unbiased estimator of the likelihood, not the log-likelihood. Consequently, a bias correction is needed:
<span class="math display">\[
\hat{p}(\mathbf{y} \mid \boldsymbol{\theta}) = \exp\left\{ \hat{\ell}(\mathbf{y} \mid \boldsymbol{\theta}) - \frac{1}{2} \sigma^2_{\hat{\ell}}(\boldsymbol{\theta}) \right\},
\]</span>
where <span class="math inline">\(\sigma^2_{\hat{\ell}}(\boldsymbol{\theta})\)</span> denotes the variance of <span class="math inline">\(\hat{\ell}(\mathbf{y} \mid \boldsymbol{\theta})\)</span> <span class="citation">(<a href="#ref-ceperley1999penalty">Ceperley and Dewing 1999</a>)</span>. This correction is exact if <span class="math inline">\(\sigma^2_{\hat{\ell}}(\boldsymbol{\theta})\)</span> is known and <span class="math inline">\(\hat{\ell}(\mathbf{y} \mid \boldsymbol{\theta})\)</span> follows a normal distribution.</p>
<p>Given the importance of controlling the variance of the log-likelihood estimator in subsampling methods, and the limitations of simple random sampling in achieving low variability, <span class="citation">Quiroz et al. (<a href="#ref-quiroz2019speeding">2019</a>)</span> propose a highly efficient, unbiased estimator of the log-likelihood based on <em>control variates</em>, specifically through data-expanded and parameter-expanded control variates.</p>
<p>The key idea is to construct a function <span class="math inline">\(q_i(\boldsymbol{\theta})\)</span> that is highly correlated with the log-likelihood contribution <span class="math inline">\(\ell_i(y_i \mid \boldsymbol{\theta})\)</span>, thereby stabilizing the log-likelihood estimator. In particular, <span class="citation">Quiroz et al. (<a href="#ref-quiroz2019speeding">2019</a>)</span> introduce a <em>difference estimator</em> of the form:
<span class="math display">\[
\hat{\ell}_{\mathrm{DE}}(\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z}) = \sum_{i=1}^N q_i(\boldsymbol{\theta}) + \frac{N}{n} \sum_{i: z_i = 1} \left( \ell_i(y_i \mid \boldsymbol{\theta}) - q_i(\boldsymbol{\theta}) \right).
\]</span>
This estimator <span class="math inline">\(\hat{\ell}_{\mathrm{DE}}(\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z})\)</span> is unbiased for the full log-likelihood <span class="math inline">\(\log p(\mathbf{y} \mid \boldsymbol{\theta})\)</span>.</p>
<p><span class="citation">Quiroz et al. (<a href="#ref-quiroz2019speeding">2019</a>)</span> propose constructing <span class="math inline">\(q_i(\boldsymbol{\theta})\)</span> using a second-order Taylor expansion of the log-likelihood around a central value of <span class="math inline">\(\boldsymbol{\theta}\)</span>, such as the mode. An alternative approach is to perform a second-order Taylor expansion around the nearest centroid of each observation, where the centroids are obtained from a pre-clustering of the data.</p>
<p>The first approach can perform poorly when the current draw <span class="math inline">\(\boldsymbol{\theta}\)</span> is far from the central expansion point, leading to inaccurate approximations. The second approach encounters difficulties in high-dimensional settings due to the curse of dimensionality: many observations will be far from their assigned centroid. To address these issues, the authors propose an adaptive strategy: initialize the algorithm using data-expanded control variates, and switch to parameter-expanded control variates once the sampler reaches a region closer to the center of the parameter space.</p>
<p>It is important to note that this strategy targets an approximation to the posterior distribution, due to the small bias introduced by the difference estimator. However, this bias diminishes rapidly and has a negligible impact on the quality of the posterior inference.</p>
<p>Once a good estimator of the log-likelihood is obtained, meaning it has low variance, the likelihood can be recovered using the appropriate bias correction. This corrected likelihood estimator is then used within the acceptance probability of the Metropolis–Hastings algorithm (see Section <a href="sec51.html#sec512">4.1.2</a>), resulting in the so-called pseudo-marginal Metropolis–Hastings (PMMH) method. This strategy significantly reduces computational cost in tall data settings.</p>
<p>Another class of subsampling methods, which does not rely on the pseudo-marginal likelihood, consists of stochastic gradient MCMC algorithms. These methods are based on ideas from stochastic gradient descent <span class="citation">(<a href="#ref-robbins1951stochastic">Robbins and Monro 1951</a>)</span> and Langevin diffusion-based stochastic differential equations.</p>
<p>The starting point is the unnormalized posterior distribution:
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{\theta} \mid \mathbf{y}) \propto \pi(\boldsymbol{\theta}) \prod_{i=1}^{N} p(y_i \mid \boldsymbol{\theta})
&amp; = \exp\left\{ \sum_{i=1}^N \left[ \frac{1}{N} \log \pi(\boldsymbol{\theta}) + \log p(y_i \mid \boldsymbol{\theta}) \right] \right\}\\
&amp; = \exp\left\{ -\sum_{i=1}^N U_i(\boldsymbol{\theta}) \right\}\\
&amp; = \exp\left\{ -U(\boldsymbol{\theta}) \right\},
\end{align*}\]</span>
where <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^K\)</span>, <span class="math inline">\(U_i(\boldsymbol{\theta}) = -\frac{1}{N} \log \pi(\boldsymbol{\theta}) - \log p(y_i \mid \boldsymbol{\theta})\)</span>, and <span class="math inline">\(U(\boldsymbol{\theta}) = \sum_{i=1}^N U_i(\boldsymbol{\theta})\)</span> is assumed to be continuous and differentiable almost everywhere.</p>
<p>The advantage of this formulation is that, under mild regularity conditions <span class="citation">(<a href="#ref-roberts1996exponential">Roberts and Tweedie 1996</a>)</span>, the Langevin diffusion process
<span class="math display">\[
d\boldsymbol{\theta}(s) = -\frac{1}{2} \nabla U(\boldsymbol{\theta}(s))\,ds + d\mathbf{B}_s,
\]</span>
has <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \mathbf{y})\)</span> as its stationary distribution. Here, <span class="math inline">\(\nabla U(\boldsymbol{\theta}(s))\)</span> is the drift term, and <span class="math inline">\(\mathbf{B}_s\)</span> is a <span class="math inline">\(K\)</span>-dimensional Brownian motion.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Using an Euler-Maruyama discretization of the Langevin diffusion gives a proposal draw from the posterior:
<span class="math display">\[
\boldsymbol{\theta}^{(c)} = \boldsymbol{\theta}^{(s)} - \frac{\epsilon}{2} \nabla U(\boldsymbol{\theta}^{(s)}) + \boldsymbol{\psi},
\]</span>
where <span class="math inline">\(\boldsymbol{\psi} \sim \mathcal{N}(\mathbf{0}, \epsilon \mathbf{I}_K)\)</span> and <span class="math inline">\(\epsilon &gt; 0\)</span> is a suitably chosen step size (learning rate). This proposal is used within a Metropolis–Hastings algorithm (see Section <a href="sec51.html#sec512">4.1.2</a>) to correct for the discretization error introduced by the Euler approximation. This method is known as the <em>Metropolis-adjusted Langevin algorithm</em> (MALA) <span class="citation">(<a href="#ref-roberts1996exponential">Roberts and Tweedie 1996</a>)</span>.</p>
<p>A simpler variant, known as the <em>unadjusted Langevin algorithm</em> (ULA), omits the acceptance step. As a result, ULA produces a biased approximation of the posterior distribution. However, a major computational bottleneck in both MALA and ULA is the requirement to evaluate the full gradient <span class="math inline">\(\nabla U(\boldsymbol{\theta}) = \sum_{i=1}^N \nabla U_i(\boldsymbol{\theta})\)</span> at every iteration, which becomes computationally prohibitive when <span class="math inline">\(N\)</span> is large.</p>
<p>To overcome this limitation, <span class="citation">Welling and Teh (<a href="#ref-welling2011bayesian">2011</a>)</span> proposed the <em>Stochastic Gradient Langevin Dynamics</em> (SGLD), which replaces the full gradient with an unbiased estimate computed using a mini-batch of data. Given a random sample of size <span class="math inline">\(n \ll N\)</span>, the stochastic gradient estimate at iteration <span class="math inline">\(s\)</span> is:
<span class="math display" id="eq:grad">\[\begin{equation}
\hat{\nabla} U(\boldsymbol{\theta})^{(n)} = \frac{N}{n} \sum_{i \in \mathcal{S}_n} \nabla U_i(\boldsymbol{\theta}),
\tag{12.1}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{S}_n \subset \{1, 2, \dots, N\}\)</span> is a randomly selected subset of size <span class="math inline">\(n\)</span>, sampled without replacement.</p>
<p>Therefore,
<span class="math display">\[
\boldsymbol{\theta}^{(s+1)} = \boldsymbol{\theta}^{(s)} - \frac{\epsilon_s}{2} \hat{\nabla} U(\boldsymbol{\theta}^{(s)})^{(n)} + \boldsymbol{\psi}_s,
\]</span>
such that <span class="math inline">\(\sum_{s=1}^{\infty}\epsilon_s = \infty\)</span> and <span class="math inline">\(\sum_{s=1}^{\infty}\epsilon_s^2 &lt; \infty\)</span>. These conditions guarantee almost sure convergence: the former ensures continued exploration of the parameter space (no premature convergence), and the latter ensures that the cumulative noise remains bounded.</p>
<p><span class="citation">Teh, Thiery, and Vollmer (<a href="#ref-teh2016consistency">2016</a>)</span> formally show that, under verifiable assumptions, the SGLD algorithm is consistent. That is, given a test function <span class="math inline">\(\phi(\boldsymbol{\theta}): \mathbb{R}^K \rightarrow \mathbb{R}\)</span>,
<span class="math display">\[
\lim_{S \rightarrow \infty} \frac{\epsilon_1 \phi(\boldsymbol{\theta}_1) + \epsilon_2 \phi(\boldsymbol{\theta}_2) + \dots + \epsilon_S \phi(\boldsymbol{\theta}_S)}{\sum_{s=1}^S \epsilon_s} = \int_{\mathbb{R}^K} \phi(\boldsymbol{\theta}) \pi(\boldsymbol{\theta}) \, d\boldsymbol{\theta}.
\]</span></p>
<p>Moreover, the algorithm satisfies a central limit theorem: <span class="math inline">\(\lim_{S \rightarrow \infty} \pi_S(\phi(\boldsymbol{\theta})) = \pi(\phi(\boldsymbol{\theta}))\)</span>, and its asymptotic bias–variance decomposition is characterized by a functional of <span class="math inline">\(\epsilon_s\)</span>, such that the optimal step size that minimizes the asymptotic mean squared error is proportional to <span class="math inline">\(s^{-1/3}\)</span>. In the common practice of using a constant step size, it has been shown that the optimal choice to minimize the asymptotic mean squared error is of order <span class="math inline">\(S^{-1/3}\)</span> <span class="citation">(<a href="#ref-vollmer2016exploration">Vollmer, Zygalakis, and Teh 2016</a>)</span>. However, we recommend tuning this parameter based on the specific application, guided by the theoretical results presented here.</p>
<p>Importantly, this iterative process does not require the computation of acceptance probabilities, which significantly reduces the computational burden. Empirical evidence suggests that SGLD often outperforms the Metropolis–Hastings algorithm when applied to large datasets under a fixed computational budget <span class="citation">(<a href="#ref-li2016scalable">Li, Ahn, and Welling 2016</a>)</span>.</p>
<p>The following Algorithm summarizes the SGLD procedure <span class="citation">(<a href="#ref-nemeth2021stochastic">Nemeth and Fearnhead 2021</a>)</span>.</p>
<div class="algorithm">
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">
<p><strong>Algorithm: Stochastic gradient Langevin dynamic</strong></p>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(\boldsymbol{\theta}^{(0)}\)</span> and the step size schedule <span class="math inline">\(\epsilon_s\)</span></p></li>
<li><p>For <span class="math inline">\(s = 1, \dots, S\)</span>:</p></li>
</ol>
<ul>
<li>Draw <span class="math inline">\(\mathcal{S}_n\)</span> of size <span class="math inline">\(n\)</span> from <span class="math inline">\(i=\left\{1,2,\dots,N\right\}\)</span> without replacement
<ul>
<li>Calculate <span class="math inline">\(\hat{\nabla} U(\boldsymbol{\theta})^{(n)}\)</span> using <span class="math inline">\(\hat{\nabla} U(\boldsymbol{\theta})^{(n)} = \frac{N}{n} \sum_{i \in \mathcal{S}_n} \nabla U_i(\boldsymbol{\theta})\)</span></li>
<li>Draw <span class="math inline">\(\boldsymbol{\psi}_s\sim N(\mathbf{0},\epsilon_s\mathbf{I}_K)\)</span></li>
<li>Update <span class="math inline">\(\boldsymbol{\theta}^{(s+1)}\leftarrow \boldsymbol{\theta}^{(s)} -\frac{\epsilon_s}{2}\hat{\nabla} U(\boldsymbol{\theta}^{(s)})^{(n)}+\boldsymbol{\psi}_s\)</span></li>
</ul></li>
</ul>
<p>End for</p>
</div>
</div>
<p>A critical component of the SGLD algorithm is the estimation of the stochastic gradient (Equation <a href="sec12_5.html#eq:grad">(12.1)</a>), particularly because high variability in this estimator can lead to algorithmic instability, a challenge also encountered in pseudo-marginal methods, as described previously. To mitigate this issue, the literature also employs <em>control variates</em> to reduce the variance of the estimator. The core idea is to construct a simple function <span class="math inline">\(u_i(\boldsymbol{\theta})\)</span> that is highly correlated with <span class="math inline">\(\nabla U_i(\boldsymbol{\theta})\)</span> and has a known expectation. This correlation allows the fluctuations in <span class="math inline">\(u_i(\boldsymbol{\theta})\)</span> to “cancel out” some of the noise in <span class="math inline">\(\nabla U_i(\boldsymbol{\theta})\)</span>, thereby stabilizing the stochastic gradient estimates. Specifically,</p>
<p><span class="math display">\[
\sum_{i=1}^N \nabla U_i(\boldsymbol{\theta}) = \sum_{i=1}^N u_i(\boldsymbol{\theta}) + \sum_{i=1}^N \left( \nabla U_i(\boldsymbol{\theta}) - u_i(\boldsymbol{\theta}) \right),
\]</span></p>
<p>which leads to the following unbiased estimator:</p>
<p><span class="math display">\[
\sum_{i=1}^N u_i(\boldsymbol{\theta}) + \frac{N}{n} \sum_{i \in \mathcal{S}_n} \left( \nabla U_i(\boldsymbol{\theta}) - u_i(\boldsymbol{\theta}) \right).
\]</span></p>
<p>To construct effective control variates, one common strategy is to first approximate the posterior mode <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> using stochastic gradient descent (SGD), which serves as the initialization point for SGLD Algorithm. SGD proceeds via a stochastic approximation of the gradient:</p>
<p><span class="math display">\[
\boldsymbol{\theta}^{(s+1)} = \boldsymbol{\theta}^{(s)} - \epsilon_s \frac{1}{n} \sum_{i \in \mathcal{S}_n} \nabla U_i(\boldsymbol{\theta}^{(s)}).
\]</span></p>
<p>This approximation introduces stochasticity into the updates but significantly reduces computational cost.</p>
<p>Two commonly used learning rate (or step size) schedules are <span class="math inline">\(\epsilon_s = s^{-\kappa}\)</span> and <span class="math inline">\(\epsilon_s = \epsilon_0 / (1 + s / \tau)^{\kappa}\)</span>, where <span class="math inline">\(\epsilon_0\)</span> is the initial learning rate, <span class="math inline">\(\tau\)</span> is a stability constant that slows down early decay (larger values lead to more stable early behavior), and <span class="math inline">\(\kappa \in (0.5, 1]\)</span> controls the long-run decay rate. If <span class="math inline">\(\kappa\)</span> is too large, the learning rate decays too quickly and the algorithm may stagnate. Conversely, if <span class="math inline">\(\kappa\)</span> is too small, the algorithm may remain unstable or fail to converge.</p>
<p>An important distinction to note is that SGLD operates with gradient <em>sums</em>, while SGD typically uses <em>averages</em>. This distinction affects how step sizes and noise scaling should be interpreted in practice.</p>
<p>After convergence, we obtain a reliable estimate of the posterior mode <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>. Based on this, we define the control variate as <span class="math inline">\(u_i(\boldsymbol{\theta}) = \nabla U_i(\hat{\boldsymbol{\theta}})\)</span>. The resulting control variate estimator of the gradient is:</p>
<p><span class="math display">\[
\hat{\nabla}_{\text{cv}} U(\boldsymbol{\theta}) = \sum_{i=1}^N \nabla U_i(\hat{\boldsymbol{\theta}}) + \frac{N}{n} \sum_{i \in \mathcal{S}_n} \left( \nabla U_i(\boldsymbol{\theta}) - \nabla U_i(\hat{\boldsymbol{\theta}}) \right).
\]</span></p>
<p><strong>Example: Simulation exercise to study the performance of CMC and SGLD</strong></p>
<p>In this example, we follow the logistic regression simulation setup introduced by <span class="citation">Nemeth and Fearnhead (<a href="#ref-nemeth2021stochastic">2021</a>)</span>:</p>
<p><span class="math display">\[
P(y_i = 1 \mid \boldsymbol{\beta}, \mathbf{x}_i) = \frac{\exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\}}{1 + \exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\}},
\]</span></p>
<p>with log-likelihood function given by:</p>
<p><span class="math display">\[
\log p(\mathbf{y} \mid \boldsymbol{\beta}, \mathbf{x})
= \sum_{i=1}^N y_i \left( \mathbf{x}_i^{\top} \boldsymbol{\beta} - \log \left(1 + \exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\} \right) \right)
+ (1 - y_i) \left( - \log \left(1 + \exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\} \right) \right),
\]</span></p>
<p>which simplifies to:</p>
<p><span class="math display">\[
\log p(\mathbf{y} \mid \boldsymbol{\beta}, \mathbf{x}) = \sum_{i=1}^N y_i \mathbf{x}_i^{\top} \boldsymbol{\beta} - \log \left(1 + \exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\} \right).
\]</span></p>
<p>This implies that the gradient vector is:</p>
<p><span class="math display">\[
\nabla \log p(\mathbf{y} \mid \boldsymbol{\beta}, \mathbf{x}) = \sum_{i=1}^N \left(y_i - \frac{\exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\}}{1+\exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\}}\right)\mathbf{x}_i.
\]</span></p>
<p>We assume a prior distribution for <span class="math inline">\(\boldsymbol{\beta} \sim \mathcal{N}(\mathbf{0}, 10 \mathbf{I}_K)\)</span>, leading to the log-prior:</p>
<p><span class="math display">\[
\log \pi(\boldsymbol{\beta}) = -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log\left( \lvert 10 \mathbf{I}_K \rvert \right) - \frac{1}{2} \boldsymbol{\beta}^{\top} (10^{-1} \mathbf{I}_K) \boldsymbol{\beta}.
\]</span></p>
<p>The gradient of the log-prior is:</p>
<p><span class="math display">\[
\nabla \log \pi(\boldsymbol{\beta}) = -\frac{1}{10}\boldsymbol{\beta}.
\]</span></p>
<p>Also note that:</p>
<p><span class="math display">\[\begin{align*}
\pi(\boldsymbol{\beta})^{1/B} &amp; \propto \left\{\exp\left(-\frac{1}{2 \cdot 10} \boldsymbol{\beta}^\top \boldsymbol{\beta}\right)\right\}^{1/B}\\
&amp; = \exp\left(-\frac{1}{2 \cdot 10 \cdot B} \boldsymbol{\beta}^\top \boldsymbol{\beta} \right).
\end{align*}\]</span></p>
<p>This implies that, when implementing CMC, the prior variance must be scaled by the number of batches <span class="math inline">\(B\)</span>. That is, each subposterior should use a prior with variance <span class="math inline">\(10 \cdot B\)</span> so that the product of the <span class="math inline">\(B\)</span> subposteriors reconstructs the correct full posterior.</p>
<p>We set <span class="math inline">\(K = 10\)</span>, <span class="math inline">\(\boldsymbol{\beta} = 0.5 \cdot \mathbf{1}_K\)</span>, and <span class="math inline">\(N = 10^5\)</span>. The covariates <span class="math inline">\(\mathbf{x}_i \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma})\)</span>, where the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}^{(i,j)} = U[-\rho, \rho]^{|i-j|}\)</span> with <span class="math inline">\(\rho = 0.4\)</span>, and <span class="math inline">\(\mathbf{1}_K\)</span> denotes a <span class="math inline">\(K\)</span>-dimensional vector of ones.</p>
<p>We run 2,000 MCMC iterations initialized at zero, and discard the first 500 as burn-in. We scale the regressors beforehand, as this is generally recommended to improve numerical stability and convergence. The following code simulates the model and sets the hyperparameters of the algorithms.</p>
<p>The following code implements SMC Algorithm, running five parallel MCMC chains and combining the resulting subposteriors using three different weighting schemes: equal weights, weights based on marginal variances, and weights based on the full covariance matrices.</p>
<p>By running the code, you can verify that the computational time of the CMC algorithm is lower than that of the Metropolis–Hastings algorithm. The first figure shows the posterior distributions of <span class="math inline">\(\beta_4\)</span> and <span class="math inline">\(\beta_5\)</span>. We observe that all three weighting schemes perform reasonably well, yielding posterior modes similar to those obtained from the full-data MCMC algorithm. However, the consensus Monte Carlo (CMC) methods produce more dispersed draws, particularly when using equal weights. In contrast, the weighting schemes based on marginal variances and the full covariance matrices yield comparable and more concentrated posterior distributions.</p>
<p>To implement the SGLD algorithm, we set <span class="math inline">\(n = 0.01 \cdot N\)</span>, and the step size to <span class="math inline">\(1 \times 10^{-4}\)</span>. The following code illustrates how to implement the SGLD algorithm.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>In Exercise 8, you are asked to implement the control variate version of SGLD. Begin by running 1,500 SGD iterations to locate the posterior mode. This mode should then be used as the initial value for a subsequent run of 1,000 SGLD iterations.</p>
<p>By running the code, you can verify that the computational time of the SGLD algorithm is lower than that of the Metropolis–Hastings algorithm. The second figure shows the posterior distributions of the fifth location parameter obtained from SGLD and Metropolis–Hastings. We observe that both modes are centered around the true population value; however, the SGLD distribution exhibits greater dispersion compared to the Metropolis–Hastings distribution.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="sec12_5.html#cb51-1" tabindex="-1"></a><span class="do">####### CMC and SDLD #######</span></span>
<span id="cb51-2"><a href="sec12_5.html#cb51-2" tabindex="-1"></a><span class="do">#### Simulation</span></span>
<span id="cb51-3"><a href="sec12_5.html#cb51-3" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()); <span class="fu">set.seed</span>(<span class="dv">10101</span>)</span>
<span id="cb51-4"><a href="sec12_5.html#cb51-4" tabindex="-1"></a><span class="fu">library</span>(mvtnorm); <span class="fu">library</span>(MCMCpack)</span></code></pre></div>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## 
## Attaching package: &#39;coda&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:Boom&#39;:
## 
##     thin</code></pre>
<pre><code>## ##
## ## Markov Chain Monte Carlo Package (MCMCpack)</code></pre>
<pre><code>## ## Copyright (C) 2003-2025 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park</code></pre>
<pre><code>## ##
## ## Support provided by the U.S. National Science Foundation</code></pre>
<pre><code>## ## (Grants SES-0350646 and SES-0350613)
## ##</code></pre>
<pre><code>## 
## Attaching package: &#39;MCMCpack&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:Boom&#39;:
## 
##     ddirichlet, dinvgamma, rdirichlet, rinvgamma</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="sec12_5.html#cb61-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2); <span class="fu">library</span>(dplyr)</span>
<span id="cb61-2"><a href="sec12_5.html#cb61-2" tabindex="-1"></a><span class="fu">library</span>(parallel); <span class="fu">library</span>(GGally)</span></code></pre></div>
<pre><code>## Warning: package &#39;GGally&#39; was built under R version 4.5.1</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="sec12_5.html#cb64-1" tabindex="-1"></a><span class="co">#--- Generate correlated covariates</span></span>
<span id="cb64-2"><a href="sec12_5.html#cb64-2" tabindex="-1"></a>genCovMat <span class="ot">&lt;-</span> <span class="cf">function</span>(K, <span class="at">rho =</span> <span class="fl">0.4</span>) {</span>
<span id="cb64-3"><a href="sec12_5.html#cb64-3" tabindex="-1"></a>    Sigma0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span>, K, K)</span>
<span id="cb64-4"><a href="sec12_5.html#cb64-4" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>K) {</span>
<span id="cb64-5"><a href="sec12_5.html#cb64-5" tabindex="-1"></a>        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(i <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb64-6"><a href="sec12_5.html#cb64-6" tabindex="-1"></a>            Sigma0[i, j] <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="sc">-</span>rho, rho)<span class="sc">^</span>(i <span class="sc">-</span> j)</span>
<span id="cb64-7"><a href="sec12_5.html#cb64-7" tabindex="-1"></a>        }</span>
<span id="cb64-8"><a href="sec12_5.html#cb64-8" tabindex="-1"></a>    }</span>
<span id="cb64-9"><a href="sec12_5.html#cb64-9" tabindex="-1"></a>    Sigma0 <span class="ot">&lt;-</span> Sigma0 <span class="sc">*</span> <span class="fu">t</span>(Sigma0)</span>
<span id="cb64-10"><a href="sec12_5.html#cb64-10" tabindex="-1"></a>    <span class="fu">diag</span>(Sigma0) <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb64-11"><a href="sec12_5.html#cb64-11" tabindex="-1"></a>    <span class="fu">return</span>(Sigma0)</span>
<span id="cb64-12"><a href="sec12_5.html#cb64-12" tabindex="-1"></a>}</span>
<span id="cb64-13"><a href="sec12_5.html#cb64-13" tabindex="-1"></a></span>
<span id="cb64-14"><a href="sec12_5.html#cb64-14" tabindex="-1"></a><span class="co">#--- Simulate logistic regression data</span></span>
<span id="cb64-15"><a href="sec12_5.html#cb64-15" tabindex="-1"></a>simulate_logit_data <span class="ot">&lt;-</span> <span class="cf">function</span>(K, N, beta_true) {</span>
<span id="cb64-16"><a href="sec12_5.html#cb64-16" tabindex="-1"></a>    Sigma0 <span class="ot">&lt;-</span> <span class="fu">genCovMat</span>(K)</span>
<span id="cb64-17"><a href="sec12_5.html#cb64-17" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(N, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>, K), <span class="at">sigma =</span> Sigma0)</span>
<span id="cb64-18"><a href="sec12_5.html#cb64-18" tabindex="-1"></a>    linpred <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true</span>
<span id="cb64-19"><a href="sec12_5.html#cb64-19" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>linpred))</span>
<span id="cb64-20"><a href="sec12_5.html#cb64-20" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N, <span class="dv">1</span>, p)</span>
<span id="cb64-21"><a href="sec12_5.html#cb64-21" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">y =</span> y, <span class="at">X =</span> X)</span>
<span id="cb64-22"><a href="sec12_5.html#cb64-22" tabindex="-1"></a>}</span>
<span id="cb64-23"><a href="sec12_5.html#cb64-23" tabindex="-1"></a></span>
<span id="cb64-24"><a href="sec12_5.html#cb64-24" tabindex="-1"></a><span class="co">#--- Parameters</span></span>
<span id="cb64-25"><a href="sec12_5.html#cb64-25" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb64-26"><a href="sec12_5.html#cb64-26" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb64-27"><a href="sec12_5.html#cb64-27" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fl">0.5</span>, K)</span>
<span id="cb64-28"><a href="sec12_5.html#cb64-28" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb64-29"><a href="sec12_5.html#cb64-29" tabindex="-1"></a>batch_prop <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb64-30"><a href="sec12_5.html#cb64-30" tabindex="-1"></a>Prior_prec <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb64-31"><a href="sec12_5.html#cb64-31" tabindex="-1"></a>n_iter <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb64-32"><a href="sec12_5.html#cb64-32" tabindex="-1"></a>burnin <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb64-33"><a href="sec12_5.html#cb64-33" tabindex="-1"></a>stepsize <span class="ot">&lt;-</span> <span class="fl">1e-4</span></span>
<span id="cb64-34"><a href="sec12_5.html#cb64-34" tabindex="-1"></a>k_target1 <span class="ot">&lt;-</span> <span class="dv">4</span>  <span class="co"># beta5</span></span>
<span id="cb64-35"><a href="sec12_5.html#cb64-35" tabindex="-1"></a>k_target2 <span class="ot">&lt;-</span> <span class="dv">5</span>  <span class="co"># beta5</span></span>
<span id="cb64-36"><a href="sec12_5.html#cb64-36" tabindex="-1"></a>ks <span class="ot">&lt;-</span> k_target1<span class="sc">:</span>k_target2</span>
<span id="cb64-37"><a href="sec12_5.html#cb64-37" tabindex="-1"></a><span class="co">#--- Simulate data</span></span>
<span id="cb64-38"><a href="sec12_5.html#cb64-38" tabindex="-1"></a>sim_data <span class="ot">&lt;-</span> <span class="fu">simulate_logit_data</span>(K, N, beta_true)</span>
<span id="cb64-39"><a href="sec12_5.html#cb64-39" tabindex="-1"></a>y <span class="ot">&lt;-</span> sim_data<span class="sc">$</span>y</span>
<span id="cb64-40"><a href="sec12_5.html#cb64-40" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">scale</span>(sim_data<span class="sc">$</span>X)</span>
<span id="cb64-41"><a href="sec12_5.html#cb64-41" tabindex="-1"></a></span>
<span id="cb64-42"><a href="sec12_5.html#cb64-42" tabindex="-1"></a><span class="co">#--- Run MCMCpack logit</span></span>
<span id="cb64-43"><a href="sec12_5.html#cb64-43" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(X)</span>
<span id="cb64-44"><a href="sec12_5.html#cb64-44" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span>K)</span>
<span id="cb64-45"><a href="sec12_5.html#cb64-45" tabindex="-1"></a>df<span class="sc">$</span>y <span class="ot">&lt;-</span> y</span>
<span id="cb64-46"><a href="sec12_5.html#cb64-46" tabindex="-1"></a>formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">&quot;y ~&quot;</span>, <span class="fu">paste</span>(<span class="fu">colnames</span>(df)[<span class="dv">1</span><span class="sc">:</span>K], <span class="at">collapse =</span> <span class="st">&quot; + &quot;</span>), <span class="st">&quot;-1&quot;</span>))</span>
<span id="cb64-47"><a href="sec12_5.html#cb64-47" tabindex="-1"></a>posterior_mh <span class="ot">&lt;-</span> <span class="fu">MCMClogit</span>(formula, <span class="at">data =</span> df, <span class="at">b0 =</span> <span class="dv">0</span>, <span class="at">B0 =</span> Prior_prec,</span>
<span id="cb64-48"><a href="sec12_5.html#cb64-48" tabindex="-1"></a><span class="at">burnin =</span> burnin, <span class="at">mcmc =</span> n_iter)</span>
<span id="cb64-49"><a href="sec12_5.html#cb64-49" tabindex="-1"></a>full_posterior <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(posterior_mh)[, <span class="dv">1</span><span class="sc">:</span>K]</span>
<span id="cb64-50"><a href="sec12_5.html#cb64-50" tabindex="-1"></a><span class="do">#### CMC</span></span>
<span id="cb64-51"><a href="sec12_5.html#cb64-51" tabindex="-1"></a><span class="co">#--- Split data</span></span>
<span id="cb64-52"><a href="sec12_5.html#cb64-52" tabindex="-1"></a>batch_ids <span class="ot">&lt;-</span> <span class="fu">split</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="fu">sort</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>B, <span class="at">length.out =</span> N)))</span>
<span id="cb64-53"><a href="sec12_5.html#cb64-53" tabindex="-1"></a><span class="co">#--- Function to run MCMC on a subset</span></span>
<span id="cb64-54"><a href="sec12_5.html#cb64-54" tabindex="-1"></a>mcmc_batch <span class="ot">&lt;-</span> <span class="cf">function</span>(batch_index, X, y, n_iter, burnin) {</span>
<span id="cb64-55"><a href="sec12_5.html#cb64-55" tabindex="-1"></a>    ids <span class="ot">&lt;-</span> batch_ids[[batch_index]]</span>
<span id="cb64-56"><a href="sec12_5.html#cb64-56" tabindex="-1"></a>    X_b <span class="ot">&lt;-</span> X[ids, ]</span>
<span id="cb64-57"><a href="sec12_5.html#cb64-57" tabindex="-1"></a>    y_b <span class="ot">&lt;-</span> y[ids]</span>
<span id="cb64-58"><a href="sec12_5.html#cb64-58" tabindex="-1"></a>    mcmc_out <span class="ot">&lt;-</span> <span class="fu">MCMClogit</span>(y_b <span class="sc">~</span> X_b <span class="sc">-</span> <span class="dv">1</span>, <span class="at">burnin =</span> burnin, <span class="at">mcmc =</span> n_iter, <span class="at">verbose =</span> <span class="dv">0</span>, <span class="at">b0 =</span> <span class="dv">0</span>, <span class="at">B0 =</span> Prior_prec <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>B))</span>
<span id="cb64-59"><a href="sec12_5.html#cb64-59" tabindex="-1"></a>    <span class="fu">return</span>(mcmc_out)</span>
<span id="cb64-60"><a href="sec12_5.html#cb64-60" tabindex="-1"></a>}</span>
<span id="cb64-61"><a href="sec12_5.html#cb64-61" tabindex="-1"></a><span class="co">#--- Run in parallel</span></span>
<span id="cb64-62"><a href="sec12_5.html#cb64-62" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(B)</span>
<span id="cb64-63"><a href="sec12_5.html#cb64-63" tabindex="-1"></a><span class="fu">clusterExport</span>(cl, <span class="fu">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="st">&quot;batch_ids&quot;</span>, <span class="st">&quot;n_iter&quot;</span>, <span class="st">&quot;burnin&quot;</span>, <span class="st">&quot;mcmc_batch&quot;</span>, <span class="st">&quot;Prior_prec&quot;</span>, <span class="st">&quot;B&quot;</span>))</span>
<span id="cb64-64"><a href="sec12_5.html#cb64-64" tabindex="-1"></a><span class="fu">clusterEvalQ</span>(cl, <span class="fu">library</span>(MCMCpack))</span></code></pre></div>
<pre><code>## [[1]]
##  [1] &quot;MCMCpack&quot;  &quot;MASS&quot;      &quot;coda&quot;      &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot;
##  [7] &quot;utils&quot;     &quot;datasets&quot;  &quot;methods&quot;   &quot;base&quot;     
## 
## [[2]]
##  [1] &quot;MCMCpack&quot;  &quot;MASS&quot;      &quot;coda&quot;      &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot;
##  [7] &quot;utils&quot;     &quot;datasets&quot;  &quot;methods&quot;   &quot;base&quot;     
## 
## [[3]]
##  [1] &quot;MCMCpack&quot;  &quot;MASS&quot;      &quot;coda&quot;      &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot;
##  [7] &quot;utils&quot;     &quot;datasets&quot;  &quot;methods&quot;   &quot;base&quot;     
## 
## [[4]]
##  [1] &quot;MCMCpack&quot;  &quot;MASS&quot;      &quot;coda&quot;      &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot;
##  [7] &quot;utils&quot;     &quot;datasets&quot;  &quot;methods&quot;   &quot;base&quot;     
## 
## [[5]]
##  [1] &quot;MCMCpack&quot;  &quot;MASS&quot;      &quot;coda&quot;      &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot;
##  [7] &quot;utils&quot;     &quot;datasets&quot;  &quot;methods&quot;   &quot;base&quot;</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="sec12_5.html#cb66-1" tabindex="-1"></a>chains <span class="ot">&lt;-</span> <span class="fu">parLapply</span>(cl, <span class="dv">1</span><span class="sc">:</span>B, <span class="cf">function</span>(b) <span class="fu">mcmc_batch</span>(b, X, y, n_iter, burnin))</span>
<span id="cb66-2"><a href="sec12_5.html#cb66-2" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span>
<span id="cb66-3"><a href="sec12_5.html#cb66-3" tabindex="-1"></a><span class="co"># Stack MCMC results</span></span>
<span id="cb66-4"><a href="sec12_5.html#cb66-4" tabindex="-1"></a>posteriors <span class="ot">&lt;-</span> <span class="fu">lapply</span>(chains, <span class="cf">function</span>(x) x[, <span class="dv">1</span><span class="sc">:</span>K])  <span class="co"># remove intercept if added</span></span>
<span id="cb66-5"><a href="sec12_5.html#cb66-5" tabindex="-1"></a><span class="co"># CMC posteriors</span></span>
<span id="cb66-6"><a href="sec12_5.html#cb66-6" tabindex="-1"></a>equal_cmc <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, posteriors) <span class="sc">/</span> B</span>
<span id="cb66-7"><a href="sec12_5.html#cb66-7" tabindex="-1"></a>invvar_cmc <span class="ot">&lt;-</span> {</span>
<span id="cb66-8"><a href="sec12_5.html#cb66-8" tabindex="-1"></a>    vars <span class="ot">&lt;-</span> <span class="fu">lapply</span>(posteriors, <span class="cf">function</span>(x) <span class="fu">apply</span>(x, <span class="dv">2</span>, var))</span>
<span id="cb66-9"><a href="sec12_5.html#cb66-9" tabindex="-1"></a>    weights <span class="ot">&lt;-</span> <span class="fu">lapply</span>(vars, <span class="cf">function</span>(v) <span class="dv">1</span> <span class="sc">/</span> v)</span>
<span id="cb66-10"><a href="sec12_5.html#cb66-10" tabindex="-1"></a>    weights_sum <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, weights)</span>
<span id="cb66-11"><a href="sec12_5.html#cb66-11" tabindex="-1"></a>    </span>
<span id="cb66-12"><a href="sec12_5.html#cb66-12" tabindex="-1"></a>    weighted_post <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, <span class="fu">Map</span>(<span class="cf">function</span>(x, w) <span class="fu">sweep</span>(x, <span class="dv">2</span>, w, <span class="st">&quot;*&quot;</span>), posteriors, weights))</span>
<span id="cb66-13"><a href="sec12_5.html#cb66-13" tabindex="-1"></a>    <span class="fu">sweep</span>(weighted_post, <span class="dv">2</span>, weights_sum, <span class="st">&quot;/&quot;</span>)</span>
<span id="cb66-14"><a href="sec12_5.html#cb66-14" tabindex="-1"></a>}</span>
<span id="cb66-15"><a href="sec12_5.html#cb66-15" tabindex="-1"></a>invmat_cmc <span class="ot">&lt;-</span> {</span>
<span id="cb66-16"><a href="sec12_5.html#cb66-16" tabindex="-1"></a>    covs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(posteriors, cov)                      <span class="co"># Get posterior covariances</span></span>
<span id="cb66-17"><a href="sec12_5.html#cb66-17" tabindex="-1"></a>    invs <span class="ot">&lt;-</span> <span class="fu">lapply</span>(covs, solve)                          <span class="co"># Invert each covariance</span></span>
<span id="cb66-18"><a href="sec12_5.html#cb66-18" tabindex="-1"></a>    weight_sum <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, invs)                      <span class="co"># Total weight matrix</span></span>
<span id="cb66-19"><a href="sec12_5.html#cb66-19" tabindex="-1"></a>    </span>
<span id="cb66-20"><a href="sec12_5.html#cb66-20" tabindex="-1"></a>    consensus <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_iter, <span class="at">ncol =</span> K)</span>
<span id="cb66-21"><a href="sec12_5.html#cb66-21" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb66-22"><a href="sec12_5.html#cb66-22" tabindex="-1"></a>        draws <span class="ot">&lt;-</span> <span class="fu">lapply</span>(posteriors, <span class="cf">function</span>(p) <span class="fu">matrix</span>(p[i, ], <span class="at">ncol =</span> <span class="dv">1</span>))  <span class="co"># Ensure column matrix</span></span>
<span id="cb66-23"><a href="sec12_5.html#cb66-23" tabindex="-1"></a>        weighted_sum <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(<span class="st">&quot;+&quot;</span>, <span class="fu">Map</span>(<span class="cf">function</span>(w, d) w <span class="sc">%*%</span> d, invs, draws))  <span class="co"># Weighted matrix product</span></span>
<span id="cb66-24"><a href="sec12_5.html#cb66-24" tabindex="-1"></a>        consensus[i, ] <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="fu">solve</span>(weight_sum, weighted_sum))  <span class="co"># Solve system and convert to vector</span></span>
<span id="cb66-25"><a href="sec12_5.html#cb66-25" tabindex="-1"></a>    }</span>
<span id="cb66-26"><a href="sec12_5.html#cb66-26" tabindex="-1"></a>    consensus</span>
<span id="cb66-27"><a href="sec12_5.html#cb66-27" tabindex="-1"></a>}</span>
<span id="cb66-28"><a href="sec12_5.html#cb66-28" tabindex="-1"></a><span class="co"># Combine all for plotting</span></span>
<span id="cb66-29"><a href="sec12_5.html#cb66-29" tabindex="-1"></a>build_df <span class="ot">&lt;-</span> <span class="cf">function</span>(mat, method) {</span>
<span id="cb66-30"><a href="sec12_5.html#cb66-30" tabindex="-1"></a>    df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(mat)</span>
<span id="cb66-31"><a href="sec12_5.html#cb66-31" tabindex="-1"></a>    <span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;x&quot;</span>, ks)</span>
<span id="cb66-32"><a href="sec12_5.html#cb66-32" tabindex="-1"></a>    df<span class="sc">$</span>method <span class="ot">&lt;-</span> method</span>
<span id="cb66-33"><a href="sec12_5.html#cb66-33" tabindex="-1"></a>    <span class="fu">return</span>(df)</span>
<span id="cb66-34"><a href="sec12_5.html#cb66-34" tabindex="-1"></a>}</span>
<span id="cb66-35"><a href="sec12_5.html#cb66-35" tabindex="-1"></a></span>
<span id="cb66-36"><a href="sec12_5.html#cb66-36" tabindex="-1"></a>df_full <span class="ot">&lt;-</span> <span class="fu">build_df</span>(full_posterior[,ks], <span class="st">&quot;overall&quot;</span>)</span>
<span id="cb66-37"><a href="sec12_5.html#cb66-37" tabindex="-1"></a>df_equal <span class="ot">&lt;-</span> <span class="fu">build_df</span>(equal_cmc[,ks], <span class="st">&quot;equal&quot;</span>)</span>
<span id="cb66-38"><a href="sec12_5.html#cb66-38" tabindex="-1"></a>df_scalar <span class="ot">&lt;-</span> <span class="fu">build_df</span>(invvar_cmc[,ks], <span class="st">&quot;scalar&quot;</span>)</span>
<span id="cb66-39"><a href="sec12_5.html#cb66-39" tabindex="-1"></a>df_matrix <span class="ot">&lt;-</span> <span class="fu">build_df</span>(invmat_cmc[,ks], <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb66-40"><a href="sec12_5.html#cb66-40" tabindex="-1"></a></span>
<span id="cb66-41"><a href="sec12_5.html#cb66-41" tabindex="-1"></a>df_plot <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df_full, df_matrix, df_scalar, df_equal)</span>
<span id="cb66-42"><a href="sec12_5.html#cb66-42" tabindex="-1"></a></span>
<span id="cb66-43"><a href="sec12_5.html#cb66-43" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb66-44"><a href="sec12_5.html#cb66-44" tabindex="-1"></a><span class="fu">ggpairs</span>(</span>
<span id="cb66-45"><a href="sec12_5.html#cb66-45" tabindex="-1"></a>df_plot,</span>
<span id="cb66-46"><a href="sec12_5.html#cb66-46" tabindex="-1"></a><span class="fu">aes</span>(<span class="at">color =</span> method, <span class="at">fill =</span> method, <span class="at">alpha =</span> <span class="fl">0.4</span>),</span>
<span id="cb66-47"><a href="sec12_5.html#cb66-47" tabindex="-1"></a><span class="at">upper =</span> <span class="fu">list</span>(<span class="at">continuous =</span> GGally<span class="sc">::</span><span class="fu">wrap</span>(<span class="st">&quot;density&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)),</span>
<span id="cb66-48"><a href="sec12_5.html#cb66-48" tabindex="-1"></a><span class="at">lower =</span> <span class="fu">list</span>(<span class="at">continuous =</span> GGally<span class="sc">::</span><span class="fu">wrap</span>(<span class="st">&quot;density&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>)),</span>
<span id="cb66-49"><a href="sec12_5.html#cb66-49" tabindex="-1"></a><span class="at">diag =</span> <span class="fu">list</span>(<span class="at">continuous =</span> GGally<span class="sc">::</span><span class="fu">wrap</span>(<span class="st">&quot;densityDiag&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>))</span>
<span id="cb66-50"><a href="sec12_5.html#cb66-50" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-7-1.svg" width="672" /></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="sec12_5.html#cb69-1" tabindex="-1"></a><span class="do">#### SGLD</span></span>
<span id="cb69-2"><a href="sec12_5.html#cb69-2" tabindex="-1"></a>SGLD_step <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, y, X, stepsize, batch_size, <span class="at">prior_var =</span> <span class="dv">10</span>) {</span>
<span id="cb69-3"><a href="sec12_5.html#cb69-3" tabindex="-1"></a>    N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X);   K <span class="ot">&lt;-</span> <span class="fu">length</span>(beta)</span>
<span id="cb69-4"><a href="sec12_5.html#cb69-4" tabindex="-1"></a>    ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="at">size =</span> batch_size, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb69-5"><a href="sec12_5.html#cb69-5" tabindex="-1"></a>    grad <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb69-6"><a href="sec12_5.html#cb69-6" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> ids) {</span>
<span id="cb69-7"><a href="sec12_5.html#cb69-7" tabindex="-1"></a>        xi <span class="ot">&lt;-</span> X[i, ]</span>
<span id="cb69-8"><a href="sec12_5.html#cb69-8" tabindex="-1"></a>        eta <span class="ot">&lt;-</span> <span class="fu">sum</span>(xi <span class="sc">*</span> beta)</span>
<span id="cb69-9"><a href="sec12_5.html#cb69-9" tabindex="-1"></a>        pi <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>eta))</span>
<span id="cb69-10"><a href="sec12_5.html#cb69-10" tabindex="-1"></a>        grad_i <span class="ot">&lt;-</span> <span class="sc">-</span>(y[i] <span class="sc">-</span> pi) <span class="sc">*</span> xi</span>
<span id="cb69-11"><a href="sec12_5.html#cb69-11" tabindex="-1"></a>        grad <span class="ot">&lt;-</span> grad <span class="sc">+</span> grad_i</span>
<span id="cb69-12"><a href="sec12_5.html#cb69-12" tabindex="-1"></a>    }</span>
<span id="cb69-13"><a href="sec12_5.html#cb69-13" tabindex="-1"></a>    grad <span class="ot">&lt;-</span> grad <span class="sc">/</span> batch_size <span class="sc">*</span> N</span>
<span id="cb69-14"><a href="sec12_5.html#cb69-14" tabindex="-1"></a>    grad <span class="ot">&lt;-</span> grad <span class="sc">+</span> beta <span class="sc">/</span> prior_var  <span class="co"># gradient of log-prior</span></span>
<span id="cb69-15"><a href="sec12_5.html#cb69-15" tabindex="-1"></a>    noise <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(K, <span class="dv">0</span>, <span class="fu">sqrt</span>(stepsize))</span>
<span id="cb69-16"><a href="sec12_5.html#cb69-16" tabindex="-1"></a>    beta_new <span class="ot">&lt;-</span> beta <span class="sc">-</span> <span class="fl">0.5</span> <span class="sc">*</span> stepsize <span class="sc">*</span> grad <span class="sc">+</span> noise</span>
<span id="cb69-17"><a href="sec12_5.html#cb69-17" tabindex="-1"></a>    <span class="fu">return</span>(beta_new)</span>
<span id="cb69-18"><a href="sec12_5.html#cb69-18" tabindex="-1"></a>}</span>
<span id="cb69-19"><a href="sec12_5.html#cb69-19" tabindex="-1"></a><span class="co">#--- SGLD algorithm</span></span>
<span id="cb69-20"><a href="sec12_5.html#cb69-20" tabindex="-1"></a>run_SGLD <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, stepsize, batch_prop, n_iter, burnin, <span class="at">beta_init =</span> <span class="cn">NULL</span>) {</span>
<span id="cb69-21"><a href="sec12_5.html#cb69-21" tabindex="-1"></a>    N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb69-22"><a href="sec12_5.html#cb69-22" tabindex="-1"></a>    K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb69-23"><a href="sec12_5.html#cb69-23" tabindex="-1"></a>    batch_size <span class="ot">&lt;-</span> <span class="fu">round</span>(batch_prop <span class="sc">*</span> N)</span>
<span id="cb69-24"><a href="sec12_5.html#cb69-24" tabindex="-1"></a>    </span>
<span id="cb69-25"><a href="sec12_5.html#cb69-25" tabindex="-1"></a>    beta_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n_iter <span class="sc">+</span> burnin, K)</span>
<span id="cb69-26"><a href="sec12_5.html#cb69-26" tabindex="-1"></a>    beta_mat[<span class="dv">1</span>, ] <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">is.null</span>(beta_init)) <span class="fu">rep</span>(<span class="dv">0</span>, K) <span class="cf">else</span> beta_init</span>
<span id="cb69-27"><a href="sec12_5.html#cb69-27" tabindex="-1"></a>    </span>
<span id="cb69-28"><a href="sec12_5.html#cb69-28" tabindex="-1"></a>    <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(n_iter <span class="sc">+</span> burnin)) {</span>
<span id="cb69-29"><a href="sec12_5.html#cb69-29" tabindex="-1"></a>        beta_mat[s, ] <span class="ot">&lt;-</span> <span class="fu">SGLD_step</span>(beta_mat[s <span class="sc">-</span> <span class="dv">1</span>, ], y, X, stepsize, batch_size)</span>
<span id="cb69-30"><a href="sec12_5.html#cb69-30" tabindex="-1"></a>    }</span>
<span id="cb69-31"><a href="sec12_5.html#cb69-31" tabindex="-1"></a>    beta_mat[(burnin <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(n_iter <span class="sc">+</span> burnin), ]</span>
<span id="cb69-32"><a href="sec12_5.html#cb69-32" tabindex="-1"></a>}</span>
<span id="cb69-33"><a href="sec12_5.html#cb69-33" tabindex="-1"></a><span class="co">#--- Run SGLD</span></span>
<span id="cb69-34"><a href="sec12_5.html#cb69-34" tabindex="-1"></a>posterior_sgld <span class="ot">&lt;-</span> <span class="fu">run_SGLD</span>(<span class="at">y =</span> y, <span class="at">X =</span> X, stepsize, batch_prop, n_iter, burnin)</span>
<span id="cb69-35"><a href="sec12_5.html#cb69-35" tabindex="-1"></a><span class="co">#--- Compare densities for beta5</span></span>
<span id="cb69-36"><a href="sec12_5.html#cb69-36" tabindex="-1"></a>df_plot <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb69-37"><a href="sec12_5.html#cb69-37" tabindex="-1"></a><span class="at">value =</span> <span class="fu">c</span>(posterior_sgld[, k_target2], posterior_mh[, k_target2]),</span>
<span id="cb69-38"><a href="sec12_5.html#cb69-38" tabindex="-1"></a><span class="at">method =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;SGLD&quot;</span>, <span class="st">&quot;MCMC&quot;</span>), <span class="at">each =</span> n_iter)</span>
<span id="cb69-39"><a href="sec12_5.html#cb69-39" tabindex="-1"></a>)</span>
<span id="cb69-40"><a href="sec12_5.html#cb69-40" tabindex="-1"></a><span class="fu">ggplot</span>(df_plot, <span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">fill =</span> method, <span class="at">color =</span> method)) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> beta_true[k_target2], <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Posterior density of &quot;</span>, beta[<span class="dv">5</span>])), <span class="at">x =</span> <span class="fu">expression</span>(beta[<span class="dv">5</span>]), <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-7-2.svg" width="672" /></p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-andrieu2009pseudoefficient" class="csl-entry">
Andrieu, Christophe, and Gareth O. Roberts. 2009. <span>“The Pseudo-Marginal Approach for Efficient Monte Carlo Computations.”</span> <em>Annals of Statistics</em> 37 (2): 697–725. <a href="https://doi.org/10.1214/07-AOS574">https://doi.org/10.1214/07-AOS574</a>.
</div>
<div id="ref-baker2019sgmcmc" class="csl-entry">
Baker, Jack, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. 2019. <span>“Sgmcmc: An r Package for Stochastic Gradient Markov Chain Monte Carlo.”</span> <em>Journal of Statistical Software</em> 91: 1–27.
</div>
<div id="ref-bardenet2014towards" class="csl-entry">
Bardenet, Rémi, Arnaud Doucet, and Chris Holmes. 2014. <span>“Towards Scaling up Markov Chain Monte Carlo: An Adaptive Subsampling Approach.”</span> In <em>International Conference on Machine Learning</em>, 405–13. PMLR.
</div>
<div id="ref-bardenet2017markov" class="csl-entry">
———. 2017. <span>“On Markov Chain Monte Carlo Methods for Tall Data.”</span> <em>Journal of Machine Learning Research</em> 18 (47): 1–43.
</div>
<div id="ref-ceperley1999penalty" class="csl-entry">
Ceperley, DM, and Mark Dewing. 1999. <span>“The Penalty Method for Random Walks with Uncertain Energies.”</span> <em>The Journal of Chemical Physics</em> 110 (20): 9812–20.
</div>
<div id="ref-huang2005sampling" class="csl-entry">
Huang, Zhen, and Andrew Gelman. 2005. <span>“Sampling for Bayesian Computation with Large Datasets.”</span> Technical Report. Department of Statistics, Columbia University.
</div>
<div id="ref-li2016scalable" class="csl-entry">
Li, Wei, Sangwoo Ahn, and Max Welling. 2016. <span>“Scalable MCMC for Mixed Membership Stochastic Blockmodels.”</span> In <em>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, edited by JMLR Workshop and Conference Proceedings, 51:723–31.
</div>
<div id="ref-Maclaurin2015" class="csl-entry">
Maclaurin, Dougal, and Ryan P. Adams. 2015. <span>“Firefly Monte Carlo: Exact MCMC with Subsets of Data.”</span> In <em>The Twenty-Fourth International Joint Conference on Artificial Intelligence</em>, 4279–95. IJCAI.
</div>
<div id="ref-minsker2015scalable" class="csl-entry">
Minsker, Stanislav. 2015. <span>“Scalable and Robust Bayesian Inference via the Median Posterior.”</span> Edited by Francis Bach and David Blei. <em>Proceedings of the 31st International Conference on Machine Learning (ICML)</em> 37: 1656–64.
</div>
<div id="ref-minsker2017robust" class="csl-entry">
Minsker, Stanislav, Subhabrata Srivastava, Lin Lin, and David B. Dunson. 2017. <span>“Robust and Scalable Bayes via a Median of Subset Posterior Measures.”</span> <em>Journal of Machine Learning Research</em> 18 (1): 4488–4527.
</div>
<div id="ref-neiswanger2013asymptotically" class="csl-entry">
Neiswanger, Willie, Chong Wang, and Eric P Xing. 2013. <span>“Asymptotically Exact, Embarrassingly Parallel MCMC.”</span> In <em>Proceedings of the Thirtieth International Conference on Machine Learning</em>, 623–32. PMLR.
</div>
<div id="ref-nemeth2021stochastic" class="csl-entry">
Nemeth, Christopher, and Paul Fearnhead. 2021. <span>“Stochastic Gradient Markov Chain Monte Carlo.”</span> <em>Journal of the American Statistical Association</em> 116 (533): 433–50.
</div>
<div id="ref-pitt2012some" class="csl-entry">
Pitt, Michael K, Ralph dos Santos Silva, Paolo Giordani, and Robert Kohn. 2012. <span>“On Some Properties of Markov Chain Monte Carlo Simulation Methods Based on the Particle Filter.”</span> <em>Journal of Econometrics</em> 171 (2): 134–51.
</div>
<div id="ref-quiroz2019speeding" class="csl-entry">
Quiroz, Matias, Robert Kohn, Mattias Villani, and Minh-Ngoc Tran. 2019. <span>“Speeding up MCMC by Efficient Data Subsampling.”</span> <em>Journal of the American Statistical Association</em>.
</div>
<div id="ref-quiroz2018subsampling" class="csl-entry">
Quiroz, Matias, Mattias Villani, Robert Kohn, Minh-Ngoc Tran, and Khue-Dung Dang. 2018. <span>“Subsampling MCMC—a Review for the Survey Statistician.”</span> <em>arXiv Preprint arXiv:1807.08409</em>.
</div>
<div id="ref-rendell2020global" class="csl-entry">
Rendell, Lewis J, Adam M Johansen, Anthony Lee, and Nick Whiteley. 2020. <span>“Global Consensus Monte Carlo.”</span> <em>Journal of Computational and Graphical Statistics</em> 30 (2): 249–59.
</div>
<div id="ref-robbins1951stochastic" class="csl-entry">
Robbins, Herbert, and Sutton Monro. 1951. <span>“A Stochastic Approximation Method.”</span> <em>The Annals of Mathematical Statistics</em>, 400–407.
</div>
<div id="ref-roberts1996exponential" class="csl-entry">
Roberts, Gareth O., and Richard L. Tweedie. 1996. <span>“Exponential Convergence of Langevin Distributions and Their Discrete Approximations.”</span> <em>Bernoulli</em> 2 (4): 341–63. <a href="https://doi.org/10.2307/3318418">https://doi.org/10.2307/3318418</a>.
</div>
<div id="ref-scott2016bayes" class="csl-entry">
Scott, Steven L., Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. 2016. <span>“Bayes and Big Data: The Consensus Monte Carlo Algorithm.”</span> <em>International Journal of Management Science and Engineering Management</em> 11 (2): 78–88. <a href="https://doi.org/10.1080/17509653.2016.1142191">https://doi.org/10.1080/17509653.2016.1142191</a>.
</div>
<div id="ref-scott2022bayes" class="csl-entry">
Scott, Steven L, Alexander W Blocker, Fernando V Bonassi, Hugh A Chipman, Edward I George, and Robert E McCulloch. 2022. <span>“Bayes and Big Data: The Consensus Monte Carlo Algorithm.”</span> In <em>Big Data and Information Theory</em>, 8–18. Routledge.
</div>
<div id="ref-teh2016consistency" class="csl-entry">
Teh, Yee Whye, Alexandre H. Thiery, and Sebastian J. Vollmer. 2016. <span>“Consistency and Fluctuations for Stochastic Gradient Langevin Dynamics.”</span> <em>Journal of Machine Learning Research</em> 17 (1): 193–225.
</div>
<div id="ref-vollmer2016exploration" class="csl-entry">
Vollmer, Sebastian J., Konstantinos C. Zygalakis, and Yee Whye Teh. 2016. <span>“Exploration of the (Non-)asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics.”</span> <em>Journal of Machine Learning Research</em> 17 (159): 1–48.
</div>
<div id="ref-wang2013parallelizing" class="csl-entry">
Wang, Xiangyu, and David B Dunson. 2013. <span>“Parallelizing MCMC via Weierstrass Sampler.”</span> <em>arXiv Preprint arXiv:1312.4605</em>.
</div>
<div id="ref-welling2011bayesian" class="csl-entry">
Welling, Max, and Yee W Teh. 2011. <span>“Bayesian Learning via Stochastic Gradient Langevin Dynamics.”</span> In <em>Proceedings of the 28th International Conference on Machine Learning (ICML-11)</em>, 681–88. Citeseer.
</div>
<div id="ref-wu2017average" class="csl-entry">
Wu, Changye, and Christian P Robert. 2017. <span>“Average of Recentered Parallel MCMC for Big Data.”</span> <em>arXiv Preprint arXiv:1706.04780</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>A Brownian motion is a continuous-time stochastic process that starts at zero, has independent increments with <span class="math inline">\(B(s) - B(t) \sim \mathcal{N}(0, s - t)\)</span>, and is continuous almost surely but nowhere differentiable.<a href="sec12_5.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>There is an <strong>R</strong> package called <em>sgmcmc</em>, developed by <span class="citation">Baker et al. (<a href="#ref-baker2019sgmcmc">2019</a>)</span>, which provides implementations of various stochastic gradient MCMC methods, including SGLD and SGHMC. However, this package depends on version 1 of the <em>tensorflow</em> package, while the current version is 2, and <em>sgmcmc</em> has not been updated on CRAN. We attempted to install the package from its GitHub repository using the <code>devtools::install\_github("STOR-i/sgmcmc"</code> command, but encountered compatibility issues due to conflicting TensorFlow versions.<a href="sec12_5.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12_4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="id_13_6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/13-RecentDev.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
