<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Multivariate linear regression: The conjugate normal-normal/inverse Wishart model | Introduction to Bayesian Data Modeling</title>
  <meta name="description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Multivariate linear regression: The conjugate normal-normal/inverse Wishart model | Introduction to Bayesian Data Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Multivariate linear regression: The conjugate normal-normal/inverse Wishart model | Introduction to Bayesian Data Modeling" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2025-05-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec43.html"/>
<link rel="next" href="sec45.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Chap1.html"><a href="Chap1.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec14.html"><a href="sec14.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chap2.html"><a href="Chap2.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and Frequentist approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="sec28.html"><a href="sec28.html"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chap3.html"><a href="Chap3.html"><i class="fa fa-check"></i><b>3</b> Cornerstone models: Conjugate families</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>3.1</b> Motivation of conjugate families</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec41.html"><a href="sec41.html#examples-of-exponential-family-distributions"><i class="fa fa-check"></i><b>3.1.1</b> Examples of exponential family distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>3.2</b> Conjugate prior to exponential family</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec42.html"><a href="sec42.html#sec421"><i class="fa fa-check"></i><b>3.2.1</b> Examples: Theorem 4.2.1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec43.html"><a href="sec43.html"><i class="fa fa-check"></i><b>3.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="3.4" data-path="sec44.html"><a href="sec44.html"><i class="fa fa-check"></i><b>3.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="3.5" data-path="sec45.html"><a href="sec45.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="sec46.html"><a href="sec46.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chap4.html"><a href="Chap4.html"><i class="fa fa-check"></i><b>4</b> Simulation methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec51.html"><a href="sec51.html"><i class="fa fa-check"></i><b>4.1</b> Markov Chain Monte Carlo methods</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec51.html"><a href="sec51.html#sec511"><i class="fa fa-check"></i><b>4.1.1</b> Gibbs sampler</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec51.html"><a href="sec51.html#sec512"><i class="fa fa-check"></i><b>4.1.2</b> Metropolis-Hastings</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec51.html"><a href="sec51.html#sec513"><i class="fa fa-check"></i><b>4.1.3</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec52.html"><a href="sec52.html"><i class="fa fa-check"></i><b>4.2</b> Importance sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sec53.html"><a href="sec53.html"><i class="fa fa-check"></i><b>4.3</b> Particle filtering</a></li>
<li class="chapter" data-level="4.4" data-path="sec54.html"><a href="sec54.html"><i class="fa fa-check"></i><b>4.4</b> Convergence diagnostics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec54.html"><a href="sec54.html#numerical-standard-error"><i class="fa fa-check"></i><b>4.4.1</b> Numerical standard error</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec54.html"><a href="sec54.html#effective-number-of-simulation-draws"><i class="fa fa-check"></i><b>4.4.2</b> Effective number of simulation draws</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec54.html"><a href="sec54.html#tests-of-convergence"><i class="fa fa-check"></i><b>4.4.3</b> Tests of convergence</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec54.html"><a href="sec54.html#checking-for-errors-in-the-posterior-simulator"><i class="fa fa-check"></i><b>4.4.4</b> Checking for errors in the posterior simulator</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec55.html"><a href="sec55.html"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="sec56.html"><a href="sec56.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Chap5.html"><a href="Chap5.html"><i class="fa fa-check"></i><b>5</b> Graphical user interface</a>
<ul>
<li class="chapter" data-level="5.1" data-path="secGUI1.html"><a href="secGUI1.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="secGUI2.html"><a href="secGUI2.html"><i class="fa fa-check"></i><b>5.2</b> Univariate models</a></li>
<li class="chapter" data-level="5.3" data-path="secGUI3.html"><a href="secGUI3.html"><i class="fa fa-check"></i><b>5.3</b> Multivariate models</a></li>
<li class="chapter" data-level="5.4" data-path="secGUI4.html"><a href="secGUI4.html"><i class="fa fa-check"></i><b>5.4</b> Time series models</a></li>
<li class="chapter" data-level="5.5" data-path="secGUI5.html"><a href="secGUI5.html"><i class="fa fa-check"></i><b>5.5</b> Longitudinal/panel models</a></li>
<li class="chapter" data-level="5.6" data-path="secGUI6.html"><a href="secGUI6.html"><i class="fa fa-check"></i><b>5.6</b> Bayesian model average</a></li>
<li class="chapter" data-level="5.7" data-path="secGUI7.html"><a href="secGUI7.html"><i class="fa fa-check"></i><b>5.7</b> Help</a></li>
<li class="chapter" data-level="5.8" data-path="secGUI8.html"><a href="secGUI8.html"><i class="fa fa-check"></i><b>5.8</b> Warning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Chap6.html"><a href="Chap6.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="sec61.html"><a href="sec61.html"><i class="fa fa-check"></i><b>6.1</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="6.2" data-path="sec62.html"><a href="sec62.html"><i class="fa fa-check"></i><b>6.2</b> The logit model</a></li>
<li class="chapter" data-level="6.3" data-path="sec63.html"><a href="sec63.html"><i class="fa fa-check"></i><b>6.3</b> The probit model</a></li>
<li class="chapter" data-level="6.4" data-path="sec64.html"><a href="sec64.html"><i class="fa fa-check"></i><b>6.4</b> The multinomial probit model</a></li>
<li class="chapter" data-level="6.5" data-path="sec65.html"><a href="sec65.html"><i class="fa fa-check"></i><b>6.5</b> The multinomial logit model</a></li>
<li class="chapter" data-level="6.6" data-path="sec66.html"><a href="sec66.html"><i class="fa fa-check"></i><b>6.6</b> Ordered probit model</a></li>
<li class="chapter" data-level="6.7" data-path="negative-binomial-model.html"><a href="negative-binomial-model.html"><i class="fa fa-check"></i><b>6.7</b> Negative binomial model</a></li>
<li class="chapter" data-level="6.8" data-path="sec68.html"><a href="sec68.html"><i class="fa fa-check"></i><b>6.8</b> Tobit model</a></li>
<li class="chapter" data-level="6.9" data-path="sec69.html"><a href="sec69.html"><i class="fa fa-check"></i><b>6.9</b> Quantile regression</a></li>
<li class="chapter" data-level="6.10" data-path="sec610.html"><a href="sec610.html"><i class="fa fa-check"></i><b>6.10</b> Bayesian bootstrap regression</a></li>
<li class="chapter" data-level="6.11" data-path="sec611.html"><a href="sec611.html"><i class="fa fa-check"></i><b>6.11</b> Summary</a></li>
<li class="chapter" data-level="6.12" data-path="sec612.html"><a href="sec612.html"><i class="fa fa-check"></i><b>6.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="Chap7.html"><a href="Chap7.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec71.html"><a href="sec71.html"><i class="fa fa-check"></i><b>7.1</b> Multivariate regression</a></li>
<li class="chapter" data-level="7.2" data-path="sec72.html"><a href="sec72.html"><i class="fa fa-check"></i><b>7.2</b> Seemingly Unrelated Regression</a></li>
<li class="chapter" data-level="7.3" data-path="sec73.html"><a href="sec73.html"><i class="fa fa-check"></i><b>7.3</b> Instrumental variable</a></li>
<li class="chapter" data-level="7.4" data-path="sec74.html"><a href="sec74.html"><i class="fa fa-check"></i><b>7.4</b> Multivariate probit model</a></li>
<li class="chapter" data-level="7.5" data-path="sec75.html"><a href="sec75.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="sec76.html"><a href="sec76.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Chap8.html"><a href="Chap8.html"><i class="fa fa-check"></i><b>8</b> Time series</a>
<ul>
<li class="chapter" data-level="8.1" data-path="sec81.html"><a href="sec81.html"><i class="fa fa-check"></i><b>8.1</b> State-space representation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="sec81.html"><a href="sec81.html#gaussian-linear-state-space-models"><i class="fa fa-check"></i><b>8.1.1</b> Gaussian linear state-space models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sec82.html"><a href="sec82.html"><i class="fa fa-check"></i><b>8.2</b> ARMA processes</a></li>
<li class="chapter" data-level="8.3" data-path="sec83.html"><a href="sec83.html"><i class="fa fa-check"></i><b>8.3</b> Stochastic volatility models</a></li>
<li class="chapter" data-level="8.4" data-path="sec84.html"><a href="sec84.html"><i class="fa fa-check"></i><b>8.4</b> Vector Autoregressive models</a></li>
<li class="chapter" data-level="8.5" data-path="sec85.html"><a href="sec85.html"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
<li class="chapter" data-level="8.6" data-path="sec86.html"><a href="sec86.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="Chap9.html"><a href="Chap9.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sec91.html"><a href="sec91.html"><i class="fa fa-check"></i><b>9.1</b> Normal model</a></li>
<li class="chapter" data-level="9.2" data-path="sec92.html"><a href="sec92.html"><i class="fa fa-check"></i><b>9.2</b> Logit model</a></li>
<li class="chapter" data-level="9.3" data-path="sec93.html"><a href="sec93.html"><i class="fa fa-check"></i><b>9.3</b> Poisson model</a></li>
<li class="chapter" data-level="9.4" data-path="sec94.html"><a href="sec94.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="sec95.html"><a href="sec95.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="Chap10.html"><a href="Chap10.html"><i class="fa fa-check"></i><b>10</b> Bayesian model averaging in variable selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="sec10_1.html"><a href="sec10_1.html"><i class="fa fa-check"></i><b>10.1</b> Foundation</a></li>
<li class="chapter" data-level="10.2" data-path="sec102.html"><a href="sec102.html"><i class="fa fa-check"></i><b>10.2</b> The Gaussian linear model</a></li>
<li class="chapter" data-level="10.3" data-path="sec103.html"><a href="sec103.html"><i class="fa fa-check"></i><b>10.3</b> Generalized linear models</a></li>
<li class="chapter" data-level="10.4" data-path="sec10_4.html"><a href="sec10_4.html"><i class="fa fa-check"></i><b>10.4</b> Calculating the marginal likelihood</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Savage-Dickey density ratio</a></li>
<li class="chapter" data-level="10.4.2" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Chib’s methods</a></li>
<li class="chapter" data-level="10.4.3" data-path="sec10_4.html"><a href="sec10_4.html#sec10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Gelfand-Dey method</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="sec10_5.html"><a href="sec10_5.html"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="sec10_6.html"><a href="sec10_6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="Chap11.html"><a href="Chap11.html"><i class="fa fa-check"></i><b>11</b> Semi-parametric and non-parametric models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="sec11_1.html"><a href="sec11_1.html"><i class="fa fa-check"></i><b>11.1</b> Mixture models</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="sec11_1.html"><a href="sec11_1.html#sec11_11"><i class="fa fa-check"></i><b>11.1.1</b> Finite Gaussian mixtures</a></li>
<li class="chapter" data-level="11.1.2" data-path="sec11_1.html"><a href="sec11_1.html#sec11_12"><i class="fa fa-check"></i><b>11.1.2</b> Direchlet processes</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="sec11_2.html"><a href="sec11_2.html"><i class="fa fa-check"></i><b>11.2</b> Splines</a></li>
<li class="chapter" data-level="11.3" data-path="sec11_3.html"><a href="sec11_3.html"><i class="fa fa-check"></i><b>11.3</b> Summary</a></li>
<li class="chapter" data-level="11.4" data-path="sec11_4.html"><a href="sec11_4.html"><i class="fa fa-check"></i><b>11.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="Chap12.html"><a href="Chap12.html"><i class="fa fa-check"></i><b>12</b> Machine learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="sec12_1.html"><a href="sec12_1.html"><i class="fa fa-check"></i><b>12.1</b> Cross validation and Bayes factors</a></li>
<li class="chapter" data-level="12.2" data-path="sec12_2.html"><a href="sec12_2.html"><i class="fa fa-check"></i><b>12.2</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="sec12_2.html"><a href="sec12_2.html#sec12_21"><i class="fa fa-check"></i><b>12.2.1</b> Bayesian LASSO</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec12_2.html"><a href="sec12_2.html#sec12_22"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic search variable selection</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec12_2.html"><a href="sec12_2.html#sec12_23"><i class="fa fa-check"></i><b>12.2.3</b> Non-local priors</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec12_3.html"><a href="sec12_3.html"><i class="fa fa-check"></i><b>12.3</b> Bayesian additive regression trees</a></li>
<li class="chapter" data-level="12.4" data-path="sec12_4.html"><a href="sec12_4.html"><i class="fa fa-check"></i><b>12.4</b> Gaussian processes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Chap13.html"><a href="Chap13.html"><i class="fa fa-check"></i><b>13</b> Causal inference</a>
<ul>
<li class="chapter" data-level="13.1" data-path="sec13_1.html"><a href="sec13_1.html"><i class="fa fa-check"></i><b>13.1</b> Instrumental variables</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="sec13_1.html"><a href="sec13_1.html#sec13_11"><i class="fa fa-check"></i><b>13.1.1</b> Semi-parametric IV model</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="sec13_2.html"><a href="sec13_2.html"><i class="fa fa-check"></i><b>13.2</b> Regression discontinuity design</a></li>
<li class="chapter" data-level="13.3" data-path="sec13_3.html"><a href="sec13_3.html"><i class="fa fa-check"></i><b>13.3</b> Regression kink design</a></li>
<li class="chapter" data-level="13.4" data-path="sec13_4.html"><a href="sec13_4.html"><i class="fa fa-check"></i><b>13.4</b> Synthetic control</a></li>
<li class="chapter" data-level="13.5" data-path="sec13_5.html"><a href="sec13_5.html"><i class="fa fa-check"></i><b>13.5</b> Difference in difference estimation</a></li>
<li class="chapter" data-level="13.6" data-path="sec13_6.html"><a href="sec13_6.html"><i class="fa fa-check"></i><b>13.6</b> Event Analysis</a></li>
<li class="chapter" data-level="13.7" data-path="sec13_7.html"><a href="sec13_7.html"><i class="fa fa-check"></i><b>13.7</b> Bayesian exponential tilted empirical likelihood</a></li>
<li class="chapter" data-level="13.8" data-path="sec13_8.html"><a href="sec13_8.html"><i class="fa fa-check"></i><b>13.8</b> Double-Debiased machine learning causal effects</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Chap14.html"><a href="Chap14.html"><i class="fa fa-check"></i><b>14</b> Approximate Bayesian methods</a>
<ul>
<li class="chapter" data-level="14.1" data-path="sec14_1.html"><a href="sec14_1.html"><i class="fa fa-check"></i><b>14.1</b> Simulation-based approaches</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="sec14_1.html"><a href="sec14_1.html#sec14_11"><i class="fa fa-check"></i><b>14.1.1</b> Approximate Bayesian computation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sec14_1.html"><a href="sec14_1.html#sec14_12"><i class="fa fa-check"></i><b>14.1.2</b> Bayesian synthetic likelihood</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sec14_2.html"><a href="sec14_2.html"><i class="fa fa-check"></i><b>14.2</b> Optimization approaches</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="sec14_2.html"><a href="sec14_2.html#sec14_21"><i class="fa fa-check"></i><b>14.2.1</b> Integrated nested Laplace approximations</a></li>
<li class="chapter" data-level="14.2.2" data-path="sec14_2.html"><a href="sec14_2.html#sec14_22"><i class="fa fa-check"></i><b>14.2.2</b> Variational Bayes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>14.3</b> Summary</a></li>
<li class="chapter" data-level="14.4" data-path="sec14_4.html"><a href="sec14_4.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Data Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec44" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model<a href="sec44.html#sec44" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s study the multivariate regression setting where there are <span class="math inline">\(N\)</span>-dimensional vectors <span class="math inline">\({\boldsymbol{y}}_m\)</span>, for <span class="math inline">\(m = 1, 2, \dots, M\)</span>, such that <span class="math inline">\({\boldsymbol{y}}_m = {\boldsymbol{X}} \boldsymbol{\beta}_m + \boldsymbol{\mu}_m\)</span>. Here, <span class="math inline">\({\boldsymbol{X}}\)</span> represents the set of common regressors, and <span class="math inline">\(\boldsymbol{\mu}_m\)</span> is the <span class="math inline">\(N\)</span>-dimensional vector of stochastic errors for each equation. We assume that <span class="math inline">\({\boldsymbol{U}} = [\boldsymbol{\mu}_1 \ \boldsymbol{\mu}_2 \ \dots \ \boldsymbol{\mu}_M] \sim MN_{N,M}({\boldsymbol{0}}, {\boldsymbol{I}}_N, {\boldsymbol{\Sigma}})\)</span>, which is a matrix variate normal distribution where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix of each <span class="math inline">\(i\)</span>-th row of <span class="math inline">\({\boldsymbol{U}}\)</span>, for <span class="math inline">\(i = 1, 2, \dots, N\)</span>, and we assume independence between the rows. Consequently, we have that <span class="math inline">\(vec({\boldsymbol{U}}) \sim N_{N \times M}({\boldsymbol{0}}, \boldsymbol{\Sigma} \otimes {\boldsymbol{I}}_N)\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>This framework can be written in matrix form
<span class="math display">\[\begin{align*}
    \underbrace{
        \begin{bmatrix}
            y_{11} &amp; y_{12} &amp; \dots &amp; y_{1M}\\
            y_{21} &amp; y_{22} &amp; \dots &amp; y_{2M}\\
            \vdots &amp; \vdots &amp; \dots &amp; \vdots\\
            y_{N1} &amp; y_{N2} &amp; \dots &amp; y_{NM}\\
    \end{bmatrix}}_{\boldsymbol{Y}}
    &amp;=
    \underbrace{\begin{bmatrix}
            x_{11} &amp; x_{12} &amp; \dots &amp; x_{1K}\\
            x_{21} &amp; x_{22} &amp; \dots &amp; x_{2K}\\
            \vdots &amp; \vdots &amp; \dots &amp; \vdots\\
            x_{N1} &amp; x_{N2} &amp; \dots &amp; x_{NK}\\
    \end{bmatrix}}_{\boldsymbol{X}}
    \underbrace{
        \begin{bmatrix}
            \beta_{11} &amp; \beta_{12} &amp; \dots &amp; \beta_{1M}\\
            \beta_{21} &amp; \beta_{22} &amp; \dots &amp; \beta_{2M}\\
            \vdots &amp; \vdots &amp; \dots &amp; \vdots\\
            \beta_{K1} &amp; \beta_{K2} &amp; \dots &amp; \beta_{KM}\\
    \end{bmatrix}}_{\boldsymbol{B}}\\
    &amp;+
    \underbrace{\begin{bmatrix}
            \mu_{11} &amp; \mu_{12} &amp; \dots &amp; \mu_{1M}\\
            \mu_{21} &amp; \mu_{22} &amp; \dots &amp; \mu_{2M}\\
            \vdots &amp; \vdots &amp; \dots &amp; \vdots\\
            \mu_{N1} &amp; \mu_{N2} &amp; \dots &amp; \mu_{NM}\\
    \end{bmatrix}}_{\boldsymbol{U}}.
\end{align*}\]</span></p>
<p>Therefore, <span class="math inline">\({\boldsymbol{Y}}\sim N_{N\times M}({\boldsymbol{X}}{\boldsymbol{B}},\boldsymbol{\Sigma}\otimes {\boldsymbol{I}}_N)\)</span>,<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>
<span class="math display">\[\begin{align*}
    p({\boldsymbol{Y}}\mid  {\boldsymbol{B}},{\boldsymbol{\Sigma}}, {\boldsymbol{X}})&amp;\propto |{{\boldsymbol \Sigma}}|^{-N/2}\exp\left\lbrace -\frac{1}{2}tr\left[({\boldsymbol{Y}}-{\boldsymbol{X}}{\boldsymbol{B}})^{\top}({\boldsymbol{Y}}-{\boldsymbol{X}}{\boldsymbol{B}}){{\boldsymbol \Sigma}}^{-1}\right]\right\rbrace
    \\
    &amp;=|{{\boldsymbol \Sigma}}|^{-N/2}\exp\left\lbrace -\frac{1}{2}tr\left[\left({\boldsymbol{S}}+({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}})\right){{\boldsymbol \Sigma}}^{-1}\right]\right\rbrace,
\end{align*}\]</span></p>
<p>where <span class="math inline">\({\boldsymbol{S}}= ({\boldsymbol{Y}}-{\boldsymbol{X}}\widehat{\boldsymbol{B}})^{\top}({\boldsymbol{Y}}-{\boldsymbol{X}}\widehat{\boldsymbol{B}})\)</span>, <span class="math inline">\(\widehat{\boldsymbol{B}}= ({\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}{\boldsymbol{X}}^{\top}{\boldsymbol{Y}}\)</span> (see Exercise 9).</p>
<p>The conjugate prior for this model is <span class="math inline">\(\pi({\boldsymbol{B}},{\boldsymbol{\Sigma}})=\pi({\boldsymbol{B}}\mid {\boldsymbol{\Sigma}})\pi({\boldsymbol{\Sigma}})\)</span> where <span class="math inline">\({\boldsymbol{B}}\mid {\boldsymbol \Sigma}\sim N_{K\times M}({\boldsymbol{B}}_{0},{\boldsymbol{V}}_{0},{\boldsymbol{\Sigma}})\)</span> and <span class="math inline">\({\boldsymbol{\Sigma}}\sim IW({\boldsymbol{\Psi}}_{0},\alpha_{0})\)</span>, that is,
<span class="math display">\[\begin{align*}
    \pi ({\boldsymbol{B}},{\boldsymbol{\Sigma}})\propto &amp;\left|{\boldsymbol{\Sigma}} \right|^{-K/2}\exp\left\lbrace -\frac{1}{2}tr\left[({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0}){\boldsymbol \Sigma}^{-1}\right]\right\rbrace \\
    &amp; \times \left|{\boldsymbol \Sigma} \right|^{-(\alpha_{0}+M+1)/2}\exp\left\lbrace -\frac{1}{2}tr \left[ {\boldsymbol{\Psi}}_{0} {\boldsymbol \Sigma}^{-1}\right] \right\rbrace.
\end{align*}\]</span></p>
<p>The posterior distribution is given by
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{B}},{\boldsymbol{\Sigma}}\mid {\boldsymbol{Y}},{\boldsymbol{X}})&amp;\propto  p({\boldsymbol{Y}}\mid {\boldsymbol{B}},{\boldsymbol{\Sigma}},{\boldsymbol{X}}) \pi({\boldsymbol{B}}\mid  {\boldsymbol \Sigma})\pi({\boldsymbol{\Sigma}})\\
    &amp;\propto \left|{\boldsymbol{\Sigma}} \right|^{-\frac{N+K+\alpha_{0}+M+1}{2}}\\
    &amp;\times\exp\left\lbrace -\frac{1}{2}tr\left[(\boldsymbol{\Psi}_{0}+{\boldsymbol{S}} +({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0})\right.\right.\\
    &amp;\left.\left.   +({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}))\boldsymbol{\Sigma}^{-1}\right]\right\rbrace .
\end{align*}\]</span>
Completing the squares on <span class="math inline">\({\boldsymbol{B}}\)</span> and collecting the remaining terms in the bracket yields</p>
<p><span class="math display">\[\begin{align*}
    {\boldsymbol{\Psi}}_{0}+{\boldsymbol{S}} +({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0})+({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}})
    &amp; = ({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)+{\boldsymbol{\Psi}}_n,
\end{align*}\]</span></p>
<p>where
<span class="math display">\[\begin{align*}
    {\boldsymbol{B}}_n = &amp;({\boldsymbol{V}}_{0}^{-1}+{\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}({\boldsymbol{V}}_{0}^{-1}{\boldsymbol{B}}_{0}+{\boldsymbol{X}}^{\top}{\boldsymbol{Y}})=({\boldsymbol{V}}_{0}^{-1}+{\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1}({\boldsymbol{V}}_{0}^{-1}{\boldsymbol{B}}_{0}+{\boldsymbol{X}}^{\top}{\boldsymbol{X}}\widehat{\boldsymbol{B}}),\\
    {\boldsymbol{V}}_n = &amp;({\boldsymbol{V}}_{0}^{-1}+{\boldsymbol{X}}^{\top}{\boldsymbol{X}})^{-1},\\
    {\boldsymbol{\Psi}}_n= &amp;{\boldsymbol{\Psi}}_{0}+{\boldsymbol{S}}+{\boldsymbol{B}}_{0}^{\top}{\boldsymbol{V}}_{0}^{-1}{\boldsymbol{B}}_{0}+\widehat{\boldsymbol{B}}^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}\widehat{\boldsymbol{B}}-{\boldsymbol{B}}_n^{\top}{\boldsymbol{V}}_n^{-1}{\boldsymbol{B}}_n.
\end{align*}\]</span>
Thus, the posterior distribution can be written as
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{B}},{\boldsymbol \Sigma}\mid  {\boldsymbol{Y}}, {\boldsymbol{X}})\propto &amp;\left|{\boldsymbol \Sigma} \right|^{-K/2}\exp\left\lbrace -\frac{1}{2} tr\left[({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)   {\boldsymbol \Sigma}^{-1}\right]\right\rbrace \\
    \times &amp; \left|{\boldsymbol \Sigma} \right|^{-\frac{N+\alpha_{0}+M+1}{2}}\exp\left\lbrace -\frac{1}{2} tr \left[ {\boldsymbol{\Psi}}_n{\boldsymbol \Sigma}^{-1}\right] \right\rbrace .
\end{align*}\]</span>
That is <span class="math inline">\(\pi({\boldsymbol{B}},{\boldsymbol \Sigma}\mid  {\boldsymbol{Y}}, {\boldsymbol{X}})=\pi ({\boldsymbol{B}}\mid  {\boldsymbol \Sigma},{\boldsymbol{Y}},{\boldsymbol{X}})\pi({\boldsymbol \Sigma}\mid  {\boldsymbol{Y}},{\boldsymbol{X}})\)</span> where <span class="math inline">\({\boldsymbol{B}}\mid  {\boldsymbol \Sigma},{\boldsymbol{Y}}, {\boldsymbol{X}} \sim N_{K\times M}({\boldsymbol{B}}_n,{\boldsymbol{V}}_n,{\boldsymbol \Sigma})\)</span> and <span class="math inline">\({\boldsymbol \Sigma}\mid  {\boldsymbol{Y}},{\boldsymbol{X}} \sim IW({\boldsymbol{\Psi}}_n,{\alpha}_n)\)</span>, <span class="math inline">\(\alpha_n= N+\alpha_{0}\)</span>. Observe again that we can write down the posterior mean as a weighted average between prior and sample information such that <span class="math inline">\({\boldsymbol{V}}_0\rightarrow\infty\)</span> implies <span class="math inline">\({\boldsymbol{B}}_n\rightarrow\hat{{\boldsymbol{B}}}\)</span>, as we show in the univariate linear model.</p>
<p>The marginal posterior for <span class="math inline">\({\boldsymbol{B}}\)</span> is given by
<span class="math display">\[\begin{align*}
    \pi({\boldsymbol{B}}\mid {\boldsymbol{Y}},{\boldsymbol{X}})&amp;\propto \int_{\boldsymbol{\mathcal{S}}} \left|{\boldsymbol \Sigma} \right|^{-(\alpha_n+K+M+1)/2}\\
    &amp;\times\exp\left\lbrace -\frac{1}{2} tr\left\{\left[({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)+{\boldsymbol{\Psi}}_n \right]  {\boldsymbol \Sigma}^{-1}\right\}\right\rbrace d{\boldsymbol{\Sigma}} \\
    &amp;\propto|({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)+{\boldsymbol{\Psi}}_n|^{-(K+\alpha_n)/2}\\
    &amp;=\left[|{\boldsymbol{\Psi}}_n|\times|{\boldsymbol{I}}_K+{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n){\boldsymbol{\Psi}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}|\right]^{-(\alpha_n+1-M+K+M-1)/2}\\
    &amp;\propto|{\boldsymbol{I}}_K+{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n){\boldsymbol{\Psi}}_n^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_n)^{\top}|^{-(\alpha_n+1-M+K+M-1)/2}.
\end{align*}\]</span></p>
<p>The second line uses the inverse Wishart distribution, the third line the Sylverter’s theorem, and the last line is the kernel of a matrix <span class="math inline">\(t\)</span>-distribution, that is, <span class="math inline">\({\boldsymbol{B}}\mid {\boldsymbol{Y}},{\boldsymbol{X}}\sim T_{K\times M}({\boldsymbol{B}}_n,{\boldsymbol{V}}_n,{\boldsymbol{\Psi}}_n)\)</span> with <span class="math inline">\(\alpha_n+1-M\)</span> degrees of freedom.</p>
<p>Observe that <span class="math inline">\(vec({\boldsymbol{B}})\)</span> has mean <span class="math inline">\(vec({\boldsymbol{B}}_n)\)</span> and variance <span class="math inline">\(({\boldsymbol{V}}_n\otimes{\boldsymbol{\Psi}}_n)/(\alpha_n-M-1)\)</span> based on its marginal distribution. On the other hand, the variance based on the conditional distribution is <span class="math inline">\({\boldsymbol{V}}_n\otimes{\boldsymbol{\Sigma}}\)</span>, where the mean of <span class="math inline">\({\boldsymbol{\Sigma}}\)</span> is <span class="math inline">\({\boldsymbol{\Psi}}_n/(\alpha_n-M-1)\)</span>.</p>
<p>The marginal likelihood is the following,
<span class="math display">\[\begin{align*}
    p({\boldsymbol{Y}})&amp;=\int_{\mathcal{B}}\int_{\mathcal{S}}\left\{ (2\pi)^{-NM/2} |{{\boldsymbol \Sigma}}|^{-N/2}\exp\left\lbrace -\frac{1}{2}tr\left[{\boldsymbol{S}}+({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}})\right]{{\boldsymbol \Sigma}}^{-1}\right\rbrace\right.\\
    &amp;\times (2\pi)^{-KM/2}\left|{\boldsymbol V}_0 \right|^{-M/2} \left|{\boldsymbol{\Sigma}} \right|^{-K/2}\exp\left\lbrace -\frac{1}{2}tr\left[({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0}){\boldsymbol \Sigma}^{-1}\right]\right\rbrace \\
    &amp;\left. \times \frac{|\Psi_0|^{\alpha_0/2}}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)} \left|{\boldsymbol \Sigma} \right|^{-(\alpha_{0}+M+1)/2}\exp\left\lbrace -\frac{1}{2}tr \left[ {\boldsymbol{\Psi}}_{0} {\boldsymbol \Sigma}^{-1}\right] \right\rbrace \right\} d{\boldsymbol{\Sigma}} d{\boldsymbol B}\\
    &amp;=(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}\\
    &amp;\times\int_{\mathcal{B}}\int_{\mathcal{S}} \left\{ \left|{\boldsymbol \Sigma} \right|^{-(\alpha_{0}+N+K+M+1)/2}\right.\\
    &amp;\left. \exp\left\lbrace -\frac{1}{2}tr\left[{\boldsymbol{S}}+({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}})+({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0})+{\boldsymbol{\Psi}}_0\right]{{\boldsymbol \Sigma}}^{-1}\right\rbrace\right\}d{\boldsymbol{\Sigma}} d{\boldsymbol B}\\
    &amp;=(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}2^{M(\alpha_n+K)/2}\Gamma_M((\alpha_n+K)/2)\\
    &amp;\times \int_{\mathcal{B}}\left|{\boldsymbol{S}}+({\boldsymbol{B}}-\widehat{\boldsymbol{B}})^{\top}{\boldsymbol{X}}^{\top}{\boldsymbol{X}}({\boldsymbol{B}}-\widehat{\boldsymbol{B}})+({\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{0}^{-1}({\boldsymbol{B}}-{\boldsymbol{B}}_{0})+{\boldsymbol{\Psi}}_0\right|^{-(\alpha_n+K)/2}d{\boldsymbol{B}}\\
    &amp;=(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}2^{M(\alpha_n+K)/2}\Gamma_M((\alpha_n+K)/2)\\
    &amp;\times \int_{\mathcal{B}}\left|({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n)^{\top}{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n)+{\boldsymbol{\Psi}}_n\right|^{-(\alpha_n+K)/2}d{\boldsymbol{B}}\\
    &amp;=(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}2^{M(\alpha_n+K)/2}\Gamma_M((\alpha_n+K)/2)\\
    &amp;\times \int_{\mathcal{B}}\left[|{\boldsymbol{\Psi}}_n|\times |{\boldsymbol{I}}_{K}+{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n){\boldsymbol{\Psi}}_n^{-1}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n)^{\top}|\right]^{-(\alpha_n+K)/2}d{\boldsymbol{B}}\\
    &amp;=|{\boldsymbol{\Psi}}_n|^{-(\alpha_n+K)/2}(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}2^{M(\alpha_n+K)/2}\Gamma_M((\alpha_n+K)/2)}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}\\
    &amp;\times \int_{\mathcal{{\boldsymbol{B}}}}\left| {\boldsymbol{I}}_{K}+{\boldsymbol{V}}_n^{-1}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n){\boldsymbol{\Psi}}_n^{-1}({\boldsymbol{B}}-\widehat{\boldsymbol{B}}_n)^{\top}\right|^{-(\alpha_n+1-M+K+M-1)/2}d{\boldsymbol{B}}\\
    &amp;=|{\boldsymbol{\Psi}}_n|^{-(\alpha_n+K)/2}(2\pi)^{-M(N+K)/2}\left|{\boldsymbol V}_0\right|^{-M/2}\frac{|\Psi_0|^{\alpha_0/2}2^{M(\alpha_n+K)/2}\Gamma_M((\alpha_n+K)/2)}{2^{\alpha_0M/2}\Gamma_M(\alpha_0/2)}\\
    &amp;\times \pi^{MK/2}\frac{\Gamma_M((\alpha_n+1-M+M-1)/2)}{\Gamma_M((\alpha_n+1-M+K+M-1)/2)}|{\boldsymbol{\Psi}}_n|^{K/2}|{\boldsymbol{V}}_n|^{M/2}\\
    &amp;=\frac{|{\boldsymbol{V}}_n|^{M/2}}{|{\boldsymbol{V}}_0|^{M/2}}\frac{|{\boldsymbol{\Psi}}_0|^{\alpha_0/2}}{|{\boldsymbol{\Psi}}_n|^{\alpha_n/2}}\frac{\Gamma_M(\alpha_n/2)}{\Gamma_M(\alpha_0/2)}\pi^{-MN/2}.  
\end{align*}\]</span></p>
<p>The third equality follows from the kernel of an inverse Wishart distribution, the fifth from Sylvester’s theorem, and the seventh from the kernel of a matrix <span class="math inline">\(t\)</span>-distribution.</p>
<p>Observe that this last expression is the multivariate case of the marginal likelihood of the univariate regression model. Taking into account that
<span class="math display">\[\begin{align*}
    ({\boldsymbol{A}}+{\boldsymbol{B}})^{-1}&amp;={\boldsymbol{A}}^{-1}-({\boldsymbol{A}}^{-1}+{\boldsymbol{B}}^{-1})^{-1}{\boldsymbol{A}}^{-1}\\
    &amp;={\boldsymbol{B}}^{-1}-({\boldsymbol{A}}^{-1}+{\boldsymbol{B}}^{-1})^{-1}{\boldsymbol{B}}^{-1}\\
    &amp;={\boldsymbol{A}}^{-1}({\boldsymbol{A}}^{-1}+{\boldsymbol{B}}^{-1}){\boldsymbol{B}}^{-1},
\end{align*}\]</span></p>
<p>we can show that <span class="math inline">\({\boldsymbol{\Psi}}_{n}={\boldsymbol{\Psi}}_{0}+{\boldsymbol{S}}+(\hat{\boldsymbol{B}}-{\boldsymbol{B}}_{0})^{\top}{\boldsymbol{V}}_{n}(\hat{\boldsymbol{B}}-{\boldsymbol{B}}_{0})\)</span> (see Exercise 7). Therefore, the marginal likelihood rewards fit (smaller sum of squares, <span class="math inline">\({\boldsymbol{S}}\)</span>), similarity between prior and sample information regarding location parameters, and information gains in variability from <span class="math inline">\({\boldsymbol{V}}_0\)</span> to <span class="math inline">\({\boldsymbol{V}}_n\)</span>.</p>
<p>Given a matrix of regressors <span class="math inline">\({\boldsymbol{X}}_0\)</span> for <span class="math inline">\(N_0\)</span> unobserved units, the predictive density of <span class="math inline">\({\boldsymbol{Y}}_0\)</span> given <span class="math inline">\({\boldsymbol{Y}}\)</span>, <span class="math inline">\(\pi({\boldsymbol{Y}}_0\mid {\boldsymbol{Y}})\)</span> is a matrix t distribution <span class="math inline">\(T_{N_0,M}(\alpha_n-M+1,{\boldsymbol{X}}_0{\boldsymbol{B}}_n,{\boldsymbol{I}}_{N_0}+{\boldsymbol{X}}_0{\boldsymbol{V}}_n{\boldsymbol{X}}_0^{\top},{\boldsymbol{\Psi}}_n)\)</span> (see Exercise 6). Observe that the prediction is centered at <span class="math inline">\({\boldsymbol{X}}_0{\boldsymbol{B}}_n\)</span>, and the covariance matrix of <span class="math inline">\(vec({\boldsymbol{Y}}_0)\)</span> is <span class="math inline">\(\frac{({\boldsymbol{I}}_{N_0}+{\boldsymbol{X}}_0{\boldsymbol{V}}_n{\boldsymbol{X}}_0^{\top})\otimes{\boldsymbol{\Psi}}_n}{\alpha_n-M-1}\)</span>.</p>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p><span class="math inline">\(vec\)</span> denotes the vectorization operation, and <span class="math inline">\(\otimes\)</span> denotes the Kronecker product.<a href="sec44.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>We can write down the former expression in a more familiar way using vectorization properties,
<span class="math inline">\(\underbrace{vec(Y)}_{\boldsymbol{y}}=\underbrace{({\boldsymbol{I}}_M\otimes {\boldsymbol{X}})}_{{\boldsymbol{Z}}}\underbrace{vec({\boldsymbol{B}})}_{\boldsymbol{\beta}}+\underbrace{vec({\boldsymbol{U}})}_{\mu}\)</span>, where <span class="math inline">\({\boldsymbol{y}}\sim N_{N\times M}({\boldsymbol{Z}}\boldsymbol{\beta},\boldsymbol{\Sigma}\otimes {\boldsymbol{I}}_N)\)</span>.<a href="sec44.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec43.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec45.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
