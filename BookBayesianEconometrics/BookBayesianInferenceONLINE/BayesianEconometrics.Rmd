---
title: "Introduction to Bayesian Data Modeling"
subtitle: "A GUIded toolkit using R"
author: "Andrés Ramírez-Hassan"
date: "`r Sys.Date()`"
knit: "bookdown::render_book"
documentclass: book #krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: no
lof: no
fontsize: 12pt
monofont: "Source Code Pro"
monofontoptions: "Scale=0.7"
site: bookdown::bookdown_site
description: "The subject of this textbook is Bayesian data modeling, with the primary aim of providing an introduction to its theoretical foundations and facilitating the application of Bayesian inference using a GUI."
github-repo: https://github.com/aramir21/IntroductionBayesianEconometricsBook
always_allow_html: yes
---

# Introduction {-}

Placeholder


## To instructors and students {-}
## Acknowledgments {-}

<!--chapter:end:index.Rmd-->


# Basic formal concepts {#Chap1}

Placeholder


## The Bayes' rule {#sec11}
## Bayesian framework: A brief summary of theory {#sec12}
## Bayesian reports: Decision theory under uncertainty {#sec14}
## Summary
## Exercises

<!--chapter:end:01-Basics.Rmd-->


# Conceptual differences between the Bayesian and Frequentist approaches {#Chap2}

Placeholder


## The concept of probability {#sec21}
## Subjectivity is not the key {#sec22}
## Estimation, hypothesis testing and prediction {#sec23}
## The likelihood principle {#sec24}
## Why is not the Bayesian approach that popular? {#sec25}
## A simple working example {#sec26}
## Summary {#sec27}
## Exercises {#sec28}

<!--chapter:end:02-BayFreq.Rmd-->


# Cornerstone models: Conjugate families {#Chap3}

Placeholder


## Motivation of conjugate families {#sec41}
## Conjugate prior to exponential family {#sec42}
## Linear regression: The conjugate normal-normal/inverse gamma model {#sec43} 
## Multivariate linear regression: The conjugate normal-normal/inverse Wishart model
## Computational examples
## Summary: Chapter 4
## Exercises: Chapter 4

<!--chapter:end:04-Conjugate.Rmd-->


# Simulation methods {#Chap4}

Placeholder


## Markov Chain Monte Carlo methods {#sec51}
### Gibbs sampler {#sec511}
### Metropolis-Hastings {#sec512}
### Hamiltonian Monte Carlo {#sec513}
## Importance sampling {#sec52}
## Particle filtering {#sec53}
## Convergence diagnostics {#sec54}
### Numerical standard error
### Effective number of simulation draws
### Tests of convergence
### Checking for errors in the posterior simulator
## Summary {#sec55}
## Exercises {#sec56}

<!--chapter:end:05-Simulation.Rmd-->

# Graphical user interface {#Chap5}

This chapter presents our graphical user interface (GUI) to carry out Bayesian regression analysis in a very friendly environment without any programming skills (drag and drop). Our GUI is based on an interactive web application using *shiny* [@Chang2018], and packages like *MCMCpack* [@Martin2018] and *bayesm* [@Rossi2017] from **R** software [@R2023], and is designed for teaching and applied purposes at an introductory level. In the next chapters of the second part of this book, we carry out some applications to highlight the potential of our GUI for applied researchers and practitioners.


<!--chapter:end:06-GUI.Rmd-->

```{r setup, include = FALSE}
library(svglite)
knitr::opts_chunk$set(echo = TRUE,
                      dev = "svglite",
                      fig.ext = ".svg"
                      )
options(knitr.graphics.auto_pdf = TRUE)
```

# Univariate regression {#Chap6}

We describe how to perform Bayesian inference in some of the most common univariate models: normal-inverse gamma, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap in linear models. The point of departure is assuming a random sample of cross-sectional units. We then show the posterior distributions of the parameters and some applications. In addition, we show how to perform inference in various models using three levels of programming skills: our graphical user interface (GUI), packages from **R**, and programming the posterior distributions. The first requires no programming skills, the second requires an intermediate level, and the third demands more advanced skills. We also include mathematical and computational exercises.

We can run our GUI typing`shiny::runGitHub("besmarter/BSTApp", launch.browser=T)` in the **R** console or any **R** code editor and execute it. However, users should see Chapter \@ref(Chap5) for details.

## The Gaussian linear model {#sec61}

The Gaussian linear model specifies  

\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mu
\]

such that \( \mu \sim N(\mathbf{0}, \sigma^2 \mathbf{I}_N) \) is a stochastic error, \( \mathbf{X} \) is an \( N \times K \) matrix of regressors, \( \boldsymbol{\beta} \) is a \( K \)-dimensional vector of location coefficients, \( \sigma^2 \) is the variance of the model (scale parameter), \( \mathbf{y} \) is an \( N \)-dimensional vector of a dependent variable, and \( N \) is the sample size. We describe this model using the conjugate family in \@ref(sec43), that is,  

\[
\pi(\boldsymbol{\beta},\sigma^2) = \pi(\boldsymbol{\beta} \mid \sigma^2) \times \pi(\sigma^2),
\]

which allows obtaining the posterior marginal distribution for \( \boldsymbol{\beta} \) and \( \sigma^2 \).

We assume independent priors in this section, that is,  

\[
\pi(\boldsymbol{\beta},\sigma^2) = \pi(\boldsymbol{\beta}) \times \pi(\sigma^2),
\]

where \( \boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \mathbf{B}_0) \) and \( \sigma^2 \sim IG(\alpha_0/2, \delta_0/2) \), with \( \alpha_0/2 \) and \( \delta_0/2 \) as the shape and rate parameters. This setting allows deriving the posterior conditional distributions,  

\[
\pi(\boldsymbol{\beta} \mid \sigma^2, \mathbf{y}, \mathbf{X})
\]

and  

\[
\pi(\sigma^2 \mid \boldsymbol{\beta}, \mathbf{y}, \mathbf{X}),
\]

which in turn enables the use of the Gibbs sampler algorithm to perform posterior inference on \( \boldsymbol{\beta} \) and \( \sigma^2 \).

The likelihood function in this model is  

\begin{align}
p(\mathbf{y} \mid \boldsymbol{\beta}, \sigma^2, \mathbf{X}) = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^{\top}(\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \right\}.
\end{align}

Then, the conditional posterior distributions are  

\begin{align}
\boldsymbol{\beta} \mid \sigma^2, \mathbf{y}, \mathbf{X} \sim N(\boldsymbol{\beta}_n, \mathbf{B}_n),
\end{align}

and  

\begin{align}
\sigma^2 \mid \boldsymbol{\beta}, \mathbf{y}, \mathbf{X} \sim IG(\alpha_n/2, \delta_n/2),
\end{align}

where  

\[
\mathbf{B}_n = (\mathbf{B}_0^{-1} + \sigma^{-2} \mathbf{X}^{\top} \mathbf{X})^{-1},
\]

\[
\boldsymbol{\beta}_n= \mathbf{B}_n (\mathbf{B}_0^{-1} \boldsymbol{\beta}_0 + \sigma^{-2} \mathbf{X}^{\top} \mathbf{y}),
\]

\[
\alpha_n = \alpha_0 + N,
\]

\[
\delta_n = \delta_0 + (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^{\top} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}).
\]

This model can be extended to consider heteroskedasticity such that \( y_i \sim N(\mathbf{x}_i^{\top} \boldsymbol{\beta}, \sigma^2/\tau_i) \), where \( \tau_i \sim G(v/2,v/2) \). See Exercise 2 for details.

**Example: The market value of soccer players in Europe**

Let's analyze the determinants of the market value of soccer players in Europe. In particular, we use the dataset *1ValueFootballPlayers.csv*, which is in the folder **DataApp** in our GitHub repository: **https://github.com/besmarter/BSTApp**. This dataset was used by @Serna2018 to find the determinants of high-performance soccer players in the five most important national leagues in Europe.

The specification of the model is  

\begin{align}
\log(\text{Value}_i) &= \beta_1 + \beta_2 \text{Perf}_i + \beta_3 \text{Age}_i + \beta_4 \text{Age}^2_i + \beta_5 \text{NatTeam}_i \\
&\quad + \beta_6 \text{Goals}_i + \beta_7 \text{Exp}_i + \beta_8 \text{Exp}^2_i + \mu_i,
\end{align}

where *Value* is the market value in Euros (2017), *Perf* is a measure of performance, *Age* is the player's age in years, *NatTeam* is an indicator variable that takes the value of 1 if the player has been on the national team, *Goals* is the number of goals scored by the player during his career, and *Exp* is his experience in years.  

We assume that the dependent variable follows a normal distribution, so we use a normal-inverse gamma model with vague conjugate priors where  

\[
\mathbf{B}_0 = 1000 \mathbf{I}_{8}, \quad \boldsymbol{\beta}_0 = \mathbf{0}_{8}, \quad \alpha_0 = 0.001, \quad \delta_0 = 0.001.
\]

We perform a Gibbs sampler with 5,000 MCMC iterations, plus a burn-in of 5,000, and a thinning parameter equal to 1.

Once our GUI is displayed (see the beginning of this chapter), we should follow the next Algorithm to run linear Gaussian models in our GUI (see Chapter \@ref(Chap5) for details).

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Gaussian linear model in the GUI**  

1. *Select* Univariate Models on the top panel  
2. **Choose** the Normal model using the left radio button  
3. *Upload* the dataset by selecting if there is a header and specifying the separator (comma, semicolon, or tab)  
4. *Use* the *Browse* button to select the file and preview the dataset  
5. *Adjust* MCMC iterations, burn-in, and thinning using the *Range sliders*  
6. *Specify* dependent and independent variables using the *Formula builder*  
7. *Click* the *Build formula* button to generate the model formula in **R**  
8. *Modify* the formula in the **Main equation** box if necessary  
9. *Set* hyperparameters (mean vector, covariance matrix, shape, and scale parameters) if needed  
10. *Click* the *Go!* button  
11. *Analyze* results  
12. *Download* posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

We can see in the following **R** code examples how to perform the linear Gaussian model using the *MCMCregress* command from the *MCMCpack* package, as well as how to program the Gibbs sampler ourselves. We should obtain similar results using all three approaches: GUI, package, and our function. In fact, our GUI relies on the *MCMCregress* command. For instance, the value of a top soccer player in Europe increases by 134\% ($\exp(0.85)-1$) on average when he has played for the national team, with a 95\% credible interval of (86\%, 197\%).

```{r}
rm(list = ls())
set.seed(010101)
########################## Linear regression: Value of soccer players ##########################
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior))
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
bhat <- solve(XtX)%*%t(X)%*%y
an <- a0 + N
# Gibbs sampling functions
PostSig2 <- function(Beta){
	dn <- d0 + t(y - X%*%Beta)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*XtX%*%bhat)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2)
	PostBetas[s,] <- Beta
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))

```

## The logit model {#sec62}

In the logit model, the dependent variable is binary, \(y_i=\left\{1,0\right\}\), which follows a Bernoulli distribution, \(y_i \stackrel{ind}{\sim} B(\pi_i)\), such that \(p(y_i=1)=\pi_i\), where \(\pi_i = \frac{\exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}}{1 + \exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}}\), and \(\mathbf{x}_i\) is a \(K\)-dimensional vector of regressors.

The likelihood function of the logit model is:

\[
p({\mathbf{y}} \mid \boldsymbol{\beta}, {\mathbf{X}}) = \prod_{i=1}^N \pi_i^{y_i}(1 - \pi_i)^{1 - y_i}
\]
\[
= \prod_{i=1}^N \left( \frac{\exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}}{1 + \exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}} \right)^{y_i} \left( \frac{1}{1 + \exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}} \right)^{1 - y_i}.
\]

We can specify a Normal distribution as a prior, \(\boldsymbol{\beta} \sim N({\boldsymbol{\beta}}_0, {\mathbf{B}}_0)\). Then, the posterior distribution is:

\[
\pi(\boldsymbol{\beta} \mid {\mathbf{y}}, {\mathbf{X}}) \propto \prod_{i=1}^N \left( \frac{\exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}}{1 + \exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}} \right)^{y_i} \left( \frac{1}{1 + \exp\left\{{\mathbf{x}}_i^{\top}\boldsymbol{\beta}\right\}} \right)^{1 - y_i}
\]
\[
\times \exp\left\{-\frac{1}{2}(\boldsymbol{\beta} - \boldsymbol{\beta}_0)^{\top} {\mathbf{B}}_0^{-1} (\boldsymbol{\beta} - \boldsymbol{\beta}_0)\right\}.
\]

The logit model does not have a standard posterior distribution. Therefore, a random walk Metropolis–Hastings algorithm can be used to obtain draws from the posterior distribution. A potential proposal distribution is a multivariate normal, centered at the current value, with covariance matrix \(\tau^2({\mathbf{B}}_0^{-1} + \widehat{{\mathbf{\Sigma}}}^{-1})^{-1}\), where \(\tau > 0\) is a tuning parameter and \(\widehat{\mathbf{\Sigma}}\) is the sample covariance matrix obtained from the maximum likelihood estimation [@Martin2011].

Tuning parameters should be set in a way that ensures reasonable diagnostic criteria and acceptance rates.

Observe that:
\[
\log(p({\mathbf{y}} \mid \boldsymbol{\beta}, {\mathbf{X}})) = \sum_{i=1}^N y_i {\mathbf{x}}_i^{\top} \boldsymbol{\beta} - \log(1 + \exp({\mathbf{x}}_i^{\top} \boldsymbol{\beta})).
\]

This expression can be used when calculating the acceptance parameter in the computational implementation of the Metropolis-Hastings algorithm. In particular, the acceptance parameter is:

\[
\alpha = \min\left\{1, \exp\left(\log(p({\mathbf{y}} \mid \boldsymbol{\beta}^{c}, {\mathbf{X}})) + \log(\pi(\boldsymbol{\beta}^c)) - \left(\log(p({\mathbf{y}} \mid \boldsymbol{\beta}^{(s-1)}, {\mathbf{X}})) + \log(\pi(\boldsymbol{\beta}^{(s-1)}))\right)\right)\right\},
\]
where \(\boldsymbol{\beta}^c\) and \(\boldsymbol{\beta}^{(s-1)}\) are the draws from the proposal distribution and the previous iteration of the Markov chain, respectively.

Formulating the acceptance rate using \(\log\) helps mitigate computational problems.

**Example: Simulation exercise**

Let's do a simulation exercise to check the performance of the algorithm. Set \(\boldsymbol{\beta} = \begin{bmatrix}0.5 & 0.8 & -1.2\end{bmatrix}^{\top}\), \(x_{ik} \sim N(0,1)\), \(k=2,3\) and \(i=1,2,\dots,10000\).

We set as hyperparameters \(\boldsymbol{\beta}_0 = [0 \ 0 \ 0]^{\top}\) and \({\mathbf{B}}_0 = 1000 {\mathbf{I}}_3\). The tuning parameter for the Metropolis-Hastings algorithm is equal to 1.

Once our GUI is displayed (see beginning of this chapter), we should follow the next Algorithm to run logit models in our GUI (see Chapter \@ref(Chap5) for details):

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Logit model in the GUI**  

1. Select *Univariate Models* on the top panel
2. Select *Logit* model using the left radio button
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*
5. Select dependent and independent variables using the *Formula builder* table
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**
7. Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
8. Select the tuning parameter for the Metropolis-Hastings algorithm. This step is not necessary as by default our GUI sets the tuning parameter at 1.1
9. Click the *Go!* button
10. Analyze results
11. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons

</div>
:::

We can see in the following **R** code how to perform the logit model using the *MCMClogit* command from the *MCMCpack* package, as well as by programming the Metropolis-Hastings algorithm ourselves. 

We should obtain similar results using the three approaches: GUI, package, and our function. Our GUI relies on the *MCMClogit* command. In particular, we achieve an acceptance rate of 0.46, and the diagnostics suggest that the posterior chains behave well. In general, the 95\% credible intervals encompass the population values, and both the mean and median are very close to these values.

```{r}
########################## Logit: Simulation ##########################
# Simulate data
rm(list = ls())
set.seed(010101)
N <- 10000 # Sample size
B <- c(0.5, 0.8, -1.2) # Population location parameters
x2 <- rnorm(N) # Regressor
x3 <- rnorm(N) # Regressor
X <- cbind(1, x2, x3) # Regressors
XB <- X%*%B
PY <- exp(XB)/(1 + exp(XB)) # Probability of Y = 1
Y <- rbinom(N, 1, PY) # Draw Y's
table(Y) # Frequency
# write.csv(cbind(Y, x2, x3), file = "DataSimulations/LogitSim.csv") # Export data
# MCMC parameters
iter <- 5000; burnin <- 1000; thin <- 5; tune <- 1
# Hyperparameters
K <- dim(X)[2] 
b0 <- rep(0, K)
c0 <- 1000
B0 <- c0*diag(K)
B0i <- solve(B0)
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
RegLog <- MCMCpack::MCMClogit(Y~X-1, mcmc = iter, burnin = burnin, thin = thin, b0 = b0, B0 = B0i, tune = tune)
summary(RegLog)
# Posterior distributions programming the Metropolis-Hastings algorithm
MHfunc <- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 6000, burnin = 1000, thin = 5){
	Xm <- cbind(1, X) # Regressors
	K <- dim(Xm)[2] # Number of location parameters
	BETAS <- matrix(0, iter + burnin, K) # Space for posterior chains
	Reg <- glm(y ~ Xm - 1, family = binomial(link = "logit")) # Maximum likelihood estimation
	BETA <- Reg$coefficients # Maximum likelihood parameter estimates 
	tot <- iter + burnin # Total iterations M-H algorithm
	COV <- vcov(Reg) # Maximum likelihood covariance matrix
	COVt <- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution
	Accep <- rep(0, tot) # Space for calculating the acceptance rate
	# Create progress bar in case that you want to see iterations progress
	pb <- winProgressBar(title = "progress bar", min = 0,
	max = tot, width = 300)
	for(it in 1:tot){
		BETAc <- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter
		likecand <- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood for the candidate
		likepast <- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log likelihood for the actual draw
		priorcand <- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior for candidate
		priorpast <- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior for actual draw
		alpha <- min(1, exp((likecand + priorcand) - (likepast + priorpast))) #Probability of selecting candidate
		u <- runif(1) # Decision rule for selecting candidate
		if(u < alpha){
			BETA <- BETAc # Changing reference for candidate if selected
			Accep[it] <- 1 # Indicator if the candidate is accepted
		} 
		BETAS[it, ] <- BETA # Saving draws
		setWinProgressBar(pb, it, title=paste( round(it/tot*100, 0),
		"% done"))
	}
	close(pb)
	keep <- seq(burnin, tot, thin)
	return(list(Bs = BETAS[keep[-1], ], AceptRate = mean(Accep[keep[-1]])))
}
Posterior <- MHfunc(y = Y, X = cbind(x2, x3), iter = iter, burnin = burnin, thin = thin) # Running our M-H function changing some default parameters.
paste("Acceptance rate equal to", round(Posterior$AceptRate, 2), sep = " ")
"Acceptance rate equal to 0.46"
PostPar <- coda::mcmc(Posterior$Bs)
# Names
colnames(PostPar) <- c("Cte", "x1", "x2")
# Summary posterior draws
summary(PostPar)
# Trace and density plots
plot(PostPar)
# Autocorrelation plots
coda::autocorr.plot(PostPar)
# Convergence diagnostics
coda::geweke.diag(PostPar)
coda::raftery.diag(PostPar,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(PostPar)
```


## The probit model {#sec63}

The probit model also has a binary dependent variable. In this case, there is a latent variable (\(y_i^*\), which is unobserved) that defines the structure of the estimation problem.

In particular,

\[
y_i = 
\begin{cases}
0, & \text{if } y_i^* \leq 0, \\
1, & \text{if } y_i^* > 0.
\end{cases}
\]

such that \(y_i^* = \mathbf{x}_i^{\top}\boldsymbol{\beta} + \mu_i\), where \(\mu_i \stackrel{i.i.d.}{\sim} N(0,1)\).^[The variance in this model is set to 1 due to identification restrictions. Observe that \(P(y_i = 1 \mid \mathbf{x}_i) = P(y_i^* > 0 \mid \mathbf{x}_i) = P(\mathbf{x}_i^{\top} \boldsymbol{\beta} + \mu_i > 0 \mid \mathbf{x}_i) = P(\mu_i > -\mathbf{x}_i^{\top} \boldsymbol{\beta} \mid \mathbf{x}_i) = P(c \times \mu_i > -c \times \mathbf{x}_i^{\top} \boldsymbol{\beta} \mid \mathbf{x}_i)\) for all \(c > 0\). Multiplying by a positive constant does not affect the probability of \(y_i = 1\).] This implies \(P(y_i = 1) = \pi_i = \Phi(\mathbf{x}_i^{\top} \boldsymbol{\beta})\), where \(\mathbf{x}_i\) is a \(K\)-dimensional vector of regressors.

@Albert1993 implemented data augmentation [@Tanner1987] to apply a Gibbs sampling algorithm to this model. Augmenting this model with \(y_i^*\), we can express the likelihood contribution from observation \(i\) as:

\[
p(y_i \mid y_i^*) = \mathbb{1}({y_i = 0}) \mathbb{1}({y_i^* \leq 0}) + \mathbb{1}({y_i = 1}) \mathbb{1}({y_i^* > 0}),
\]

where \(\mathbb{1}(A)\) is an indicator function that takes the value of 1 when the condition \(A\) is satisfied.

The posterior distribution is:

\[
\pi(\boldsymbol{\beta}, \mathbf{y^*} \mid \mathbf{y}, \mathbf{X}) \propto \prod_{i=1}^N \left[\mathbb{1}({y_i = 0}) \mathbb{1}({y_i^* \leq 0}) + \mathbb{1}({y_i = 1}) \mathbb{1}({y_i^* > 0}) \right]
\]

\[
\times N_N(\mathbf{y^*} \mid \mathbf{X\boldsymbol{\beta}}, \mathbf{I}_n) \times N_K(\boldsymbol{\beta} \mid \boldsymbol{\beta}_0, \mathbf{B}_0),
\]

where we assume a Gaussian prior for \(\boldsymbol{\beta}\): \(\boldsymbol{\beta} \sim N_K(\boldsymbol{\beta}_0, \mathbf{B}_0)\).

This implies

\[
y_i^* \mid \boldsymbol{\beta}, \mathbf{y}, \mathbf{X} \sim
\begin{cases}
TN_{(-\infty,0]}(\mathbf{x}_i^{\top} \boldsymbol{\beta}, 1) & \text{if } y_i = 0, \\
TN_{(0,\infty)}(\mathbf{x}_i^{\top} \boldsymbol{\beta}, 1) & \text{if } y_i = 1,
\end{cases},
\]

where \(TN\) denotes a truncated normal density.

\[
\boldsymbol{\beta} \mid \mathbf{y}^*, \mathbf{X} \sim N(\boldsymbol{\beta}_n, \mathbf{B}_n),
\]

where \(\mathbf{B}_n = (\mathbf{B}_0^{-1} + \mathbf{X}^{\top} \mathbf{X})^{-1}\), and \(\boldsymbol{\beta}_n = \mathbf{B}_n (\mathbf{B}_0^{-1} \boldsymbol{\beta}_0 + \mathbf{X}^{\top} \mathbf{y}^*)\).

**Example: Determinants of hospitalization**

We use the dataset named **2HealthMed.csv**, which is located in the **DataApp** folder of our GitHub repository (https://github.com/besmarter/BSTApp), and was used by @Ramirez2013. The dependent variable is a binary indicator, taking the value 1 if an individual was hospitalized in 2007, and 0 otherwise.

The specification of the model is

$$
\text{Hosp}_i = \boldsymbol{\beta}_1 + \boldsymbol{\beta}_2 \text{SHI}_i + \boldsymbol{\beta}_3 \text{Female}_i + \boldsymbol{\beta}_4 \text{Age}_i + \boldsymbol{\beta}_5 \text{Age}_i^2 + \boldsymbol{\beta}_6 \text{Est2}_i + \boldsymbol{\beta}_7 \text{Est3}_i + \boldsymbol{\beta}_8 \text{Fair}_i + \boldsymbol{\beta}_9 \text{Good}_i + \boldsymbol{\beta}_{10} \text{Excellent}_i,
$$

where *SHI* is a binary variable equal to 1 if the individual is enrolled in a subsidized health care program and 0 otherwise, *Female* is an indicator of gender, *Age* is in years, *Est2* and *Est3* are indicators of socioeconomic status, with *Est1* being the reference category (the lowest status), and *HealthStatus* is a self-perception of health status, where *bad* is the reference category.

We set $\boldsymbol{\beta}_0 = {\boldsymbol{0}}_{10}$, ${\boldsymbol{B}}_0 = {\boldsymbol{I}}_{10}$, with iterations, burn-in, and thinning parameters equal to 10000, 1000, and 1, respectively. We can use the next Algorithm to run the probit model in our GUI. Our GUI relies on the *rbprobitGibbs* command from the *bayesm* package to perform inference in the probit model. The following **R** code shows how to run this example using the *rbprobitGibbs* command. We also asked you to implement a Gibbs sampler algorithm to perform inference in the probit model in the exercises.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Probit model in the GUI**  

1. Select *Univariate Models* on the top panel  
2. Select *Probit* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**  
7. Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors  
8. Select the tuning parameter for the Metropolis-Hastings algorithm. This step is not necessary as by default our GUI sets the tuning parameter at 1.1  
9. Click the *Go!* button  
10. Analyze results  
11. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

Our analysis finds evidence that gender and self-perceived health status significantly affect the probability of hospitalization. Women have a higher probability of being hospitalized than men, and individuals with a better perception of their health status have a lower probability of hospitalization.

```{r}
mydata <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv", sep = ",", header = TRUE, quote = "")
attach(mydata)
str(mydata)
K <- 10 # Number of regressors
b0 <- rep(0, K) # Prio mean
B0i <- diag(K) # Prior precision (inverse of covariance)
Prior <- list(betabar = b0, A = B0i) # Prior list
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
Data <- list(y = y, X = X) # Data list
Mcmc <- list(R = 10000, keep = 1, nprint = 0) # MCMC parameters
RegProb <- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package
PostPar <- coda::mcmc(RegProb$betadraw) # Posterior draws
colnames(PostPar) <- c("Cte", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent") # Names
summary(PostPar) # Posterior summary
```

## The multinomial probit model {#sec64}

The multinomial probit model is used to model the choice of the $l$-th alternative over a set of $L$ mutually exclusive options. We observe the following:

$$
y_{il} =
\begin{cases}
1, & \text{if } y_{il}^* \geq \max\left\{\boldsymbol{y}_i^*\right\}, \\
0, & \text{otherwise,}
\end{cases}
$$

where $\boldsymbol{y}_i^* = \boldsymbol{X}_{i} \boldsymbol{\delta} + \boldsymbol{\mu}_i$, with $\boldsymbol{\mu}_i \stackrel{i.i.d.}{\sim} N(\boldsymbol{0}, \boldsymbol{\Sigma})$. The vector $\boldsymbol{y}_i^*$ is an unobserved latent vector of dimension $L$. The matrix $\boldsymbol{X}_i = \left[(1 \ \boldsymbol{c}_i^{\top}) \otimes \boldsymbol{I}_L \ \boldsymbol{A}_i\right]$ is an $L \times j$ matrix of regressors for each alternative, where $l = 1, 2, \dots, L$, and $j = L \times (1 + \text{dim}(\boldsymbol{c}_i)) + a$. Here, $\boldsymbol{c}_i$ is a vector of individual-specific characteristics, $\boldsymbol{A}_i$ is an $L \times a$ matrix of alternative-varying regressors, $a$ is the number of alternative-varying regressors, and $\boldsymbol{\delta}$ is a $j$-dimensional vector of parameters.

We take into account simultaneously the alternative-varying regressors (alternative attributes) and alternative-invariant regressors (individual characteristics).^[Note that this model is not identified if $\boldsymbol{\Sigma}$ is unrestricted. The likelihood function remains the same if a scalar random variable is added to each of the $L$ latent regressions.] The vector $\boldsymbol{y}_i^*$ can be stacked into a multiple regression model with correlated stochastic errors, i.e., $\boldsymbol{y}^* = \boldsymbol{X} \boldsymbol{\delta} + \boldsymbol{\mu}$, where $\boldsymbol{y}^* = \left[\boldsymbol{y}_1^{*\top} \ \boldsymbol{y}_2^{*\top} \ \dots \ \boldsymbol{y}_N^{*\top}\right]$, $\boldsymbol{X} = \left[\boldsymbol{X}_1^{\top} \ \boldsymbol{X}_2^{\top} \ \dots \ \boldsymbol{X}_N^{\top}\right]^{\top}$, and $\boldsymbol{\mu} = \left[\boldsymbol{\mu}_1^{\top} \ \boldsymbol{\mu}_2^{\top} \ \dots \ \boldsymbol{\mu}_N^{\top}\right]^{\top}$.

Following the practice of expressing $y_{il}^*$ relative to $y_{iL}^*$ by letting $\boldsymbol{w}_i = \left[w_{i1} \ w_{i2} \ \dots \ w_{iL-1}\right]^{\top}$, where $w_{il} = y_{il}^* - y_{iL}^*$, we can write $\boldsymbol{w}_i = \boldsymbol{R}_i \boldsymbol{\beta} + \boldsymbol{\epsilon}_i$, with $\boldsymbol{\epsilon}_i \sim N(\boldsymbol{0}, \boldsymbol{\Omega})$, where $\boldsymbol{R}_i = \left[(1 \ \boldsymbol{c}_i^{\top}) \otimes \boldsymbol{I}_{L-1} \ \boldsymbol{\Delta A}_i\right]$ is an $(L-1) \times K$ matrix, with $\Delta \boldsymbol{A}_i = \boldsymbol{A}_{li} - \boldsymbol{A}_{Li}$, for $l = 1, 2, \dots, L-1$. That is, the last row of $\boldsymbol{A}_i$ is subtracted from each row of $\boldsymbol{A}_i$, and $\boldsymbol{\beta}$ is a $K$-dimensional vector, where $K = (L-1) \times (1 + \text{dim}(\boldsymbol{c}_i)) + a$.

Observe that $\boldsymbol{\beta}$ contains the same last $a$ elements as $\boldsymbol{\delta}$, that is, the alternative-specific attribute coefficients. However, the first $(L-1) \times (1 + \text{dim}(\boldsymbol{c}_i))$ elements of $\boldsymbol{\beta}$ are the differences $\delta_{jl} - \delta_{jL}$, for $j = 1, \dots, \text{dim}(\boldsymbol{c}_i)$ and $l = 1, 2, \dots, L-1$. That is, these elements represent the difference between the coefficients of each qualitative response and the $L$-th alternative for the individuals' characteristics. This makes it difficult to interpret the multinomial probit coefficients.

Note that in multinomial models, for each alternative-specific attribute, it is only necessary to estimate one coefficient for all alternatives. However, for individuals' characteristics (non-alternative-specific regressors), it is required to estimate $L-1$ coefficients, since the coefficient for the base alternative is set equal to 0.

The likelihood function in this model is given by 
\[
p(\boldsymbol{\beta}, \boldsymbol{\Omega} \mid \boldsymbol{y}, \boldsymbol{R}) = \prod_{i=1}^N \prod_{l=1}^L p_{il}^{y_{il}},
\]
where \( p_{il} = p(y_{il}^* \geq \max(\boldsymbol{y}_i^*)) \).

We assume independent priors for the parameters: 
\[
\boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \boldsymbol{B}_0) \quad \text{and} \quad \boldsymbol{\Omega}^{-1} \sim W(\alpha_0, \boldsymbol{\Sigma}_0), 
\]
where \( W \) denotes the Wishart density.


We can employ Gibbs sampling in this model, as it is a standard Bayesian linear regression model when data augmentation is used for \( \boldsymbol{w} \). The posterior conditional distributions are given by

\begin{equation*}
	\boldsymbol{\beta}\mid \boldsymbol{\Omega},\boldsymbol{w}\sim{N}(\boldsymbol{\beta}_n,\boldsymbol{B}_n),
\end{equation*}
\begin{equation*}
	\boldsymbol{\Omega}^{-1}\mid \boldsymbol{\beta},\boldsymbol{w}\sim{W}(\alpha_n,\boldsymbol{\Sigma}_n),
\end{equation*}

where $\boldsymbol{B}_n=(\boldsymbol{B}_0^{-1}+\boldsymbol{X}^{*\top}\boldsymbol{X}^*)^{-1}$, $\boldsymbol{\beta}_n=\boldsymbol{B}_n(\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0+\boldsymbol{X}^{*\top}\boldsymbol{w}^*)$, $\boldsymbol{\Omega}^{-1}=\boldsymbol{C}^{\top}\boldsymbol{C}$, $\boldsymbol{X}_i^{*\top}=\boldsymbol{C}^{\top}\boldsymbol{R}_i$, $\boldsymbol{w}_i^*=\boldsymbol{C}^{\top}\boldsymbol{w}_i$, $\boldsymbol{X}^*=\begin{bmatrix}\boldsymbol{X}_1^*\\
	\boldsymbol{X}_2^*\\
	\vdots\\
	\boldsymbol{X}_N^*
\end{bmatrix}$, $\alpha_n=\alpha_0+N$, $\boldsymbol{\Sigma}_n=(\boldsymbol{\Sigma}_0+\sum_{i=1}^N (\boldsymbol{w}_i-\boldsymbol{R}_i\boldsymbol{\beta})^{\top}(\boldsymbol{w}_i-\boldsymbol{R}_i\boldsymbol{\beta}))^{-1}$.

We can collapse the multinomial vector $\boldsymbol{y}_i$ into the indicator variable $d_i=\sum_{l=1}^{L-1}l\times \mathbb{1}({\max(\boldsymbol{w}_{l})=w_{il}})$.^[Observe that the identification issue in this model is due to scaling $w_{il}$ by a positive constant does not change the value of $d_i$.] Then the distribution of $\boldsymbol{w}_i\mid \boldsymbol{\beta},\boldsymbol{\Omega}^{-1},d_i$ is an $L-1$ dimensional Gaussian distribution truncated over the appropriate cone in $\mathcal{R}^{L-1}$.
@McCulloch1994 propose drawing from the univariate conditional distributions $w_{il}\mid \boldsymbol{w}_{i,-l},\boldsymbol{\beta},\boldsymbol{\Omega}^{-1},d_i\sim TN_{I_{il}}(m_{il},\tau_{ll}^2)$, where 
\begin{equation*}
I_{il}=\begin{Bmatrix} w_{il}>\max(\boldsymbol{w}_{i,-l},0), & d_i=l\\
	w_{il}<\max(\boldsymbol{w}_{i,-l},0), & d_i\neq l\\
\end{Bmatrix},
\end{equation*}
and permuting the columns and rows of $\boldsymbol{\Omega}^{-1}$ so that the $l$-th column and row is the last,
\begin{equation*}
	\boldsymbol{\Omega}^{-1}=\begin{bmatrix}
		\boldsymbol{\Omega}_{-l,-l} & \boldsymbol\omega_{-l,l}\\
		\boldsymbol\omega_{l,-1} & \omega_{l,l}\\
	\end{bmatrix}^{-1}
	=\begin{bmatrix}
		\boldsymbol{\Omega}_{-l,-l}^{-1}+{\tau}^{-2}_{ll}\boldsymbol{f}_l\boldsymbol{f}_l^{\top} & -\boldsymbol{f}_l\tau^{-2}_{ll}\\
		-{\tau}^{-2}_{ll}\boldsymbol{f}_l^{\top} & {\tau}^{-2}_{ll}\\
	\end{bmatrix}
\end{equation*}
\noindent where $\boldsymbol{f}_l=\boldsymbol{\Omega}_{-l,-l}^{-1}\boldsymbol{\omega}_{-l,l}$, $\tau_{ll}^2= \omega_{ll}-\boldsymbol{\omega}_{l,-l}\boldsymbol{\Omega}^{-1}_{-l,-1}\boldsymbol{\omega}_{-l,l}$, $m_{il}=\boldsymbol{r}_{il}^{\top}\boldsymbol{\beta}+\boldsymbol{f}_l^{\top}(\boldsymbol{w}_{i,-l}-\boldsymbol{R}_{i,-l}\boldsymbol{\beta})$, $\boldsymbol{w}_{i,-l}$ is an $L-2$ dimensional vector of all components of $\boldsymbol{w}_i$ excluding $w_{il}$, $\boldsymbol{r}_{il}$ is the $l$-th row of $\boldsymbol{R}_i$, $l=1,2,\dots,L-1$. 

The identified parameters are obtained by normalizing with respect to one of the diagonal elements $\frac{1}{\omega_{1,1}^{0.5}}\boldsymbol{\beta}$ and $\frac{1}{\omega_{1,1}}\boldsymbol{\Omega}$.^[Our GUI is based on the *bayesm* package that takes into account this identification restriction to display the outcomes of the posterior chains.]


**Warning:** This model is an example where decisions must be made about setting the model in an identified parameter space versus an unidentified parameter space. The mixing properties of the posterior draws can be better in the latter case [@mcculloch2000bayesian], which typically results in less computational burden. However, it is important to recover the identified space in a final stage. Additionally, defining priors in the unidentified space may have unintended consequences on the posterior distributions in the identified space [@nobile2000comment]. The multinomial probit model presented in this section is set in the unidentified space [@McCulloch1994], while a version of the multinomial probit in the identified space is presented by @mcculloch2000bayesian.

**Example: Choice of fishing mode**

We used in this application the dataset *3Fishing.csv* from @cameron05. The dependent variable is mutually exclusive alternatives regarding fishing modes (mode), where beach is equal to 1, pier is equal to 2, private boat is equal to 3, and chartered boat (baseline alternative) is equal to 4. In this model, we have

\[
\mathbf{X}_i = \begin{bmatrix}
1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & 0 & \text{Price}_{i,1} & \text{Catch rate}_{i,1}\\ 
0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & \text{Price}_{i,2} & \text{Catch rate}_{i,2}\\
0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & \text{Price}_{i,3} & \text{Catch rate}_{i,3}\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & \text{Price}_{i,4} & \text{Catch rate}_{i,4}\\
\end{bmatrix}
\]

In this example, chartered boat is the base category, the number of choice categories is four, there are two alternative-specific regressors (price and catch rate), and one non-alternative-specific regressor (income). This setting involves the estimation of eight location parameters (\(\boldsymbol{\beta}\)): three intercepts, three for income, one for price, and one for catch rate. This is the order of the posterior chains in our GUI. Note that the location coefficients are set equal to 0 for the baseline category. For multinomial models, we strongly recommend using the last category as the baseline.

We also get posterior estimates for a \(3\times 3\) covariance matrix (four alternatives minus one), where the element (1,1) is equal to 1 due to identification restrictions, and elements 2 and 4 are the same, as well as 3 and 7, and 6 and 8, due to symmetry. 

Observe that this identification restriction implies *NaN* values in @Geweke1992 and @Heidelberger1983 tests for element (1,1) of the covariance matrix, and just eight dependence factors associated with the remaining elements of the covariance matrix.

Once our GUI is displayed (see beginning of this chapter), we should follow the next Algorithm to run multinomial probit models in our GUI (see Chapter \@ref(Chap5) for details), which in turn uses the command *rmnpGibbs* from the *bayesm* package.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Multinomial Probit model in the GUI**  

1. Select *Univariate Models* on the top panel  
2. Select *Multinomial Probit* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Select the number of the **Base Alternative**  
7. Select the **Number of choice categorical alternatives**  
8. Select the **Number of alternative specific variables**  
9. Select the **Number of Non-alternative specific variables**  
10. Click the *Build formula* button to generate the formula in **R** syntax.  
11. Set the hyperparameters: mean vector, covariance matrix, scale matrix, and degrees of freedom. This step is not necessary as by default our GUI uses non-informative priors  
12. Click the *Go!* button  
13. Analyze results  
14. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

We ran 100,000 MCMC iterations plus 10,000 as burn-in with a thinning parameter equal to 5, where all priors use default values for the hyperparameters in our GUI. We found that the 95\% credible intervals of the coefficient associated with income for beach and private boat alternatives are equal to  (8.58e-06, 8.88e-05) and (3.36e-05, 1.45e-04). This suggests that the probability of choosing these alternatives increases compared to a chartered boat when income increases. In addition, an increase in the price or a decrease in the catch rate for specific fishing alternatives imply lower probabilities of choosing them as the 95\% credible intervals are (-9.91e-03, -3.83e-03) and (1.40e-01, 4.62e-01), respectively. However, the posterior chain diagnostics suggest there are convergence issues with the posterior draws (see Exercise 5).

## The multinomial logit model {#sec65}

The multinomial logit model is used to model mutually exclusive discrete outcomes or qualitative response variables. However, this model assumes the independence of irrelevant alternatives (IIA), meaning that the choice between two alternatives does not depend on a third alternative. We consider the multinomial mixed logit model (not to be confused with the random parameters logit model), which accounts for both alternative-varying regressors (conditional) and alternative-invariant regressors (multinomial) simultaneously.^[The multinomial mixed logit model can be implemented as a conditional logit model.]

In this setting, there are \( L \) mutually exclusive alternatives, and the dependent variable \( y_{il} \) is equal to 1 if the \( l \)th alternative is chosen by individual \( i \), and 0 otherwise, where \( l=\left\{1,2,\dots,L\right\} \). The likelihood function is 

\[
p(\boldsymbol{\beta} \mid \boldsymbol{y}, \boldsymbol{X}) = \prod_{i=1}^{N} \prod_{l=1}^{L} p_{il}^{y_{il}},
\]

where the probability that individual \( i \) chooses the alternative \( l \) is given by

\[
p_{il} := p(y_i = l \mid \boldsymbol{\beta}, \boldsymbol{X}) = \frac{\exp\left\{\boldsymbol{x}_{il}^{\top} \boldsymbol{\beta}_l\right\}}{\sum_{j=1}^{L} \exp\left\{\boldsymbol{x}_{ij}^{\top} \boldsymbol{\beta}_j\right\}},
\]

\(\boldsymbol{y}\) and \(\boldsymbol{X}\) are the vector and matrix of the dependent variable and regressors, respectively, and \(\boldsymbol{\beta}\) is the vector containing all the coefficients.

Remember that coefficients associated with alternative-invariant regressors are set to 0 for the baseline category, and the coefficients associated with the alternative-varying regressors are the same for all the categories. In addition, we assume \( \boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \boldsymbol{B}_0) \) as the prior distribution. Thus, the posterior distribution is

\[
\pi(\boldsymbol{\beta} \mid \boldsymbol{y}, \boldsymbol{X}) \propto p(\boldsymbol{\beta} \mid \boldsymbol{y}, \boldsymbol{X}) \times \pi(\boldsymbol{\beta}).
\]

As the multinomial logit model does not have a standard posterior distribution, @rossi2012bayesian propose a "tailored" independent Metropolis-Hastings algorithm where the proposal distribution is a multivariate Student's \( t \) distribution with \( v \) degrees of freedom (tuning parameter), mean equal to the maximum likelihood estimator, and scale equal to the inverse of the Hessian matrix.

**Example: Simulation exercise**

Let's conduct a simulation exercise to evaluate the performance of the Metropolis-Hastings algorithm for inference in the multinomial logit model. We consider a scenario with three alternatives, one alternative-invariant regressor (plus the intercept), and three alternative-varying regressors. The population parameters are given by \( \boldsymbol{\beta}_1 = [1 \ -2.5 \ 0.5 \ 0.8 \ -3]^{\top} \), \( \boldsymbol{\beta}_2 = [1 \ -3.5 \ 0.5 \ 0.8 \ -3]^{\top} \), and \( \boldsymbol{\beta}_3 = [0 \ 0 \ 0.5 \ 0.8 \ -3]^{\top} \), where the first two elements of each vector correspond to the intercept and the alternative-invariant regressor, while the last three elements correspond to the alternative-varying regressors. The sample size is 1000, and all regressors are simulated from standard normal distributions.

We can deploy our GUI using the command line at the beginning of this chapter. We should follow the next Algorithm to run multinomial logit models in our GUI (see Chapter \@ref(Chap5) for details).

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Multinomial logit models**  

1. Select *Univariate Models* on the top panel  
2. Select *Multinomial Logit* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Select the **Base Alternative**  
7. Select the **Number of choice categorical alternatives**  
8. Select the **Number of alternative specific variables**  
9. Select the **Number of Non-alternative specific variables**  
10. Click the *Build formula* button to generate the formula in **R** syntax.  
11. Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors  
12. Select the tuning parameter for the Metropolis-Hastings algorithm, that is, the **Degrees of freedom: Multivariate Student's t distribution**  
13. Click the *Go!* button  
14. Analyze results  
15. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following code in **R** demonstrates how to implement the M-H algorithm from scratch. The first part simulates the dataset, the second part constructs the log-likelihood function, and the third part implements the M-H algorithm. We use vague priors centered at zero, with a covariance matrix of $1000\mathbf{I}_7$. We observe that the posterior estimates closely match the population parameters, and all 95\% credible intervals contain the population parameters.

```{r}
remove(list = ls())
set.seed(12345)
# Simulation of data
N<-1000  # Sample Size
B<-c(0.5,0.8,-3); B1<-c(-2.5,-3.5,0); B2<-c(1,1,0)
# Alternative specific attributes of choice 1, for instance, price, quality and duration of choice 1
X1<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) 
# Alternative specific attributes of choice 2, for instance, price, quality and duration of choice 2
X2<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
# Alternative specific attributes of choice 3, for instance, price, quality and duration of choice 3
X3<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
X4<-matrix(rnorm(N,1,1),N,1)
V1<-B2[1]+X1%*%B+B1[1]*X4; V2<-B2[2]+X2%*%B+B1[2]*X4; V3<-B2[3]+X3%*%B+B1[3]*X4
suma<-exp(V1)+exp(V2)+exp(V3)
p1<-exp(V1)/suma; p2<-exp(V2)/suma; p3<-exp(V3)/suma
p<-cbind(p1,p2,p3)
y<- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE))
y1<-y==1; y2<-y==2; y3<-y==3
# Log likelihood
log.L<- function(Beta){
	V1<-Beta[1]+Beta[3]*X4+X1%*%Beta[5:7]
	V2<-Beta[2]+Beta[4]*X4+X2%*%Beta[5:7]
	V3<- X3%*%Beta[5:7]
	suma<-exp(V1)+exp(V2)+exp(V3)
	p11<-exp(V1)/suma; 	p22<-exp(V2)/suma; 	p33<-exp(V3)/suma
	suma2<-NULL
	for(i in 1:N){
		suma1<-y1[i]*log(p11[i])+y2[i]*log(p22[i])+y3[i]*log(p33[i])
		suma2<-c(suma2,suma1)}
	logL<-sum(suma2)
	return(-logL)
}
# Parameters: Proposal
k <- 7
res.optim<-optim(rep(0, k), log.L, method="BFGS", hessian=TRUE)
MeanT <- res.optim$par
ScaleT <- as.matrix(Matrix::forceSymmetric(solve(res.optim$hessian))) # Force this matrix to be symmetric
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
MHfunction <- function(iter, tuning){
	Beta <- rep(0, k); Acept <- NULL 
	BetasPost <- matrix(NA, iter, k)
	pb <- winProgressBar(title = "progress bar", min = 0, max = iter, width = 300)
	for(s in 1:iter){
		LogPostBeta <- -log.L(Beta) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(LaplacesDemon::rmvt(n=1, mu = MeanT, S = ScaleT, df = tuning))
		LogPostBetaC <- -log.L(BetaC) + mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
		alpha <- min(exp((LogPostBetaC-mvtnorm::dmvt(BetaC, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))-(LogPostBeta-mvtnorm::dmvt(Beta, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))) ,1)
		u <- runif(1)
		if(u <= alpha){
			Acepti <- 1; Beta <- BetaC
		}else{
			Acepti <- 0; Beta <- Beta
		}
		BetasPost[s, ] <- Beta; Acept <- c(Acept, Acepti)
		setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),"% done"))
	}
	close(pb); AcepRate <- mean(Acept)
	Results <- list(AcepRate = AcepRate, BetasPost = BetasPost)
	return(Results)
}
# MCMC parameters
mcmc <- 10000; burnin <- 1000; thin <- 5; iter <- mcmc + burnin; keep <- seq(burnin, iter, thin); tuning <- 6 # Degrees of freedom
ResultsPost <- MHfunction(iter = iter, tuning = tuning)
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
```

## Ordered probit model {#sec66}

The ordered probit model is used when there is a natural order in the categorical response variable. In this case, there is a latent variable \( y_i^* = \mathbf{x}_i^{\top}\boldsymbol{\beta} + \mu_i \), where \( \mathbf{x}_i \) is a \( K \)-dimensional vector of regressors, \( \mu_i \stackrel{i.i.d.}{\sim} N(0,1) \), such that \( y_i = l \) if and only if \( \alpha_{l-1} < y_i^* \leq \alpha_l \), for \( l \in \{1, 2, \dots, L\} \), where \( \alpha_0 = -\infty \), \( \alpha_1 = 0 \), and \( \alpha_L = \infty \).^[Identification issues necessitate setting the variance in this model equal to 1 and \( \alpha_1 = 0 \). Observe that multiplying \( y_i^* \) by a positive constant or adding a constant to all of the cut-offs and subtracting the same constant from the intercept does not affect \( y_i \).] Then, the probability of observing \( y_i = l \) is given by:

\[
p(y_i = l) = \Phi(\alpha_l - \mathbf{x}_i^{\top}\boldsymbol{\beta}) - \Phi(\alpha_{l-1} - \mathbf{x}_i^{\top}\boldsymbol{\beta}),
\]

and the likelihood function is:

\[
p(\boldsymbol{\beta}, \boldsymbol{\alpha} \mid \mathbf{y}, \mathbf{X}) = \prod_{i=1}^{N} p(y_i = l \mid \boldsymbol{\beta}, \boldsymbol{\alpha}, \mathbf{X}).
\]

The priors in this model are independent, i.e., \( \pi(\boldsymbol{\beta}, \boldsymbol{\gamma}) = \pi(\boldsymbol{\beta}) \times \pi(\boldsymbol{\gamma}) \), where \( \boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \boldsymbol{B}_0) \) and \( \boldsymbol{\gamma} \sim N(\boldsymbol{\gamma}_0, \boldsymbol{\Gamma}_0) \), and \( \boldsymbol{\gamma} = \left[ \gamma_2 \ \gamma_3 \ \dots \ \gamma_{L-1} \right]^{\top} \), such that:

\[
\boldsymbol{\alpha} = \left[ \exp\left\{\gamma_2\right\} \ \sum_{l=2}^{3} \exp\left\{\gamma_l\right\} \ \dots \ \sum_{l=2}^{L-1} \exp\left\{\gamma_l\right\} \right]^{\top}.
\]

This structure imposes the ordinal condition on the cut-offs.

This model does not have a standard conditional posterior distribution for \( \boldsymbol{\gamma} \) (\( \boldsymbol{\alpha} \)), but it does have a standard conditional distribution for \( \boldsymbol{\beta} \) once data augmentation is used. We can then use a Metropolis-within-Gibbs sampling algorithm. In particular, we use Gibbs sampling to draw \( \boldsymbol{\beta} \) and \( \boldsymbol{y}^* \), where:

\[
\boldsymbol{\beta} \mid \boldsymbol{y}^*, \boldsymbol{\alpha}, \boldsymbol{X} \sim N(\boldsymbol{\beta}_n, \boldsymbol{B}_n),
\]

with \( \boldsymbol{B}_n = (\boldsymbol{B}_0^{-1} + \boldsymbol{X}^{\top}\boldsymbol{X})^{-1} \), \( \boldsymbol{\beta}_n = \boldsymbol{B}_n(\boldsymbol{B}_0^{-1}\boldsymbol{\beta}_0 + \boldsymbol{X}^{\top}\boldsymbol{y}^*) \), and:

\[
y_i^* \mid \boldsymbol{\beta}, \boldsymbol{\alpha}, \boldsymbol{y}, \boldsymbol{X} \sim TN_{(\alpha_{y_i-1}, \alpha_{y_i})}(\boldsymbol{x}_i^{\top}\boldsymbol{\beta}, 1).
\]

We use a random-walk Metropolis--Hastings algorithm for \( \boldsymbol{\gamma} \) with a proposal distribution that is Gaussian with mean equal to the current value and covariance matrix \( s^2(\boldsymbol{\Gamma}_0^{-1} + \hat{\boldsymbol{\Sigma}}_{\gamma}^{-1})^{-1} \), where \( s > 0 \) is a tuning parameter and \( \hat{\boldsymbol{\Sigma}}_{\gamma} \) is the sample covariance matrix associated with \( \gamma \) from the maximum likelihood estimation.

**Example: Determinants of preventive health care visits**

We used the file named *2HealthMed.csv* in this application. In particular, the dependent variable is *MedVisPrevOr*, which is an ordered variable equal to 1 if the individual did not visit a physician for preventive reasons, 2 if the individual visited once in that year, and so on, until it is equal to 6 for visiting five or more times. The latter category represents 1.6% of the sample. Observe that the dependent variable has six categories.

In this example, the set of regressors is given by *SHI*, which is an indicator of being in the subsidized health care system (1 means being in the system), sex (*Female*), age (both linear and squared), socioeconomic conditions indicator (*Est2* and *Est3*), with the lowest being the baseline category, self-perception of health status (*Fair*, *Good*, and *Excellent*), where *Bad* is the baseline, and education level (*PriEd*, *HighEd*, *VocEd*, *UnivEd*), with *no education* as the baseline category.

We ran this application with 50,000 MCMC iterations plus 10,000 as burn-in, and a thinning parameter equal to 5. This setting means 10,000 effective posterior draws. We set \( \boldsymbol{\beta}_0 = \boldsymbol{0}_{11} \), \( \boldsymbol{B}_0 = 1000\boldsymbol{I}_{11} \), \( \boldsymbol{\gamma}_0 = \boldsymbol{0}_4 \), \( \boldsymbol{\Gamma}_0 = \boldsymbol{I}_4 \), and the tuning parameter is 1.

We can run the ordered probit models in our GUI following the steps in the next Algorithm.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Ordered probit models**  

1. Select *Univariate Models* on the top panel  
2. Select *Ordered Probit* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. Remember that this formula must have -1 to omit the intercept in the specification.  
7. Set the hyperparameters: mean vectors and covariance matrices. This step is not necessary as by default our GUI uses non-informative priors
8. Select the number of ordered alternatives
9. Set the hyperparameters: mean and covariance matrix of the cutoffs. This step is not necessary as by default our GUI uses non-informative prior 
10. Select the tuning parameter for the Metropolis-Hastings algorithm  
11. Click the *Go!* button  
12. Analyze results  
13. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following **R** code shows how to perform inference in this model using the command *rordprobitGibbs* from the *bayesm* library, which is the command that our GUI uses.


```{r}
rm(list = ls())
set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- MedVisPrevOr 
# MedVisPrevOr: Oredered variable for preventive visits to doctors in one year: 1 (none), 2 (once), ... 6 (five or more)
X <- cbind(SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent, PriEd, HighEd, VocEd, UnivEd)
k <- dim(X)[2]
L <- length(table(y))
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
gamma0 <- rep(0, L-2); Gamma0 <- diag(L-2)
# MCMC parameters
mcmc <- 60000+1; thin <- 5; tuningPar <- 1/(L-2)^0.5
DataApp <- list(y = y, X = X, k = L)
Prior <- list(betabar = b0, A = solve(B0), dstarbar = gamma0, Ad = Gamma0)
mcmcpar <- list(R = mcmc, keep = 5, s = tuningPar, nprint = 0)
PostBeta <- bayesm::rordprobitGibbs(Data = DataApp, Prior = Prior, Mcmc = mcmcpar)
BetasPost <- coda::mcmc(PostBeta[["betadraw"]])
colnames(BetasPost) <- c("SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent", "PriEd", "HighEd", "VocEd", "UnivEd")
summary(BetasPost)		
# Convergence diagnostics
coda::geweke.diag(BetasPost)
coda::raftery.diag(BetasPost,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(BetasPost)
# Cut offs
Cutoffs <- PostBeta[["cutdraw"]]
summary(Cutoffs)
coda::geweke.diag(Cutoffs)
coda::heidel.diag(Cutoffs)
coda::raftery.diag(Cutoffs[,-1],q=0.5,r=0.05,s = 0.95)
```


The results suggest that older individuals (at a decreasing rate) in the subsidized health program, characterized by being in the second socioeconomic status, with an increasing self-perception of good health, and not having high school as their highest education degree, have a higher probability of visiting a physician for preventive health purposes. Convergence diagnostics look well, except for the self-health perception draws.

We also obtained the posterior estimates of the cutoffs in the ordered probit model. These estimates are necessary to calculate the probability that an individual is in a specific category of physician visits. Due to identification restrictions, the first cutoff is set equal to 0. This is why we observe *NaN* values in @Geweke1992 and @Heidelberger1983 tests, and only four values in the @Raftery1992 test, which correspond to the remaining free cutoffs. It seems that these cutoff estimates have some convergence issues when using the @Raftery1992 test as a diagnostic tool. Furthermore, their dependence factors are also very high.

## Negative binomial model

The dependent variable in the negative binomial model is a nonnegative integer or count. In contrast to the Poisson model, the negative binomial model accounts for over-dispersion. The Poisson model assumes equi-dispersion, meaning the mean and variance are equal.

We assume that $y_i \stackrel{i.i.d.} {\sim} \text{NB}(\gamma, \theta_i)$, where the density function for individual $i$ is 
$$
\frac{\Gamma(y_i + \gamma)}{\Gamma(\gamma) y_i!} (1 - \theta_i)^{y_i} \theta_i^{\gamma},
$$
with the success probability $\theta_i = \frac{\gamma}{\lambda_i + \gamma}$, where $\lambda_i = \exp\left\{\mathbf{x}_i^{\top} \boldsymbol{\beta}\right\}$ is the mean, and $\gamma = \exp\left\{\alpha \right\}$ is the target for the number of successful trials, or the dispersion parameter, and $\mathbf{x}_i$ is a $K$-dimensional vector of regressors.

We assume independent priors for this model: $\boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \mathbf{B}_0)$ and $\alpha \sim G(\alpha_0, \delta_0)$.

This model does not have standard conditional posterior distributions. Therefore, @rossi2012bayesian propose using a random-walk Metropolis–Hastings algorithm where the proposal distribution for $\boldsymbol{\beta}$ is Gaussian, centered at the current stage, with covariance matrix $s_{\boldsymbol{\beta}}^2 \hat{\mathbf{\Sigma}}_{\boldsymbol{\beta}}$, where $s_{\boldsymbol{\beta}}$ is a tuning parameter and $\hat{\mathbf{\Sigma}}_{\boldsymbol{\beta}}$ is the maximum likelihood covariance estimator. Additionally, the proposal for $\alpha$ is normal, centered at the current value, with variance $s_{\alpha}^2 \hat{\sigma}_{\alpha}^2$, where $s_{\alpha}$ is a tuning parameter and $\hat{\sigma}_{\alpha}^2$ is the maximum likelihood variance estimator.

**Example: Simulation exercise**

Let's do a simulation exercise to check the performance of the M-H algorithms in the negative binomial model. There are two regressors, $x_{i1} \sim U(0,1)$ and $x_{i2} \sim N(0,1)$, and the intercept. The dispersion parameter is $\gamma = \exp\left\{1.2\right\}$, and $\boldsymbol{\beta} = \left[1 \ 1 \ 1\right]^{\top}$. The sample size is 1,000.

We run this simulation using 10,000 MCMC iterations, a burn-in equal to 1,000, and a thinning parameter equal to 5. We set vague priors for the location parameters, particularly, $\boldsymbol{\beta}_0 = \boldsymbol{0}_{3}$ and $\boldsymbol{B}_0 = 1000\boldsymbol{I}_{3}$, and $\alpha_0 = 0.5$ and $\delta_0 = 0.1$, which are the default values in the *rnegbinRw* command from the *bayesm* package in **R**. In addition, the tuning parameters of the Metropolis--Hastings algorithms are $s_{\boldsymbol{\beta}} = 2.93/k^{1/2}$ and $s_{\alpha} = 2.93$, which are also the default parameters in *rnegbinRw*, where $k$ is the number of location parameters.

We can run the negative binomial models in our GUI following the steps in the next Algorithm.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Negative binomial models**  

1. Select *Univariate Models* on the top panel  
2. Select *Negative Binomial (Poisson)* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**  
7. Set the hyperparameters: mean vector, covariance matrix, shape, and scale parameters. This step is not necessary as by default our GUI uses non-informative priors  
8. Select the tuning parameters for the Metropolis-Hastings algorithms  
9. Click the *Go!* button  
10. Analyze results  
11. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following **R** code shows how to perform inference in the negative binomial model programming the M-H algorithms from scratch. We ask to estimate this example using the *rnegbinRw* command in Exercise 8.

We observe from the results that all 95\% credible intervals encompass the population parameters, and the posterior means are very close to the population parameters.

```{r}
rm(list = ls())
set.seed(010101)
N <- 2000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2); k <- dim(X)[2]; B <- rep(1, k)
alpha <- 1.2; gamma <- exp(alpha); lambda <- exp(X%*%B)
y <- rnbinom(N, mu = lambda, size = gamma)
# log likelihood
logLik <- function(par){
	alpha <- par[1]; beta <- par[2:(k+1)]
	gamma <- exp(alpha)
	lambda <- exp(X%*%beta)
	logLikNB <- sum(sapply(1:N, function(i){dnbinom(y[i], size = gamma, mu = lambda[i], log = TRUE)}))
	return(-logLikNB)
}
# Parameters: Proposal
par0 <- rep(0.5, k+1)
res.optim <- suppressWarnings(optim(par0, logLik, method="BFGS", hessian=TRUE))
res.optim$par
res.optim$convergence
Covar <- solve(res.optim$hessian) 
CovarBetas <- Covar[2:(k+1),2:(k+1)]
VarAlpha <- Covar[1:1]
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
alpha0 <- 0.5; delta0 <- 0.1
# Metropolis-Hastings function 
MHfunction <- function(iter, sbeta, salpha){
	Beta <- rep(0, k); 	Acept1 <- NULL; Acept2 <- NULL
	BetasPost <- matrix(NA, iter, k); alpha <- 1
	alphaPost <- rep(NA, iter); par <- c(alpha, Beta)
	pb <- winProgressBar(title = "progress bar", min = 0, max = iter, width = 300)
	for(s in 1:iter){
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(MASS::mvrnorm(1, mu = Beta, Sigma = sbeta^2*CovarBetas))
		parC <- c(alpha, BetaC)
		LogPostBetaC <- -logLik(parC) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
		alpha1 <- min(exp((LogPostBetaC - mvtnorm::dmvnorm(BetaC, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))-(LogPostBeta - mvtnorm::dmvnorm(Beta, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))),1)
		u1 <- runif(1)
		if(u1 <= alpha1){Acept1i <- 1; Beta <- BetaC}else{
			Acept1i <- 0; Beta <- Beta
		}
		par <- c(alpha, Beta)
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alphaC <- rnorm(1, mean = alpha, sd = salpha*VarAlpha^0.5)
		parC <- c(alphaC, Beta)
		LogPostBetaC <- -logLik(parC) + dgamma(alphaC, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alpha2 <- min(exp((LogPostBetaC - dnorm(alphaC, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))-(LogPostBeta - dnorm(alpha, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))),1)
		u2 <- runif(1)
		if(u2 <= alpha2){Acept2i <- 1; alpha <- alphaC}else{
			Acept2i <- 0; alpha <- alpha
		}
		
		BetasPost[s, ] <- Beta; alphaPost[s] <- alpha
		Acept1 <- c(Acept1, Acept1i); Acept2 <- c(Acept2, Acept2i)
		setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),"% done"))
	}
	close(pb)
	AcepRateBeta <- mean(Acept1); AcepRateAlpha <- mean(Acept2)
	Results <- list(AcepRateBeta = AcepRateBeta, AcepRateAlpha = AcepRateAlpha, BetasPost = BetasPost, alphaPost = alphaPost)
	return(Results)
}
# MCMC parameters
mcmc <- 10000
burnin <- 1000
thin <- 5
iter <- mcmc + burnin
keep <- seq(burnin, iter, thin)
sbeta <- 2.93/sqrt(k); salpha <- 2.93
# Run M-H
ResultsPost <- MHfunction(iter = iter, sbeta = sbeta, salpha = salpha)
ResultsPost$AcepRateBeta
ResultsPost$AcepRateAlpha
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
summary(coda::mcmc(ResultsPost$alphaPost[keep[-1]]))
```

## Tobit model {#sec68}

The dependent variable is partially observed in Tobit models due to sampling schemes, whereas the regressors are completely observed. In particular,

\begin{equation}
	y_i = \begin{Bmatrix}
		L, & \quad y_i^* < L, \\
		y_i^*, & \quad L \leq y_i^* < U, \\
		U, & \quad y_i^* \geq U,
	\end{Bmatrix}
\end{equation}

where \( y_i^* \stackrel{i.i.d.}{\sim} N(\mathbf{x}_i^{\top} \boldsymbol{\beta}, \sigma^2) \), \( \mathbf{x}_i \) is a \( K \)-dimensional vector of regressors.^[We can set \( L \) or \( U \) equal to \( -\infty \) or \( \infty \) to model data censored on just one side.]

We use conjugate independent priors \( \boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \mathbf{B}_0) \) and 
\( \sigma^2 \sim IG(\alpha_0/2, \delta_0/2) \), and data augmentation using \( \mathbf{y}^*_C \) such that \( y_{C_i}^* \stackrel{i.n.d.}{\thicksim} N(\mathbf{x}_i^{\top} \boldsymbol{\beta}, \sigma^2) \), \( y_{C_i} = \left\{ y_{C_i^L}^* \cup y_{C_i^U}^* \right\} \) are lower and upper censored data. This allows implementing the Gibbs sampling algorithm [@Chib1992].

Then,

\begin{align}
\pi(\boldsymbol{\beta}, \sigma^2, \mathbf{y^*} \mid \mathbf{y}, \mathbf{X}) &\propto \prod_{i=1}^N \left[ \mathbb{1}({y_i = L}) \mathbb{1}({y_{C_i^L}^* < L}) + \mathbb{1}({L \leq y_i < U}) + \mathbb{1}({y_i = U}) \mathbb{1}({y_{C_i^U}^* \geq U}) \right] \\
&\times N(y_i^* \mid \mathbf{x}_i^{\top} \boldsymbol{\beta}, \sigma^2) \times N(\boldsymbol{\beta} \mid \boldsymbol{\beta}_0, \mathbf{B}_0) \times IG(\sigma^2 \mid \alpha_0/2, \delta_0/2)
\end{align}

The posterior distributions are:

\begin{equation}
	y_{C_i}^* \mid \boldsymbol{\beta}, \sigma^2, \mathbf{y}, \mathbf{X} \sim \begin{Bmatrix}
		TN_{(-\infty,L)}(\mathbf{x}_i^{\top}\boldsymbol{\beta}, \sigma^2) \ , \ y_i = L \\
		TN_{[U, \infty)}(\mathbf{x}_i^{\top}\boldsymbol{\beta}, \sigma^2) \ \ , \ y_i = U
	\end{Bmatrix}
\end{equation}

\begin{equation}
	\boldsymbol{\beta} \mid \sigma^2, \mathbf{y}, \mathbf{X} \sim N(\boldsymbol{\beta}_n, \sigma^2 \mathbf{B}_n)
\end{equation}

\begin{equation}
	\sigma^2 \mid \boldsymbol{\beta}, \mathbf{y}, \mathbf{X} \sim IG(\alpha_n/2, \delta_n/2)
\end{equation}

where \( \mathbf{B}_n = (\mathbf{B}_0^{-1} + \sigma^{-2} \mathbf{X}^{\top} \mathbf{X})^{-1} \), \( \boldsymbol{\beta}_n = \mathbf{B}_n(\mathbf{B}_0^{-1} \boldsymbol{\beta}_0 + \sigma^{-2} \mathbf{X}^{\top} \mathbf{y}^*) \), \( \alpha_n = \alpha_0 + N \), and \( \delta_n = \delta_0 + (\mathbf{y}^* - \mathbf{X} \boldsymbol{\beta})^{\top}(\mathbf{y}^* - \mathbf{X} \boldsymbol{\beta}) \).

**Example: The market value of soccer players in Europe continues**

We continue with the example of the market value of soccer players from Section \@ref(sec61). We specify the same equation but assume that the sample is censored from below, such that we only have information for soccer players whose market value is higher than one million euros. The dependent variable is *log(ValueCens)*, and the left censoring point is 13.82.

The following Algorithm illustrates how to estimate Tobit models in our GUI. Our GUI utilizes the *MCMCtobit* command from the *MCMCpack* package.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Tobit models**  

1. Select *Univariate Models* on the top panel  
2. Select *Tobit* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**  
7. Set the left and right censoring points. To censor above only, specify *-Inf* in the left censoring box, and to censor below only, specify *Inf* in the right censoring box  
8. Set the hyperparameters: mean vector, covariance matrix, shape, and scale parameters. This step is not necessary as by default our GUI uses non-informative priors  
9. Click the *Go!* button  
10. Analyze results  
11. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

We run this application using the same hyperparameters that we set in the example of Section \@ref(sec61). All results seem similar to those in the example of linear models. In addition, the posterior chains seem to achieve good diagnostics.

```{r}
rm(list = ls()); set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(ValueCens) 
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001; a0 <- 0.001
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 50000
burnin <- 10000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCtobit(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf)
summary(coda::mcmc(posterior))
# Gibbs sampling functions
XtX <- t(X)%*%X
PostBeta <- function(Yl, sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*t(X)%*%Yl)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostYl <- function(Beta, L, U, i){
	Ylmean <- X[i,]%*%Beta
	if(y[i] == L){
		Yli <- truncnorm::rtruncnorm(1, a = -Inf, b = L, mean = Ylmean, sd = sig2^0.5)
	}else{
		if(y[i] == U){
			Yli <- truncnorm::rtruncnorm(1, a = U, b = Inf, mean = Ylmean, sd = sig2^0.5)
		}else{
			Yli <- y[i]
		}
	}
	return(Yli)
}
PostSig2 <- function(Beta, Yl){
	an <- a0 + length(y)
	dn <- d0 + t(Yl - X%*%Beta)%*%(Yl - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBetas <- matrix(0, mcmc+burnin, k); Beta <- rep(0, k)
PostSigma2 <- rep(0, mcmc+burnin); sig2 <- 1
L <- log(1000000); U <- Inf
# create progress bar in case that you want to see iterations progress
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	Yl <- sapply(1:N, function(i){PostYl(Beta = Beta, L = L, U = U, i)})
	Beta <- PostBeta(Yl = Yl, sig2)
	sig2 <- PostSig2(Beta = Beta, Yl = Yl) 
	PostBetas[s,] <- Beta; PostSigma2[s] <- sig2
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), "% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
summary(coda::mcmc(PostSigma2[keep]))
```

## Quantile regression {#sec69}

In quantile regression, the location parameters vary according to the quantile of the dependent variable. Let $q_{\tau}(\boldsymbol{x}_i) = \boldsymbol{x}_i^{\top} \boldsymbol{\beta}_{\tau}$ denote the $\tau$-th quantile regression function of $y_i$ given $\boldsymbol{x}_i$, where $\boldsymbol{x}_i$ is a $K$-dimensional vector of regressors, and $0 < \tau < 1$. Specifically, we have the model $y_i = \boldsymbol{x}_i^{\top} \boldsymbol{\beta}_{\tau} + \mu_i$, with the condition $\int_{-\infty}^{0} f_{\tau}(\mu_i) \, d\mu_i = \tau$, meaning that the $\tau$-th quantile of $\mu_i$ is 0.

In particular, @Kozumi2011 propose the asymmetric Laplace distribution for $f_{\tau}(\mu_i)$, given by

$$ f_{\tau}(\mu_i) = \tau(1 - \tau) \exp\left\{- \mu_i(\tau - \mathbb{1}({\mu_i < 0})) \right\}, $$

where $\mu_i(\tau - \mathbb{1}({\mu_i < 0}))$ is the check (loss) function. These authors also propose a location-scale mixture of normals representation, given by

$$ \mu_i = \theta e_i + \psi \sqrt{e_i} z_i, $$

where $\theta = \frac{1 - 2\tau}{\tau(1 - \tau)}$, $\psi^2 = \frac{2}{\tau(1 - \tau)}$, $e_i \sim E(1)$, and $z_i \sim N(0,1)$, with $e_i \perp z_i$.^[ $E$ denotes an exponential density. ] As a result of this representation and the fact that the sample is i.i.d., the likelihood function is

$$ p(\boldsymbol{y} \mid \boldsymbol{\beta}_{\tau}, \boldsymbol{e}, \boldsymbol{X}) \propto \left( \prod_{i=1}^{N} e_i^{-1/2} \right) \exp\left\{- \sum_{i=1}^{N} \frac{(y_i - \boldsymbol{x}_i^{\top} \boldsymbol{\beta}_{\tau} - \theta e_i)^2}{2 \psi^2 e_i} \right\}. $$

Assuming a normal prior for $\boldsymbol{\beta}_{\tau}$, i.e., $\boldsymbol{\beta}_{\tau} \sim N(\boldsymbol{\beta}_{\tau 0}, \boldsymbol{B}_{\tau 0})$, and using data augmentation for $\boldsymbol{e}$, we can implement a Gibbs sampling algorithm for this model. The posterior distributions are as follows:

\begin{equation*}
    \boldsymbol{\beta}_{\tau} \mid \boldsymbol{e}, \boldsymbol{y}, \boldsymbol{X} \sim N(\boldsymbol{\beta}_{n\tau}, \boldsymbol{B}_{n\tau}),
\end{equation*}

\begin{equation*}
    e_i \mid \boldsymbol{\beta}_{\tau}, \boldsymbol{y}, \boldsymbol{X} \sim \text{GIG}\left( \frac{1}{2}, \alpha_{ni}, \delta_{ni} \right),
\end{equation*}

where

\begin{align*}
    \boldsymbol{B}_{n\tau} &= \left( \boldsymbol{B}_{\tau 0}^{-1} + \sum_{i=1}^{N} \frac{\boldsymbol{x}_i \boldsymbol{x}_i^{\top}}{\psi^2 e_i} \right)^{-1}, \\
    \boldsymbol{\beta}_{n\tau} &= \boldsymbol{B}_{n\tau} \left( \boldsymbol{B}_{\tau 0}^{-1} \boldsymbol{\beta}_{\tau 0} + \sum_{i=1}^{N} \frac{\boldsymbol{x}_i (y_i - \theta e_i)}{\psi^2 e_i} \right), \\
    \alpha_{ni} &= \frac{(y_i - \boldsymbol{x}_i^{\top} \boldsymbol{\beta}_{\tau})^2}{\psi^2}, \quad \delta_{ni} = 2 + \frac{\theta^2}{\psi^2}.
\end{align*}

**Example: The market value of soccer players in Europe continues**

We continue the example of the market value of soccer players from Section \@ref(sec61). Now, we want to examine whether the marginal effect of having been on the national team varies with the quantile of the market value of top soccer players in Europe. Thus, we use the same regressors as in the previous example, but analyze the effects at the 0.5-th and 0.9-th quantiles of *NatTeam*.

The following Algorithm shows how to estimate quantile regression models in our GUI. Our GUI uses the command *MCMCquantreg* from the package *MCMCpack*. The following code demonstrates how to perform this analysis using the package.

The results show that at the median market value (0.5-th quantile), the 95% credible interval for the coefficient associated with *national team* is (0.34, 1.02), with a posterior mean of 0.69. At the 0.9-th quantile, these values are (0.44, 1.59) and 1.03, respectively. It appears that being on the national team increases the market value of more expensive players more significantly on average, although there is some overlap in the credible intervals.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Quantile regression**  

1. Select *Univariate Models* on the top panel  
2. Select *Quantile* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select MCMC iterations, burn-in, and thinning parameters using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**  
7. Set the quantile to be analyzed, by default it is 0.5  
8. Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors  
9. Click the *Go!* button  
10. Analyze results  
11. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

```{r}
rm(list = ls()); set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(ValueCens) 
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
k <- dim(X)[2]; N <- dim(X)[1]
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k); B0i <- solve(B0)
# MCMC parameters
mcmc <- 50000; burnin <- 10000
tot <- mcmc + burnin; thin <- 1
# Quantile
q <- 0.5
posterior05  <- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf)
summary(coda::mcmc(posterior05))
q <- 0.9
posterior09  <- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf)
summary(coda::mcmc(posterior09))
```

## Bayesian bootstrap regression {#sec610}

We implement the Bayesian bootstrap [@Rubin1981] for linear regression models. In particular, the Bayesian bootstrap simulates the posterior distributions by assuming that the sample cumulative distribution function (CDF) is the population CDF (this assumption is also implicit in the frequentist bootstrap [@Efron1979]).

Given \( y_i \stackrel{i.i.d.}{\sim} \mathcal{F} \), where \( \mathcal{F} \) does not specify a particular parametric family of distributions, but instead sets \( \mathbb{E}(y_i \mid \boldsymbol{x}_i) = \boldsymbol{x}_i^{\top} \boldsymbol{\beta} \), with \( \boldsymbol{x}_i \) being a \( K \)-dimensional vector of regressors and \( \boldsymbol{\beta} \) a \( K \)-dimensional vector of parameters, the Bayesian bootstrap generates posterior probabilities for each \( y_i \), where the values of \( \boldsymbol{y} \) that are not observed have zero posterior probability.

The algorithm to implement the Bayesian bootstrap is the following:

```{=html}
<figcaption><b>Algorithm: Bayesian bootstrap</b></figcaption>
<figure id="alg:Gibbs">
<pre>
Draw <b>g</b> ∼ Dir(α<sub>1</sub>, α<sub>2</sub>, ..., α<sub>N</sub>) such that α<sub>i</sub>=1 for all i 
<b>g</b>=(g<sub>1</sub>, g<sub>2</sub>, ..., g<sub>N</sub>) is the vector of probabilities to attach to (y<sub>1</sub>,<b>x</b><sub>1</sub>), (y<sub>2</sub>,<b>x</b><sub>2</sub>), ..., (y<sub>N</sub>,<b>x</b><sub>N</sub>) for each Bayesian bootstrap replication.
Sample (y<sub>i</sub>,<b>x</b><sub>i</sub>) N times with replacement and probabilities g<sub>i</sub>, i=1,2, ...,N.
Estimate <b>β</b> using ordinary least squares in the model E(<b>y</b>|<b>X</b>)=<b>X</b><b>β</b>, <b>y</b> being a S<sub>1</sub> dimensional vector, and <b>X</b> a S<sub>1</sub> X K matrix from the previous stage. 
Repeat this process S times.
The distribution of <b>β</b><sup>(s)</sup> is the Bayesian distribution of <b>β</b>.
</pre>
</figure>
```

**Example: Simulation exercise**  

Let's perform a simulation exercise to evaluate the performance of the previous Algorithm for inference using the Bayesian bootstrap. The data-generating process is defined by two regressors, each distributed as standard normal. The location vector is $\boldsymbol{\beta} = \left[1 \ 1 \ 1\right]^{\top}$, with a variance of $\sigma^2 = 1$, and the sample size is 1,000.  

The following Algorithm illustrates how to use our GUI to run the Bayesian bootstrap. Our GUI is based on the *bayesboot* command from the *bayesboot* package in **R**. Exercise 11 asks about using this package to perform inference in this simulation and compares the results with those obtained using our GUI with $S = 10000$.

::: {.algorithm}
<div style="background-color: #f8f9fa; padding: 15px; border-left: 5px solid #343a40; box-shadow: 4px 4px 10px rgba(0, 0, 0, 0.1); border-radius: 5px;">

**Algorithm: Bayesian Bootstrap in Linear Regression**  

1. Select *Univariate Models* on the top panel  
2. Select *Bootstrap* model using the left radio button  
3. Upload the dataset by first selecting whether there is a header in the file, and the kind of separator in the *csv* file of the dataset (comma, semicolon, or tab). Then, use the *Browse* button under the **Choose File** legend. You should see a preview of the dataset  
4. Select number of bootstrap replications using the *Range sliders*  
5. Select dependent and independent variables using the *Formula builder* table  
6. Click the *Build formula* button to generate the formula in **R** syntax. You can modify the formula in the **Main equation** box using valid arguments of the *formula* command structure in **R**  
7. Click the *Go!* button  
8. Analyze results  
9. Download posterior chains and diagnostic plots using the *Download Posterior Chains* and *Download Posterior Graphs* buttons  

</div>
:::

The following **R** code shows how to program the Bayesian bootstrap from scratch. We observe from the results that all 95\% credible intervals encompass the population parameters, and the posterior means are close to the population parameters.

```{r}
rm(list = ls()); set.seed(010101)
N <- 1000; x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(x1, x2); k <- dim(X)[2]
B <- rep(1, k+1); sig2 <- 1
u <- rnorm(N, 0, sig2); y <- cbind(1, X)%*%B + u
data <- as.data.frame(cbind(y, X))
names(data) <- c("y", "x1", "x2")
Reg <- function(d){
	Reg <- lm(y ~ x1 + x2, data = d)
	Bhat <- Reg$coef
	return(Bhat)
}
S <- 10000; alpha <- 1
BB <- function(S, df, alpha){
	Betas <- matrix(NA, S, dim(df)[2])
	N <- dim(df)[1]
	pb <- winProgressBar(title = "progress bar", min = 0, max = S, width = 300)
	for(s in 1:S){
		g <- LaplacesDemon::rdirichlet(N, alpha)
		ids <- sample(1:N, size = N, replace = TRUE, prob = g)
		datas <- df[ids,]
		names(datas) <- names(df)
		Bs <- Reg(d = datas)
		Betas[s, ] <- Bs
		setWinProgressBar(pb, s, title=paste( round(s/S*100, 0), "% done"))
	}
	close(pb)
	return(Betas)
}
BBs <- BB(S = S, df = data, alpha = alpha)
summary(coda::mcmc(BBs))
```

## Summary {#sec611}

In this chapter, we present the core univariate regression models and demonstrate how to perform Bayesian inference using Markov Chain Monte Carlo (MCMC) methods. Specifically, we cover a range of algorithms: Gibbs sampling, Metropolis-Hastings, nested Metropolis-Hastings, and Metropolis-Hastings-within-Gibbs. These algorithms form the foundation for performing Bayesian inference in more complex settings using cross-sectional datasets.

## Exercises {#sec612}

1. Get the posterior conditional distributions of the Gaussian linear model assuming independent priors \(\pi(\boldsymbol{\beta}, \sigma^2) = \pi(\boldsymbol{\beta}) \times \pi(\sigma^2)\), where \(\boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \boldsymbol{B}_0)\) and \(\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)\).

2. Given the model \(y_i \sim N(\boldsymbol{x}_i^{\top} \boldsymbol{\beta}, \sigma^2/\tau_i)\) (Gaussian linear model with heteroskedasticity) with independent priors, \(\pi(\boldsymbol{\beta}, \sigma^2, \boldsymbol{\tau}) = \pi(\boldsymbol{\beta}) \times \pi(\sigma^2) \times \prod_{i=1}^N \pi(\tau_i)\), where \(\boldsymbol{\beta} \sim N(\boldsymbol{\beta}_0, \boldsymbol{B}_0)\), \(\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)\), and \(\tau_i \sim G(v/2, v/2)\). Show that 
   \[
   \boldsymbol{\beta} \mid \sigma^2, \boldsymbol{\tau}, \boldsymbol{y}, \boldsymbol{X} \sim N(\boldsymbol{\beta}_n, \boldsymbol{B}_n), \quad \sigma^2 \mid \boldsymbol{\beta}, \boldsymbol{\tau}, \boldsymbol{y}, \boldsymbol{X} \sim IG(\alpha_n/2, \delta_n/2),
   \]
   and 
   \[
   \tau_i \mid \boldsymbol{\beta}, \sigma^2, \boldsymbol{y}, \boldsymbol{X} \sim G(v_{1n}/2, v_{2in}/2),
   \]
   where \(\boldsymbol{\tau} = [\tau_1 \dots \tau_n]^{\top}\), \(\boldsymbol{B}_n = (\boldsymbol{B}_0^{-1} + \sigma^{-2} \boldsymbol{X}^{\top} \Psi \boldsymbol{X})^{-1}\), 
   \[
   \boldsymbol{\beta}_n = \boldsymbol{B}_n (\boldsymbol{B}_0^{-1} \boldsymbol{\beta}_0 + \sigma^{-2} \boldsymbol{X}^{\top} \Psi \boldsymbol{y}),
   \]
   \(\alpha_n = \alpha_0 + N\), \(\delta_n = \delta_0 + (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^{\top} \Psi (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})\),
   \(v_{1n} = v + 1\), \(v_{2in} = v + \sigma^{-2}(y_i - \boldsymbol{x}_i^{\top} \boldsymbol{\beta})^2\), and \(\Psi = \text{diagonal}\{\tau_i\}\).

3. **The market value of soccer players in Europe continues**  
   Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the market value of soccer players in Europe, setting \(v = 5\) and the same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case?

4. **Example: Determinants of hospitalization continues**  
   Program a Gibbs sampling algorithm in the application of determinants of hospitalization.

5. **Choice of the fishing mode continues**
   - Run the Algorithm of the Multinomial Probit of the book to show the results of the Geweke [@Geweke1992], Raftery [@Raftery1992], and Heidelberger [@Heidelberger1983] tests using our GUI.
   - Use the command *rmnpGibbs* to do the example of the choice of the fishing mode.

6. **Simulation exercise of the multinomial logit model continues**  
   Perform inference in the simulation of the multinomial logit model using the command *rmnlIndepMetrop* from the *bayesm* package of **R** and using our GUI.

7. **Simulation of the ordered probit model**  
   Simulate an ordered probit model where the first regressor distributes \(N(6, 5)\) and the second distributes \(G(1, 1)\), the location vector is \(\boldsymbol{\beta} = \left[ 0.5, -0.25, 0.5 \right]^{\top}\), and the cutoffs are in the vector \(\boldsymbol{\alpha} = \left[ 0, 1, 2.5 \right]^{\top}\). Program from scratch a Metropolis-within-Gibbs sampling algorithm to perform inference in this simulation.

8. **Simulation of the negative binomial model continues**  
   Perform inference in the simulation of the negative binomial model using the *bayesm* package in **R** software.

9. **The market value of soccer players in Europe continues**  
   Perform the application of the value of soccer players with left censoring at one million Euros in our GUI using the Algorithm of the Tobit models, and the hyperparameters of the example.

10. **The market value of soccer players in Europe continues**  
    Program from scratch the Gibbs sampling algorithm in the example of the market value of soccer players at the 0.75 quantile.

11. Use the *bayesboot* package to perform inference in the simulation exercise of Section \@ref(sec610), and compare the results with the ones that we get using our GUI, setting \(S = 10000\).



<!--chapter:end:06-Univariatereg.Rmd-->

# Time series {#Chap8}

We will show the state-space representation of time series models with their theory foundation, and perform applications using R and our GUI. We will have mathematical and computational exercises in our GUI and in R.

<!--chapter:end:08-Timeseries.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:15-References.Rmd-->

