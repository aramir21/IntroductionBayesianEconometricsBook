\chapter{Machine learning}\label{chap13}

Machine learning approaches are characterized by high-dimensional parameter spaces that are implicit in non-parametric inference. Take into account that non-parametric inference refers to models of potentially infinite parameters, rather than absence of these ones.

\section{Cross validation and Bayes factors}\label{sec13_1}
The issue of overfitting in Bayesian inference is mitigated due to its inherent shrinkage property when proper priors are used. Remember that the posterior distributions is a compromise between the sample information and the prior information.

\section{Regularization}\label{sec13_2}
The linear normal model using the conjugate family is ridge regression \cite{Ishwaran2005}. We can use empirical Bayes to select the scale parameter of the prior covariance matrix of the location parameters, which is in turn the regularization parameter in the ridge regression (see my class notes in MSc in Data Science and Analytic).

\subsection{Bayesian LASSO}\label{sec13_21}

\subsection{Stochastic search variable selection}\label{sec13_22}

\subsection{Non-local priors}\label{sec13_23}

\cite{johnson2012bayesian}
R package: mombf (Model Selection with Bayesian Methods and Information Criteria)
link: https://cran.r-project.org/web/packages/mombf/index.html

\section{Bayesian additive regression trees}\label{sec13_3}

\section{Gaussian processes}\label{13_4}

Remember the relevance of GP in Bayesian optimization, and consequently, in pseudo-marginal MCMC, which is a case where the likelihood function cannot be evaluated exactly but can be estimated unbiasedly. This method is particularly useful in scenarios where latent variables or complex models make direct likelihood computations intractable.