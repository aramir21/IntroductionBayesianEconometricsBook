\chapter{Longitudinal/Panel data models}\label{chap9}

We describe how to perform inference in longitudinal/panel models using a Bayesian framework. In this context, multiple cross-sectional units are observed repeatedly over time, a structure referred to as panel data by econometricians and longitudinal data by statisticians. Specifically, we present models for continuous (normal), binary (logit), and count (Poisson) responses. Applications and exercises illustrate the potential of these models.

In longitudinal/panel data sets, we have $y_{it}$ where $i=1,2,\dots,N$ and $t=1,2,\dots,T_i$. If $T_i=T$ for all $i$, the dataset is \textit{balanced}; otherwise, it is \textit{unbalanced}. Longitudinal data typically involves by far more cross-sectional units than time periods, this is called typically a \textit{short panel}. It assumes that cross-sectional units are independent, though serial correlation exists within each unit over time, and unobserved heterogeneity for each unit must be accounted for. We can treat this unobserved heterogeneity as random variables, assuming it is either independent or dependent on control variables. Econometricians refer to these cases as \textit{random effects} and \textit{fixed effects}, respectively. The Bayesian literature takes a different approach, modeling the panel structure hierarchically, where the unobserved heterogeneity may or may not depend on other controls.\footnote{See \cite{rendon2013fixed} for a nice comparison of Frequentist and Bayesian treatments of panel data models.}

Remember that the easiest way to run our GUI is typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
	\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor, and once our GUI is deployed, select \textit{Hierarchical Longitudinal Models}. However, users should see Chapter \ref{chapGUI} for details.

\section{Normal model}\label{sec91}

The longitudinal/panel normal model establishes $\bm{y}_i=\bm{X}_i\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\mu}_i$ where $\bm{y}_i$ are $T_i$-dimensional vectors corresponding to units $i=1,2,\dots,N$, $\bm{X}_i$ and $\bm{W}_i$ are $T_i\times K_1$ and $T_i\times K_2$ matrices, respectively. In the statistical literature, $\bm{\beta}$ is a $K_1$-dimensional vector of \textit{fixed effects}, and $\bm{b}_i$ is a $K_2$-dimensional vector of unit-specific \textit{random effects} that allow unit-specific means, and enable to capture marginal dependence among the observations on the cross-sectional units. We assume normal stochastic errors, $\bm{\mu}_i\sim{N}(\bm{0},\sigma^2\bm{I}_{T_i})$, which means that the likelihood function is
{\footnotesize
\begin{align*}
	p(\bm{\beta},\bm{b},\sigma^2\mid \bm{y}, \bm{X},\bm{W}) & \propto \prod_{i=1}^N |\sigma^2\bm{I}_{T_i}|^{-1/2}\exp\left\{-\frac{1}{2\sigma^2}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	& = (\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\},
\end{align*} 
}
where $\bm{b}=[\bm{b}_1^{\top}, \bm{b}_2^{\top},\dots, \bm{b}_N^{\top}]^{\top}$.

Panel data modeling in the Bayesian approach assumes a hierarchical structure in the \textit{random effects}. Following \cite{Chib1999}, there is a first stage where $\bm{b}_i\sim{N}(\bm{0},\bm{D})$, $\bm{D}$ allows serial correlation within each cross-sectional unit $i$, and then, there is a second stage where $\bm{D}\sim{I}{W}(d_0,d_0\bm{D}_0)$. Thus, we can see that there is an additional layer of priors as there is a prior on the hyperparameter $\bm{D}$. 

In addition, we have standard conjugate prior distributions for $\bm{\beta}$ and $\sigma^2$, $\bm{\beta} \sim {N}(\bm{\beta}_0,\bm{B}_0)$ and 
$\sigma^2 \sim {I}{G}(\alpha_0, \delta_0)$. 

\cite{Chib1999} propose a blocking algorithm to perform inference in longitudinal hierarchical models by considering the distribution of $\bm{y}_i$ marginalized over the random effects. Given that $\bm{y}_i\mid  \bm{\beta},\bm{b}_i,\sigma^2,\bm{X}_i,\bm{W}_i\sim N(\bm{X}_i\bm{\beta}+\bm{W}_i\bm{b}_i,\sigma^2\bm{I}_{T_i})$, we can see that    $\bm{y}_i\mid \bm{\beta},\bm{D},\sigma^2,\bm{X}_i,\bm{W}_i\sim{N}(\bm{X}_i\bm{\beta},\bm{V}_i)$, where $\bm{V}_i=\sigma^2\bm{I}_{T_i}+\bm{W}_i\bm{D}\bm{W}_i^{\top}$ given that $\mathbb{E}[\bm{b}_i]=\bm{0}$ and $Var[\bm{b}_i]=\bm{D}$. If we have just random intercepts, then $\bm{W}_i=\bm{i}_{T_i}$, where $\bm{i}_{T_i}$ is a $T_i$-dimensional vector of ones. Thus, $\bm{V}_i=\sigma^2\bm{I}_{T_i}+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$, the variance is $\sigma^2+\sigma^2_{b}$ and the covariance is $\sigma^2_{b}$ within each cross-sectional unit through time.

We can deduce the posterior distribution of $\bm{\beta}$ given $\sigma^2$ and $\bm{D}$,
\begin{align*}
	\pi(\bm{\beta}\mid \sigma^2, \bm{D},\bm{y}, \bm{X}, \bm{W}) & \propto \exp\left\{-\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta})^{\top}\bm{V}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bm{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}.
\end{align*} 
This implies that (see Exercise 1)  
\begin{equation*}
	\bm{\beta}\mid \sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$.

We can use the likelihood $p(\bm{\beta},\bm{b}_i,\sigma^2\mid \bm{y}, \bm{X},\bm{W})$ to get the posterior distributions of $\bm{b}_i$, $\sigma^2$ and $\bm{D}$. In particular,
\begin{align*}
	\pi(\bm{b}_i\mid \bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W})&\propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	&\times \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\propto\exp\left\{-\frac{1}{2}\sum_{i=1}^N(-2\bm{b}_i^{\top}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))+ \bm{b}_i^{\top}(\sigma^{-2}\bm{W}_i^{\top}\bm{W}_i+\bm{D}^{-1})\bm{b}_i)\right\}\\
	&\propto\exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i)\right\}\\
	&=\exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i)\right\}, 
\end{align*}
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))$.

We can complete the square in this expression by adding and subtracting $\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}$. Thus,
\begin{align*}
	\pi(\bm{b}_i\mid \bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W})&\propto \exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i+\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}-\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni})\right\}\\
	&\propto \exp\left\{(\bm{b}_i-\bm{b}_{ni})^{\top}\bm{B}_{ni}^{-1}(\bm{b}_i-\bm{b}_{ni})\right\}. 
\end{align*}
This is the kernel of a multivariate normal distribution with mean $\bm{b}_{ni}$ and variance $\bm{B}_{ni}$. Thus,
\begin{equation*}
	\bm{b}_i\mid \bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
Let's see the posterior distribution of $\sigma^2$,
\begin{align*}
	\pi(\sigma^2\mid \bm{\beta},\bm{b},\bm{y}, \bm{X}, \bm{W})&\propto (\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	&\times (\sigma^2)^{-\alpha_0-1}\exp\left\{-\frac{\delta_0}{\sigma^2}\right\}\\
	&=(\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}-\alpha_0-1}\\
	&\times \exp\left\{-\frac{1}{\sigma^2}\left(\delta_0+\sum_{i=1}^N\frac{(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)}{2}\right)\right\}. 
\end{align*}
Thus,
\begin{equation*}
	\sigma^2\mid  \bm{\beta}, \bm{b}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.

The posterior distribution of $\bm{D}$ is the following,
\begin{align*}
	\pi(\bm{D}\mid \bm{b})&\propto  |\bm{D}|^{-N/2} \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\times |\bm{D}|^{-(d_0+K_2+1)/2}\exp\left\{-\frac{1}{2}tr(d_0\bm{D}_0\bm{D}^{-1})\right\}\\
	&=|\bm{D}|^{-(d_0+N+K_2+1)/2}\exp\left\{-\frac{1}{2}tr\left(\left(d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}\right)\bm{D}^{-1}\right) \right\}. 
\end{align*}
This is the kernel of an inverse Wishart distribution with degrees of freedom $d_n=d_0+N$ and scale matrix $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}$. Thus,   
\begin{equation*}
	\bm{D}\mid  \bm{b} \sim {I}{W}(d_n, \bm{D}_n).
\end{equation*}
Observe that the posterior distribution of $\bm{D}$ dependents just on $\bm{b}$. 

All the posterior conditional distributions belong to standard families, this implies that we can use a Gibbs sampling algorithm to perform inference in these hierarchical normal models.\\

\textbf{Example: The relation between productivity and public investment}

We used the dataset named \textit{8PublicCap.csv} used by \cite{Ramirez2017} to analyze the relation  between public investment and gross state product in the setting of a spatial panel dataset consisting of 48 US states from 1970 to 1986.
In particular, we perform inference based on the following equation 
\begin{equation*}
	\log(\text{gsp}_{it})=b_i+\beta_1+\beta_2\log(\text{pcap}_{it})+\beta_3\log(\text{pc}_{it})+\beta_4\log(\text{emp}_{it})+\beta_5\text{unemp}_{it}+\mu_{it},
\end{equation*}

where gsp in the gross state product, pcap is public capital, and pc is private capital all in USD, emp is employment (people), and unemp is the unemployment rate in percentage.

Algorithm \ref{alg:HLN} shows how to perform inference in hierarchical longitudinal normal models in our GUI. See also Chapter \ref{chapGUI} for details regarding the dataset structure.

\begin{algorithm}[h!]
	\caption{Hierarchical longitudinal normal models}\label{alg:HLN}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Hierarchical Longitudinal Model} on the top panel
		\State Select \textit{Normal} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the formula of the \textit{fixed effects} equation in the \textbf{Main Equation: Fixed Effects} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Write down the formula of the \textit{random effects} equation in the \textbf{Main Equation: Random Effects} box without writing the dependent variable, that is, starting the equation with the \textit{tilde} (``$\sim$") symbol. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation. If there are just random intercepts do not write anything in this box
		\State Write down the name of the grouping variable, that is, the variable that indicates the cross-sectional units 
		\State Set the hyperparameters of the \textit{fixed effects}: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Set the hyperparameters of the \textit{random effects}: degrees of freedom and scale matrix of the inverse Wishart distribution. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We ask in Exercise 2 to run this application in our GUI using 10000 MCMC iterations plus a burn-in equal to 5000 iterations, and a thinning parameter equal to 1. We also used the default values for the hyperparameters of the prior distributions, that is, $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=5$ and $\bm{D}_0=\bm{I}_1$. It seems that all posterior draws come from stationary distributions, as suggested by the diagnostics and posterior plots (see Exercise 2).

The following code uses the command \textit{MCMChregress} from the package \textit{MCMCpack} to run this application. This command is also used by our GUI to perform inference in hierarchical longitudinal normal models.

We can see that the 95\% symmetric credible intervals for public capital, private capital, employment, and unemployment are (-2.54e-02, -2.06e-02), (2.92e-01, 2.96e-01), (7.62e-01, 7.67e-01) and (-5.47e-03, -5.31e-03), respectively. The posterior mean elasticity estimate of public capital to gsp is -0.023, that is, an increase by 1\% in public capital means a 0.023\% decrease in gross state product. The posterior mean estimates of private capital and employment elasticities are 0.294 and 0.765, respectively. In addition, 1 percentage point increase in the unemployment rate means a decrease of 0.54\% in gsp. It seems that all these variables are statistically relevant. In addition, the posterior mean estimates of the variance associated with the unobserved heterogeneity and stochastic errors are 1.06e-01 and 1.45e-03. We obtained the posterior chain of the proportion of the variance associated with the unobserved heterogeneity. The 95\% symmetric credible interval is (0.98, 0.99) for this proportion, that is, unobserved heterogeneity is very important to explain the total variability.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, MCMChregress command}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
DataGSP <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/8PublicCap.csv", sep = ",", header = TRUE, quote = "")
attach(DataGSP)
K1 <- 5; K2 <- 1
b0 <- rep(0, K1); B0 <- diag(K1)
r0 <- 5; R0 <- diag(K2)
a0 <- 0.001; d0 <- 0.001
Resultshreg <- MCMCpack::MCMChregress(fixed = log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, random = ~1, group = "id", data = DataGSP, burnin = 5000, mcmc = 10000, thin = 1, r = r0, R = R0, nu = a0, delta = d0)
Betas <- Resultshreg[["mcmc"]][,1:K1]
Sigma2RanEff <- Resultshreg[["mcmc"]][,54]
Sigma2 <- Resultshreg[["mcmc"]][,55]
summary(Betas)
Quantiles for each variable:
						2.5%       25%       50%      75%     97.5%
beta.(Intercept)  2.3145  2.3246  2.3301  2.335  2.3455
beta.log(pcap)   -0.0254 -0.0239 -0.0231 -0.022 -0.0206
beta.log(pc)      0.2917  0.2930  0.2937  0.294  0.2957
beta.log(emp)     0.7619  0.7637  0.7646  0.765  0.7672
beta.unemp       -0.0054 -0.0054 -0.0053 -0.005 -0.0053
summary(Sigma2RanEff)
Quantiles for each variable:
2.5%     25%     50%     75%   97.5% 
0.07208 0.09086 0.10331 0.11751 0.15600 
summary(Sigma2)
Quantiles for each variable:
2.5%      25%      50%      75%    97.5% 
0.001316 0.001403 0.001451 0.001501 0.001606 
summary(Sigma2RanEff/(Sigma2RanEff+Sigma2))
Quantiles for each variable:
2.5%    25%    50%    75%  97.5% 
0.9799 0.9842 0.9861 0.9879 0.9909
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

There are many extensions of this model, for instance, \cite{Chib1999} propose to introduce heteroskedasticity in this model by assuming $\mu_{it}\mid \tau_{it}\sim N(0, \sigma^2/\tau_{it})$, $\tau_{it}\sim G(v/2,v/2)$. We ask in Exercise 2 to perform inference in the relation between productivity and public investment example using this setting. Another potential extension is to allow dependence between $\bm{b}_i$ and some controls, let's say $\bm{z}_i$, a $K_3$-dimensional vector, and assume $\bm{b}_i\sim N(\bm{Z}_i\bm{\gamma},\bm{D})$ where $\bm{Z}_i=\bm{I}_{K_2}\otimes \bm{z}_i^{\top}$, and complete the model using a prior for $\bm{\gamma}$, $\bm{\gamma}\sim N(\bm{\gamma}_0,\bm{\Gamma}_0)$. We ask to perform a simulation using this setting in Exercise 3.\\   

\textbf{Example: Simulation exercise of the longitudinal normal model with heteroskedasticity}

Let's perform a simulation exercise to assess some potential extensions of the longitudinal hierarchical normal model. The point of departure is to assume that
\[y_{it}=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+b_i+w_{it1}b_{i1}+\mu_{it},\]
where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\mu_{it}\sim N(0, (0.1/\tau)^{1/2})$, $\tau_{it}\sim G(v/2,v/2)$ and $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6]^{\top}$, $i=1,2,\dots,50$. The sample size is 2000 in an \textit{unbalanced panel structure}. 

Following same stages as in this section and Exercise 1, the posterior conditional distributions assuming that $\mu_{it}\mid \tau_{it}\sim N(0, \sigma^2/\tau_{it})$, $\tau_{it}\sim G(v/2,v/2)$ are given by 
\begin{equation*}
	\bm{\beta}\mid \sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{\tau}=[\tau_{it}]^{\top}$, $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$, $\bm{V}_i=\sigma^2\bm{\Psi}_i+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$ and $\bm{\Psi}_i=diag\left\{\tau_{it}^{-1}\right\}$.
\begin{equation*}
	\bm{b}_i\mid \bm{\beta},\sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}))$.
\begin{equation*}
	\sigma^2\mid  \bm{\beta}, \bm{b}, \bm{\tau}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.  
\begin{equation*}
	\bm{D}\mid  \bm{b} \sim {I}{W}(d_n, \bm{D}_n),
\end{equation*}
where $d_n=d_0+N$ and $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}$. And
\begin{equation*}
	\tau_{it}\mid \sigma^2, \bm{\beta}, \bm{b}, \bm{y}, \bm{X}, \bm{W} \sim {G}(v_{1n}/2, v_{2ni}/2),
\end{equation*}
where $v_{1n}=v+1$ and $v_{2ni}=v+\sigma^{-2}(y_{it}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^2$.

The following code implements this simulation, and gets draws of the posterior distributions. We set MCMC iterations, burn-in and thinning parameters equal to 5000, 1000 and 1, respectively. In addition, $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=2$, $\bm{D}_0=\bm{I}_2$ and $v=5$.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
NT <- 2000; N <- 50
id <- c(1:N, sample(1:N, NT - N,replace=TRUE))
table(id)
x1 <- rnorm(NT); x2 <- rnorm(NT); x3 <- rnorm(NT) 
X <- cbind(1, x1, x2, x3); K1 <- dim(X)[2]
w1 <- rnorm(NT); W <- cbind(1, w1)
K2 <- dim(W)[2]; B <- c(0.5, 0.4, 0.6, -0.6)
D <- c(0.7, 0.6)
b1 <- rnorm(N, 0, sd = D[1]^0.5)
b2 <- rnorm(N, 0, sd = D[2]^0.5)
b <- cbind(b1, b2)
v <- 5; tau <- rgamma(NT, shape = v/2, rate = v/2)
sig2 <- 0.1; u <- rnorm(NT, 0, sd = (sig2/tau)^0.5)
y <- NULL
for(i in 1:NT){
	yi <- X[i,]%*%B + W[i,]%*%b[id[i],] + u[i] 
	y <- c(y, yi)
}
Data <- as.data.frame(cbind(y, x1, x2, x3, w1, id))
mcmc <- 5000; burnin <- 1000; thin <- 1; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
PostBeta <- function(sig2, D, tau){
	XVX <- matrix(0, K1, K1)
	XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Wi <- W[ids, ]
		taui <- tau[ids]
		Vi <- sig2*solve(diag(1/taui)) + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi)
		Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		yi <- y[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
Postb <- function(Beta, sig2, D, tau){
	Di <- solve(D); 	bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]; Xi <- X[ids, ]
		yi <- y[ids]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		Wtei <- sig2^(-1)*t(Wi)%*%Taui%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Taui%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostSig2 <- function(Beta, bs, tau){
	an <- a0 + 0.5*NT
	ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]; yi <- y[ids]
		Wi <- W[ids, ]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
		etei <- t(ei)%*%Taui%*%ei
		ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
PostD <- function(bs){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]
		btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostTau <- function(sig2, Beta, bs){
	v1n <- v + 1
	v2n <- NULL
	for(i in 1:NT){
		Xi <- X[i, ]; yi <- y[i]
		Wi <- W[i, ]; bi <- bs[id[i],]
		v2ni <- v + sig2^(-1)*(yi - Xi%*%Beta - Wi%*%bi)^2
		v2n <- c(v2n, v2ni)
	}
	tau <- rgamma(NT, shape = rep(v1n/2, NT), rate = v2n/2)
	return(tau)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1); PostDs <- matrix(0, tot, K2*(K2+1)/2)
PostSig2s <- rep(0, tot); Postbs <- array(0, c(N, K2, tot))
PostTaus <- matrix(0, tot, NT); RegLS <- lm(y ~ X - 1)
SumLS <- summary(RegLS)
Beta <- SumLS[["coefficients"]][,1]
sig2 <- SumLS[["sigma"]]^2; D <- diag(K2)
tau <- rgamma(NT, shape = v/2, rate = v/2) 
pb <- txtProgressBar(min = 0, max = tot, style = 3)
for(s in 1:tot){
	bs <- Postb(Beta = Beta, sig2 = sig2, D = D, tau = tau)
	D <- PostD(bs = bs)
	Beta <- PostBeta(sig2 = sig2, D = D, tau = tau)
	sig2 <- PostSig2(Beta = Beta, bs = bs, tau = tau)
	tau <- PostTau(sig2 = sig2, Beta = Beta, bs = bs)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	PostSig2s[s] <- sig2
	Postbs[, , s] <- bs
	PostTaus[s,] <- tau
	setTxtProgressBar(pb, s)
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]; Ds <- PostDs[keep,]
bs <- Postbs[, , keep]; sig2s <- PostSig2s[keep]
taus <- PostTaus[keep,]
summary(coda::mcmc(Bs))
Quantiles for each variable:
         	2.5%     25%     50%     75%   97.5%
var1  0.07833  0.2412  0.3232  0.4022  0.5619
var2  0.34101  0.3598  0.3697  0.3793  0.3988
var3  0.59596  0.6150  0.6251  0.6351  0.6574
var4 -0.63722 -0.6165 -0.6067 -0.5966 -0.5785
summary(coda::mcmc(Ds))
Quantiles for each variable:
       2.5%     25%      50%      75%   97.5%
var1  0.4720  0.5995  0.68858  0.79206 1.05285
var2 -0.2721 -0.1405 -0.08185 -0.02482 0.09186
var3  0.3689  0.4644  0.52978  0.60946 0.81999
summary(coda::mcmc(sig2s))
 Quantiles for each variable:
2.5%    25%    50%    75%  97.5% 
0.1022 0.1157 0.1324 0.1683 0.3217 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
We can see that all the 95\% credible intervals encompass the population parameters, except for the second \textit{fixed effect} and the variance of the model, but both for a tiny margin.  

\section{Logit model}\label{sec92}

We can use the framework of Section \ref{sec91} to perform inference in models with longitudinal/panel data of binary response variables. In particular, let $y_{it} \sim B(\pi_{it})$, where $\text{logit}(\pi_{it}) = \log\left(\frac{\pi_{it}}{1 - \pi_{it}}\right) \equiv y_{it}^*$, such that $y_{it}^* \sim N(\bm{x}_{it}^{\top} \bm{\beta} + \bm{w}_{it}^{\top} \bm{b}_i, \sigma^2)$. Thus, we can \textit{augment} the model with the latent variable $y_{it}^*$ and perform inference using a Metropolis-within-Gibbs sampling algorithm based on the posterior conditional distributions from the previous section. 

We can implement a Gibbs sampling algorithm to sample draws from the posterior conditional distributions of $\bm{\beta}$, $\sigma^2$, $\bm{b}_i$, and $\bm{D}$ using the equations in Section \ref{sec91} conditional on $\bm{y}_i^*$. Then, we can use a random walk Metropolis-Hastings algorithm to sample $y_{it}^*$, where the proposal distribution is Gaussian with mean $y_{it}^*$ and variance $v^2$, that is, $y_{it}^{*c} = y_{it}^* + \epsilon_{it}$, where $\epsilon_{it} \sim \mathcal{N}(0, v^2)$, and $v$ is a tuning parameter to achieve good acceptance rates. 

Finally, for making predictions, we should take into account that $\mathbb{E}[\pi_{it}] = \frac{1}{1 + \exp\left\{(\bm{x}_{it}^{\top} \bm{\beta} + \bm{w}_{it}^{\top} \bm{b}_i)/\sqrt{1 + \left(\frac{16\sqrt{3}}{15\pi}\right)^2 \sigma^2}\right\}}$ \cite[~pag. 136]{diggle2002analysis}.

The posterior distribution of this model is
\begin{align*}
	\pi(\bm{\beta},\sigma^2, \bm{b}_i, \bm{D}, \bm{y}^*\mid \bm{y}, \bm{X}, \bm{W})&\propto \prod_{i=1}^N \prod_{t=1}^{T_i}\left\{\pi_{it}^{y_{it}}(1-\pi_{it})^{1-y_{it}}\right.\\
	&\left.\times (\sigma^2)^{-1}\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^*-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^*-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bm{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}\\
	&\times \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\times (\sigma^2)^{-\alpha_0-1}\exp\left\{-\frac{\delta_0}{\sigma^2}\right\}\\
	&\times |\bm{D}|^{-(d_0+K_2+1)/2}\exp\left\{-\frac{1}{2}tr(d_0\bm{D}_0\bm{D}^{-1})\right\}.	
\end{align*}

We can get samples of $y_{it}^*$ from a normal distribution with mean equal to $\bm{x}_{it}^{\top}\bm{\beta}+\bm{w}_{it}^{\top}\bm{b}_i$ and variance $\sigma^2$, and use these samples to get $\pi_{it}=\frac{1}{1+e^{-y_{it}^*}}$, $y_{it}^{*c}=y_{it}^{*}+\epsilon_{it}$ and $\pi_{it}^c=\frac{1}{1+e^{-y_{it}^{*c}}}$, and calculate the acceptance rate of the Metropolis-Hastings algorithm, 
{\footnotesize
\begin{align*}
	\alpha=\min\left(1,\frac{ \pi_{it}^{cy_{it}}(1-\pi_{it}^c)^{(1-y_{it})}\times\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^{c*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^{c*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}}{\pi_{it}^{y_{it}}(1-\pi_{it})^{(1-y_{it})}\times\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^{*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^{*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}}\right).
\end{align*}
}\\

\textbf{Example: Doctor visits in Germany}

We used the dataset \textit{9VisitDoc.csv} provided by \cite{Winkelmann2004}\footnote{See \textit{http://qed.econ.queensu.ca/jae/2004-v19.4/winkelmann/} for details}. We analyze the determinants of a binary variable (\text{DocVis}), which equals 1 if an individual visited a physician in the last three months and 0 otherwise. The dataset contains 32,837 observations of 9,197 individuals in an \textit{unbalanced longitudinal/panel} dataset over the years 1995--1999 from the German Socioeconomic Panel Data.

The specification is given by
\begin{align*}
	\text{logit}(\pi_{it}) &= \beta_1 + \beta_2 \text{Age} + \beta_3 \text{Male} + \beta_4 \text{Sport} + \beta_5 \text{LogInc} \\
	&\quad + \beta_6 \text{GoodHealth} + \beta_7 \text{BadHealth} + b_i + b_{i1} \text{Sozh},
\end{align*}
where $\pi_{it} = p(\text{DocVis}_{it} = 1)$.

This specification controls for age, a gender indicator (with 1 representing male), whether the individual practices any sport (with 1 for sport), the logarithm of monthly gross income, and self-perception of health status, where “good” and “bad” are compared to a baseline of “regular”. Additionally, we assume that unobserved heterogeneity is linked to whether the individual receives welfare payments (with \text{Sozh} equal to 1 for receiving welfare). 

We set 10,000 MCMC iterations, plus 1,000 burn-in, and a thinning parameter equal to 10. In addition, $\bm{\beta}_0 = \bm{0}_7$, $\bm{B}_0 = \bm{I}_7$, $\alpha_0 = \delta_0 = 0.001$, $d_0 = 5$, and $\bm{D}_0 = \bm{I}_2$.

The Algorithm \ref{alg:HLogit} shows how to perform inference of the hierarchical longitudinal logit model using our GUI. We show in the following code how to perform inference of this example using the command \textit{MCMChlogit} from the \textit{MCMCpack} package. We fixed the variance for over-dispersion ($\sigma^2$) setting \textit{FixOD = 1} in this example. Our GUI does not fix this value, that is, it sets \textit{FixOD = 0}, which is the default value in the command \textit{MCMChlogit}. We ask to replicate this example using our GUI in Exercise 4. The command \textit{MCMChlogit} uses an adaptive algorithm to tune $v$ based on an optimal acceptance rate equal to 0.44. 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Doctor visits in Germany}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/9VisitDoc.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
K1 <- 7; K2 <- 2; N <- 9197
b0 <- rep(0, K1); B0 <- diag(K1)
r0 <- 5; R0 <- diag(K2)
a0 <- 0.001; d0 <- 0.001
RegLogit <- glm(DocVis ~ Age + Male + Sport + LogInc + GoodHealth + BadHealth, family = binomial(link = "logit"))
SumLogit <- summary(RegLogit)
Beta0 <- SumLogit[["coefficients"]][,1]
mcmc <- 10000; burnin <- 1000; thin <- 10
# MCMChlogit
Resultshlogit <- MCMCpack::MCMChlogit(fixed = DocVis ~ Age + Male + Sport + LogInc + GoodHealth + BadHealth, random = ~Sozh, group="id", data = Data, burnin = burnin, mcmc = mcmc, thin = thin, mubeta = b0, Vbeta = B0, r = r0, R = R0, nu = a0, delta = d0, beta.start = Beta0, FixOD = 1)
Betas <- Resultshlogit[["mcmc"]][,1:K1]
Sigma2RanEff <- Resultshlogit[["mcmc"]][,c(K2*N+K1+1, 2*N+K1+K2^2)]
summary(Betas)
Quantiles for each variable:
						2.5%       25%       50%      75%    97.5%
beta.(Intercept) -1.1085 -0.6428 -0.4169 -0.166  0.280
beta.Age          0.0051  0.0078  0.0095  0.010  0.013
beta.Male        -1.1914 -1.1325 -1.0981 -1.065 -1.008
beta.Sport        0.2256  0.2846  0.3159  0.348  0.401
beta.LogInc       0.1782  0.2357  0.2661  0.299  0.367
beta.GoodHealth  -1.1648 -1.1046 -1.0701 -1.040 -0.983
beta.BadHealth    1.2233  1.3242  1.3716  1.426  1.533
summary(Sigma2RanEff)
Quantiles for each variable:
								              2.5%    25%   50%   75% 97.5%
VCV.(Intercept).(Intercept) 2.0749 2.1709 2.238 2.303 2.422
VCV.Sozh.Sozh               0.3536 0.4875 0.626 0.906 1.271
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

The results suggest that age, sports, income and a bad perception of health status increase the probability of visiting the physician, the posterior estimates have 95\% symmetric credible intervals equal to (5.1e-03, 1.3e-02), (0.23, 0.40), (0.18, 0.37) and (1.22, 1.53), whereas men have a lower probability of visiting a physician, the 95\% credible interval is (-1.19, -1.01), and individuals who have a good perception of their health status also have a lower probability of visiting the doctor, the 95\% credible interval is (-1.16, -0.98). The 95\% credible interval of the variances of the unobserved heterogeneity associated with the welfare program is (0.35, 1.27).\\

\begin{algorithm}[h!]
	\caption{Hierarchical longitudinal logit models}\label{alg:HLogit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Hierarchical Longitudinal Model} on the top panel
		\State Select \textit{Logit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the formula of the \textit{fixed effects} equation in the \textbf{Main Equation: Fixed Effects} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Write down the formula of the \textit{random effects} equation in the \textbf{Main Equation: Random Effects} box without writing the dependent variable, that is, starting the equation with the \textit{tilde} (``$\sim$") symbol. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation. If there are just random intercepts do not write anything in this box
		\State Write down the name of the grouping variable, that is, the variable that indicates the cross-sectional units 
		\State Set the hyperparameters of the \textit{fixed effects}: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Set the hyperparameters of the \textit{random effects}: degrees of freedom and scale matrix of the inverse Wishart distribution. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

\section{Poisson model}\label{sec93}

We can use same ideas as in Section \ref{sec92} to perform inference in longitudinal/panel datasets where the dependent variable takes non-negative integers. Let's assume that $y_{it}\sim P(\lambda_{it})$ where $\log(\lambda_{it})=y_{it}^*$ such that $y_{it}^*\sim N(\bm{x}_{it}^{\top}\bm{\beta}+\bm{w}_{it}^{\top}\bm{b}_i,\sigma^2)$. We can \textit{augment} the model with the latent variable $y_{it}^{*}$, and again use a Metropolis-within-Gibbs algorithm to perform inference in this model.

The posterior distribution of this model is
\begin{align*}
	\pi(\bm{\beta},\sigma^2, \bm{b}_i, \bm{D}, \bm{y}^*\mid \bm{y}, \bm{X}, \bm{W})&\propto \prod_{i=1}^N \prod_{t=1}^{T_i}\left\{\lambda_{it}^{y_{it}}\exp\left\{-\lambda_{it}\right\}\right.\\
	&\left.\times (\sigma^2)^{-1}\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^*-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^*-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bm{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}\\
	&\times \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\times (\sigma^2)^{-\alpha_0-1}\exp\left\{-\frac{\delta_0}{\sigma^2}\right\}\\
	&\times |\bm{D}|^{-(d_0+K_2+1)/2}\exp\left\{-\frac{1}{2}tr(d_0\bm{D}_0\bm{D}^{-1})\right\}.	
\end{align*}

We can get samples of $y_{it}^*$ from a normal distribution with mean equal to $\bm{x}_{it}^{\top}\bm{\beta}+\bm{w}_{it}^{\top}\bm{b}_i$ and variance $\sigma^2$, and use these samples to get $\lambda_{it}=\exp(y_{it}^*)$, $y_{it}^{*c}=y_{it}^{*}+\epsilon_{it}$, where $\epsilon_{it}\sim\mathcal{N}(0,v^2)$, $v$ is a tuning parameter to get good acceptance rates, and $\lambda_{it}^c=\exp(y_{it}^{*c})$. The acceptance rate of the Metropolis-Hastings algorithm is 
{\footnotesize
	\begin{align*}
		\alpha=\min\left(1,\frac{ \lambda_{it}^{cy_{it}}\exp(-\lambda_{it}^c)\times\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^{c*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^{c*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}}{\lambda_{it}^{y_{it}}\exp(-\lambda_{it})\times\exp\left\{-\frac{1}{2\sigma^2}(y_{it}^{*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^{\top}(y_{it}^{*}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)\right\}}\right).
	\end{align*}
}

In addition, we should use the posterior conditional distributions from Section \ref{sec91} to complete the algorithm getting samples of $\bm{\beta}$, $\sigma^2$, $\bm{b}_i$ and $\bm{D}$ replacing $y_{it}$ by ${y}_{it}^*$.

We should take into account for doing predictions that $\mathbb{E}[{\lambda}_{it}]=\exp\left\{\bm{x}_{it}^{\top}\bm{\beta}+\bm{w}_{it}^{\top}\bm{b}_i+0.5\sigma^2\right\}$ \cite[~pag. 137]{diggle2002analysis}.\\

\textbf{Example: Simulation exercise}

Let's perform a simulation exercise to assess the performance of the hierarchical longitudinal Poisson model. The point of departure is to assume that 
\[
y_{it}^* = \beta_0 + \beta_1 x_{it1} + \beta_2 x_{it2} + \beta_3 x_{it3} + b_i + w_{it1} b_{i1},
\]
where $x_{itk} \sim N(0,1)$ for $k = 1, 2, 3$, $w_{it1} \sim N(0,1)$, $b_i \sim N(0, 0.7^{1/2})$, $b_{i1} \sim N(0, 0.6^{1/2})$, and $\bm{\beta} = [0.5 \ 0.4 \ 0.6 \ -0.6]^{\top}$, with $i = 1, 2, \dots, 50$. Additionally, $y_{it} \sim P(\lambda_{it})$, where $\lambda_{it} = \exp(y_{it}^*)$. The sample size is 1000 in an \textit{unbalanced panel structure}.

We set the priors as $\bm{\beta}_0 = \bm{0}_4$, $\bm{B}_0 = \bm{I}_4$, $\alpha_0 = \delta_0 = 0.001$, $d_0 = 2$, and $\bm{D}_0 = \bm{I}_2$. The number of MCMC iterations, burn-in, and thinning parameters are 15,000, 5,000, and 10, respectively.

The following code shows how to perform inference in the hierarchical longitudinal Poisson model programming the Metropolis-within-Gibbs sampler.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal Poisson model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
NT <- 1000; N <- 50
id <- c(1:N, sample(1:N, NT - N,replace=TRUE))
x1 <- rnorm(NT); x2 <- rnorm(NT); x3 <- rnorm(NT) 
X <- cbind(1, x1, x2, x3)
K1 <- dim(X)[2]; w1 <- rnorm(NT) 
W <- cbind(1, w1); K2 <- dim(W)[2]
B <- c(0.5, 0.4, 0.6, -0.6)
D <- c(0.7, 0.6); sig2 <- 0.1
b1 <- rnorm(N, 0, sd = D[1]^0.5)
b2 <- rnorm(N, 0, sd = D[2]^0.5)
b <- cbind(b1, b2)
yl <- NULL
for(i in 1:NT){
	ylmeani <- X[i,]%*%B + W[i,]%*%b[id[i],]
	yli <- rnorm(1, ylmeani, sig2^0.5)
	yl <- c(yl, yli)
}
lambdait <- exp(yl); y <- rpois(NT, lambdait)
Data <- as.data.frame(cbind(y, x1, x2, x3, w1, id))
mcmc <- 15000; burnin <- 5000; thin <- 10; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
LatentMHV1 <- function(tuning, Beta, bs, sig2){
	ylhat <- rep(0, NT)
	accept <- NULL
	for(i in 1:NT){
		ids <- which(id == i)
		yi <- y[i]
		ylhatmeani <- X[i,]%*%Beta + W[i,]%*%bs[id[i],]
		ylhati <- rnorm(1, ylhatmeani, sd = sig2^0.5)
		lambdahati <- exp(ylhati)
		ei <- rnorm(1, 0, sd = tuning)
		ylpropi <- ylhati + ei
		lambdapropi <- exp(ylpropi)
		logPosthati <- sum(dpois(yi, lambdahati, log = TRUE) + dnorm(ylhati, ylhatmeani, sig2^0.5, log = TRUE))
		logPostpropi <- sum(dpois(yi, lambdapropi, log = TRUE) + dnorm(ylpropi, ylhatmeani, sig2^0.5, log = TRUE))
		alphai <- min(1, exp(logPostpropi - logPosthati))
		ui <- runif(1)
		if(ui <= alphai){
			ylhati <- ylpropi; accepti <- 1
		}else{
			ylhati <- ylhati; accepti <- 0
		}
		ylhat[i] <- ylhati
		accept <- c(accept, accepti)
	}
	res <- list(ylhat = ylhat, accept = mean(accept))
	return(res)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal Poisson model}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBeta <- function(D, ylhat, sig2){
	XVX <- matrix(0, K1, K1); XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i); Ti <- length(ids)
		Wi <- W[ids, ]
		Vi <- diag(Ti)*sig2 + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi); Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		yi <- ylhat[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX); bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
Postb <- function(Beta, D, ylhat, sig2){
	Di <- solve(D); bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]; Xi <- X[ids, ]
		yi <- ylhat[ids]
		Wtei <- sig2^(-1)*t(Wi)%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostD <- function(bs){
	rn <- r0 + N; btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]; btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostSig2 <- function(Beta, bs, ylhat){
	an <- a0 + 0.5*NT; ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]
		yi <- ylhat[ids]
		Wi <- W[ids, ]
		ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
		etei <- t(ei)%*%ei
		ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal Poisson model}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1); PostDs <- matrix(0, tot, K2*(K2+1)/2)
Postbs <- array(0, c(N, K2, tot)); PostSig2s <- rep(0, tot)
Accepts <- rep(NULL, tot)
RegPois <- glm(y ~ X - 1, family = poisson(link = "log"))
SumPois <- summary(RegPois)
Beta <- SumPois[["coefficients"]][,1]
sig2 <- sum(SumPois[["deviance.resid"]]^2)/SumPois[["df.residual"]]
D <- diag(K2); bs1 <- rnorm(N, 0, sd = D[1,1]^0.5)
bs2 <- rnorm(N, 0, sd = D[2,2]^0.5); bs <- cbind(bs1, bs2)
tuning <- 0.1; ropt <- 0.44
tunepariter <- seq(round(tot/10, 0), tot, round(tot/10, 0));   l <- 1
pb <- txtProgressBar(min = 0, max = tot, style = 3)
for(s in 1:tot){
	LatY <- LatentMHV1(tuning = tuning, Beta = Beta, bs = bs, sig2 = sig2)
	ylhat <- LatY[["ylhat"]]
	bs <- Postb(Beta = Beta, D = D, ylhat=ylhat, sig2 = sig2)
	D <- PostD(bs = bs)
	Beta <- PostBeta(D = D, ylhat = ylhat, sig2 = sig2)
	sig2 <- PostSig2(Beta = Beta, bs = bs, ylhat = ylhat)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	Postbs[, , s] <- bs; PostSig2s[s] <- sig2
	AcceptRate <- LatY[["accept"]]
	Accepts[s] <- AcceptRate
	if(AcceptRate > ropt){
		tuning = tuning*(2-(1-AcceptRate)/(1-ropt))
	}else{
		tuning = tuning/(2-AcceptRate/ropt)
	}
	if(s == tunepariter[l]){
		print(AcceptRate); l <- l + 1
	}
	setTxtProgressBar(pb, s)
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]; Ds <- PostDs[keep,]
bs <- Postbs[, , keep]; sig2s <- PostSig2s[keep]
summary(coda::mcmc(Bs))
 Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
var1  0.1038  0.3803  0.5199  0.6534  0.9259
var2  0.2432  0.3166  0.3608  0.4003  0.4796
var3  0.4213  0.5017  0.5453  0.5885  0.6682
var4 -0.7038 -0.6149 -0.5729 -0.5269 -0.4459
summary(coda::mcmc(Ds))
Quantiles for each variable:
			2.5%     25%     50%      75%   97.5%
var1  0.3331  0.4788  0.5732  0.69316 0.99135
var2 -0.3354 -0.1926 -0.1277 -0.06692 0.03674
var3  0.1252  0.2182  0.2780  0.34731 0.51055
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
We can see that all 95\% credible intervals encompass the population parameters of the \textit{fixed effects}, the posterior medians are relatively near the population values. However, we do not get good posterior estimates of the covariance matrix of the \textit{random effects} as the 95\% credible intervals do not encompass the second element of the diagonal of this matrix. In addition, the posterior draws of this algorithm over-estimates the over-dispersion parameter.

We can perform inference for the hierarchical longitudinal Poisson model in our GUI using Algorithm \ref{alg:HPosi}. Our GUI is based on the \textit{MCMChpoisson} command from the \textit{MCMCpack} package.

\begin{algorithm}[h!]
	\caption{Hierarchical longitudinal Poisson models}\label{alg:HPosi}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Hierarchical Longitudinal Model} on the top panel
		\State Select \textit{Poisson} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the formula of the \textit{fixed effects} equation in the \textbf{Main Equation: Fixed Effects} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Write down the formula of the \textit{random effects} equation in the \textbf{Main Equation: Random Effects} box without writing the dependent variable, that is, starting the equation with the \textit{tilde} (``$\sim$") symbol. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation. If there are just random intercepts do not write anything in this box
		\State Write down the name of the grouping variable, that is, the variable that indicates the cross-sectional units 
		\State Set the hyperparameters of the \textit{fixed effects}: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Set the hyperparameters of the \textit{random effects}: degrees of freedom and scale matrix of the inverse Wishart distribution. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic}
	$^*${\footnotesize{At the time of writing this book there was an issue with the function \textit{MCMChpoisson} from \textit{MCMCpack}. We contact the maintainer, but users may have issues running this algorithm or running this function directly in \textbf{R}.}} 
\end{algorithm}


\section{Summary}\label{sec94}
In this chapter, we present how to perform inference in longitudinal/panel data models from a Bayesian perspective. In particular, the Bayesian approach uses a hierarchical structure, where the \textit{random effects} have priors that depend on hyperparameters, which in turn also have priors. We cover the three most common cases: continuous, binary, and count dependent variables. The basic models presented in this chapter can be easily extended to more flexible cases, given the hierarchical structure.

\section{Exercises}\label{sec95}

\begin{enumerate}
	
	\item Show that the posterior distribution of $\bm{\beta}\mid \sigma^2,\bm{D}$ is $N(\bm{\beta}_n,\bm{B}_n)$, where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$.
	
	\item \textbf{The relation between productivity and public investment example continues}
	
	\begin{itemize}
		\item Perform inference of this example using our GUI.
		\item Program from scratch a Gibbs sampling algorithm to perform this application. Set $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=5$ and $\bm{D}_0=\bm{I}_1$.
		\item Perform inference in this example assuming that $\mu_{it}\mid \tau_{it}\sim N(0, \sigma^2/\tau_{it})$ and $\tau_{it}\sim G(v/2,v/2)$ setting $v=5$. 
	\end{itemize}
  
	\item \textbf{Simulation exercise of the longitudinal normal model continues}
	
	
	Assume that 
	\[y_{it}=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+\beta_4 z_{i1}+b_i+w_{it1}b_{i1}+\mu_{it},\]
	where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $z_{i1}\sim B(0.5)$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\mu_{it}\sim N(0, 0.1^{1/2})$ $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6 \ 0.7]^{\top}$, $i=1,2,\dots,50$, and the sample size is 2,000 in an \textit{unbalanced panel structure}. In addition, we assume that $\bm{b}_i$ dependents on $\bm{z}_i=[1 \ z_{i1}]^{\top}$ such that $\bm{b}_i\sim N(\bm{Z}_i\bm{\gamma},\bm{D})$ where $\bm{Z}_i=\bm{I}_{K_2}\otimes \bm{z}_i^{\top}$, and $\bm{\gamma}=[1 \ 1 \ 1 \ 1]$. The prior for $\bm{\gamma}$ is $N(\bm{\gamma}_0,\bm{\Gamma}_0)$ where we set $\bm{\gamma}_0=\bm{0}_4$ and $\bm{\Gamma}_0=\bm{I}_4$. 
	
	\begin{itemize}
		\item Perform inference in this model without taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters.
		\item Perform inference in this model taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters. 
	\end{itemize}

\item \textbf{Doctor visits in Germany continues I}

Replicate this example using our GUI, which by default does not fix the over-dispersion parameter ($\sigma^2$), and compare the results with the results of this example in Section \ref{sec92}.

\item \textbf{Simulation exercise of the longitudinal logit model}

Perform a simulation exercise to assess the performance of the hierarchical longitudinal logit model. The point of departure is to assume that \[y_{it}^*=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+b_i+w_{it1}b_{i1},\]
where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6]^{\top}$, $i=1,2,\dots,50$, and $y_{it}\sim B(\pi_{it}$, where $\pi_{it}=1/(1+\exp(y_{it}^*))$. The sample size is 1,000 in an \textit{unbalanced panel structure}.

\begin{itemize}
	\item Perform inference using the command $MCMChlogit$ fixing the over-dispersion parameter, and using $\bm{\beta}_0=\bm{0}_4$, $\bm{B}_0=\bm{I}_4$, $\alpha_0=\delta_0=0.001$, $d_0=2$ and $\bm{D}_0=\bm{I}_2$.
	\item Program from scratch a Metropolis-within-Gibbs algorithm to perform inference in this simulation.  
\end{itemize} 

\item \textbf{Doctor visits in Germany continues II} 

Take a sub-sample of the first 500 individuals of the datatset \textit{9VisitDoc.csv} to perform inference in the number of visits to doctors (\textit{DocNum}) with the same specification of the example of \textbf{Doctor visits in Germany} of Section \ref{sec92}.	
\end{enumerate}