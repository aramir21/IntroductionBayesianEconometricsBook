\chapter{Longitudinal/Panel data models}\label{chap9}

We describe how to perform inference in panel/longitudinal models using a Bayesian framework. In this context, multiple cross-sectional units are observed repeatedly over time, a structure referred to as panel data by econometricians and longitudinal data by statisticians. Specifically, we present models for continuous (normal), binary (logit), and count (Poisson) responses. Applications and exercises illustrate the potential of these models.

In panel/longitudinal data sets, we have $y_{it}$ where $i=1,2,\dots,N$ and $t=1,2,\dots,T_i$. If $T_i=T$ for all $i$, the dataset is \textit{balanced}; otherwise, it is \textit{unbalanced}. Longitudinal data typically involves by far more cross-sectional units than time periods, this is called typically a \textit{short panel}. It assumes that cross-sectional units are independent, though serial correlation exists within each unit over time, and unobserved heterogeneity for each unit must be accounted for. We can treat this unobserved heterogeneity as random variables, assuming it is either independent or dependent on control variables. Econometricians refer to these cases as \textit{random effects} and \textit{fixed effects}, respectively. The Bayesian literature takes a different approach, modeling the panel structure hierarchically, where the unobserved heterogeneity may or may not depend on other controls.\footnote{See \cite{rendon2013fixed} for a nice comparison of Frequentist and Bayesian treatments of panel data models.}

Remember that we can run our GUI typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
	\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor, and once our GUI is deployed, select \textit{Hierarchical Longitudinal Models}.



\section{Normal model}\label{sec91}

The panel/longitudinal normal model establishes $\bm{y}_i=\bm{X}_i\bm{\beta}+\bm{W}_i\bm{b}_i+\bm{\mu}_i$ where $\bm{y}_i$ are $T_i$-dimensional vectors corresponding to units $i=1,2,\dots,N$, $\bm{X}_i$ and $\bm{W}_i$ are $T_i\times K_1$ and $T_i\times K_2$ matrices, respectively. In the statistical literature, $\bm{\beta}$ is a $K_1$-dimensional vector of \textit{fixed effects}, and $\bm{b}_i$ is a $K_2$-dimensional vector of unit-specific \textit{random effects} that allow unit-specific means, and enable to capture marginal dependence among the observations on the cross-sectional units. We assume normal stochastic errors, $\bm{\mu}_i\sim{N}(\bm{0},\sigma^2\bm{I}_{T_i})$, which means that the likelihood function is

{\footnotesize
\begin{align*}
	p(\bm{\beta},\bm{b},\sigma^2|\bm{y}, \bm{X},\bm{W}) & \propto \prod_{i=1}^N |\sigma^2\bm{I}_{T_i}|^{-1/2}\exp\left\{-\frac{1}{2\sigma^2}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	& = (\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\},
\end{align*} 
}
where $\bm{b}=[\bm{b}_1^{\top}, \bm{b}_2^{\top},\dots, \bm{b}_N^{\top}]^{\top}$.

Panel data modeling in the Bayesian approach assumes a hierarchical structure in the \textit{random effects}. Following \cite{Chib1999}, there is a first stage where $\bm{b}_i\sim{N}(\bm{0},\bm{D})$, $\bm{D}$ allows serial correlation within each cross-sectional unit $i$, and then, there is a second stage where $\bm{D}\sim{I}{W}(d_0,d_0\bm{D}_0)$. Thus, we can see that there is an additional layer of priors as there is a prior on the hyperparameter $\bm{D}$. 

In addition, we have standard conjugate prior distributions for $\bm{\beta}$ and $\sigma^2$, $\bm{\beta} \sim {N}(\bm{\beta}_0,\bm{B}_0)$ and 
$\sigma^2 \sim {I}{G}(\alpha_0, \delta_0)$. 

\cite{Chib1999} propose a blocking algorithm to perform inference in longitudinal hierarchical models by considering the distribution of $\bm{y}_i$ marginalized over the random effects. Given that $\bm{y}_i| \bm{\beta},\bm{b}_i,\sigma^2,\bm{X}_i,\bm{W}_i\sim N(\bm{X}_i\bm{\beta}+\bm{W}_i\bm{b}_i,\sigma^2\bm{I}_{T_i})$, we can see that    $\bm{y}_i|\bm{\beta},\bm{D},\sigma^2,\bm{X}_i,\bm{W}_i\sim{N}(\bm{X}_i\bm{\beta},\bm{V}_i)$, where $\bm{V}_i=\sigma^2\bm{I}_{T_i}+\bm{W}_i\bm{D}\bm{W}_i^{\top}$ given that $\mathbb{E}[\bm{b}_i]=\bm{0}$ and $Var[\bm{b}_i]=\bm{D}$. If we have just random intercepts, then $\bm{W}_i=\bm{i}_{T_i}$, where $\bm{i}_{T_i}$ is a $T_i$-dimensional vector of ones. Thus, $\bm{V}_i=\sigma^2\bm{I}_{T_i}+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$, the variance is $\sigma^2+\sigma^2_{b}$ and the covariance is $\sigma^2_{b}$ within each cross-sectional unit through time.

We can deduce the posterior distribution of $\bm{\beta}$ given $\sigma^2$ and $\bm{D}$,
\begin{align*}
	\pi(\bm{\beta}|\sigma^2, \bm{D},\bm{y}, \bm{X}, \bm{W}) & \propto \exp\left\{-\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta})^{\top}\bm{V}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bm{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}.
\end{align*} 
This implies that (see Exercise 1)  
\begin{equation*}
	\bm{\beta}|\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$.

We can use the likelihood $p(\bm{\beta},\bm{b}_i,\sigma^2|\bm{y}, \bm{X},\bm{W})$ to get the posterior distributions of $\bm{b}_i$, $\sigma^2$ and $\bm{D}$. In particular,
\begin{align*}
	\pi(\bm{b}_i|\bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W})&\propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	&\times \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\propto\exp\left\{-\frac{1}{2}\sum_{i=1}^N(-2\bm{b}_i^{\top}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))+ \bm{b}_i^{\top}(\sigma^{-2}\bm{W}_i^{\top}\bm{W}_i+\bm{D}^{-1})\bm{b}_i)\right\}\\
	&\propto\exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i)\right\}\\
	&=\exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i)\right\}, 
\end{align*}
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}))$.

We can complete the square in this expression by adding and subtracting $\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}$. Thus,
\begin{align*}
	\pi(\bm{b}_i|\bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W})&\propto \exp\left\{-\frac{1}{2}(-2\bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}+ \bm{b}_i^{\top}\bm{B}_{ni}^{-1}\bm{b}_i+\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni}-\bm{b}_{ni}^{\top}\bm{B}_{ni}^{-1}\bm{b}_{ni})\right\}\\
	&\propto \exp\left\{(\bm{b}_i-\bm{b}_{ni})^{\top}\bm{B}_{ni}^{-1}(\bm{b}_i-\bm{b}_{ni})\right\}. 
\end{align*}
This is the kernel of a multivariate normal distribution with mean $\bm{b}_{ni}$ and variance $\bm{B}_{ni}$. Thus,
\begin{equation*}
	\bm{b}_i|\bm{\beta},\sigma^2,\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
Let's see the posterior distribution of $\sigma^2$,
\begin{align*}
	\pi(\sigma^2|\bm{\beta},\bm{b},\bm{y}, \bm{X}, \bm{W})&\propto (\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\}\\
	&\times (\sigma^2)^{-\alpha_0-1}\exp\left\{-\frac{\delta_0}{\sigma^2}\right\}\\
	&=(\sigma^2)^{-\frac{\sum_{i=1}^N T_i}{2}-\alpha_0-1}\\
	&\times \exp\left\{-\frac{1}{\sigma^2}\left(\delta_0+\sum_{i=1}^N\frac{(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)}{2}\right)\right\}. 
\end{align*}
Thus,
\begin{equation*}
	\sigma^2| \bm{\beta}, \bm{b}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.

The posterior distribution of $\bm{D}$ is the following,
\begin{align*}
	\pi(\bm{D}|\bm{b})&\propto  |\bm{D}|^{-N/2} \exp\left\{-\frac{1}{2}\sum_{i=1}^N \bm{b}_i^{\top}\bm{D}^{-1}\bm{b}_i\right\}\\
	&\times |\bm{D}|^{-(d_0+K_2+1)/2}\exp\left\{-\frac{1}{2}tr(d_0\bm{D}_0\bm{D}^{-1})\right\}\\
	&=|\bm{D}|^{-(d_0+N+K_2+1)/2}\exp\left\{-\frac{1}{2}tr\left(\left(d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}\right)\bm{D}^{-1}\right) \right\}. 
\end{align*}
This is the kernel of an inverse Wishart distribution with degrees of freedom $d_n=d_0+N$ and scale matrix $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}$. Thus,   
\begin{equation*}
	\bm{D}| \bm{b} \sim {I}{W}(d_n, \bm{D}_n).
\end{equation*}
Observe that the posterior distribution of $\bm{D}$ dependents just on $\bm{b}$. 

All the posterior conditional distributions belong to standard families, this implies that we can use a Gibbs sampling algorithm to perform inference in these hierarchical normal models.\\

\textbf{Example: The relation between productivity and public investment}

We used the dataset named \textit{8PublicCap.csv} used by \cite{Ramirez2017} to analyze the relation  between public investment and gross state product in the setting of a spatial panel dataset consisting of 48 US states from 1970 to 1986.
In particular, we perform inference based on the following equation 
\begin{equation*}
	\log(\text{gsp})=b_i+\beta_1+\beta_2\log(\text{pcap})+\beta_3\log(\text{pc})+\beta_4\log(\text{emp})+\beta_5\text{unemp}+\mu_i,
\end{equation*}

where gsp in the gross state product, pcap is public capital, and pc is private capital all in USD, emp is employment (people), and unemp is the unemployment rate in percentage.

Algorithm \ref{alg:HLN} shows how to perform inference in hierarchical longitudinal normal models in our GUI. See also Chapter \ref{chapGUI} for details regarding the dataset structure.

\begin{algorithm}[h!]
	\caption{Hierarchical longitudinal normal models}\label{alg:HLN}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Hierarchical Longitudinal Model} on the top panel
		\State Select \textit{Normal} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the formula of the \textit{fixed effects} equation in the \textbf{Main Equation: Fixed Effects} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Write down the formula of the \textit{random effects} equation in the \textbf{Main Equation: Random Effects} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation. If there are just random intercepts do not write anything in this box
		\State Write down the name of the grouping variable, that is, the variable that indicates the cross-sectional units 
		\State Set the hyperparameters of the \textit{fixed effects}: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Set the hyperparameters of the \textit{random effects}: degrees of freedom and scale matrix of the inverse Wishart distribution. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We ask in Exercise 2 to run this application in our GUI using 10000 MCMC iterations plus a burn-in equal to 5000 iterations, and a thinning parameter equal to 1. We also used the default values for the hyperparameters of the prior distributions, that is, $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=1$ and $\bm{D}_0=\bm{I}_1$. It seems that all posterior draws come from stationary distributions, as suggested by the diagnostics and posterior plots (see Exercise 2).

The following code uses the command \textit{MCMChregress} from the package \textit{MCMCpack} to run this application. This command is also used by our GUI to perform inference in hierarchical longitudinal normal models.

We can see that the 95\% symmetric credible intervals for public capital, private capital, employment, and unemployment are (-2.54e-02, -2.06e-02), (2.92e-01, 2.96e-01), (7.62e-01, 7.67e-01) and (-5.47e-03, -5.31e-03), respectively. The posterior mean elasticity estimate of public capital to gsp is -0.023, that is, an increase by 1\% in public capital means a 0.023\% decrease in gross state product. The posterior mean estimates of private capital and employment elasticities are 0.294 and 0.765, respectively. In addition, 1 percentage point increase in the unemployment rate means a decrease of 0.54\% in gsp. It seems that all these variables are statistically relevant. In addition, the posterior mean estimates of the variance associated with the unobserved heterogeneity and stochastic errors are 1.06e-01 and 1.45e-03. We obtained the posterior chain of the proportion of the variance associated with the unobserved heterogeneity. The 95\% symmetric credible interval is (0.98, 0.99) for this proportion, that is, unobserved heterogeneity is very important to explain the total variability.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, MCMChregress command}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
DataGSP <- read.csv("DataApplications/8PublicCap.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(DataGSP)
K1 <- 5; K2 <- 1
b0 <- rep(0, K1); B0 <- diag(K1)
r0 <- 5; R0 <- diag(K2)
a0 <- 0.001; d0 <- 0.001
Resultshreg <- MCMCpack::MCMChregress(fixed = log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, random = ~1, 
group = "id", data = DataGSP, burnin = 5000, mcmc = 10000, thin = 1,
r = r0, R = R0, nu = a0, delta = d0)
Betas <- Resultshreg[["mcmc"]][,1:K1]
Sigma2RanEff <- Resultshreg[["mcmc"]][,54]
Sigma2 <- Resultshreg[["mcmc"]][,55]
summary(Betas)
Quantiles for each variable:
						2.5%       25%       50%      75%     97.5%
beta.(Intercept)  2.3145  2.3246  2.3301  2.335  2.3455
beta.log(pcap)   -0.0254 -0.0239 -0.0231 -0.022 -0.0206
beta.log(pc)      0.2917  0.2930  0.2937  0.294  0.2957
beta.log(emp)     0.7619  0.7637  0.7646  0.765  0.7672
beta.unemp       -0.0054 -0.0054 -0.0053 -0.005 -0.0053
summary(Sigma2RanEff)
Quantiles for each variable:
2.5%     25%     50%     75%   97.5% 
0.07208 0.09086 0.10331 0.11751 0.15600 
summary(Sigma2)
Quantiles for each variable:
2.5%      25%      50%      75%    97.5% 
0.001316 0.001403 0.001451 0.001501 0.001606 
summary(Sigma2RanEff/(Sigma2RanEff+Sigma2))
Quantiles for each variable:
2.5%    25%    50%    75%  97.5% 
0.9799 0.9842 0.9861 0.9879 0.9909
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

There are many extensions of this model, for instance, \cite{Chib1999} propose to introduce heteroskedasticity in this model by assuming $\mu_{it}|\tau_{it}\sim N(0, \sigma^2/\tau_{it})$, $\tau_{it}\sim G(v/2,v/2)$. We ask in Exercise 2 to perform inference in the relation between productivity and public investment example using this setting. Another potential extension is to allow dependence between $\bm{b}_i$ and some controls, let's say $\bm{z}_i$, a $K_3$-dimensional vector, and assume $\bm{b}_i\sim N(\bm{Z}_i\bm{\gamma},\bm{D})$ where $\bm{Z}_i=\bm{I}_{K_2}\otimes \bm{z}_i^{\top}$, and complete the model using a prior for $\bm{\gamma}$, $\bm{\gamma}\sim N(\bm{\gamma}_0,\bm{\Gamma}_0)$. We ask to perform a simulation using this setting in Exercise 3.\\   

\textbf{Example: Simulation exercise}

Let's perform a simulation exercise to assess some potential extensions of the longitudinal hierarchical normal model. The point of departure is to assume that $y_{it}=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+b_i+w_{it1}b_{i1}+\mu_{it}$ where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\mu_{it}\sim N(0, 0.1^{1/2})$ $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6]^{\top}$, $i=1,2,\dots,50$, and the sample size is 2000 in an \textit{unbalanced panel structure}. 

Following same stages as in this section and Exercise 1, the posterior conditional distributions assuming that $\mu_{it}|\tau_{it}\sim N(0, \sigma^2/\tau_{it})$, $\tau_{it}\sim G(v/2,v/2)$ are given by 
\begin{equation*}
	\bm{\beta}|\sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{\tau}=[\tau_{it}]^{\top}$, $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$, $\bm{V}_i=\sigma^2\bm{\Psi}_i+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$ and $\bm{\Psi}_i=diag\left\{\tau_{it}^{-1}\right\}$.
\begin{equation*}
	\bm{b}_i|\bm{\beta},\sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}))$.
\begin{equation*}
	\sigma^2| \bm{\beta}, \bm{b}, \bm{\tau}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.  
\begin{equation*}
	\bm{D}| \bm{b} \sim {I}{W}(d_n, \bm{D}_n),
\end{equation*}
where $d_n=d_0+N$ and $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}$.
\begin{equation*}
	\tau_{it}|\sigma^2, \bm{\beta}, \bm{b}, \bm{y}, \bm{X}, \bm{W} \sim {G}(v_{1n}/2, v_{2ni}/2),
\end{equation*}
where $v_{1n}=v+1$ and $v_{2ni}=v+\sigma^{-2}(y_{it}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^2$.

The following code implements this simulation, and gets draws of the posterior distributions. We set MCMC iterations, burn-in and thinning parameters equal to 5000, 1000 and 1, respectively. In addition, $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=2$, $\bm{D}_0=\bm{I}_2$ and $v=5$.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
NT <- 2000; N <- 50
id <- c(1:N, sample(1:N, NT - N,replace=TRUE))
table(id)
x1 <- rnorm(NT); x2 <- rnorm(NT); x3 <- rnorm(NT) 
X <- cbind(1, x1, x2, x3); K1 <- dim(X)[2]
w1 <- rnorm(NT); W <- cbind(1, w1)
K2 <- dim(W)[2]; B <- c(0.5, 0.4, 0.6, -0.6)
D <- c(0.7, 0.6)
b1 <- rnorm(N, 0, sd = D[1]^0.5)
b2 <- rnorm(N, 0, sd = D[2]^0.5)
b <- cbind(b1, b2)
v <- 5; tau <- rgamma(NT, shape = v/2, rate = v/2)
sig2 <- 0.1; u <- rnorm(NT, 0, sd = (sig2/tau)^0.5)
y <- NULL
for(i in 1:NT){
	yi <- X[i,]%*%B + W[i,]%*%b[id[i],] + u[i] 
	y <- c(y, yi)
}
Data <- as.data.frame(cbind(y, x1, x2, x3, w1, id))
mcmc <- 5000; burnin <- 1000; thin <- 1; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
PostBeta <- function(sig2, D, tau){
	XVX <- matrix(0, K1, K1)
	XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Wi <- W[ids, ]
		taui <- tau[ids]
		Vi <- sig2*solve(diag(1/taui)) + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi)
		Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		yi <- y[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
Postb <- function(Beta, sig2, D, tau){
	Di <- solve(D); 	bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]; Xi <- X[ids, ]
		yi <- y[ids]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		Wtei <- sig2^(-1)*t(Wi)%*%Taui%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Taui%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostSig2 <- function(Beta, bs, tau){
	an <- a0 + 0.5*NT
	ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]; yi <- y[ids]
		Wi <- W[ids, ]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
		etei <- t(ei)%*%Taui%*%ei
		ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
PostD <- function(bs){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]
		btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostTau <- function(sig2, Beta, bs){
	v1n <- v + 1
	v2n <- NULL
	for(i in 1:NT){
		Xi <- X[i, ]; yi <- y[i]
		Wi <- W[i, ]; bi <- bs[id[i],]
		v2ni <- v + sig2^(-1)*(yi - Xi%*%Beta - Wi%*%bi)^2
		v2n <- c(v2n, v2ni)
	}
	tau <- rgamma(NT, shape = rep(v1n/2, NT), rate = v2n/2)
	return(tau)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with heteroskedasticity from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1); PostDs <- matrix(0, tot, K2*(K2+1)/2)
PostSig2s <- rep(0, tot); Postbs <- array(0, c(N, K2, tot))
PostTaus <- matrix(0, tot, NT); RegLS <- lm(y ~ X - 1)
SumLS <- summary(RegLS)
Beta <- SumLS[["coefficients"]][,1]
sig2 <- SumLS[["sigma"]]^2; D <- diag(K2)
tau <- rgamma(NT, shape = v/2, rate = v/2) 
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	bs <- Postb(Beta = Beta, sig2 = sig2, D = D, tau = tau)
	D <- PostD(bs = bs)
	Beta <- PostBeta(sig2 = sig2, D = D, tau = tau)
	sig2 <- PostSig2(Beta = Beta, bs = bs, tau = tau)
	tau <- PostTau(sig2 = sig2, Beta = Beta, bs = bs)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	PostSig2s[s] <- sig2
	Postbs[, , s] <- bs
	PostTaus[s,] <- tau
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]; Ds <- PostDs[keep,]
bs <- Postbs[, , keep]; sig2s <- PostSig2s[keep]
taus <- PostTaus[keep,]
summary(coda::mcmc(Bs))
Quantiles for each variable:
         	2.5%     25%     50%     75%   97.5%
var1  0.07833  0.2412  0.3232  0.4022  0.5619
var2  0.34101  0.3598  0.3697  0.3793  0.3988
var3  0.59596  0.6150  0.6251  0.6351  0.6574
var4 -0.63722 -0.6165 -0.6067 -0.5966 -0.5785
summary(coda::mcmc(Ds))
Quantiles for each variable:
       2.5%     25%      50%      75%   97.5%
var1  0.4720  0.5995  0.68858  0.79206 1.05285
var2 -0.2721 -0.1405 -0.08185 -0.02482 0.09186
var3  0.3689  0.4644  0.52978  0.60946 0.81999
summary(coda::mcmc(sig2s))
 Quantiles for each variable:
2.5%    25%    50%    75%  97.5% 
0.1022 0.1157 0.1324 0.1683 0.3217 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
We can see that all the 95\% credible intervals encompass the population parameters, except for the second \textit{fixed effect} and the variance of the model, but both for a tiny margin.  

\section{Logit model}\label{sec92}

\section{Poisson model}\label{sec93}

\section{Summary}\label{sec94}

\section{Exercises}\label{sec95}

\begin{enumerate}
	
	\item Show that the posterior distribution of $\bm{\beta}|\sigma^2,\bm{D}$ is $N(\bm{\beta}_n,\bm{B}_n)$, where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$.
	
	\item \textbf{The relation between productivity and public investment example continues}
	
	\begin{itemize}
		\item Perform inference of this example using our GUI.
		\item Program from scratch a Gibbs sampling algorithm to perform this application.
		\item Perform inference in this example assuming that $\mu_{it}|\tau_{it}\sim N(0, \sigma^2/\tau_{it})$ and $\tau_{it}\sim G(v/2,v/2)$ setting $v=5$. 
	\end{itemize}
  
	\item Assume that $y_{it}=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+\beta_4 z_{i1}+b_i+w_{it1}b_{i1}+\mu_{it}$ where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $z_{i1}\sim B(0.5)$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\mu_{it}\sim N(0, 0.1^{1/2})$ $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6 \ 0.7]^{\top}$, $i=1,2,\dots,50$, and the sample size is 2000 in an \textit{unbalanced panel structure}. In addition, we assume that $\bm{b}_i$ dependents on $\bm{z}_i=[1 \ z_{i1}]^{\top}$ such that $\bm{b}_i\sim N(\bm{Z}_i\bm{\gamma},\bm{D})$ where $\bm{Z}_i=\bm{I}_{K_2}\otimes \bm{z}_i^{\top}$, where $\bm{\gamma}=[1 \ 1 \ 1 \ 1]$. The prior for $\bm{\gamma}$ is $N(\bm{0}_4,\bm{I}_4)$.
	
	\begin{itemize}
		\item Perform inference in this model without taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters.
		\item Perform inference in this model taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters. 
	\end{itemize}
	
	
	
\end{enumerate}