\chapter{Univariate models}\label{chap6}

We describe how to perform Bayesian inference in some of the most common univariate models: normal-inverse gamma, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap in linear models. We show the posterior distributions of the parameters and some applications. In addition, we demonstrate how to perform inference in various models using three levels of programming skills: our graphical user interface (GUI), packages from \textbf{R}, and direct programming of the posterior distributions. The first requires no programming skills, the second requires an intermediate level, and the third demands more advanced skills. We also include mathematical and computational exercises

\section{The Gaussian linear model}\label{sec61}

The Gaussian linear model specifies ${\bf{y}}={\bf{X}}\bm{\bm{\beta}}+\bf{\mu}$ such that $\mu\sim N(\bf{0},\sigma^2\bf{I}_N)$ is an stochastic error, ${\bf{X}}$ is a $N \times K$ matrix of regressors, $\bm{\bm{\beta}}$ is a $K$-dimensional vector of location coefficients, $\sigma^2$ is the variance of the model (scale parameter), ${\bf{y}}$ is a $N$-dimensional vector of a dependent variable, and $N$ is the sample size. We describe this model using the conjugate family in Section \ref{sec33}, that is, $\pi(\bm{\bm{\beta}},\sigma^2)=\pi(\bm{\bm{\beta}}|\sigma^2)\times\pi(\sigma^2)$, and this allowed to get the posterior marginal distribution for $\bm{\bm{\beta}}$ and $\sigma^2$.

We assume independent prior in this section, that is, $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$, $\alpha_0/2$ and $\delta_0/2$ are the shape and rate parameters. This setting allows getting the posterior conditional distributions, that is, $\pi(\bm{\beta}|\sigma^2,{\bf{y}}, {\bf{X}})$ and $\pi(\sigma^2|\bm{\beta},{\bf{y}}, {\bf{X}})$, which in turn allows to use the Gibbs sampler algorithm to perform posterior inference of $\bm{\beta}$ and $\sigma^2$.

The likelihood function in this model is
\begin{align*}
	p({\bf{y}}| \bm{\beta}, \sigma^2, {\bf{X}}) = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{y}} - \bf{X\bm{\beta}})^{\top}({\bf{y}} - \bf{X\bm{\beta}}) \right\}.
\end{align*}

Then, the conditional posterior distributions are

\begin{align*}
	\bm{\beta}|\sigma^2, {\bf{y}}, {\bf{X}} \sim N(\bm{\beta}_n, \sigma^2{\bf{B}}_n),
\end{align*}
and
\begin{align*}
	\sigma^2|\bm{\beta}, {\bf{y}}, {\bf{X}} \sim IG(\alpha_n/2, \delta_n/2),
\end{align*}

 where ${\bf{B}}_n = ({\bf{B}}_0^{-1} + \sigma^{-2} {\bf{X}}^{\top}{\bf{X}})^{-1}$, $\bm{\beta}_n= {\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0 + \sigma^{-2} {\bf{X}}^{\top}{\bf{y}})$, $\alpha_n = \alpha_0 + N$ and $\delta_n = \delta_0 + ({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}({\bf{y}}-{\bf{X}}\bm{\beta})$ (see Exercise 1 in this chapter).\footnote{This model can be extended to consider heteroskedasticity such that $y_i\sim N({\bf{x}}_i^{\top}\bm{\beta}, \sigma^2/\tau_i)$, where $\tau_i\sim G(v/2,v/2)$. See exercise 2 for details.}\\

\textbf{Example: The market value of soccer players in Europe}

Let's analyze the determinants of the market value of soccer players in Europe. In particular, we use the dataset \textit{1ValueFootballPlayers.csv} which is in folder \textbf{DataApp} in our github repository \textbf{https://github.com/besmarter/BSTApp}. This dataset was used by \cite{Serna2018} to finding the determinants of high performance soccer players in the five most important national leagues in Europe.

The specification of the model is
\begin{align*}
	\log(\text{Value}_i)&={\beta}_1+{\beta}_2\text{Perf}_i+{\beta}_3\text{Age}_i+{\beta}_4\text{Age}^2_i+{\beta}_5\text{NatTeam}_i\\
	&+{\beta}_6\text{Goals}_i+{\beta}_7\text{Exp}_i+{\beta}_{8}\text{Exp}^2_i+\mu_i,
\end{align*}

where \textit{Value} is the market value in Euros (2017), \textit{Perf} is a measure of performance, \textit{Age} is the players' age in years, \textit{NatTem} is an indicator variable that takes the value of 1 if the player has been on the national team, \textit{Goals} is the number of goals scored by the player during his career, and \textit{Exp} is his experience in years.  

We assume that the dependent variable distributes normal, then we use a normal-inverse gamma model using vague conjugate priors where ${\bf{B}}_0=1000{\bf{I}}_{8}$, $\bm{\beta}_0={\bf{0}}_{8}$, $\alpha_0=0.001$ and $\delta_0=0.001$. We perform a Gibbs sampler with 5,000 MCMC iterations plus a burn-in equal to 5,000, and a thinning parameter equal to 1.

We can see in the results of the algorithms that we get similar results using the three approaches: GUI, package and our function. For instance, the value of a top soccer player in Europe increases 134\% ($\exp(0.85)-1)$) on average when he has played in the national team, the credible interval at 95\% is (86\%, 197\%).  


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The value of soccer players, using \textbf{R} packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
########################## Linear regression: Value of soccer players ##########################
Data <- read.csv("DataApplications/1ValueFootballPlayers.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean       SD  Naive SE Time-series SE
X         3.695499 2.228060 3.151e-02      3.151e-02
XPerf     0.035445 0.004299 6.079e-05      6.079e-05
XAge      0.778410 0.181362 2.565e-03      2.565e-03
XAge2    -0.016617 0.003380 4.781e-05      4.781e-05
XNatTeam  0.850362 0.116861 1.653e-03      1.689e-03
XGoals    0.009097 0.001603 2.266e-05      2.266e-05
XExp      0.206208 0.062713 8.869e-04      8.428e-04
XExp2    -0.006992 0.002718 3.844e-05      3.719e-05
sigma2    0.969590 0.076091 1.076e-03      1.076e-03
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
bhat <- solve(XtX)%*%t(X)%*%y
an <- a0 + N
# Gibbs sampling functions
PostSig2 <- function(Beta){
	dn <- d0 + t(y - X%*%Beta)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*XtX%*%bhat)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2)
	PostBetas[s,] <- Beta
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean       SD  Naive SE Time-series SE
Intercept  3.663230 2.194363 3.103e-02      3.103e-02
Perf       0.035361 0.004315 6.102e-05      6.102e-05
Age        0.780374 0.178530 2.525e-03      2.525e-03
Age2      -0.016641 0.003332 4.713e-05      4.713e-05
NatTeam    0.850094 0.119093 1.684e-03      1.684e-03
Goals      0.009164 0.001605 2.270e-05      2.270e-05
Exp        0.205965 0.062985 8.907e-04      8.596e-04
Exp2      -0.007006 0.002731 3.862e-05      3.701e-05
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean             SD       Naive SE Time-series SE 
0.973309       0.077316       0.001093       0.001116 
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The logit model}\label{sec62}

In the logit model the dependent variable is binary, $Y_i=\left\{1,0\right\}$, then it follows a Bernoulli distribution, $Y_i\stackrel{ind} {\thicksim}B(\pi_i)$, that is, $p(Y_i=1)=\pi_i$, such that $\pi_i=\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}$.

The likelihood function of the logit model is
\begin{align*}
	p({\bf{y}}|\bm{\beta},{\bf{X}})&=\prod_{i=1}^N \pi_i^{y_i}(1-\pi_i)^{1-y_i}\\
	&=\prod_{i=1}^N\left(\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{y_i}\left(\frac{1}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{1-y_i}.
\end{align*}

We can specify a Normal distribution as prior, $\bm{\beta}\sim N({\bm{\beta}}_0,{\bf{B}}_0)$. Then, the posterior distribution is

\begin{align*}
	\pi(\bm{\beta}|{\bf{y}},{\bf{X}})&\propto\prod_{i=1}^N\left(\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{y_i}\left(\frac{1}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{1-y_i}\\
	&\times\exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}.
\end{align*}

The logit model does not have a standard posterior distribution. Then, a random walk Metropolis--Hastings algorithm can be used to obtain draws from the posterior distribution. A potential proposal is a multivariate Normal centered at the current value, with covariance matrix $\tau^2({\bf{B}}_0^{-1}+\widehat{{\bm{\Sigma}}}^{-1})^{-1}$, where $\tau>0$ is a tuning parameter and $\widehat{\bm{\Sigma}}$ is the sample covariance matrix from the maximum likelihood estimation \cite{Martin2011}.\footnote{Tuning parameters should be set in a way such that one obtains reasonable diagnostic criteria and acceptation rates.}

Observe that $\log(p({\bf{y}}|\bm{\beta},{\bf{X}}))=\sum_{i=1}^Ny_i{\bf{x}}_i^{\top}\bm{\beta}-\log(1+\exp({\bf{x}}_i^{\top}\bm{\beta}))$. We can use this expression when calculating the acceptance parameter in the computational implementation of the Metropolis-Hastings algorithm. In particular, the acceptance parameter is \begin{equation*}
	\alpha=\min\left\{1, \exp(\log(p({\bf{y}}|\bm{\beta}^{c},{\bf{X}}))+\log(\pi(\bm{\beta}^c))-(\log(p({\bf{y}}|\bm{\beta}^{(s-1)},{\bf{X}}))+\log(\pi(\bm{\beta}^{(s-1)}))))\right\},
\end{equation*}
where $\bm{\beta}^c$ and $\bm{\beta}^{(s-1)}$ are the draws from the proposal distribution and the previous iteration of the Markov chain, respectively.\footnote{Formulating the acceptance rate using $\log$ helps to mitigate computational problems.}\\

\textbf{Example: Simulation exercise}

Let's do a simulation exercise to check the performance of the algorithm. Set $\bm{\beta}=\begin{bmatrix}0.5 & 0.8 & -1.2\end{bmatrix}^{\top}$, $x_{ik}\sim N(0,1)$, $k=2,3$ and $i=1,2,\dots,10000$.

We set as hyperparameters $\bm{\beta}_0=[0 \ 0 \ 0]^{\top}$ and ${\bf{B}}_0=1000{\bf{I}}_3$. The tune parameter for the Metropolis-Hastings algorithm is equal to 1. We obtain an acceptance rate of 0.46, and the diagnostics suggest that the posterior chains behave well. We also see that the posterior results using \textit{MCMClogit} from \textit{MCMCpack} and our Metropolis-Hastings algorithm are very similar. In general, the 95\% credible intervals encompass the population values, and the mean and median are very close to these values.  

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, using R packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
########################## Logit: Simulation ##########################
# Simulate data
rm(list = ls())
set.seed(010101)
N <- 10000 # Sample size
B <- c(0.5, 0.8, -1.2) # Population location parameters
x2 <- rnorm(N) # Regressor
x3 <- rnorm(N) # Regressor
X <- cbind(1, x2, x3) # Regressors
XB <- X%*%B
PY <- exp(XB)/(1 + exp(XB)) # Probability of Y = 1
Y <- rbinom(N, 1, PY) # Draw Y's
table(Y) # Frequency
# write.csv(cbind(Y, x2, x3), file = "DataSimulations/LogitSim.csv") # Export data
# MCMC parameters
iter <- 5000; burnin <- 1000; thin <- 5; tune <- 1
# Hyperparameters
K <- dim(X)[2] 
b0 <- rep(0, K)
c0 <- 1000
B0 <- c0*diag(K)
B0i <- solve(B0)
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
RegLog <- MCMCpack::MCMClogit(Y~X-1, mcmc = iter, burnin = burnin, thin = thin, b0 = b0, B0 = B0i, tune = tune)
summary(RegLog)
Iterations = 1001:5996
Thinning interval = 5 
Number of chains = 1 
Sample size per chain = 1000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean      SD  Naive SE Time-series SE
X    0.4896 0.02550 0.0008064       0.001246
Xx2  0.8330 0.02730 0.0008632       0.001406
Xx3 -1.2104 0.03049 0.0009643       0.001536
2. Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
X    0.4424  0.4728  0.4894  0.5072  0.5405
Xx2  0.7787  0.8159  0.8327  0.8505  0.8852
Xx3 -1.2758 -1.2296 -1.2088 -1.1902 -1.1513
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Posterior distributions programming the Metropolis-Hastings algorithm
MHfunc <- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 6000, burnin = 1000, thin = 5){
	Xm <- cbind(1, X) # Regressors
	K <- dim(Xm)[2] # Number of location parameters
	BETAS <- matrix(0, iter + burnin, K) # Space for posterior chains
	Reg <- glm(y ~ Xm - 1, family = binomial(link = "logit")) # Maximum likelihood estimation
	BETA <- Reg$coefficients # Maximum likelihood parameter estimates 
	tot <- iter + burnin # Total iterations M-H algorithm
	COV <- vcov(Reg) # Maximum likelihood covariance matrix
	COVt <- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution
	Accep <- rep(0, tot) # Space for calculating the acceptance rate
	# Create progress bar in case that you want to see iterations progress
	pb <- winProgressBar(title = "progress bar", min = 0,
	max = tot, width = 300)
	for(it in 1:tot){
		BETAc <- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter
		likecand <- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood for the candidate
		likepast <- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log likelihood for the actual draw
		priorcand <- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior for candidate
		priorpast <- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior for actual draw
		alpha <- min(1, exp(likecand + priorcand - likepast - priorpast)) #Probability of selecting candidate
		u <- runif(1) # Decision rule for selecting candidate
		if(u < alpha){
			BETA <- BETAc # Changing reference for candidate if selected
			Accep[it] <- 1 # Indicator if the candidate is accepted
		} 
		BETAS[it, ] <- BETA # Saving draws
		setWinProgressBar(pb, it, title=paste( round(it/tot*100, 0),
		"% done"))
	}
	close(pb)
	keep <- seq(burnin, tot, thin)
	return(list(Bs = BETAS[keep[-1], ], AceptRate = mean(Accep[keep[-1]])))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]		
Posterior <- MHfunc(y = Y, X = cbind(x2, x3), iter = iter, burnin = burnin, thin = thin) # Running our M-H function changing some default parameters.
paste("Acceptance rate equal to", round(Posterior$AceptRate, 2), sep = " ")
"Acceptance rate equal to 0.46"
PostPar <- coda::mcmc(Posterior$Bs)
# Names
colnames(PostPar) <- c("Cte", "x1", "x2")
# Summary posterior draws
summary(PostPar)
Iterations = 1:1000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean      SD  Naive SE Time-series SE
Cte  0.4893 0.02427 0.0007674       0.001223
x1   0.8309 0.02699 0.0008536       0.001440
x2  -1.2107 0.02943 0.0009308       0.001423
2. Quantiles for each variable:
		2.5%     25%     50%     75%   97.5%
Cte  0.4431  0.4721  0.4899  0.5059  0.5344
x1   0.7817  0.8123  0.8305  0.8505  0.8833
x2  -1.2665 -1.2309 -1.2107 -1.1911 -1.1538
# Trace and density plots
plot(PostPar)
# Autocorrelation plots
coda::autocorr.plot(PostPar)
# Convergence diagnostics
coda::geweke.diag(PostPar)
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 
Cte     x1     x2 
-0.975 -3.112  1.326 
coda::raftery.diag(PostPar,q=0.5,r=0.05,s = 0.95)
Quantile (q) = 0.5
Accuracy (r) = +/- 0.05
Probability (s) = 0.95 
Burn-in  Total Lower bound  Dependence
(M)      (N)   (Nmin)       factor (I)
Cte 6        731   385          1.90      
x1  6        703   385          1.83      
x2  6        725   385          1.88 
coda::heidel.diag(PostPar)
Stationarity start     p-value
test         iteration        
Cte passed         1       0.4436 
x1  passed       101       0.3470 
x2  passed         1       0.0872 
Halfwidth Mean   Halfwidth
test                      
Cte passed     0.489 0.00240  
x1  passed     0.832 0.00268  
x2  passed    -1.211 0.00279
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The probit model}\label{sec63}

The probit model also has as dependent variable a binary outcome.
In this case, there is a latent variable ($y_i^*$, unobserved) that defines the structure of the estimation problem.
In particular,
\begin{equation*}
Y_i=\begin{Bmatrix}
	0, \ Y_i^*\leq 0 \\ 
	1, \ Y_i^*> 0 \\ 
\end{Bmatrix},
\end{equation*}

such that $Y_i^*=\bf{x}_i^{\top}\bm{\beta}+\mu_i$, $\mu_i\stackrel{i.i.d.} {\thicksim}N(0,1)$.\footnote{The variance in this model is set to 1 due to identification restrictions.	Observe that $P(Y_i=1|\bf{x}_i)=P(Y_i^*>0|\bf{x}_i)=P(\bf{x}_i^{\top}\bm{\beta}+\mu_i>0|\bf{x}_i)=P(\mu_i>-\bf{x}_i^{\top}\bm{\beta}|\bf{x}_i)=P(c\times\mu_i>-c\times\bf{x}_i^{\top}\bm{\beta}|\bf{x}_i)$ $\forall c>0$. Multiplying for a positive constant does not affect the probability of $Y_i=1$.} This implies $P(Y_i=1)=\pi_i=\Phi(\bf{x}_i^{\top}\bm{\beta})$.\\

\cite{Albert1993} implemented data augmentation \cite{Tanner1987} to apply a Gibbs sampling algorithm in this model.
Augmenting this model with $Y_i^*$, we can have the likelihood contribution from observation $i$, $p(y_i|y_i^*)=\mathbbm{1}_{y_i=0}\mathbbm{1}_{y_i^*\leq 0}+\mathbbm{1}_{y_i=1}\mathbbm{1}_{y_i^*> 0}$, where $\mathbbm{1}_A$ is an indicator function that takes the value of 1 when condition $A$ is satisfied.\\

The posterior distribution is $\pi(\bm{\beta},\bm{y^*}|\bm{y},\bm{X})\propto\prod_{i=1}^N\left[\mathbbm{1}_{y_i=0}\mathbbm{1}_{y_i^*\leq 0}+\mathbbm{1}_{y_i=1}\mathbbm{1}_{y_i^*> 0}\right] \times {N}_N(\bm{y}^*|\bm{X\bm{\beta}},\bm{I}_n)\times {N}_K(\bm{\beta}|\bm{\beta}_0,\bm{B}_0)$ when taking a Gaussian distribution as prior $\bm{\beta}\sim{N}_k(\bm{\beta}_0,\bm{B}_0)$.
This implies
\begin{equation*}
	y_i^*|\bm{\beta},\bm{y},\bm{X}\sim\begin{Bmatrix}
		TN_{(-\infty,0]}(\bf{x}_i^{\top}\bm{\beta},1) \ , \ y_i= 0 \\ 
		TN_{(0,\infty)}(\bf{x}_i^{\top}\bm{\beta},1) \ \ \ , \ y_i= 1
	\end{Bmatrix},\footnote{$TN$ denotes a truncated normal density.}
\end{equation*}
\begin{equation*}
	\bm{\beta}|\bm{y}^*, \bm{X} \sim N(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
\noindent where $\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{\top}\bm{X})^{-1}$, and $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}\bm{y}^*)$.\\

\textbf{Example: Determinants of hospitalization}

We use the dataset named \textbf{2HealthMed.csv}, which is in folder \textbf{DataApp} in our github repository \textbf{(https://github.com/besmarter/BSTApp} and was used by \cite{Ramirez2013}. Our dependent variable is a binary indicator with a value equal to 1 if an individual was hospitalized in 2007, and 0 otherwise.

The specification of the model is
\begin{align*}
	\text{Hosp}_i&={\beta}_1+{\beta}_2\text{SHI}_i+{\beta}_3\text{Female}_i+{\beta}_4\text{Age}_i+{\beta}_5\text{Age}_i^2+{\beta}_6\text{Est2}_i+{\beta}_7\text{Est3}_i\\
	&+{\beta}_8\text{Fair}_i+{\beta}_9\text{Good}_i+{\beta}_{10}\text{Excellent}_i,
\end{align*}

where \textit{SHI} is a binary variable equal to 1 if the individual is in a subsidized health care program and 0 otherwise, \textit{Female} is an indicator of gender, \textit{Age} in years, \textit{Est2} and \textit{Est3} are indicators of socioeconomic status, the reference is \textit{Est1}, which is the lowest, and self perception of health status where \textit{bad} is the reference.

Let's set $\bm{\beta}_0={\bf{0}}_{10}$, ${\bf{B}}_0={\bf{I}}_{10}$, iterations, burn-in and thinning parameters equal to 10000, 1000 and 1, respectively. We use the package \textit{bayesm} to perform this exercise. We asked to program a Gibbs sampler algorithm in the exercises.

We find evidence that gender and self-perceived health status affect the probability of hospitalization. Women have a higher probability of being hospitalized than men, and a better perception of health status decreases this probability.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization}
	\begin{VF}
		\begin{lstlisting}[language=R]	
mydata <- read.csv("DataApplications/2HealthMed.csv", header = T, sep = ",")
attach(mydata)
str(mydata)
K <- 10 # Number of regressors
b0 <- rep(0, K) # Prio mean
B0i <- diag(K) # Prior precision (inverse of covariance)
Prior <- list(betabar = b0, A = B0i) # Prior list
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
Data <- list(y = y, X = X) # Data list
Mcmc <- list(R = 10000, keep = 1, nprint = 0) # MCMC parameters
RegProb <- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package
PostPar <- coda::mcmc(RegProb$betadraw) # Posterior draws
colnames(PostPar) <- c("Cte", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent") # Names
summary(PostPar) # Posterior summary
Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000
2. Quantiles for each variable:
				2.5%        25%        50%        75%      97.5%
Cte       -1.22e+00 -1.03e+00 -9.43e-01 -8.50e-01 -0.671744
SHI       -1.24e-01 -4.63e-02 -6.30e-03  3.26e-02  0.104703
Female     2.80e-02  9.65e-02  1.28e-01  1.60e-01  0.223123
Age       -7.55e-03 -2.50e-03  1.25e-04  2.80e-03  0.007646
Age2      -4.98e-05  9.05e-06  4.02e-05  7.07e-05  0.000128
Est2      -1.89e-01 -1.23e-01 -8.84e-02 -5.32e-02  0.012714
Est3      -2.13e-01 -1.03e-01 -4.73e-02  1.01e-02  0.109527
Fair      -7.09e-01 -5.69e-01 -4.93e-01 -4.16e-01 -0.269494
Good      -1.42e+00 -1.28e+00 -1.20e+00 -1.12e+00 -0.982533
Excellent -1.33e+00 -1.15e+00 -1.06e+00 -9.74e-01 -0.795881
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The multinomial probit model}\label{sec64}
The multinomial probit model is used to model the choice of the $l$-th alternative over a set $L$ mutually exclusive options.
We observe 
\begin{equation*}
y_{il}=
\begin{Bmatrix}
	1, & y_{il}^*\geq \max\left\{\bm{y}_i^*\right\}  \\
	0, & \text{otherwise}
\end{Bmatrix},
\end{equation*}

such that $\bm{y}_i^*=\bm{X}_{i}\bm{\delta}+\bm\mu_i$, $\bm\mu_i\stackrel{i.i.d.} {\thicksim}N(\bm 0,\bm{\Sigma})$, $\bm{y}_i^*$ is an unobserved latent $L$ dimensional vector, $\bm{X}_{i}=\left[(1 \ \bm{c}_i^{\top})\otimes \bm{I}_L \ \bm{A}_i\right]$ is an $L\times j$ matrix of regressors for each alternative, $l=1,2,\dots,L$, $j=L\times (1+dim\left\{\bm{c}_i\right\})+a$, $\bm{c}_i$ is a vector of the individuals' specific characteristics, $\bm{A}_i$ is an $L\times a$ matrix of alternative-varying regressors, $a$ is the number of alternative-varying regressors, and $\bm{\delta}$ is a $j$ dimensional vector of parameters.

We take into account simultaneously the alternative-varying regressors (alternative attributes) and alternative-invariant regressors (individual characteristics).\footnote{Note that this model is not identified if $\bm{\Sigma}$ is unrestricted. The likelihood function is the same if a scalar random variable is added to each of the $L$ latent regressions.} $\bm{y}_i^*$ can be stacked up into a multiple regression with correlated stochastic errors, $\bm{y}^*=\bm{X}\bm\delta+\bm{\mu}$, where $\bm{y}^*=\left[\bm{y}_1^{*\top},\bm{y}_2^{*\top},\dots,\bm{y}_N^{*\top}\right]$,$\bm{X}=\left[\bm{X}_1^{\top},\bm{X}_2^{\top},\dots,\bm{X}_N^{\top}\right]^{\top}$, and $\bm{\mu}=\left[\bm{\mu}_1^{\top},\bm{\mu}_2^{\top},\dots,\bm{\mu}_N^{\top}\right]^{\top}$.

Following the practice of expressing $y_{il}^*$ relative to $y_{iL}^*$ by letting $\bm{w}_i=\left[w_{i1},w_{i2},\dots,w_{iL-1}\right]^{\top}$, $w_{il}=y_{il}^*-y_{iL}^*$, we can write $\bm{w}_i=\bm{R}_i\bm{\beta}+\bm{\epsilon}_i$, $\bm{\epsilon_i}\sim{N}(\bm 0,\bm{\Omega})$, where $\bm{R}_i=\left[(1 \ \bm{c}_i^{\top})\otimes \bm{I}_{L-1} \ \bm{\Delta A}_i\right]$ is an $L-1\times k$ matrix where $\Delta \bm{A}_i=\bm{A}_{li}-\bm{A}_{Li}$, $l=1,2, \dots, L-1$, that is, the last row of $\bm{A}_i$ is subtracted from each row of $\bm{A}_i$, and ${\bm{\beta}}$ is a $k$ dimensional vector, $k=(L-1)\times(1+dim\left\{\bm{c}_i\right\})+a$.

Observe that $\bm{\beta}$ contains the same last $a$ elements as $\bm{\delta}$, that is, alternative specific attributes coefficients, but the first $(L-1)\times(1+dim\left\{\bm{c}_i\right\})$-th elements are $\delta_{jl}-\delta_{jL}$, $j=1+dim\left\{\bm{c}_i\right\}$, $l=1,2,\dots,L-1$, that is, the difference between the coefficients of each qualitative response and the $L$-th alternative for the individuals' characteristics.This makes it difficult to interpret the multinomial probit coefficients.

Note that in multinomial models, for each alternative specific attribute, it is only required to estimate one coefficient for all alternatives, whereas for individuals' characteristics (non-alternative specific regressors), it is necessary to estimate $L-1$ coefficients (the coefficient of the base alternative is set equal to 0).

The likelihood function in this model is $p(\bm{\beta},\bm{\Omega}|\bm{y},\bm{R})=\prod_{i=1}^N\prod_{l=1}^L p_{il}^{y_{il}}$ where $p_{il}=p(y_{il}^*\geq \max(\bm{y}_i^*))$.

We assume independent priors, $\bm{\beta}\sim N(\bm{\beta}_0,\bm{B}_0)$ and $\bm{\Omega}^{-1}\sim W(\alpha_0,\bm{\Sigma}_0)$.\footnote{$W$ denotes the Wishart density.} We can employ Gibbs sampling in this model because this is a standard Bayesian linear regression model when data augmentation in $\bm{w}$ is used.
The posterior conditional distributions are
\begin{equation*}
	\bm{\beta}|\bm{\Omega},\bm{w}\sim{N}(\bm{\beta}_n,\bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\Omega}^{-1}|\bm{\beta},\bm{w}\sim{W}(\alpha_n,\bm{\Sigma}_n),
\end{equation*}

where $\bm{B}_n=(\bm{B}_0^{-1}+\bm{X}^{*\top}\bm{X}^*)^{-1}$, $\bm{\beta}_n=\bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0+\bm{X}^{*\top}\bm{w}^*)$, $\bm{\Omega}^{-1}=\bm{C}^{\top}\bm{C}$, $\bm{X}_i^{*\top}=\bm{C}^{\top}\bm{R}_i$, $\bm{w}_i^*=\bm{C}^{\top}\bm{w}_i$, $\bm{X}^*=\begin{bmatrix}\bm{X}_1^*\\
	\bm{X}_2^*\\
	\vdots\\
	\bm{X}_N^*
\end{bmatrix}$, $\alpha_n=\alpha_0+N$, $\bm{\Sigma}_n=(\bm{\Sigma}_0+\sum_{i=1}^N (\bm{w}_i-\bm{R}_i\bm{\beta})^{\top}(\bm{w}_i-\bm{R}_i\bm{\beta}))^{-1}$.

We can collapse the multinomial vector $\bm{y}_i$ into the indicator variable $d_i=\sum_{l=1}^{L-1}l\times \mathbbm{1}_{\max(\bm{w}_{l})=w_{il}}$.\footnote{Observe that the identification issue in this model is due to scaling $w_{il}$ by a positive constant does not change the value of $d_i$.} Then the distribution of $\bm{w}_i|\bm{\beta},\bm{\Omega}^{-1},d_i$ is an $L-1$ dimensional Gaussian distribution truncated over the appropriate cone in $\mathcal{R}^{L-1}$.
\cite{McCulloch1994} propose drawing from the univariate conditional distributions $w_{il}|\bm{w}_{i,-l},\bm{\beta},\bm{\Omega}^{-1},d_i\sim TN_{I_{il}}(m_{il},\tau_{ll}^2)$, where 

\begin{equation*}
I_{il}=\begin{Bmatrix} w_{il}>\max(\bm{w}_{i,-l},0), & d_i=l\\
	w_{il}<\max(\bm{w}_{i,-l},0), & d_i\neq l\\
\end{Bmatrix},
\end{equation*}
and permuting the columns and rows of $\bm{\Omega}^{-1}$ so that the $l$-th column and row is the last,
\begin{equation*}
	\bm{\Omega}^{-1}=\begin{bmatrix}
		\bm{\Omega}_{-l,-l} & \bm\omega_{-l,l}\\
		\bm\omega_{l,-1} & \omega_{l,l}\\
	\end{bmatrix}^{-1}
	=\begin{bmatrix}
		\bm{\Omega}_{-l,-l}^{-1}+{\tau}^{-2}_{ll}\bm{f}\bm{f}^{\top} & -\bm{f}\tau^{-2}_{ll}\\
		-{\tau}^{-2}_{ll}\bm{f}^{\top} & {\tau}^{-2}_{ll}\\
	\end{bmatrix}
\end{equation*}
\noindent where $\bm{f}=\bm{\Omega}_{-l,-l}^{-1}\bm{\omega}_{-l,l}$, $\tau_{ll}^2= \omega_{ll}-\bm{\omega}_{l,-l}\bm{\Omega}^{-1}_{-l,-1}\bm{\omega}_{-l,l}$, $m_{il}=\bm{r}_{il}^{\top}\bm{\beta}+\bm{f}^{\top}(\bm{w}_{i,-l}-\bm{R}_{i,-l}\bm{\beta})$, $\bm{w}_{i,-l}$ is an $L-2$ dimensional vector of all components of $\bm{w}_i$ excluding $w_{il}$, $\bm{r}_{il}$ is the $l$-th row of $\bm{R}_i$, $l=1,2,\dots,L-1$. 

The identified parameters are obtained by normalizing with respect to one of the diagonal elements $\frac{1}{\omega_{1,1}^{0.5}}\bm{\beta}$ and $\frac{1}{\omega_{1,1}}\bm{\Omega}$.\footnote{Our GUI is based on the \textit{bayesm} package that takes into account this identification restriction to display the outcomes of the posterior chains.}\\

\textbf{Example: Choice of fishing mode}

Let's do the famous example of \textit{choice of fishing mode} from \cite{cameron05}.




\section{Summary}\label{sec611}
We present ...

\section{Exercises}\label{sec612}

\begin{enumerate}
	\item Get the posterior conditional distributions of the Gaussian linear model assuming independent priors $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$.
	
	\item Show that the posterior conditional distributions of the Gaussian linear model with heteroskedasticity assuming independent priors $\pi(\bm{\beta},\sigma^2,\tau)=\pi(\bm{\beta})\times\pi(\sigma^2)\times\prod_{i=1}^N\pi(\tau_i)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$, $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$ and $\tau_i\sim G(v/2,v/2)$ are $\bm{\beta}|\sigma^2,\tau,{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$, $\sigma^2|\bm{\beta},\tau,{\bf{y}},{\bf{X}}\sim IG(\alpha_n,\delta_n)$ and $\tau_i|\bm{\beta},\sigma^2,{\bf{y}},{\bf{X}}\sim G(v_{1n},v_{2in})$, where $\tau=[\tau_1,\dots,\tau_n]^{\top}$, ${\bf{B}}_n=({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}\bm{\Psi}{\bf{X}})^{-1}$, $\bm{\beta}_n={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}\bm{\Psi}{\bf{y}})$, $\alpha_n=\alpha_0+N$, $\delta_n=\delta_0+({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\bm{\Psi}({\bf{y}}-{\bf{X}}\bm{\beta})$, $v_{1n}=v+1$, $v_{2in}=v+\sigma^{-2}(y_i-{\bf{x}}_i^{\top}\bm{\beta})^2$, and $\bm{\Psi}=\text{diagonal}\left\{\tau_i\right\}$.
	
	\item \textbf{The market value of soccer players in Europe continues}
	
	Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the the market value of soccer players in Europe setting $v=5$ and same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case?
	
	\item \textbf{Example: Determinants of hospitalization continues}
	
	Program a Gibbs sampling algorithm in the application of determinants of hospitalization. 
	
\end{enumerate}
