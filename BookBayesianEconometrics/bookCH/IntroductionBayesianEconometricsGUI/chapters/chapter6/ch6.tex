\chapter{Univariate models}\label{chap6}

In this chapter, we describe how to perform Bayesian inference in several of the most common univariate models, including the normal, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap models for linear regression. Our point of departure is a random sample of cross-sectional units. We then present the posterior distributions of the parameters and illustrate their use through selected applications. In addition, we demonstrate how to perform inference in these models using three levels of programming engagement: through our graphical user interface (GUI), via \textbf{R} packages, and by directly programming the posterior distributions. The first requires no programming skills, the second requires an intermediate level, and the third demands advanced skills. We also provide mathematical and computational exercises.


We can run our GUI typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
		\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor. However, users should see Chapter \ref{chapGUI} for details.

\section{The Gaussian linear model}\label{sec61}

The Gaussian linear model is specified as ${\bm{y}} = {\bm{X}}\bm{\beta} + \bm{\mu}$, where $\bm{\mu} \sim N(\bm{0}, \sigma^2 \bm{I}_N)$ is a stochastic error term, ${\bm{X}}$ is an $N \times K$ matrix of regressors, $\bm{\beta}$ is a $K$-dimensional vector of location parameters, $\sigma^2$ is the variance of the model (scale parameter), ${\bm{y}}$ is an $N$-dimensional vector of the dependent variable, and $N$ is the sample size. In Section~\ref{sec43}, we describe this model under the conjugate prior family, that is, $\pi(\bm{\beta}, \sigma^2) = \pi(\bm{\beta} \mid \sigma^2) \times \pi(\sigma^2)$, which allows us to derive the marginal posterior distributions of $\bm{\beta}$ and $\sigma^2$.

In this section, we assume independent priors, that is, $\pi(\bm{\beta}, \sigma^2) = \pi(\bm{\beta}) \times \pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bm{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$, with $\alpha_0/2$ and $\delta_0/2$ denoting the shape and rate parameters, respectively. This specification allows us to obtain the conditional posterior distributions, $\pi(\bm{\beta} \mid \sigma^2, {\bm{y}}, {\bm{X}})$ and $\pi(\sigma^2 \mid \bm{\beta}, {\bm{y}}, {\bm{X}})$, which can be used to implement the Gibbs sampler for posterior inference on $\bm{\beta}$ and $\sigma^2$.

The likelihood function of this model is given by
\begin{align*}
	p({\bm{y}} \mid \bm{\beta}, \sigma^2, {\bm{X}}) 
	= (2\pi\sigma^2)^{-\frac{N}{2}} 
	\exp\left\{ -\frac{1}{2\sigma^2} ({\bm{y}} - {\bm{X}}\bm{\beta})^{\top} ({\bm{y}} - {\bm{X}}\bm{\beta}) \right\}.
\end{align*}

The conditional posterior distributions are
\begin{align*}
	\bm{\beta} \mid \sigma^2, {\bm{y}}, {\bm{X}} &\sim N(\bm{\beta}_n, {\bm{B}}_n), \\
	\sigma^2 \mid \bm{\beta}, {\bm{y}}, {\bm{X}} &\sim IG(\alpha_n/2, \delta_n/2),
\end{align*}
where 
${\bm{B}}_n = ({\bm{B}}_0^{-1} + \sigma^{-2} {\bm{X}}^{\top}{\bm{X}})^{-1}$, 
$\bm{\beta}_n = {\bm{B}}_n ({\bm{B}}_0^{-1}\bm{\beta}_0 + \sigma^{-2} {\bm{X}}^{\top}{\bm{y}})$, 
$\alpha_n = \alpha_0 + N$, and 
$\delta_n = \delta_0 + ({\bm{y}} - {\bm{X}}\bm{\beta})^{\top}({\bm{y}} - {\bm{X}}\bm{\beta})$ 
(see Exercise~1 in this chapter).\footnote{This model can be extended to account for heteroskedasticity such that $y_i \sim N({\bm{x}}_i^{\top}\bm{\beta}, \sigma^2/\tau_i)$, where $\tau_i \sim G(v/2, v/2)$. See Exercise~2 for details.}\\

\textbf{Example: The market value of soccer players in Europe}

Let's analyze the determinants of the market value of soccer players in Europe. In particular, we use the dataset \textit{1ValueFootballPlayers.csv}, located in the \textbf{DataApp} folder of our GitHub repository \textbf{\textbf{https://github.com/besmarter/BSTApp}}. This dataset was originally used by \cite{Serna2018} to identify the determinants of high-performance soccer players in the five major European national leagues.

The model specification is as follows:
\begin{align*}
	\log(\text{Value}_i) &= {\beta}_1 + {\beta}_2 \text{Perf}_i + {\beta}_3 \text{Age}_i + {\beta}_4 \text{Age}_i^2 + {\beta}_5 \text{NatTeam}_i \\
	&\quad + {\beta}_6 \text{Goals}_i + {\beta}_7 \text{Exp}_i + {\beta}_8 \text{Exp}_i^2 + \mu_i,
\end{align*}

where \textit{Value} is the market value in euros (2017), \textit{Perf} is a measure of player performance, \textit{Age} is the player's age in years, \textit{NatTeam} is an indicator variable that equals 1 if the player has been a member of the national team, \textit{Goals} is the total number of goals scored during the player's career, and \textit{Exp} represents playing experience in years.  

We assume that the dependent variable follows a normal distribution. Accordingly, we estimate a normal–inverse-gamma model using vague conjugate priors with ${\bm{B}}_0 = 1000{\bm{I}}_{8}$, $\bm{\beta}_0 = {\bm{0}}_{8}$, $\alpha_0 = 0.001$, and $\delta_0 = 0.001$. Posterior inference is performed using a Gibbs sampler with 5,000 MCMC iterations, a burn-in period of 5,000, and a thinning parameter of 1.

Once the GUI is launched (see the beginning of this chapter), users can follow Algorithm~\ref{alg:Gaussian} to run linear Gaussian models in the interface (see Chapter~\ref{chapGUI} for further details).

\begin{algorithm}[h!]
	\caption{Gaussian linear model}\label{alg:Gaussian}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Normal} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code examples illustrate how to estimate the linear Gaussian model using the \textit{MCMCregress} command from the \textit{MCMCpack} package, as well as how to implement the Gibbs sampler manually. We should obtain similar results across all three approaches —GUI, package, and custom function— since our GUI internally relies on the \textit{MCMCregress} command. For instance, the market value of a top soccer player in Europe increases by approximately 134\% ($\exp(0.85) - 1$) on average when he has played for the national team, with a 95\% credible interval of (86\%, 197\%).

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code1}
	\textit{R code. The value of soccer players, using \textbf{R} packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
# Linear regression: Value of soccer players #
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior))
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
bhat <- solve(XtX)%*%t(X)%*%y
an <- a0 + N
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Gibbs sampling functions
PostSig2 <- function(Beta){
	dn <- d0 + t(y - X%*%Beta)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*XtX%*%bhat)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2)
	PostBetas[s,] <- Beta
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The logit model}\label{sec62}

In the logit model, the dependent variable is binary, $y_i \in \{0,1\}$, and follows a Bernoulli distribution, $y_i \stackrel{ind}{\sim} B(\pi_i)$, where $p(y_i = 1) = \pi_i$ and 
\[
\pi_i = \frac{\exp({\bm{x}}_i^{\top}\bm{\beta})}{1 + \exp({\bm{x}}_i^{\top}\bm{\beta})}.
\]
Here, $\bm{x}_i$ is a $K$-dimensional vector of regressors.

The likelihood function of the logit model is given by
\begin{align*}
	p({\bm{y}} \mid \bm{\beta}, {\bm{X}}) 
	&= \prod_{i=1}^N \pi_i^{y_i} (1 - \pi_i)^{1 - y_i} \\
	&= \prod_{i=1}^N 
	\left(\frac{\exp({\bm{x}}_i^{\top}\bm{\beta})}{1 + \exp({\bm{x}}_i^{\top}\bm{\beta})}\right)^{y_i}
	\left(\frac{1}{1 + \exp({\bm{x}}_i^{\top}\bm{\beta})}\right)^{1 - y_i}.
\end{align*}

We specify a normal prior distribution for the coefficients, $\bm{\beta} \sim N({\bm{\beta}}_0, {\bm{B}}_0)$. The posterior distribution is therefore
\begin{align*}
	\pi(\bm{\beta} \mid {\bm{y}}, {\bm{X}}) &\propto 
	\prod_{i=1}^N 
	\left(\frac{\exp({\bm{x}}_i^{\top}\bm{\beta})}{1 + \exp({\bm{x}}_i^{\top}\bm{\beta})}\right)^{y_i}
	\left(\frac{1}{1 + \exp({\bm{x}}_i^{\top}\bm{\beta})}\right)^{1 - y_i} \\
	&\quad \times 
	\exp\left\{ -\frac{1}{2} (\bm{\beta} - \bm{\beta}_0)^{\top} 
	{\bm{B}}_0^{-1} (\bm{\beta} - \bm{\beta}_0) \right\}.
\end{align*}

Since the logit model does not yield a standard posterior distribution, a random-walk Metropolis–Hastings algorithm can be employed to obtain draws from the posterior. A suitable proposal distribution is a multivariate normal centered at the current draw, with covariance matrix 
$\tau^2({\bm{B}}_0^{-1} + \widehat{{\bm{\Sigma}}}^{-1})^{-1}$, 
where $\tau > 0$ is a tuning parameter and $\widehat{\bm{\Sigma}}$ is the sample covariance matrix obtained from the maximum likelihood estimates \cite{Martin2011}.\footnote{Tuning parameters should be chosen to ensure satisfactory convergence diagnostics and acceptance rates.}

The log-likelihood function can be expressed as
\[
\log(p({\bm{y}} \mid \bm{\beta}, {\bm{X}})) 
= \sum_{i=1}^N \left[y_i {\bm{x}}_i^{\top}\bm{\beta} - \log(1 + \exp({\bm{x}}_i^{\top}\bm{\beta}))\right].
\]
This expression is useful for computing the acceptance probability in the Metropolis–Hastings algorithm, given by
\begin{align*}
\alpha & = \min\left\{1, 
\exp\left(
\log(p({\bm{y}} \mid \bm{\beta}^{c}, {\bm{X}})) + \log(\pi(\bm{\beta}^c))\right.\right.\\
&\left.\left.
- \left[\log(p({\bm{y}} \mid \bm{\beta}^{(s-1)}, {\bm{X}})) + \log(\pi(\bm{\beta}^{(s-1)}))\right]
\right)\right\},
\end{align*}
where $\bm{\beta}^c$ and $\bm{\beta}^{(s-1)}$ denote the candidate draw from the proposal distribution and the previous iteration of the Markov chain, respectively.\footnote{Expressing the acceptance probability in logarithmic form helps mitigate numerical underflow and overflow issues during computation.}\\

\textbf{Example: Simulation exercise}

Let's conduct a simulation exercise to evaluate the performance of the algorithm. Set $\bm{\beta} = \begin{bmatrix} 0.5 & 0.8 & -1.2 \end{bmatrix}^{\top}$, with $x_{ik} \sim N(0,1)$ for $k = 2, 3$ and $i = 1, 2, \dots, 10{,}000$.

We specify the hyperparameters as $\bm{\beta}_0 = [0 \ 0 \ 0]^{\top}$ and ${\bm{B}}_0 = 1000{\bm{I}}_3$. The tuning parameter for the Metropolis–Hastings algorithm is set to 1.

Once the GUI is launched (see the beginning of this chapter), users should follow Algorithm~\ref{alg:Logit} to estimate logit models in the interface (see Chapter~\ref{chapGUI} for further details).

\begin{algorithm}[h!]
	\caption{Logit model}\label{alg:Logit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Logit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameter for the Metropolis-Hastings algorithm. This step is not necessary as by default our GUI sets the tuning parameter at 1.1
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code illustrates how to estimate the logit model using the \textit{MCMClogit} command from the \textit{MCMCpack} package, as well as by implementing the Metropolis–Hastings algorithm manually.

We should obtain similar results across the three approaches —GUI, package, and custom function— since our GUI internally relies on the \textit{MCMClogit} command. In this example, the acceptance rate is 0.46, and the diagnostics indicate that the posterior chains exhibit satisfactory convergence behavior. Overall, the 95\% credible intervals contain the true population values, and both the posterior means and medians are very close to those values.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the logit model estimation using \textbf{R} packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
############# Logit: Simulation ##########
# Simulate data
rm(list = ls()); set.seed(10101)
N <- 10000 # Sample size
B <- c(0.5, 0.8, -1.2) # Population location parameters
x2 <- rnorm(N) # Regressor
x3 <- rnorm(N) # Regressor
X <- cbind(1, x2, x3) # Regressors
XB <- X%*%B
PY <- exp(XB)/(1 + exp(XB)) # Probability of Y = 1
Y <- rbinom(N, 1, PY) # Draw Y's
table(Y) # Frequency
# write.csv(cbind(Y, x2, x3), file = "DataSimulations/LogitSim.csv") # Export data
# MCMC parameters
iter <- 5000; burnin <- 1000; thin <- 5; tune <- 1
# Hyperparameters
K <- dim(X)[2] 
b0 <- rep(0, K)
c0 <- 1000
B0 <- c0*diag(K)
B0i <- solve(B0)
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
RegLog <- MCMCpack::MCMClogit(Y~X-1, mcmc = iter, burnin = burnin, thin = thin, b0 = b0, B0 = B0i, tune = tune)
summary(RegLog)
# Metropolis-Hastings algorithm
MHfunc <- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 6000, burnin = 1000, thin = 5){
	Xm <- cbind(1, X) # Regressors
	K <- dim(Xm)[2] # Number of location parameters
	BETAS <- matrix(0, iter + burnin, K) # Space for posterior chains
	Reg <- glm(y ~ Xm - 1, family = binomial(link = "logit")) # Maximum likelihood estimation
	BETA <- Reg$coefficients # Maximum likelihood parameter estimates 
	tot <- iter + burnin # Total iterations M-H algorithm
	COV <- vcov(Reg) # Maximum likelihood covariance matrix
	COVt <- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution
	Accep <- rep(0, tot) # Space for calculating the acceptance rate
	pb <- txtProgressBar(min = 0, max = tot, style = 3)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the logit model estimation programming our M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]		
	for(it in 1:tot){
		BETAc <- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter
		likecand <- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood candidate
		likepast <- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log likelihood actual draw
		priorcand <- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior candidate
		priorpast <- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior actual draw
		alpha <- min(1, exp((likecand + priorcand) - (likepast + priorpast))) #Probability of selecting candidate
		u <- runif(1) # Decision rule for selecting candidate
		if(u < alpha){
			BETA <- BETAc # Changing reference for candidate
			Accep[it] <- 1 # Indicator candidate is accepted
		} 
		BETAS[it, ] <- BETA # Saving draws
		setTxtProgressBar(pb, it)
	}
	close(pb)
	keep <- seq(burnin, tot, thin)
	return(list(Bs = BETAS[keep[-1], ], AceptRate = mean(Accep[keep[-1]])))
}
Posterior <- MHfunc(y = Y, X = cbind(x2, x3), iter = iter, burnin = burnin, thin = thin) # Running our M-H function changing some default parameters.
paste("Acceptance rate equal to", round(Posterior$AceptRate, 2), sep = " ")
"Acceptance rate equal to 0.46"
PostPar <- coda::mcmc(Posterior$Bs)
# Names
colnames(PostPar) <- c("Cte", "x1", "x2")
# Summary posterior draws
summary(PostPar)
# Trace and density plots
plot(PostPar)
# Autocorrelation plots
coda::autocorr.plot(PostPar)
# Convergence diagnostics
coda::geweke.diag(PostPar)
coda::raftery.diag(PostPar,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(PostPar)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The probit model}\label{sec63}

The probit model also features a binary dependent variable. In this case, an unobserved latent variable ($y_i^*$) defines the underlying structure of the estimation problem.

Specifically,
\begin{equation*}
	y_i =
	\begin{Bmatrix}
		0, & y_i^* \leq 0 \\ 
		1, & y_i^* > 0 \\ 
	\end{Bmatrix},
\end{equation*}
such that $y_i^* = \bm{x}_i^{\top}\bm{\beta} + \mu_i$, where $\mu_i \stackrel{i.i.d.}{\sim} N(0,1)$.\footnote{The variance in this model is normalized to one due to identification restrictions. Note that $P(y_i = 1 \mid \bm{x}_i) = P(y_i^* > 0 \mid \bm{x}_i) = P(\bm{x}_i^{\top}\bm{\beta} + \mu_i > 0 \mid \bm{x}_i) = P(\mu_i > -\bm{x}_i^{\top}\bm{\beta} \mid \bm{x}_i) = P(c \times \mu_i > -c \times \bm{x}_i^{\top}\bm{\beta} \mid \bm{x}_i)$ for all $c > 0$. Multiplying by a positive constant does not affect the probability of $y_i = 1$.} 
This implies that $P(y_i = 1) = \pi_i = \Phi(\bm{x}_i^{\top}\bm{\beta})$, where $\bm{x}_i$ is a $K$-dimensional vector of regressors.

\cite{Albert1993} introduced a data augmentation approach \cite{Tanner1987} to implement a Gibbs sampling algorithm for this model. By augmenting the data with the latent variable $y_i^*$, the likelihood contribution from observation $i$ can be written as
\[
p(y_i \mid y_i^*) = \mathbbm{1}(y_i = 0)\mathbbm{1}(y_i^* \leq 0) + \mathbbm{1}(y_i = 1)\mathbbm{1}(y_i^* > 0),
\]
where $\mathbbm{1}(A)$ is an indicator function that equals 1 when the condition $A$ is satisfied.

The joint posterior distribution is given by
\begin{align*}
	\pi(\bm{\beta}, \bm{y}^* \mid \bm{y}, \bm{X}) 
	&\propto \prod_{i=1}^N 
	\left[\mathbbm{1}(y_i = 0)\mathbbm{1}(y_i^* \leq 0) + \mathbbm{1}(y_i = 1)\mathbbm{1}(y_i^* > 0)\right] \\
	&\quad \times N_N(\bm{y}^* \mid \bm{X}\bm{\beta}, \bm{I}_N) 
	\times N_K(\bm{\beta} \mid \bm{\beta}_0, \bm{B}_0),
\end{align*}
where we assume a Gaussian prior for $\bm{\beta}$: $\bm{\beta} \sim N_K(\bm{\beta}_0, \bm{B}_0)$.

The corresponding conditional posterior distributions are:
\begin{equation*}
	y_i^* \mid \bm{\beta}, \bm{y}, \bm{X} \sim
	\begin{Bmatrix}
		TN_{(-\infty, 0]}(\bm{x}_i^{\top}\bm{\beta}, 1), & y_i = 0, \\ 
		TN_{(0, \infty)}(\bm{x}_i^{\top}\bm{\beta}, 1), & y_i = 1,
	\end{Bmatrix}
\end{equation*}
\begin{equation*}
	\bm{\beta} \mid \bm{y}^*, \bm{X} \sim N(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{\top}\bm{X})^{-1}$ and 
$\bm{\beta}_n = \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}\bm{y}^*)$.\\

\textbf{Example: Determinants of hospitalization}

We use the dataset \textbf{2HealthMed.csv}, located in the \textbf{DataApp} folder of our GitHub repository \textbf{\textbf{https://github.com/besmarter/BSTApp}}, which was previously analyzed by \cite{Ramirez2013}. The dependent variable is a binary indicator that equals 1 if an individual was hospitalized in 2007 and 0 otherwise.

The model specification is as follows:
\begin{align*}
	\text{Hosp}_i &= {\beta}_1 + {\beta}_2 \text{SHI}_i + {\beta}_3 \text{Female}_i + {\beta}_4 \text{Age}_i + {\beta}_5 \text{Age}_i^2 + {\beta}_6 \text{Est2}_i + {\beta}_7 \text{Est3}_i \\
	&\quad + {\beta}_8 \text{Fair}_i + {\beta}_9 \text{Good}_i + {\beta}_{10} \text{Excellent}_i,
\end{align*}

where \textit{SHI} is a binary variable equal to 1 if the individual is enrolled in a subsidized health care program and 0 otherwise, \textit{Female} is a gender indicator, \textit{Age} is measured in years, \textit{Est2} and \textit{Est3} are indicators of socioeconomic status (with \textit{Est1} as the reference category representing the lowest status), and \textit{HealthStatus} is the individual’s self-perceived health status, where \textit{Bad} serves as the reference category.

We set $\bm{\beta}_0 = {\bm{0}}_{10}$ and ${\bm{B}}_0 = {\bm{I}}_{10}$, with 10,000 iterations, a burn-in period of 1,000, and a thinning parameter of 1. Algorithm~\ref{alg:Probit} can be used to estimate the probit model within our GUI. The GUI internally relies on the \textit{rbprobitGibbs} command from the \textit{bayesm} package to perform Bayesian inference for the probit model. The following \textbf{R} code illustrates how to run this example using the \textit{rbprobitGibbs} command. In the exercises, you are also asked to implement a Gibbs sampling algorithm to perform inference in the probit model manually.

\begin{algorithm}[h!]
	\caption{Probit model}\label{alg:Probit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}


Our analysis finds evidence that gender and self-perceived health status significantly affect the probability of hospitalization. Women have a higher probability of being hospitalized than men, and individuals with a better perception of their health status have a lower probability of hospitalization.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization}
	\begin{VF}
		\begin{lstlisting}[language=R]			
mydata <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv", sep = ",", header = TRUE, quote = "")
attach(mydata); str(mydata)
K <- 10 ; b0 <- rep(0, K) # Regressors and prior mean
B0i <- diag(K) # Prior precision (inverse of covariance)
Prior <- list(betabar = b0, A = B0i) # Prior list
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
Data <- list(y = y, X = X) # Data list
Mcmc <- list(R = 10000, keep = 1, nprint = 0) # MCMC
RegProb <- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package
PostPar <- coda::mcmc(RegProb$betadraw) # Posterior draws
colnames(PostPar) <- c("Cte", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent") # Names
summary(PostPar) # Posterior summary\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The multinomial probit model}\label{sec64}

The multinomial probit model is used to explain the choice of the $l$-th alternative from a set of $L$ mutually exclusive options. We observe
\begin{equation*}
	y_{il} =
	\begin{cases}
		1, & \text{if } y_{il}^* \geq \max\{\bm{y}_i^*\}, \\
		0, & \text{otherwise,}
	\end{cases}
\end{equation*}
where $\bm{y}_i^* = \bm{X}_{i}\bm{\delta} + \bm{\mu}_i$, and $\bm{\mu}_i \stackrel{i.i.d.}{\sim} N(\bm{0}, \bm{\Sigma})$. The vector $\bm{y}_i^*$ is an unobserved latent vector of dimension $L$. The matrix $\bm{X}_i = \left[(1 \ \bm{c}_i^{\top}) \otimes \bm{I}_L \ \bm{A}_i\right]$ is an $L \times j$ matrix of regressors for each alternative, where $l = 1, 2, \dots, L$, and $j = L \times (1 + \text{dim}(\bm{c}_i)) + a$. Here, $\bm{c}_i$ is a vector of individual-specific characteristics, $\bm{A}_i$ is an $L \times a$ matrix of alternative-varying regressors, $a$ denotes the number of alternative-varying regressors, and $\bm{\delta}$ is a $j$-dimensional vector of parameters.

The model jointly accounts for alternative-varying regressors (alternative attributes) and alternative-invariant regressors (individual characteristics).\footnote{This model is not identified if $\bm{\Sigma}$ is unrestricted. The likelihood function remains unchanged if a scalar random variable is added to each of the $L$ latent regressions.} The vector $\bm{y}_i^*$ can be stacked into a multivariate regression model with correlated stochastic errors, i.e., $\bm{y}^* = \bm{X}\bm{\delta} + \bm{\mu}$, where $\bm{y}^* = [\bm{y}_1^{*\top}, \bm{y}_2^{*\top}, \dots, \bm{y}_N^{*\top}]^{\top}$, $\bm{X} = [\bm{X}_1^{\top}, \bm{X}_2^{\top}, \dots, \bm{X}_N^{\top}]^{\top}$, and $\bm{\mu} = [\bm{\mu}_1^{\top}, \bm{\mu}_2^{\top}, \dots, \bm{\mu}_N^{\top}]^{\top}$.

Following standard practice, we express $y_{il}^*$ relative to $y_{iL}^*$ by defining $\bm{w}_i = [w_{i1}, w_{i2}, \dots, w_{iL-1}]^{\top}$, where $w_{il} = y_{il}^* - y_{iL}^*$. Then, $\bm{w}_i = \bm{R}_i\bm{\beta} + \bm{\epsilon}_i$, with $\bm{\epsilon}_i \sim N(\bm{0}, \bm{\Omega})$, where $\bm{R}_i = \left[(1 \ \bm{c}_i^{\top}) \otimes \bm{I}_{L-1} \ \bm{\Delta A}_i\right]$ is an $(L-1) \times K$ matrix, $\Delta\bm{A}_i = \bm{A}_{li} - \bm{A}_{Li}$ for $l = 1, 2, \dots, L-1$, and $\bm{\beta}$ is a $K$-dimensional parameter vector, with $K = (L-1) \times (1 + \text{dim}(\bm{c}_i)) + a$. That is, the last row of $\bm{A}_i$ is subtracted from each row to normalize the model.

The vector $\bm{\beta}$ contains the same last $a$ elements as $\bm{\delta}$, corresponding to the coefficients on the alternative-specific attributes. However, the first $(L-1) \times (1 + \text{dim}(\bm{c}_i))$ elements of $\bm{\beta}$ are differences of the form $\delta_{jl} - \delta_{jL}$, for $j = 1, \dots, \text{dim}(\bm{c}_i)$ and $l = 1, \dots, L-1$. These represent the differences between the coefficients of each qualitative response and those of the $L$-th (base) alternative for the individual characteristics, making the interpretation of the multinomial probit coefficients less straightforward.

In multinomial models, only one coefficient per alternative-specific attribute needs to be estimated across alternatives. However, for individual characteristics (non–alternative-specific regressors), $L - 1$ coefficients must be estimated, since the coefficient for the base alternative is set to zero for identification.

The likelihood function of this model is
\[
p(\bm{\beta}, \bm{\Omega} \mid \bm{y}, \bm{R}) = \prod_{i=1}^N \prod_{l=1}^L p_{il}^{y_{il}},
\]
where $p_{il} = P(y_{il}^* \geq \max(\bm{y}_i^*))$.

We assume independent priors for the parameters:
\[
\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0) 
\quad \text{and} \quad 
\bm{\Omega}^{-1} \sim W(\alpha_0, \bm{\Sigma}_0),\footnote{$W$ denotes the Wishart distribution.}
\]
and employ Gibbs sampling to estimate the model. With data augmentation for $\bm{w}$, this model becomes a standard Bayesian linear regression. The conditional posterior distributions are
\begin{align*}
	\bm{\beta} \mid \bm{\Omega}, \bm{w} &\sim N(\bm{\beta}_n, \bm{B}_n), \\
	\bm{\Omega}^{-1} \mid \bm{\beta}, \bm{w} &\sim W(\alpha_n, \bm{\Sigma}_n),
\end{align*}
where 
$\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{*\top}\bm{X}^*)^{-1}$, 
$\bm{\beta}_n = \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{*\top}\bm{w}^*)$, 
$\bm{\Omega}^{-1} = \bm{C}^{\top}\bm{C}$, 
$\bm{X}_i^{*\top} = \bm{C}^{\top}\bm{R}_i$, 
$\bm{w}_i^* = \bm{C}^{\top}\bm{w}_i$, 
$\bm{X}^* = [\bm{X}_1^*, \bm{X}_2^*, \dots, \bm{X}_N^*]^{\top}$,
$\alpha_n = \alpha_0 + N$, and 
$\bm{\Sigma}_n = (\bm{\Sigma}_0 + \sum_{i=1}^N (\bm{w}_i - \bm{R}_i\bm{\beta})^{\top}(\bm{w}_i - \bm{R}_i\bm{\beta}))^{-1}$.

We can collapse the multinomial outcome vector $\bm{y}_i$ into an indicator variable $d_i = \sum_{l=1}^{L-1} l \times \mathbbm{1}(\max(\bm{w}_l) = w_{il})$.\footnote{The identification issue arises because scaling $w_{il}$ by a positive constant does not alter the value of $d_i$.} Then, the conditional distribution of $\bm{w}_i \mid \bm{\beta}, \bm{\Omega}^{-1}, d_i$ is an $(L-1)$-dimensional Gaussian truncated over the appropriate cone in $\mathbb{R}^{L-1}$.

\cite{McCulloch1994} propose sampling from the univariate conditional distributions $w_{il} \mid \bm{w}_{i,-l}, \bm{\beta}, \bm{\Omega}^{-1}, d_i \sim TN_{I_{il}}(m_{il}, \tau_{ll}^2)$, where
\begin{equation*}
	I_{il} =
	\begin{cases}
		w_{il} > \max(\bm{w}_{i,-l}, 0), & d_i = l, \\
		w_{il} < \max(\bm{w}_{i,-l}, 0), & d_i \neq l,
	\end{cases}
\end{equation*}
and permuting the columns and rows of $\bm{\Omega}^{-1}$ so that the $l$-th column and row appear last:
\begin{equation*}
	\bm{\Omega}^{-1} =
	\begin{bmatrix}
		\bm{\Omega}_{-l,-l} & \bm{\omega}_{-l,l} \\
		\bm{\omega}_{l,-l} & \omega_{l,l}
	\end{bmatrix}^{-1}
	=
	\begin{bmatrix}
		\bm{\Omega}_{-l,-l}^{-1} + \tau_{ll}^{-2}\bm{f}_l\bm{f}_l^{\top} & -\bm{f}_l\tau_{ll}^{-2} \\
		-\tau_{ll}^{-2}\bm{f}_l^{\top} & \tau_{ll}^{-2}
	\end{bmatrix},
\end{equation*}
where $\bm{f}_l = \bm{\Omega}_{-l,-l}^{-1}\bm{\omega}_{-l,l}$, $\tau_{ll}^2 = \omega_{ll} - \bm{\omega}_{l,-l}\bm{\Omega}_{-l,-l}^{-1}\bm{\omega}_{-l,l}$, and $m_{il} = \bm{r}_{il}^{\top}\bm{\beta} + \bm{f}_l^{\top}(\bm{w}_{i,-l} - \bm{R}_{i,-l}\bm{\beta})$. Here, $\bm{w}_{i,-l}$ is an $(L-2)$-dimensional vector excluding $w_{il}$, and $\bm{r}_{il}$ is the $l$-th row of $\bm{R}_i$, for $l = 1, 2, \dots, L-1$.

The identified parameters are obtained by normalizing with respect to one of the diagonal elements, such that $\bm{\beta}/\omega_{11}^{0.5}$ and $\bm{\Omega}/\omega_{11}$.\footnote{Our GUI is based on the \textit{bayesm} package, which applies this identification restriction when displaying the posterior outcomes.}

\textbf{Warning:} This model illustrates the importance of deciding whether to specify the model in an identified or unidentified parameter space. The posterior draws often exhibit better mixing properties in the unidentified space \cite{mcculloch2000bayesian}, reducing computational burden. However, it is crucial to recover the identified space in the final stage. Moreover, defining priors in the unidentified space may have unintended effects on the posterior distributions in the identified space \cite{nobile2000comment}. The multinomial probit model presented in this section is specified in the unidentified space following \cite{McCulloch1994}, whereas a version formulated in the identified space is proposed by \cite{mcculloch2000bayesian}.\\

\textbf{Example: Choice of fishing mode}

In this application, we use the dataset \textit{3Fishing.csv} from \cite[p.~491]{cameron05}. The dependent variable represents mutually exclusive fishing mode choices (\textit{mode}), where beach equals 1, pier equals 2, private boat equals 3, and chartered boat (the baseline alternative) equals 4. The model is specified as
{\small{
		\begin{align*}
			\bm{X}_i & = \begin{bmatrix}
				1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & 0 & \text{Price}_{i,1} & \text{Catch rate}_{i,1}\\ 
				0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & \text{Price}_{i,2} & \text{Catch rate}_{i,2}\\
				0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & \text{Price}_{i,3} & \text{Catch rate}_{i,3}\\
				0 & 0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & \text{Price}_{i,4} & \text{Catch rate}_{i,4}\\
			\end{bmatrix}.
		\end{align*}
}}

In this example, the chartered boat is the base category, and the number of choice alternatives is four. There are two alternative-specific regressors (price and catch rate) and one non–alternative-specific regressor (income). This specification involves the estimation of eight location parameters ($\bm{\beta}$): three intercepts, three for income, one for price, and one for catch rate. This is the order in which the posterior chains are displayed in our GUI. Note that the location coefficients are set to zero for the baseline category. For multinomial models, we strongly recommend using the last category as the baseline.

Posterior inference also provides estimates of a $3 \times 3$ covariance matrix (number of alternatives minus one). The element $(1,1)$ is normalized to 1 due to identification restrictions, and elements (2,4), (3,7), and (6,8) are equal due to symmetry.\footnote{This order corresponds to the layout in the \textit{pdf}, \textit{eps}, and \textit{csv} files that can be downloaded from our GUI.} Note that this identification restriction leads to \textit{NaN} values for the element $(1,1)$ of the covariance matrix in the \cite{Geweke1992} and \cite{Heidelberger1983} diagnostics, and only eight dependence factors are reported for the remaining covariance elements.

Once the GUI is launched (see the beginning of this chapter), users should follow Algorithm~\ref{alg:MultinomialProbit} to estimate multinomial probit models in the interface (see Chapter~\ref{chapGUI} for additional details). The GUI internally relies on the \textit{rmnpGibbs} command from the \textit{bayesm} package.

\begin{algorithm}[h!]
	\caption{Multinomial probit models}\label{alg:MultinomialProbit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Multinomial Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Select the number of the \textbf{Base Alternative}
		\State Select the \textbf{Number of choice categorical alternatives}
		\State Select the \textbf{Number of alternative specific variables}
		\State Select the \textbf{Number of Non-alternative specific variables} 
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax.
		\State Set the hyperparameters: mean vector, covariance matrix, scale matrix and degrees of freedom. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}


We ran 100,000 MCMC iterations with an additional 10,000 as burn-in and a thinning parameter of 5, using the default hyperparameter values for all priors in our GUI. The 95\% credible intervals for the coefficients associated with income in the beach and private boat alternatives are (8.58e-06, 8.88e-05) and (3.36e-05, 1.45e-04), respectively. These results suggest that the probability of choosing these alternatives increases relative to the chartered boat option as income rises. Furthermore, an increase in price or a decrease in the catch rate for specific fishing alternatives leads to lower probabilities of selecting those options, as indicated by the corresponding 95\% credible intervals of (-9.91e-03, -3.83e-03) and (1.40e-01, 4.62e-01), respectively. However, diagnostic checks of the posterior chains indicate some convergence issues with the posterior draws (see Exercise~5).

\section{The multinomial logit model}\label{sec65}

The multinomial logit model is used to analyze mutually exclusive discrete outcomes or qualitative response variables. However, this model assumes the independence of irrelevant alternatives (IIA), meaning that the relative odds of choosing between two alternatives are unaffected by the presence or characteristics of a third option; this assumption is not required in the multinomial probit. We consider the multinomial mixed logit model (not to be confused with the random-parameters logit model), which simultaneously accommodates both alternative-varying regressors (conditional) and alternative-invariant regressors (multinomial).\footnote{The multinomial mixed logit model can be implemented as a conditional logit model.}

In this setting, there are $L$ mutually exclusive alternatives, and the dependent variable $y_{il}$ equals 1 if individual $i$ chooses the $l$-th alternative and 0 otherwise, where $l = 1, 2, \dots, L$. The likelihood function is
\[
p(\bm{\beta} \mid \bm{y}, \bm{X}) = \prod_{i=1}^{N} \prod_{l=1}^{L} p_{il}^{y_{il}},
\]
where the probability that individual $i$ chooses alternative $l$ is given by
\[
p_{il} = p(y_i = l \mid \bm{\beta}, \bm{X}) = 
\frac{\exp\left\{\bm{x}_{il}^{\top} \bm{\beta}_l\right\}}{\sum_{j=1}^{L} \exp\left\{\bm{x}_{ij}^{\top} \bm{\beta}_j\right\}}.
\]
Here, $\bm{y}$ and $\bm{X}$ denote the vector and matrix of dependent and explanatory variables, respectively, and $\bm{\beta}$ is the vector containing all model coefficients.

Note that coefficients associated with alternative-invariant regressors are normalized to zero for the baseline category, while the coefficients on alternative-specific regressors are constrained to be identical across all categories. We assume a prior distribution $\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0)$. The posterior distribution is then
\[
\pi(\bm{\beta} \mid \bm{y}, \bm{X}) \propto p(\bm{\beta} \mid \bm{y}, \bm{X}) \times \pi(\bm{\beta}).
\]

Because the multinomial logit model does not yield a standard posterior distribution, \cite{rossi2012bayesian} propose a tailored independent Metropolis--Hastings algorithm in which the proposal distribution follows a multivariate Student-$t$ distribution with $v$ degrees of freedom (a tuning parameter), mean equal to the maximum likelihood estimate, and scale matrix equal to the inverse of the observed Hessian.\\

\textbf{Example: Simulation exercise}

Let's conduct a simulation exercise to evaluate the performance of the Metropolis–Hastings algorithm for inference in the multinomial logit model. We consider a setting with three alternatives, one alternative-invariant regressor (in addition to the intercept), and three alternative-varying regressors. The population parameters are specified as $\bm{\beta}_1 = [1 \ -2.5 \ 0.5 \ 0.8 \ -3]^{\top}$, $\bm{\beta}_2 = [1 \ -3.5 \ 0.5 \ 0.8 \ -3]^{\top}$, and $\bm{\beta}_3 = [0 \ 0 \ 0.5 \ 0.8 \ -3]^{\top}$, where the first two elements of each vector correspond to the intercept and the alternative-invariant regressor, while the last three elements correspond to the alternative-varying regressors. The sample size is 1,000, and all regressors are simulated from standard normal distributions.

We can launch our GUI using the command line introduced at the beginning of this chapter. To estimate the multinomial logit model, we follow Algorithm~\ref{alg:MultinomialLogit} in our GUI (see Chapter~\ref{chapGUI} for further details).

\begin{algorithm}[h!]
	\caption{Multinomial logit models}\label{alg:MultinomialLogit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Multinomial Logit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Select the \textbf{Base Alternative}
		\State Select the \textbf{Number of choice categorical alternatives}
		\State Select the \textbf{Number of alternative specific variables}
		\State Select the \textbf{Number of Non-alternative specific variables} 
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax.
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameter for the Metropolis-Hastings algorithm, that is, the \textbf{Degrees of freedom: Multivariate Student's t distribution} 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code demonstrates how to implement the Metropolis–Hastings (M–H) algorithm from scratch. The first part simulates the dataset, the second defines the log-likelihood function, and the third implements the M–H sampling procedure. We use vague priors centered at zero, with a covariance matrix of $1000\bm{I}_7$. The posterior estimates closely approximate the true population parameters, and all 95\% credible intervals include the corresponding population values.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]		
remove(list = ls()); set.seed(12345)
# Simulation of data
N<-1000  # Sample Size
B<-c(0.5,0.8,-3); B1<-c(-2.5,-3.5,0); B2<-c(1,1,0)
# Alternative specific attributes of choice 1
X1<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) 
# Alternative specific attributes of choice 2
X2<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
# Alternative specific attributes of choice 3
X3<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
X4<-matrix(rnorm(N,1,1),N,1)
V1<-B2[1]+X1%*%B+B1[1]*X4; V2<-B2[2]+X2%*%B+B1[2]*X4; V3<-B2[3]+X3%*%B+B1[3]*X4\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
suma<-exp(V1)+exp(V2)+exp(V3)
p1<-exp(V1)/suma; p2<-exp(V2)/suma; p3<-exp(V3)/suma
p<-cbind(p1,p2,p3)
y<- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE))
y1<-y==1; y2<-y==2; y3<-y==3		
# Log likelihood
log.L<- function(Beta){
	V1<-Beta[1]+Beta[3]*X4+X1%*%Beta[5:7]
	V2<-Beta[2]+Beta[4]*X4+X2%*%Beta[5:7]
	V3<- X3%*%Beta[5:7]
	suma<-exp(V1)+exp(V2)+exp(V3)
	p11<-exp(V1)/suma; 	p22<-exp(V2)/suma; 	p33<-exp(V3)/suma
	suma2<-NULL
	for(i in 1:N){
		suma1<-y1[i]*log(p11[i])+y2[i]*log(p22[i])+y3[i]*log(p33[i])
		suma2<-c(suma2,suma1)}
	logL<-sum(suma2)
	return(-logL)
}
# Parameters: Proposal
k <- 7
res.optim<-optim(rep(0, k), log.L, method="BFGS", hessian=TRUE)
MeanT <- res.optim$par
ScaleT <- as.matrix(Matrix::forceSymmetric(solve(res.optim$hessian))) # Force this matrix to be symmetric
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
MHfunction <- function(iter, tuning){
	Beta <- rep(0, k); Acept <- NULL 
	BetasPost <- matrix(NA, iter, k)
	pb <- txtProgressBar(min = 0, max = iter, style = 3)
	for(s in 1:iter){
		LogPostBeta <- -log.L(Beta) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(LaplacesDemon::rmvt(n=1, mu = MeanT, S = ScaleT, df = tuning))
		LogPostBetaC <- -log.L(BetaC) + mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
		alpha <- min(exp((LogPostBetaC-mvtnorm::dmvt(BetaC, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))-(LogPostBeta-mvtnorm::dmvt(Beta, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))) ,1)
		u <- runif(1)
		if(u <= alpha){
			Acepti <- 1; Beta <- BetaC
		}else{
			Acepti <- 0; Beta <- Beta
		}
		BetasPost[s, ] <- Beta; Acept <- c(Acept, Acepti)
		setTxtProgressBar(pb, s)
	}
	close(pb); AcepRate <- mean(Acept)
	Results <- list(AcepRate = AcepRate, BetasPost = BetasPost)
	return(Results)
}		
# MCMC parameters
mcmc <- 10000; burnin <- 1000; thin <- 5; iter <- mcmc + burnin; keep <- seq(burnin, iter, thin); tuning <- 6 # Degrees of freedom
ResultsPost <- MHfunction(iter = iter, tuning = tuning)
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Ordered probit model}\label{sec66}

The ordered probit model is used when the categorical response variable has a natural ordering. In this case, there exists an unobserved latent variable $y_i^* = \bm{x}_i^{\top}\bm{\beta} + \mu_i$, where $\bm{x}_i$ is a $K$-dimensional vector of regressors and $\mu_i \stackrel{i.i.d.}{\sim} N(0,1)$. The observed outcome is defined as $y_i = l$ if and only if $\alpha_{l-1} < y_i^* \leq \alpha_l$, for $l \in \{1, 2, \dots, L\}$, where $\alpha_0 = -\infty$, $\alpha_1 = 0$, and $\alpha_L = \infty$.\footnote{Identification requires fixing the variance of the latent error term to 1 and setting $\alpha_1 = 0$. Note that multiplying $y_i^*$ by a positive constant, or adding a constant to all the cut-offs and subtracting the same constant from the intercept, does not affect the observed outcome $y_i$.} 

The probability of observing category $l$ is given by
\[
p(y_i = l) = \Phi(\alpha_l - \bm{x}_i^{\top}\bm{\beta}) - \Phi(\alpha_{l-1} - \bm{x}_i^{\top}\bm{\beta}),
\]
and the likelihood function is
\[
p(\bm{\beta}, \bm{\alpha} \mid \bm{y}, \bm{X}) = \prod_{i=1}^{N} p(y_i = l \mid \bm{\beta}, \bm{\alpha}, \bm{X}).
\]

The priors are assumed to be independent, i.e., $\pi(\bm{\beta}, \bm{\gamma}) = \pi(\bm{\beta}) \times \pi(\bm{\gamma})$, where $\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0)$ and $\bm{\gamma} \sim N(\bm{\gamma}_0, \bm{\Gamma}_0)$, with $\bm{\gamma} = [\gamma_2 \ \gamma_3 \ \dots \ \gamma_{L-1}]^{\top}$ such that
\[
\bm{\alpha} = 
\begin{bmatrix}
	\exp\{\gamma_2\} \\ 
	\sum_{l=2}^{3} \exp\{\gamma_l\} \\ 
	\vdots \\ 
	\sum_{l=2}^{L-1} \exp\{\gamma_l\}
\end{bmatrix}.
\]
This parameterization enforces the ordering constraint on the cut-offs.

Although the model does not yield a standard conditional posterior distribution for $\bm{\gamma}$ (and therefore for $\bm{\alpha}$), it admits a standard conditional posterior for $\bm{\beta}$ once data augmentation is introduced. Thus, inference can be performed using a Metropolis-within-Gibbs sampling algorithm. In particular, we use Gibbs sampling to draw $\bm{\beta}$ and $\bm{y}^*$, where
\[
\bm{\beta} \mid \bm{y}^*, \bm{\alpha}, \bm{X} \sim N(\bm{\beta}_n, \bm{B}_n),
\]
with $\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{\top}\bm{X})^{-1}$ and $\bm{\beta}_n = \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}\bm{y}^*)$, and
\[
y_i^* \mid \bm{\beta}, \bm{\alpha}, \bm{y}, \bm{X} \sim TN_{(\alpha_{y_i-1}, \alpha_{y_i})}(\bm{x}_i^{\top}\bm{\beta}, 1).
\]

For $\bm{\gamma}$, we employ a random-walk Metropolis--Hastings algorithm with a Gaussian proposal distribution centered at the current value and covariance matrix $s^2(\bm{\Gamma}_0^{-1} + \hat{\bm{\Sigma}}_{\gamma}^{-1})^{-1}$, where $s > 0$ is a tuning parameter and $\hat{\bm{\Sigma}}_{\gamma}$ denotes the sample covariance matrix of $\bm{\gamma}$ obtained from the maximum likelihood estimation.\\

\textbf{Example: Determinants of preventive health care visits}

We use the file \textit{2HealthMed.csv} for this application. The dependent variable is \textit{MedVisPrevOr}, an ordered variable equal to 1 if the individual did not visit a physician for preventive reasons, 2 if the individual visited once during the year, and so on, up to 6 if the individual visited five or more times. The latter category represents 1.6\% of the sample. Thus, the dependent variable has six ordered categories.

The set of regressors includes \textit{SHI}, an indicator variable equal to 1 if the individual is enrolled in the subsidized health care system; \textit{Female}, a gender indicator; \textit{Age} and its square; socioeconomic status indicators (\textit{Est2} and \textit{Est3}), with the lowest level as the reference category; self-perceived health status (\textit{Fair}, \textit{Good}, and \textit{Excellent}), where \textit{Bad} serves as the baseline; and education level (\textit{PriEd}, \textit{HighEd}, \textit{VocEd}, \textit{UnivEd}), with \textit{No education} as the reference category.

We ran this application with 50,000 MCMC iterations, an additional 10,000 as burn-in, and a thinning parameter of 5, resulting in 10,000 effective posterior draws. The prior parameters were set as $\bm{\beta}_0 = \bm{0}_{11}$, $\bm{B}_0 = 1000\bm{I}_{11}$, $\bm{\gamma}_0 = \bm{0}_4$, $\bm{\Gamma}_0 = \bm{I}_4$, and the tuning parameter was set to 1.

The ordered probit model can be estimated in our GUI by following the steps outlined in Algorithm~\ref{alg:OrderedProbit}.
 
\begin{algorithm}[h!]
	\caption{Ordered probit models}\label{alg:OrderedProbit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Ordered Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. Remember that this formula must have -1 to omit the intercept in the specification.
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the number of ordered alternatives
		\State Set the hyperparameters: mean and covariance matrix of the cutoffs. This step is not necessary as by default our GUI uses non-informative prior 
		\State Select the tuning parameter for the Metropolis-Hastings algorithm 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code shows how to perform inference in this model using the command \textit{rordprobitGibbs} from the \textit{bayesm} library, which is the command that our GUI uses.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of preventive health care visits}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/2HealthMed.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- MedVisPrevOr 
# MedVisPrevOr: Oredered variable for preventive visits to doctors in one year: 1 (none), 2 (once), ... 6 (five or more)
X <- cbind(SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent, PriEd, HighEd, VocEd, UnivEd)
k <- dim(X)[2]
L <- length(table(y))
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
gamma0 <- rep(0, L-2); Gamma0 <- diag(L-2)
# MCMC parameters
mcmc <- 60000+1; thin <- 5; tuningPar <- 1/(L-2)^0.5
DataApp <- list(y = y, X = X, k = L)
Prior <- list(betabar = b0, A = solve(B0), dstarbar = gamma0, Ad = Gamma0)
mcmcpar <- list(R = mcmc, keep = 5, s = tuningPar)
PostBeta <- bayesm::rordprobitGibbs(Data = DataApp, Prior = Prior, Mcmc = mcmcpar)
BetasPost <- coda::mcmc(PostBeta[["betadraw"]])
colnames(BetasPost) <- c("SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent", "PriEd", "HighEd", "VocEd", "UnivEd")
summary(BetasPost)		
# Convergence diagnostics
coda::geweke.diag(BetasPost)
coda::raftery.diag(BetasPost,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(BetasPost)
# Cut offs
Cutoffs <- PostBeta[["cutdraw"]]
summary(Cutoffs)
coda::geweke.diag(Cutoffs)
coda::heidel.diag(Cutoffs)
coda::raftery.diag(Cutoffs[,-1],q=0.5,r=0.05,s = 0.95)\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

The results suggest that older individuals (at a decreasing rate), those enrolled in the subsidized health care program, belonging to the second socioeconomic stratum, with a better self-perception of health, and without a high school degree as their highest level of education, have a higher probability of visiting a physician for preventive health purposes. Convergence diagnostics appear satisfactory for most parameters, except for the draws associated with self-perceived health status.

We also obtained the posterior estimates of the cut-offs in the ordered probit model. These estimates are necessary to compute the probability that an individual falls into a specific category of physician visits. Due to identification restrictions, the first cut-off is fixed at zero. Consequently, \textit{NaN} values appear in the \cite{Geweke1992} and \cite{Heidelberger1983} diagnostics, and only four values are reported in the \cite{Raftery1992} test, corresponding to the remaining unrestricted cut-offs. However, the \cite{Raftery1992} diagnostics indicate potential convergence issues for these parameters, as their dependence factors are relatively high.

\section{Negative binomial model}\label{sec67}

The dependent variable in the negative binomial model is a nonnegative integer (count). Unlike the Poisson model, the negative binomial model accounts for over-dispersion, whereas the Poisson model assumes equi-dispersion—that is, the mean and variance are equal.

We assume that $y_i \stackrel{i.i.d.}{\sim} \text{NB}(\gamma, \theta_i)$, where the probability mass function for individual $i$ is 
\[
\frac{\Gamma(y_i + \gamma)}{\Gamma(\gamma) y_i!} (1 - \theta_i)^{y_i} \theta_i^{\gamma},
\]
with success probability $\theta_i = \frac{\gamma}{\lambda_i + \gamma}$, where $\lambda_i = \exp\{\bm{x}_i^{\top}\bm{\beta}\}$ is the conditional mean, $\gamma = \exp\{\alpha\}$ is the target number of successful trials (or dispersion parameter), and $\bm{x}_i$ is a $K$-dimensional vector of regressors.

We assume independent priors for the model parameters: $\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0)$ and $\alpha \sim G(\alpha_0, \delta_0)$.

Since this model does not yield standard conditional posterior distributions, \cite{rossi2012bayesian} propose using a random-walk Metropolis–Hastings algorithm. The proposal distribution for $\bm{\beta}$ is Gaussian, centered at the current draw, with covariance matrix $s_{\bm{\beta}}^2 \hat{\bm{\Sigma}}_{\bm{\beta}}$, where $s_{\bm{\beta}}$ is a tuning parameter and $\hat{\bm{\Sigma}}_{\bm{\beta}}$ is the covariance matrix from the maximum likelihood estimation. Similarly, the proposal distribution for $\alpha$ is normal, centered at the current value, with variance $s_{\alpha}^2 \hat{\sigma}_{\alpha}^2$, where $s_{\alpha}$ is a tuning parameter and $\hat{\sigma}_{\alpha}^2$ is the variance from the maximum likelihood estimation.\\

\textbf{Example: Simulation exercise}

Let's conduct a simulation exercise to evaluate the performance of the Metropolis–Hastings algorithm in the negative binomial model. There are two regressors, $x_{i1} \sim U(0,1)$ and $x_{i2} \sim N(0,1)$, along with an intercept. The dispersion parameter is set to $\gamma = \exp\{1.2\}$, and the coefficient vector is $\bm{\beta} = [1 \ 1 \ 1]^{\top}$. The sample size is 1,000.

We run this simulation using 10,000 MCMC iterations, a burn-in period of 1,000, and a thinning parameter of 5. Vague priors are assigned to the location parameters: $\bm{\beta}_0 = \bm{0}_{3}$, $\bm{B}_0 = 1000\bm{I}_{3}$, $\alpha_0 = 0.5$, and $\delta_0 = 0.1$, which are the default values in the \textit{rnegbinRw} command from the \textit{bayesm} package in \textbf{R}. In addition, the tuning parameters for the Metropolis–Hastings algorithm are $s_{\beta} = 2.93 / k^{1/2}$ and $s_{\alpha} = 2.93$, which are also the default values in \textit{rnegbinRw}, where $k$ denotes the number of location parameters.

The negative binomial model can be estimated in our GUI by following the steps outlined in Algorithm~\ref{alg:NegativeBinomial}.

\begin{algorithm}[h!]
	\caption{Negative binomial models}\label{alg:NegativeBinomial}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Negative Binomial (Poisson)} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameters for the Metropolis-Hastings algorithms 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code demonstrates how to perform inference in the negative binomial model by programming the Metropolis–Hastings algorithm from scratch. In Exercise~8, we ask you to estimate this example using the \textit{rnegbinRw} command.

The results show that all 95\% credible intervals contain the true population parameters, and the posterior means are very close to those values.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(010101)
N <- 2000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2); k <- dim(X)[2]; B <- rep(1, k)
alpha <- 1.2; gamma <- exp(alpha); lambda <- exp(X%*%B)
y <- rnbinom(N, mu = lambda, size = gamma)
# log likelihood
logLik <- function(par){
	alpha <- par[1]; beta <- par[2:(k+1)]
	gamma <- exp(alpha)
	lambda <- exp(X%*%beta)
	logLikNB <- sum(sapply(1:N, function(i){dnbinom(y[i], size = gamma, mu = lambda[i], log = TRUE)}))
	return(-logLikNB)
}
# Parameters: Proposal
par0 <- rep(0.5, k+1)
res.optim <- optim(par0, logLik, method="BFGS", hessian=TRUE)
res.optim$par
res.optim$convergence
Covar <- solve(res.optim$hessian) 
CovarBetas <- Covar[2:(k+1),2:(k+1)]
VarAlpha <- Covar[1:1]
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
alpha0 <- 0.5; delta0 <- 0.1
# Metropolis-Hastings function 
MHfunction <- function(iter, sbeta, salpha){
	Beta <- rep(0, k); 	Acept1 <- NULL; Acept2 <- NULL
	BetasPost <- matrix(NA, iter, k); alpha <- 1
	alphaPost <- rep(NA, iter); par <- c(alpha, Beta)
	pb <- txtProgressBar(min = 0, max = iter, style = 3)
	for(s in 1:iter){
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(MASS::mvrnorm(1, mu = Beta, Sigma = sbeta^2*CovarBetas))
		parC <- c(alpha, BetaC)
		LogPostBetaC <- -logLik(parC) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
		alpha1 <- min(exp((LogPostBetaC - mvtnorm::dmvnorm(BetaC, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))-(LogPostBeta - mvtnorm::dmvnorm(Beta, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))),1)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model, M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]
		u1 <- runif(1)
		if(u1 <= alpha1){Acept1i <- 1; Beta <- BetaC}else{
			Acept1i <- 0; Beta <- Beta
		}
		par <- c(alpha, Beta)
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alphaC <- rnorm(1, mean = alpha, sd = salpha*VarAlpha^0.5)
		parC <- c(alphaC, Beta)
		LogPostBetaC <- -logLik(parC) + dgamma(alphaC, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alpha2 <- min(exp((LogPostBetaC - dnorm(alphaC, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))-(LogPostBeta - dnorm(alpha, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))),1)
		u2 <- runif(1)
		if(u2 <= alpha2){Acept2i <- 1; alpha <- alphaC}else{
			Acept2i <- 0; alpha <- alpha
		}
		
		BetasPost[s, ] <- Beta; alphaPost[s] <- alpha
		Acept1 <- c(Acept1, Acept1i); Acept2 <- c(Acept2, Acept2i)
		setTxtProgressBar(pb, s)
	}
	close(pb)
	AcepRateBeta <- mean(Acept1); AcepRateAlpha <- mean(Acept2)
	Results <- list(AcepRateBeta = AcepRateBeta, AcepRateAlpha = AcepRateAlpha, BetasPost = BetasPost, alphaPost = alphaPost)
	return(Results)
}
# MCMC parameters
mcmc <- 10000; burnin <- 1000; thin <- 5
iter <- mcmc + burnin; keep <- seq(burnin, iter, thin)
sbeta <- 2.93/sqrt(k); salpha <- 2.93
# Run M-H
ResultsPost <- MHfunction(iter = iter, sbeta = sbeta, salpha = salpha)
ResultsPost$AcepRateBeta; ResultsPost$AcepRateAlpha
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
summary(coda::mcmc(ResultsPost$alphaPost[keep[-1]]))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Tobit model}\label{sec68}
The dependent variable in Tobit models is partially observed due to sampling or censoring, while the regressors are fully observed. In particular,
\begin{equation*}
	y_i = 
	\begin{cases}
		L, & \text{if } y_i^* < L, \\
		y_i^*, & \text{if } L \leq y_i^* < U, \\
		U, & \text{if } y_i^* \geq U,
	\end{cases}
\end{equation*}
where $y_i^* \stackrel{i.i.d.}{\sim} N(\bm{x}_i^{\top}\bm{\beta}, \sigma^2)$, and $\bm{x}_i$ is a $K$-dimensional vector of regressors.\footnote{We can set $L$ or $U$ equal to $-\infty$ or $\infty$ to model data censored on only one side.}

We assume independent conjugate priors: $\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$. Using data augmentation, we introduce $\bm{y}_C^*$ such that $y_{C_i}^* \stackrel{i.i.d.}{\sim} N(\bm{x}_i^{\top}\bm{\beta}, \sigma^2)$, where $y_{C_i} = \{y_{C_i^L}^*, y_{C_i^U}^*\}$ corresponds to the lower- and upper-censored observations. This formulation allows us to implement a Gibbs sampling algorithm \cite{Chib1992}.

The joint posterior distribution is
\begin{align*}
	\pi(\bm{\beta}, \sigma^2, \bm{y}^* \mid \bm{y}, \bm{X})
	&\propto \prod_{i=1}^N \Big[
	\mathbbm{1}(y_i = L)\mathbbm{1}(y_{C_i^L}^* < L)+ \mathbbm{1}(L \leq y_i < U)\\
	&+ \mathbbm{1}(y_i = U)\mathbbm{1}(y_{C_i^U}^* \geq U)
	\Big] \times N(y_i^* \mid \bm{x}_i^{\top}\bm{\beta}, \sigma^2) \\
	&
	\times N(\bm{\beta} \mid \bm{\beta}_0, \bm{B}_0)
	\times IG(\sigma^2 \mid \alpha_0/2, \delta_0/2).
\end{align*}

The conditional posterior distributions are:
\begin{equation*}
	y_{C_i}^* \mid \bm{\beta}, \sigma^2, \bm{y}, \bm{X} \sim
	\begin{cases}
		TN_{(-\infty, L)}(\bm{x}_i^{\top}\bm{\beta}, \sigma^2), & y_i = L, \\
		TN_{[U, \infty)}(\bm{x}_i^{\top}\bm{\beta}, \sigma^2), & y_i = U,
	\end{cases}
\end{equation*}
\begin{equation*}
	\bm{\beta} \mid \sigma^2, \bm{y}, \bm{X} \sim N(\bm{\beta}_n, \sigma^2\bm{B}_n),
\end{equation*}
\begin{equation*}
	\sigma^2 \mid \bm{\beta}, \bm{y}, \bm{X} \sim IG(\alpha_n/2, \delta_n/2),
\end{equation*}
where 
\[
\bm{B}_n = (\bm{B}_0^{-1} + \sigma^{-2}\bm{X}^{\top}\bm{X})^{-1}, \quad
\bm{\beta}_n = \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sigma^{-2}\bm{X}^{\top}\bm{y}^*), \quad
\alpha_n = \alpha_0 + N,
\] and
\(\delta_n = \delta_0 + (\bm{y}^* - \bm{X}\bm{\beta})^{\top}(\bm{y}^* - \bm{X}\bm{\beta}).\)\\

\textbf{Example: The market value of soccer players in Europe continues}

We continue with the example of the market value of soccer players from Section~\ref{sec61}. We specify the same model but now assume that the sample is left-censored, meaning we only observe soccer players whose market value exceeds one million euros. The dependent variable is \textit{log(ValueCens)}, and the left-censoring point is 13.82.

Algorithm~\ref{alg:Tobit} illustrates how to estimate Tobit models in our GUI. Our GUI implements the \textit{MCMCtobit} command from the \textit{MCMCpack} package.

\begin{algorithm}[h!]
	\caption{Tobit models}\label{alg:Tobit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Tobit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the left and right censoring points. To censor above only, specify \textit{-Inf} in the left censoring box, and to censor below only, specify \textit{Inf} in the right censoring box
		\State Set the hyperparameters: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We run this application using the same hyperparameters that we set in the example of Section \ref{sec61}. All results seem similar to those in the example of linear models. In addition, the posterior chains seem to achieve good diagnostics.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer player with left censoring}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(ValueCens) 
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001; a0 <- 0.001
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 50000; burnin <- 10000
tot <- mcmc + burnin; thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCtobit(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin, below = 13.82, above = Inf)
summary(coda::mcmc(posterior))
# Gibbs sampling functions
XtX <- t(X)%*%X
PostBeta <- function(Yl, sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*t(X)%*%Yl)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostYl <- function(Beta, sig2, L, U, i){
	Ylmean <- X[i,]%*%Beta
	if(y[i] == L){
		Yli <- truncnorm::rtruncnorm(1, a = -Inf, b = L, mean = Ylmean, sd = sig2^0.5)
	}else{
		if(y[i] == U){
			Yli <- truncnorm::rtruncnorm(1, a = U, b = Inf, mean = Ylmean, sd = sig2^0.5)
		}else{
			Yli <- y[i]
		}
	}
	return(Yli)
}\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer player with left censoring, Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostSig2 <- function(Beta, Yl){
	an <- a0 + length(y)
	dn <- d0 + t(Yl - X%*%Beta)%*%(Yl - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBetas <- matrix(0, mcmc+burnin, k); Beta <- rep(0, k)
PostSigma2 <- rep(0, mcmc+burnin); sig2 <- 1
L <- log(1000000); U <- Inf
# create progress bar in case that you want to see iterations progress
pb <- txtProgressBar(min = 0, max = tot, style = 3)
for(s in 1:tot){
	Yl <- sapply(1:N, function(i){PostYl(Beta = Beta, sig2 = sig2, L = L, U = U, i)})
	Beta <- PostBeta(Yl = Yl, sig2 = sig2)
	sig2 <- PostSig2(Beta = Beta, Yl = Yl) 
	PostBetas[s,] <- Beta; PostSigma2[s] <- sig2
	setTxtProgressBar(pb, s)
}
close(pb)
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
summary(coda::mcmc(PostSigma2[keep]))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Quantile regression}\label{sec69}

In quantile regression, the location parameters vary across quantiles of the dependent variable. Let $q_{\tau}(\bm{x}_i) = \bm{x}_i^{\top} \bm{\beta}_{\tau}$ denote the $\tau$-th quantile regression function of $y_i$ given $\bm{x}_i$, where $\bm{x}_i$ is a $K$-dimensional vector of regressors and $0 < \tau < 1$. Specifically, we consider the model $y_i = \bm{x}_i^{\top} \bm{\beta}_{\tau} + \mu_i$, subject to the condition $\int_{-\infty}^{0} f_{\tau}(\mu_i) \, d\mu_i = \tau$, meaning that the $\tau$-th quantile of $\mu_i$ is 0.

\cite{Kozumi2011} propose modeling $f_{\tau}(\mu_i)$ using the asymmetric Laplace distribution, given by 
\[
f_{\tau}(\mu_i) = \tau(1 - \tau) \exp\left\{- \mu_i \big(\tau - \mathbbm{1}(\mu_i < 0)\big) \right\},
\]
where $\mu_i \big(\tau - \mathbbm{1}(\mu_i < 0)\big)$ is the check (loss) function. These authors also show that this distribution admits a location–scale mixture of normals representation:
\[
\mu_i = \theta e_i + \psi \sqrt{e_i} z_i,
\]
where $\theta = \frac{1 - 2\tau}{\tau(1 - \tau)}$, $\psi^2 = \frac{2}{\tau(1 - \tau)}$, $e_i \sim E(1)$, and $z_i \sim N(0,1)$, with $e_i \perp z_i$.\footnote{$E$ denotes an exponential density.}

Given this representation and assuming that the sample is i.i.d., the likelihood function becomes
\[
p(\bm{y} \mid \bm{\beta}_{\tau}, \bm{e}, \bm{X}) \propto 
\left( \prod_{i=1}^{N} e_i^{-1/2} \right) 
\exp\left\{ - \sum_{i=1}^{N} 
\frac{(y_i - \bm{x}_i^{\top}\bm{\beta}_{\tau} - \theta e_i)^2}{2 \psi^2 e_i} 
\right\}.
\]

Assuming a normal prior for $\bm{\beta}_{\tau}$, i.e., $\bm{\beta}_{\tau} \sim N(\bm{\beta}_{\tau 0}, \bm{B}_{\tau 0})$, and applying data augmentation for $\bm{e}$, a Gibbs sampling algorithm can be implemented for this model. The posterior distributions are:
\begin{equation*}
	\bm{\beta}_{\tau} \mid \bm{e}, \bm{y}, \bm{X} \sim N(\bm{\beta}_{n\tau}, \bm{B}_{n\tau}),
\end{equation*}
\begin{equation*}
	e_i \mid \bm{\beta}_{\tau}, \bm{y}, \bm{X} \sim \text{GIG}\left( \frac{1}{2}, \alpha_{ni}, \delta_{ni} \right),\footnote{GIG denotes a generalized inverse Gaussian density.}
\end{equation*}
where
\[
\bm{B}_{n\tau} = \left( \bm{B}_{\tau 0}^{-1} + \sum_{i=1}^{N} \frac{\bm{x}_i \bm{x}_i^{\top}}{\psi^2 e_i} \right)^{-1}, \quad
\bm{\beta}_{n\tau} = \bm{B}_{n\tau} \left( \bm{B}_{\tau 0}^{-1} \bm{\beta}_{\tau 0} + \sum_{i=1}^{N} \frac{\bm{x}_i (y_i - \theta e_i)}{\psi^2 e_i} \right),
\]
\[
\alpha_{ni} = \frac{(y_i - \bm{x}_i^{\top}\bm{\beta}_{\tau})^2}{\psi^2}, \qquad
\delta_{ni} = 2 + \frac{\theta^2}{\psi^2}.
\]

\textbf{Example: The market value of soccer players in Europe continues}

We continue with the example of the market value of soccer players from Section~\ref{sec61}. We now examine whether the marginal effect of having played on the national team varies across the quantiles of the market value distribution of top European soccer players. We use the same set of regressors as in the previous example but focus on the effects at the 0.5 and 0.9 quantiles of \textit{NatTeam}.

Algorithm~\ref{alg:Quantile} illustrates how to estimate quantile regression models in our GUI. Our GUI implements the \textit{MCMCquantreg} command from the \textit{MCMCpack} package. The following code demonstrates how to conduct this analysis using that package.

The results show that, at the median of the market value distribution (0.5 quantile), the 95\% credible interval for the coefficient associated with \textit{NatTeam} is (0.34, 1.02), with a posterior mean of 0.69. At the 0.9 quantile, these values are (0.44, 1.59) and 1.03, respectively. These results suggest that being on the national team increases the market value of higher-valued players more strongly, although there is some overlap in the credible intervals.

\begin{algorithm}[h!]
	\caption{Quantile regression}\label{alg:Quantile}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Quantile} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the quantile to be analyzed, by default is 0.5
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer player, quantile regression}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/1ValueFootballPlayers.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
y <- log(ValueCens) 
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
k <- dim(X)[2]; N <- dim(X)[1]
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000; B0<-c0*diag(k); B0i<-solve(B0)
# MCMC parameters
mcmc <- 50000; burnin <- 10000
tot <- mcmc + burnin; thin <- 1
q <- 0.5 # quantile
posterior05  <- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior05))
q <- 0.9 # quantile
posterior09  <- MCMCpack::MCMCquantreg(y~X-1, tau = q, b0=b0, B0 = B0i, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior09))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Bayesian bootstrap regression}\label{sec610}

We implement the Bayesian bootstrap \cite{Rubin1981} for linear regression models. In particular, the Bayesian bootstrap simulates posterior distributions by assuming that the sample cumulative distribution function (CDF) represents the population CDF, an assumption also implicit in the frequentist bootstrap \cite{Efron1979}.

Let $y_i \stackrel{i.i.d.}{\sim} \mathcal{F}$, where $\mathcal{F}$ does not correspond to any specific parametric family of distributions but satisfies $\mathbb{E}(y_i \mid \bm{x}_i) = \bm{x}_i^{\top}\bm{\beta}$. Here, $\bm{x}_i$ is a $K$-dimensional vector of regressors, and $\bm{\beta}$ is a $K$-dimensional vector of parameters. The Bayesian bootstrap generates posterior probabilities associated with each $y_i$, assigning zero probability to unobserved values of $\bm{y}$.

The algorithm to implement the Bayesian bootstrap is the following:
\begin{algorithm}[!h]
	\caption{Bayesian bootstrap from scratch in linear regression}
	\label{Alg:BB}
	\begin{algorithmic}[1]
		\State Draw $\bm{g}\sim Dir(\alpha_1,\alpha_2,\dots,\alpha_N)$ such that $\alpha_i=1 \ \forall i$.
		\State $\bm{g}=(g_1,g_2,\dots,g_N)$ is the vector of probabilities to attach to $(y_1,\bm{x}_1^{\top}),(y_2,\bm{x}_2^{\top}),\dots,(y_n,\bm{x}_N^{\top})$ for each Bayesian bootstrap replication.
		\For{\texttt{$s=1,\dots,S$}}
			\State Sample $(y_i,\bm{x}_i^{\top})$ $N$ times with replacement and probabilities $g_i$, $i=1,2,\dots,N$.
			\State Estimate $\bm{\beta}^{(s)}$ using weighted least squares in the model $\mathbb{E}(\bm{y}\mid \bm{X})=\bm{X}\bm{\beta}$, where the weights are based on $g_i$.$^*$ 
		\EndFor
		\State The distribution of $\bm{\beta}^{(s)}$ is the posterior distribution of $\bm{\beta}$.		
	\end{algorithmic}
	$^*${\footnotesize{Ordinary least squares is the posterior mean of $\bm{\beta}$ using Jeffrey's prior in a linear regression.}}
\end{algorithm}

\textbf{Example: Simulation exercise}

Let's perform a simulation exercise to evaluate the performance of Algorithm~\ref{Alg:BB} for inference using the Bayesian bootstrap. The data-generating process includes two regressors, each drawn from a standard normal distribution. The location vector is $\bm{\beta} = [1 \ 1 \ 1]^{\top}$, with variance $\sigma^2 = 1$, and the sample size is 1,000.

Algorithm~\ref{alg:BayBootstrap} illustrates how to use our GUI to implement the Bayesian bootstrap. Our GUI relies on the \textit{bayesboot} command from the \textit{bayesboot} package in \textbf{R}. Exercise~11 asks you to use this package to perform inference in this simulation and compare the results with those obtained from our GUI using $S = 10{,}000$.

\begin{algorithm}[h!]
	\caption{Bayesian bootstrap in linear regression}\label{alg:BayBootstrap}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Bootstrap} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select number of bootstrap replications using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code demonstrates how to implement the Bayesian bootstrap from scratch. The results show that all 95\% credible intervals contain the true population parameters, and the posterior means are close to these values.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Bayesian bootstrap}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
#--- Data
N <- 1000; x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2); B <- c(1, 1, 1); sig2 <- 1
y <- as.numeric(X %*% B + rnorm(N, 0, sqrt(sig2)))
data <- data.frame(y, x1, x2)

#--- Bayesian bootstrap (Rubin) for OLS coefficients
BB <- function(S, df, alpha = 1) {
	N <- nrow(df)
	Betas <- matrix(NA_real_, nrow = S, ncol = 3)
	colnames(Betas) <- c("(Intercept)", "x1", "x2")
	for (s in 1:S) {
		# One Dirichlet weight vector over the N observations
		w <- as.numeric(LaplacesDemon::rdirichlet(1, rep(alpha, N)))
		# Weighted least squares = Bayesian bootstrap draw of the OLS functional
		fit <- lm(y ~ x1 + x2, data = df, weights = w)
		Betas[s, ] <- coef(fit)
	}
	Betas
}
S <- 10000
BBs <- BB(S = S, df = data, alpha = 1)
summary(coda::mcmc(BBs))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 
 
\section{Summary}\label{sec611}
In this chapter, we present the core univariate regression models and demonstrate how to perform Bayesian inference using Markov Chain Monte Carlo (MCMC) methods. Specifically, we cover several key algorithms: Gibbs sampling, Metropolis–Hastings, nested Metropolis–Hastings, and Metropolis–Hastings-within-Gibbs. These algorithms provide the foundation for conducting Bayesian inference in more complex frameworks based on cross-sectional data.

\section{Exercises}\label{sec612}

\begin{enumerate}
	\item Derive the posterior conditional distributions of the Gaussian linear model assuming independent priors $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta}) \times \pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bm{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$.
	
	\item Consider the model $y_i \sim N({\bm{x}}_i^{\top}\bm{\beta}, \sigma^2/\tau_i)$ (Gaussian linear model with heteroskedasticity) and independent priors $\pi(\bm{\beta},\sigma^2,\bm{\tau}) = \pi(\bm{\beta}) \times \pi(\sigma^2) \times \prod_{i=1}^N \pi(\tau_i)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bm{B}}_0)$, $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$, and $\tau_i \sim G(v/2, v/2)$. 
	
	Show that $\bm{\beta}\mid \sigma^2,\bm{\tau},{\bm{y}},{\bm{X}}\sim N(\bm{\beta}_n,{\bm{B}}_n)$, $\sigma^2\mid \bm{\beta},\bm{\tau},{\bm{y}},{\bm{X}}\sim IG(\alpha_n/2,\delta_n/2)$, and $\tau_i\mid \bm{\beta},\sigma^2,{\bm{y}},{\bm{X}}\sim G(v_{1n}/2,v_{2in}/2)$, where $\bm{\tau}=[\tau_1 \ \dots \ \tau_N]^{\top}$, ${\bm{B}}_n=({\bm{B}}_0^{-1}+\sigma^{-2}{{\bm{X}}}^{\top}\Psi{{\bm{X}}})^{-1}$, $\bm{\beta}_n={\bm{B}}_n({\bm{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bm{X}}^{\top}\Psi{\bm{y}})$, $\alpha_n=\alpha_0+N$, $\delta_n=\delta_0+({\bm{y}}-{\bm{X}}\bm{\beta})^{\top}\Psi({\bm{y}}-{\bm{X}}\bm{\beta})$, $v_{1n}=v+1$, $v_{2in}=v+\sigma^{-2}(y_i-{\bm{x}}_i^{\top}\bm{\beta})^2$, and $\Psi=\text{diag}\left(\tau_i\right)$.
	
	\item \textbf{The market value of soccer players in Europe continues.}  
	
	Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm for the market value of soccer players in Europe, setting $v=5$ and using the same hyperparameters as in the homoscedastic case. Is there any meaningful difference in the coefficient associated with the national team compared to the homoscedastic case?
	
	\item \textbf{Example: Determinants of hospitalization continues.}
	  
	Program a Gibbs sampling algorithm for the application on the determinants of hospitalization.
	
	\item \textbf{Choice of fishing mode continues.}  
	\begin{itemize}
		\item Run Algorithm~\ref{alg:MultinomialProbit} to report the results of the Geweke~\cite{Geweke1992}, Raftery~\cite{Raftery1992}, and Heidelberger~\cite{Heidelberger1983} tests using our GUI.
		\item Use the command \textit{rmnpGibbs} to replicate the fishing mode choice example.
	\end{itemize}
	
	\item \textbf{Simulation exercise: Multinomial logit model continues.}
	  
	Perform inference in the simulated multinomial logit model using the command \textit{rmnlIndepMetrop} from the \textit{bayesm} package in \textbf{R}, and compare the results with those obtained from our GUI.
	
	\item \textbf{Simulation of the ordered probit model.} 
	 
	Simulate an ordered probit model where the first regressor follows $N(6, 5)$ and the second follows $G(1,1)$. The location vector is $\bm{\beta} = [0.5 \ -0.25 \ 0.5]^{\top}$, and the cutoffs are $\bm{\alpha} = [0 \ 1 \ 2.5]^{\top}$. Program a Metropolis-within-Gibbs sampling algorithm from scratch to perform inference in this simulation.
	
	\item \textbf{Simulation of the negative binomial model continues.}
	  
	Perform inference in the simulated negative binomial model using the \textit{bayesm} package in \textbf{R}.
	
	\item \textbf{The market value of soccer players in Europe continues.} 
	 
	Perform the application on soccer players’ market value with left censoring at one million euros in our GUI using Algorithm~\ref{alg:Tobit} and the hyperparameters of the example.
	
	\item \textbf{The market value of soccer players in Europe continues.} 
	 
	Program from scratch the Gibbs sampling algorithm for the example of soccer players’ market value at the 0.75 quantile.
	
	\item Use the \textit{bayesboot} package to perform inference in the simulation exercise of Section~\ref{sec610} and compare the results with those obtained from our GUI, setting $S = 10{,}000$.
\end{enumerate}
