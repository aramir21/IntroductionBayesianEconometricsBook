\chapter{Univariate models}\label{chap6}

We describe how to perform Bayesian inference in some of the most common univariate models: normal-inverse gamma, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap in linear models. We show the posterior distributions of the parameters and some applications. In addition, we show how to perform inference in various models using three levels of programming skills: our graphical user interface (GUI), packages from \textbf{R}, and programming the posterior distributions. The first requires no programming skills, the second requires an intermediate level, and the third demands more advanced skills. We also include mathematical and computational exercises.

We can run our GUI typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
		\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor.

\section{The Gaussian linear model}\label{sec61}

The Gaussian linear model specifies ${\bf{y}}={\bf{X}}\bm{\bm{\beta}}+\bf{\mu}$ such that $\mu\sim N(\bf{0},\sigma^2\bf{I}_N)$ is an stochastic error, ${\bf{X}}$ is a $N \times K$ matrix of regressors, $\bm{\bm{\beta}}$ is a $K$-dimensional vector of location coefficients, $\sigma^2$ is the variance of the model (scale parameter), ${\bf{y}}$ is a $N$-dimensional vector of a dependent variable, and $N$ is the sample size. We describe this model using the conjugate family in Section \ref{sec33}, that is, $\pi(\bm{\bm{\beta}},\sigma^2)=\pi(\bm{\bm{\beta}}|\sigma^2)\times\pi(\sigma^2)$, and this allowed to get the posterior marginal distribution for $\bm{\bm{\beta}}$ and $\sigma^2$.

We assume independent prior in this section, that is, $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$, $\alpha_0/2$ and $\delta_0/2$ are the shape and rate parameters. This setting allows getting the posterior conditional distributions, that is, $\pi(\bm{\beta}|\sigma^2,{\bf{y}}, {\bf{X}})$ and $\pi(\sigma^2|\bm{\beta},{\bf{y}}, {\bf{X}})$, which in turn allows to use the Gibbs sampler algorithm to perform posterior inference of $\bm{\beta}$ and $\sigma^2$.

The likelihood function in this model is
\begin{align*}
	p({\bf{y}}| \bm{\beta}, \sigma^2, {\bf{X}}) = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{y}} - \bf{X\bm{\beta}})^{\top}({\bf{y}} - \bf{X\bm{\beta}}) \right\}.
\end{align*}

Then, the conditional posterior distributions are

\begin{align*}
	\bm{\beta}|\sigma^2, {\bf{y}}, {\bf{X}} \sim N(\bm{\beta}_n, \sigma^2{\bf{B}}_n),
\end{align*}
and
\begin{align*}
	\sigma^2|\bm{\beta}, {\bf{y}}, {\bf{X}} \sim IG(\alpha_n/2, \delta_n/2),
\end{align*}

 where ${\bf{B}}_n = ({\bf{B}}_0^{-1} + \sigma^{-2} {\bf{X}}^{\top}{\bf{X}})^{-1}$, $\bm{\beta}_n= {\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0 + \sigma^{-2} {\bf{X}}^{\top}{\bf{y}})$, $\alpha_n = \alpha_0 + N$ and $\delta_n = \delta_0 + ({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}({\bf{y}}-{\bf{X}}\bm{\beta})$ (see Exercise 1 in this chapter).\footnote{This model can be extended to consider heteroskedasticity such that $y_i\sim N({\bf{x}}_i^{\top}\bm{\beta}, \sigma^2/\tau_i)$, where $\tau_i\sim G(v/2,v/2)$. See exercise 2 for details.}\\

\textbf{Example: The market value of soccer players in Europe}

Let's analyze the determinants of the market value of soccer players in Europe. In particular, we use the dataset \textit{1ValueFootballPlayers.csv} which is in folder \textbf{DataApp} in our github repository \textbf{https://github.com/besmarter/BSTApp}. This dataset was used by \cite{Serna2018} to finding the determinants of high performance soccer players in the five most important national leagues in Europe.

The specification of the model is
\begin{align*}
	\log(\text{Value}_i)&={\beta}_1+{\beta}_2\text{Perf}_i+{\beta}_3\text{Age}_i+{\beta}_4\text{Age}^2_i+{\beta}_5\text{NatTeam}_i\\
	&+{\beta}_6\text{Goals}_i+{\beta}_7\text{Exp}_i+{\beta}_{8}\text{Exp}^2_i+\mu_i,
\end{align*}

where \textit{Value} is the market value in Euros (2017), \textit{Perf} is a measure of performance, \textit{Age} is the players' age in years, \textit{NatTem} is an indicator variable that takes the value of 1 if the player has been on the national team, \textit{Goals} is the number of goals scored by the player during his career, and \textit{Exp} is his experience in years.  

We assume that the dependent variable distributes normal, then we use a normal-inverse gamma model using vague conjugate priors where ${\bf{B}}_0=1000{\bf{I}}_{8}$, $\bm{\beta}_0={\bf{0}}_{8}$, $\alpha_0=0.001$ and $\delta_0=0.001$. We perform a Gibbs sampler with 5,000 MCMC iterations plus a burn-in equal to 5,000, and a thinning parameter equal to 1.

Once our GUI is displayed (see beginning of this chapter), we should follow Algorithm \ref{alg:Gaussian} to run linear Gaussian models in our GUI (see Chapter \ref{chapGUI} for details):
\begin{algorithm}[h!]
	\caption{Linear Gaussian model}\label{alg:Gaussian}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Normal} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We can see in the next \textbf{R} codes how to perform the linear Gaussian model using the command \textit{MCMCregress} of the \textit{MCMCpack} package, and programming the Gibbs sampler ourselves. We should get similar results using the three approaches: GUI, package and our function. In fact, our GUI relies on the \textit{MCMCregress} command. For instance, the value of a top soccer player in Europe increases 134\% ($\exp(0.85)-1)$) on average when he has played in the national team, the credible interval at 95\% is (86\%, 197\%).  


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code1}
	\textit{R code. The value of soccer players, using \textbf{R} packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
########################## Linear regression: Value of soccer players ##########################
Data <- read.csv("DataApplications/1ValueFootballPlayers.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
posterior  <- MCMCpack::MCMCregress(y~X-1, b0=b0, B0 = B0i, c0 = a0, d0 = d0, burnin = burnin, mcmc = mcmc, thin = thin)
summary(coda::mcmc(posterior))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
					Mean       SD  Naive SE Time-series SE
X         3.695499 2.228060 3.151e-02      3.151e-02
XPerf     0.035445 0.004299 6.079e-05      6.079e-05
XAge      0.778410 0.181362 2.565e-03      2.565e-03
XAge2    -0.016617 0.003380 4.781e-05      4.781e-05
XNatTeam  0.850362 0.116861 1.653e-03      1.689e-03
XGoals    0.009097 0.001603 2.266e-05      2.266e-05
XExp      0.206208 0.062713 8.869e-04      8.428e-04
XExp2    -0.006992 0.002718 3.844e-05      3.719e-05
sigma2    0.969590 0.076091 1.076e-03      1.076e-03
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
bhat <- solve(XtX)%*%t(X)%*%y
an <- a0 + N
# Gibbs sampling functions
PostSig2 <- function(Beta){
	dn <- d0 + t(y - X%*%Beta)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2){
	Bn <- solve(B0i + sig2^(-1)*XtX)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*XtX%*%bhat)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2)
	PostBetas[s,] <- Beta
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
					Mean       SD  Naive SE Time-series SE
Intercept  3.663230 2.194363 3.103e-02      3.103e-02
Perf       0.035361 0.004315 6.102e-05      6.102e-05
Age        0.780374 0.178530 2.525e-03      2.525e-03
Age2      -0.016641 0.003332 4.713e-05      4.713e-05
NatTeam    0.850094 0.119093 1.684e-03      1.684e-03
Goals      0.009164 0.001605 2.270e-05      2.270e-05
Exp        0.205965 0.062985 8.907e-04      8.596e-04
Exp2      -0.007006 0.002731 3.862e-05      3.701e-05
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean             SD       Naive SE Time-series SE 
0.973309       0.077316       0.001093       0.001116 
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The logit model}\label{sec62}

In the logit model the dependent variable is binary, $Y_i=\left\{1,0\right\}$, then it follows a Bernoulli distribution, $Y_i\stackrel{ind} {\thicksim}B(\pi_i)$, that is, $p(Y_i=1)=\pi_i$, such that $\pi_i=\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}$.

The likelihood function of the logit model is
\begin{align*}
	p({\bf{y}}|\bm{\beta},{\bf{X}})&=\prod_{i=1}^N \pi_i^{y_i}(1-\pi_i)^{1-y_i}\\
	&=\prod_{i=1}^N\left(\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{y_i}\left(\frac{1}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{1-y_i}.
\end{align*}

We can specify a Normal distribution as prior, $\bm{\beta}\sim N({\bm{\beta}}_0,{\bf{B}}_0)$. Then, the posterior distribution is

\begin{align*}
	\pi(\bm{\beta}|{\bf{y}},{\bf{X}})&\propto\prod_{i=1}^N\left(\frac{\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{y_i}\left(\frac{1}{1+\exp\left\{{\bf{x}}_i^{\top}\bm{\beta}\right\}}\right)^{1-y_i}\\
	&\times\exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}.
\end{align*}

The logit model does not have a standard posterior distribution. Then, a random walk Metropolis--Hastings algorithm can be used to obtain draws from the posterior distribution. A potential proposal is a multivariate Normal centered at the current value, with covariance matrix $\tau^2({\bf{B}}_0^{-1}+\widehat{{\bm{\Sigma}}}^{-1})^{-1}$, where $\tau>0$ is a tuning parameter and $\widehat{\bm{\Sigma}}$ is the sample covariance matrix from the maximum likelihood estimation \cite{Martin2011}.\footnote{Tuning parameters should be set in a way such that one obtains reasonable diagnostic criteria and acceptation rates.}

Observe that $\log(p({\bf{y}}|\bm{\beta},{\bf{X}}))=\sum_{i=1}^Ny_i{\bf{x}}_i^{\top}\bm{\beta}-\log(1+\exp({\bf{x}}_i^{\top}\bm{\beta}))$. We can use this expression when calculating the acceptance parameter in the computational implementation of the Metropolis-Hastings algorithm. In particular, the acceptance parameter is \begin{equation*}
	\alpha=\min\left\{1, \exp(\log(p({\bf{y}}|\bm{\beta}^{c},{\bf{X}}))+\log(\pi(\bm{\beta}^c))-(\log(p({\bf{y}}|\bm{\beta}^{(s-1)},{\bf{X}}))+\log(\pi(\bm{\beta}^{(s-1)}))))\right\},
\end{equation*}
where $\bm{\beta}^c$ and $\bm{\beta}^{(s-1)}$ are the draws from the proposal distribution and the previous iteration of the Markov chain, respectively.\footnote{Formulating the acceptance rate using $\log$ helps to mitigate computational problems.}\\

\textbf{Example: Simulation exercise}

Let's do a simulation exercise to check the performance of the algorithm. Set $\bm{\beta}=\begin{bmatrix}0.5 & 0.8 & -1.2\end{bmatrix}^{\top}$, $x_{ik}\sim N(0,1)$, $k=2,3$ and $i=1,2,\dots,10000$.

We set as hyperparameters $\bm{\beta}_0=[0 \ 0 \ 0]^{\top}$ and ${\bf{B}}_0=1000{\bf{I}}_3$. The tune parameter for the Metropolis-Hastings algorithm is equal to 1.

Once our GUI is displayed (see beginning of this chapter), we should follow Algorithm \ref{alg:Logit} to run logit models in our GUI (see Chapter \ref{chapGUI} for details):
\begin{algorithm}[h!]
	\caption{Logit model}\label{alg:Logit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Logit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. You can modify the formula in the \textbf{Main equation} box using valid arguments of the \textit{formula} command structure in \textbf{R}
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameter for the Metropolis-Hastings algorithm
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We can see in the next \textbf{R} codes how to perform the logit model using the command \textit{MCMClogit} of the \textit{MCMCpack} package, and programming the Metropolis-Hastings algorithm ourselves. 

We should get similar results using the three approaches: GUI, package and our function. Our GUI relies on the \textit{MCMClogit} command. In particular, we obtain an acceptance rate of 0.46, and the diagnostics suggest that the posterior chains behave well. In general, the 95\% credible intervals encompass the population values, and the mean and median are very close to these values.  

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the logit model estimation using \textbf{R} packages}
	\begin{VF}
		\begin{lstlisting}[language=R]		
########################## Logit: Simulation ##########################
# Simulate data
rm(list = ls())
set.seed(010101)
N <- 10000 # Sample size
B <- c(0.5, 0.8, -1.2) # Population location parameters
x2 <- rnorm(N) # Regressor
x3 <- rnorm(N) # Regressor
X <- cbind(1, x2, x3) # Regressors
XB <- X%*%B
PY <- exp(XB)/(1 + exp(XB)) # Probability of Y = 1
Y <- rbinom(N, 1, PY) # Draw Y's
table(Y) # Frequency
# write.csv(cbind(Y, x2, x3), file = "DataSimulations/LogitSim.csv") # Export data
# MCMC parameters
iter <- 5000; burnin <- 1000; thin <- 5; tune <- 1
# Hyperparameters
K <- dim(X)[2] 
b0 <- rep(0, K)
c0 <- 1000
B0 <- c0*diag(K)
B0i <- solve(B0)
# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix
RegLog <- MCMCpack::MCMClogit(Y~X-1, mcmc = iter, burnin = burnin, thin = thin, b0 = b0, B0 = B0i, tune = tune)
summary(RegLog)
Iterations = 1001:5996
Thinning interval = 5 
Number of chains = 1 
Sample size per chain = 1000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
			Mean      SD  Naive SE Time-series SE
X    0.4896 0.02550 0.0008064       0.001246
Xx2  0.8330 0.02730 0.0008632       0.001406
Xx3 -1.2104 0.03049 0.0009643       0.001536
2. Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
X    0.4424  0.4728  0.4894  0.5072  0.5405
Xx2  0.7787  0.8159  0.8327  0.8505  0.8852
Xx3 -1.2758 -1.2296 -1.2088 -1.1902 -1.1513
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the logit model estimation programming our M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Posterior distributions programming the Metropolis-Hastings algorithm
MHfunc <- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 6000, burnin = 1000, thin = 5){
	Xm <- cbind(1, X) # Regressors
	K <- dim(Xm)[2] # Number of location parameters
	BETAS <- matrix(0, iter + burnin, K) # Space for posterior chains
	Reg <- glm(y ~ Xm - 1, family = binomial(link = "logit")) # Maximum likelihood estimation
	BETA <- Reg$coefficients # Maximum likelihood parameter estimates 
	tot <- iter + burnin # Total iterations M-H algorithm
	COV <- vcov(Reg) # Maximum likelihood covariance matrix
	COVt <- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution
	Accep <- rep(0, tot) # Space for calculating the acceptance rate
	# Create progress bar in case that you want to see iterations progress
	pb <- winProgressBar(title = "progress bar", min = 0,
	max = tot, width = 300)
	for(it in 1:tot){
		BETAc <- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter
		likecand <- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood for the candidate
		likepast <- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log likelihood for the actual draw
		priorcand <- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior for candidate
		priorpast <- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior for actual draw
		alpha <- min(1, exp((likecand - priorcand) - (likepast - priorpast))) #Probability of selecting candidate
		u <- runif(1) # Decision rule for selecting candidate
		if(u < alpha){
			BETA <- BETAc # Changing reference for candidate if selected
			Accep[it] <- 1 # Indicator if the candidate is accepted
		} 
		BETAS[it, ] <- BETA # Saving draws
		setWinProgressBar(pb, it, title=paste( round(it/tot*100, 0),
		"% done"))
	}
	close(pb)
	keep <- seq(burnin, tot, thin)
	return(list(Bs = BETAS[keep[-1], ], AceptRate = mean(Accep[keep[-1]])))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the logit model programming our M-H algorithm, results}
	\begin{VF}
		\begin{lstlisting}[language=R]		
Posterior <- MHfunc(y = Y, X = cbind(x2, x3), iter = iter, burnin = burnin, thin = thin) # Running our M-H function changing some default parameters.
paste("Acceptance rate equal to", round(Posterior$AceptRate, 2), sep = " ")
"Acceptance rate equal to 0.46"
PostPar <- coda::mcmc(Posterior$Bs)
# Names
colnames(PostPar) <- c("Cte", "x1", "x2")
# Summary posterior draws
summary(PostPar)
Iterations = 1:1000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean      SD  Naive SE Time-series SE
Cte  0.4893 0.02427 0.0007674       0.001223
x1   0.8309 0.02699 0.0008536       0.001440
x2  -1.2107 0.02943 0.0009308       0.001423
2. Quantiles for each variable:
		2.5%     25%     50%     75%   97.5%
Cte  0.4431  0.4721  0.4899  0.5059  0.5344
x1   0.7817  0.8123  0.8305  0.8505  0.8833
x2  -1.2665 -1.2309 -1.2107 -1.1911 -1.1538
# Trace and density plots
plot(PostPar)
# Autocorrelation plots
coda::autocorr.plot(PostPar)
# Convergence diagnostics
coda::geweke.diag(PostPar)
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 
Cte     x1     x2 
-0.975 -3.112  1.326 
coda::raftery.diag(PostPar,q=0.5,r=0.05,s = 0.95)
Quantile (q) = 0.5
Accuracy (r) = +/- 0.05
Probability (s) = 0.95 
Burn-in  Total Lower bound  Dependence
(M)      (N)   (Nmin)       factor (I)
Cte 6        731   385          1.90      
x1  6        703   385          1.83      
x2  6        725   385          1.88 
coda::heidel.diag(PostPar)
Stationarity start     p-value
test         iteration        
Cte passed         1       0.4436 
x1  passed       101       0.3470 
x2  passed         1       0.0872 
Halfwidth Mean   Halfwidth
test                      
Cte passed     0.489 0.00240  
x1  passed     0.832 0.00268  
x2  passed    -1.211 0.00279
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The probit model}\label{sec63}

The probit model also has as dependent variable a binary outcome.
In this case, there is a latent variable ($y_i^*$, unobserved) that defines the structure of the estimation problem.
In particular,
\begin{equation*}
Y_i=\begin{Bmatrix}
	0, \ Y_i^*\leq 0 \\ 
	1, \ Y_i^*> 0 \\ 
\end{Bmatrix},
\end{equation*}

such that $Y_i^*=\bf{x}_i^{\top}\bm{\beta}+\mu_i$, $\mu_i\stackrel{i.i.d.} {\thicksim}N(0,1)$.\footnote{The variance in this model is set to 1 due to identification restrictions.	Observe that $P(Y_i=1|\bf{x}_i)=P(Y_i^*>0|\bf{x}_i)=P(\bf{x}_i^{\top}\bm{\beta}+\mu_i>0|\bf{x}_i)=P(\mu_i>-\bf{x}_i^{\top}\bm{\beta}|\bf{x}_i)=P(c\times\mu_i>-c\times\bf{x}_i^{\top}\bm{\beta}|\bf{x}_i)$ $\forall c>0$. Multiplying for a positive constant does not affect the probability of $Y_i=1$.} This implies $P(Y_i=1)=\pi_i=\Phi(\bf{x}_i^{\top}\bm{\beta})$.\\

\cite{Albert1993} implemented data augmentation \cite{Tanner1987} to apply a Gibbs sampling algorithm in this model.
Augmenting this model with $Y_i^*$, we can have the likelihood contribution from observation $i$, $p(y_i|y_i^*)=\mathbbm{1}_{y_i=0}\mathbbm{1}_{y_i^*\leq 0}+\mathbbm{1}_{y_i=1}\mathbbm{1}_{y_i^*> 0}$, where $\mathbbm{1}_A$ is an indicator function that takes the value of 1 when condition $A$ is satisfied.\\

The posterior distribution is $\pi(\bm{\beta},\bm{y^*}|\bm{y},\bm{X})\propto\prod_{i=1}^N\left[\mathbbm{1}_{y_i=0}\mathbbm{1}_{y_i^*\leq 0}+\mathbbm{1}_{y_i=1}\mathbbm{1}_{y_i^*> 0}\right] \times {N}_N(\bm{y}^*|\bm{X\bm{\beta}},\bm{I}_n)\times {N}_K(\bm{\beta}|\bm{\beta}_0,\bm{B}_0)$ when taking a Gaussian distribution as prior $\bm{\beta}\sim{N}_k(\bm{\beta}_0,\bm{B}_0)$.
This implies
\begin{equation*}
	y_i^*|\bm{\beta},\bm{y},\bm{X}\sim\begin{Bmatrix}
		TN_{(-\infty,0]}(\bf{x}_i^{\top}\bm{\beta},1) \ , \ y_i= 0 \\ 
		TN_{(0,\infty)}(\bf{x}_i^{\top}\bm{\beta},1) \ \ \ , \ y_i= 1
	\end{Bmatrix},\footnote{$TN$ denotes a truncated normal density.}
\end{equation*}
\begin{equation*}
	\bm{\beta}|\bm{y}^*, \bm{X} \sim N(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
\noindent where $\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{\top}\bm{X})^{-1}$, and $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}\bm{y}^*)$.\\

\textbf{Example: Determinants of hospitalization}

We use the dataset named \textbf{2HealthMed.csv}, which is in folder \textbf{DataApp} in our github repository \textbf{(https://github.com/besmarter/BSTApp} and was used by \cite{Ramirez2013}. Our dependent variable is a binary indicator with a value equal to 1 if an individual was hospitalized in 2007, and 0 otherwise.

The specification of the model is
\begin{align*}
	\text{Hosp}_i&={\beta}_1+{\beta}_2\text{SHI}_i+{\beta}_3\text{Female}_i+{\beta}_4\text{Age}_i+{\beta}_5\text{Age}_i^2+{\beta}_6\text{Est2}_i+{\beta}_7\text{Est3}_i\\
	&+{\beta}_8\text{Fair}_i+{\beta}_9\text{Good}_i+{\beta}_{10}\text{Excellent}_i,
\end{align*}

where \textit{SHI} is a binary variable equal to 1 if the individual is in a subsidized health care program and 0 otherwise, \textit{Female} is an indicator of gender, \textit{Age} in years, \textit{Est2} and \textit{Est3} are indicators of socioeconomic status, the reference is \textit{Est1}, which is the lowest, and self perception of health status where \textit{bad} is the reference.

Let's set $\bm{\beta}_0={\bf{0}}_{10}$, ${\bf{B}}_0={\bf{I}}_{10}$, iterations, burn-in and thinning parameters equal to 10000, 1000 and 1, respectively. We can use the Algorithm \ref{alg:Gaussian} to run the probit model in our GUI. We should select \textit{Probit} model in stage 2. Our GUI relies in the command \textit{rbprobitGibbs} from the package \textit{bayesm} to perform inference in the Probit model. The following \textbf{R} code shows how to run this example using the command \textit{rbprobitGibbs}. We asked to program a Gibbs sampler algorithm to perform inference in the probit model in the exercises.

We find evidence that gender and self-perceived health status affect the probability of hospitalization. Women have a higher probability of being hospitalized than men, and a better perception of health status decreases this probability.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization}
	\begin{VF}
		\begin{lstlisting}[language=R]	
mydata <- read.csv("DataApplications/2HealthMed.csv", header = T, sep = ",")
attach(mydata)
str(mydata)
K <- 10 # Number of regressors
b0 <- rep(0, K) # Prio mean
B0i <- diag(K) # Prior precision (inverse of covariance)
Prior <- list(betabar = b0, A = B0i) # Prior list
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
Data <- list(y = y, X = X) # Data list
Mcmc <- list(R = 10000, keep = 1, nprint = 0) # MCMC parameters
RegProb <- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package
PostPar <- coda::mcmc(RegProb$betadraw) # Posterior draws
colnames(PostPar) <- c("Cte", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent") # Names
summary(PostPar) # Posterior summary
Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000
2. Quantiles for each variable:
				2.5%        25%        50%        75%      97.5%
Cte       -1.22e+00 -1.03e+00 -9.43e-01 -8.50e-01 -0.671744
SHI       -1.24e-01 -4.63e-02 -6.30e-03  3.26e-02  0.104703
Female     2.80e-02  9.65e-02  1.28e-01  1.60e-01  0.223123
Age       -7.55e-03 -2.50e-03  1.25e-04  2.80e-03  0.007646
Age2      -4.98e-05  9.05e-06  4.02e-05  7.07e-05  0.000128
Est2      -1.89e-01 -1.23e-01 -8.84e-02 -5.32e-02  0.012714
Est3      -2.13e-01 -1.03e-01 -4.73e-02  1.01e-02  0.109527
Fair      -7.09e-01 -5.69e-01 -4.93e-01 -4.16e-01 -0.269494
Good      -1.42e+00 -1.28e+00 -1.20e+00 -1.12e+00 -0.982533
Excellent -1.33e+00 -1.15e+00 -1.06e+00 -9.74e-01 -0.795881
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{The multinomial probit model}\label{sec64}
The multinomial probit model is used to model the choice of the $l$-th alternative over a set $L$ mutually exclusive options.
We observe 
\begin{equation*}
y_{il}=
\begin{Bmatrix}
	1, & y_{il}^*\geq \max\left\{\bm{y}_i^*\right\}  \\
	0, & \text{otherwise}
\end{Bmatrix},
\end{equation*}

such that $\bm{y}_i^*=\bm{X}_{i}\bm{\delta}+\bm\mu_i$, $\bm\mu_i\stackrel{i.i.d.} {\thicksim}N(\bm 0,\bm{\Sigma})$, $\bm{y}_i^*$ is an unobserved latent $L$ dimensional vector, $\bm{X}_{i}=\left[(1 \ \bm{c}_i^{\top})\otimes \bm{I}_L \ \bm{A}_i\right]$ is an $L\times j$ matrix of regressors for each alternative, $l=1,2,\dots,L$, $j=L\times (1+dim\left\{\bm{c}_i\right\})+a$, $\bm{c}_i$ is a vector of the individuals' specific characteristics, $\bm{A}_i$ is an $L\times a$ matrix of alternative-varying regressors, $a$ is the number of alternative-varying regressors, and $\bm{\delta}$ is a $j$ dimensional vector of parameters.

We take into account simultaneously the alternative-varying regressors (alternative attributes) and alternative-invariant regressors (individual characteristics).\footnote{Note that this model is not identified if $\bm{\Sigma}$ is unrestricted. The likelihood function is the same if a scalar random variable is added to each of the $L$ latent regressions.} $\bm{y}_i^*$ can be stacked up into a multiple regression with correlated stochastic errors, $\bm{y}^*=\bm{X}\bm\delta+\bm{\mu}$, where $\bm{y}^*=\left[\bm{y}_1^{*\top},\bm{y}_2^{*\top},\dots,\bm{y}_N^{*\top}\right]$,$\bm{X}=\left[\bm{X}_1^{\top},\bm{X}_2^{\top},\dots,\bm{X}_N^{\top}\right]^{\top}$, and $\bm{\mu}=\left[\bm{\mu}_1^{\top},\bm{\mu}_2^{\top},\dots,\bm{\mu}_N^{\top}\right]^{\top}$.

Following the practice of expressing $y_{il}^*$ relative to $y_{iL}^*$ by letting $\bm{w}_i=\left[w_{i1},w_{i2},\dots,w_{iL-1}\right]^{\top}$, $w_{il}=y_{il}^*-y_{iL}^*$, we can write $\bm{w}_i=\bm{R}_i\bm{\beta}+\bm{\epsilon}_i$, $\bm{\epsilon_i}\sim{N}(\bm 0,\bm{\Omega})$, where $\bm{R}_i=\left[(1 \ \bm{c}_i^{\top})\otimes \bm{I}_{L-1} \ \bm{\Delta A}_i\right]$ is an $L-1\times k$ matrix where $\Delta \bm{A}_i=\bm{A}_{li}-\bm{A}_{Li}$, $l=1,2, \dots, L-1$, that is, the last row of $\bm{A}_i$ is subtracted from each row of $\bm{A}_i$, and ${\bm{\beta}}$ is a $k$ dimensional vector, $k=(L-1)\times(1+dim\left\{\bm{c}_i\right\})+a$.

Observe that $\bm{\beta}$ contains the same last $a$ elements as $\bm{\delta}$, that is, alternative specific attributes coefficients, but the first $(L-1)\times(1+dim\left\{\bm{c}_i\right\})$-th elements are $\delta_{jl}-\delta_{jL}$, $j=1+dim\left\{\bm{c}_i\right\}$, $l=1,2,\dots,L-1$, that is, the difference between the coefficients of each qualitative response and the $L$-th alternative for the individuals' characteristics.This makes it difficult to interpret the multinomial probit coefficients.

Note that in multinomial models, for each alternative specific attribute, it is only required to estimate one coefficient for all alternatives, whereas for individuals' characteristics (non-alternative specific regressors), it is necessary to estimate $L-1$ coefficients (the coefficient of the base alternative is set equal to 0).

The likelihood function in this model is $p(\bm{\beta},\bm{\Omega}|\bm{y},\bm{R})=\prod_{i=1}^N\prod_{l=1}^L p_{il}^{y_{il}}$ where $p_{il}=p(y_{il}^*\geq \max(\bm{y}_i^*))$.

We assume independent priors, $\bm{\beta}\sim N(\bm{\beta}_0,\bm{B}_0)$ and $\bm{\Omega}^{-1}\sim W(\alpha_0,\bm{\Sigma}_0)$.\footnote{$W$ denotes the Wishart density.} We can employ Gibbs sampling in this model because this is a standard Bayesian linear regression model when data augmentation in $\bm{w}$ is used.
The posterior conditional distributions are
\begin{equation*}
	\bm{\beta}|\bm{\Omega},\bm{w}\sim{N}(\bm{\beta}_n,\bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\Omega}^{-1}|\bm{\beta},\bm{w}\sim{W}(\alpha_n,\bm{\Sigma}_n),
\end{equation*}

where $\bm{B}_n=(\bm{B}_0^{-1}+\bm{X}^{*\top}\bm{X}^*)^{-1}$, $\bm{\beta}_n=\bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0+\bm{X}^{*\top}\bm{w}^*)$, $\bm{\Omega}^{-1}=\bm{C}^{\top}\bm{C}$, $\bm{X}_i^{*\top}=\bm{C}^{\top}\bm{R}_i$, $\bm{w}_i^*=\bm{C}^{\top}\bm{w}_i$, $\bm{X}^*=\begin{bmatrix}\bm{X}_1^*\\
	\bm{X}_2^*\\
	\vdots\\
	\bm{X}_N^*
\end{bmatrix}$, $\alpha_n=\alpha_0+N$, $\bm{\Sigma}_n=(\bm{\Sigma}_0+\sum_{i=1}^N (\bm{w}_i-\bm{R}_i\bm{\beta})^{\top}(\bm{w}_i-\bm{R}_i\bm{\beta}))^{-1}$.

We can collapse the multinomial vector $\bm{y}_i$ into the indicator variable $d_i=\sum_{l=1}^{L-1}l\times \mathbbm{1}_{\max(\bm{w}_{l})=w_{il}}$.\footnote{Observe that the identification issue in this model is due to scaling $w_{il}$ by a positive constant does not change the value of $d_i$.} Then the distribution of $\bm{w}_i|\bm{\beta},\bm{\Omega}^{-1},d_i$ is an $L-1$ dimensional Gaussian distribution truncated over the appropriate cone in $\mathcal{R}^{L-1}$.
\cite{McCulloch1994} propose drawing from the univariate conditional distributions $w_{il}|\bm{w}_{i,-l},\bm{\beta},\bm{\Omega}^{-1},d_i\sim TN_{I_{il}}(m_{il},\tau_{ll}^2)$, where 

\begin{equation*}
I_{il}=\begin{Bmatrix} w_{il}>\max(\bm{w}_{i,-l},0), & d_i=l\\
	w_{il}<\max(\bm{w}_{i,-l},0), & d_i\neq l\\
\end{Bmatrix},
\end{equation*}
and permuting the columns and rows of $\bm{\Omega}^{-1}$ so that the $l$-th column and row is the last,
\begin{equation*}
	\bm{\Omega}^{-1}=\begin{bmatrix}
		\bm{\Omega}_{-l,-l} & \bm\omega_{-l,l}\\
		\bm\omega_{l,-1} & \omega_{l,l}\\
	\end{bmatrix}^{-1}
	=\begin{bmatrix}
		\bm{\Omega}_{-l,-l}^{-1}+{\tau}^{-2}_{ll}\bm{f}\bm{f}^{\top} & -\bm{f}\tau^{-2}_{ll}\\
		-{\tau}^{-2}_{ll}\bm{f}^{\top} & {\tau}^{-2}_{ll}\\
	\end{bmatrix}
\end{equation*}
\noindent where $\bm{f}=\bm{\Omega}_{-l,-l}^{-1}\bm{\omega}_{-l,l}$, $\tau_{ll}^2= \omega_{ll}-\bm{\omega}_{l,-l}\bm{\Omega}^{-1}_{-l,-1}\bm{\omega}_{-l,l}$, $m_{il}=\bm{r}_{il}^{\top}\bm{\beta}+\bm{f}^{\top}(\bm{w}_{i,-l}-\bm{R}_{i,-l}\bm{\beta})$, $\bm{w}_{i,-l}$ is an $L-2$ dimensional vector of all components of $\bm{w}_i$ excluding $w_{il}$, $\bm{r}_{il}$ is the $l$-th row of $\bm{R}_i$, $l=1,2,\dots,L-1$. 

The identified parameters are obtained by normalizing with respect to one of the diagonal elements $\frac{1}{\omega_{1,1}^{0.5}}\bm{\beta}$ and $\frac{1}{\omega_{1,1}}\bm{\Omega}$.\footnote{Our GUI is based on the \textit{bayesm} package that takes into account this identification restriction to display the outcomes of the posterior chains.}\\

\textbf{Example: Choice of fishing mode}

We used in this application the dataset \textit{3Fishing.csv} from \cite[p.~491]{cameron05}. The dependent variable is mutually exclusive alternatives regarding fishing modes (mode), where beach is equal to 1, pier is equal to 2, private boat is equal to 3, and chartered boat (baseline alternative) is equal to 4. In this model, we have
{\small{
\begin{align*}
	\bm{X}_i & = \begin{Bmatrix}
		1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & 0 & \text{Price}_{i,1} & \text{Catch rate}_{i,1}\\ 
		0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & 0 & \text{Price}_{i,2} & \text{Catch rate}_{i,2}\\
		0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & 0 & \text{Price}_{i,3} & \text{Catch rate}_{i,3}\\
		0 & 0 & 0 & 1 & 0 & 0 & 0 & \text{Income}_i & \text{Price}_{i,4} & \text{Catch rate}_{i,4}\\
	\end{Bmatrix}.
\end{align*}
}}

In this example chartered boat is the base category, the number of choice categories is four, there are two alternative-specific regressors (price and catch rate), and one non alternative-specific regressor (income). This setting involves the estimation of eight location parameters ($\bm\beta$): three intercepts, three for income, one for price, and one for catch rate. This is the order of the posterior chains in our GUI. Note that the location coefficients are set equal to 0 for the baseline category. For multinomial models, we strongly recommend using the last category as the baseline.

We also get posterior estimates for a $3\times 3$ covariance matrix (four alternatives minus one), where the element (1,1) is equal to 1 due to identification restrictions, and elements 2 and 4 are the same, as well as 3 and 7, and 6 and 8, due to symmetry.\footnote{This is the order in the pdf, eps and csv files that can be downloaded from our GUI.} Observe that this identification restriction implies \textit{NaN} values in \cite{Geweke1992} and \cite{Heidelberger1983} tests for element (1,1) of the covariance matrix, and just eight dependence factors associated with the remaining elements of the covariance matrix. 

Once our GUI is displayed (see beginning of this chapter), we should follow Algorithm \ref{alg:MultinomialProbit} to run multinomial probit models in our GUI (see Chapter \ref{chapGUI} for details):

\begin{algorithm}[h!]
	\caption{Multinomial probit models}\label{alg:MultinomialProbit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Multinomial Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Select the number of the \textbf{Base Alternative}
		\State Select the \textbf{Number of choice categorical alternatives}
		\State Select the \textbf{Number of alternative specific variables}
		\State Select the \textbf{Number of Non-alternative specific variables} 
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax.
		\State Set the hyperparameters: mean vector, covariance matrix, scale matrix and degrees of freedom. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}


We ran 100,000 MCMC iterations plus 10,000 as burn-in with a thinning parameter equal to 5, where all priors use default values for the hyperparameters in our GUI. We found that the 95\% credible intervals of the coefficient associated with income for beach and private boat alternatives are equal to  (8.58e-06, 8.88e-05) and (3.36e-05, 1.45e-04). This suggests that the probability of choosing these alternatives increases compared to a chartered boat when income increases. In addition, an increase in the price or a decrease in the catch rate for specific fishing alternatives imply lower probabilities of choosing them as the 95\% credible intervals are (-9.91e-03, -3.83e-03) and (1.40e-01, 4.62e-01), respectively. However, the chain diagnostics suggest there are convergence issues with the posterior draws (see exercise 5). 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Choice of fishing mode, results}
	\begin{VF}
		\begin{lstlisting}[language=R]	
Iterations = 10005:110000
Thinning interval = 5 
Number of chains = 1 
Sample size per chain = 20000 
Quantiles for each variable:
				2.5%        25%        50%        75%      97.5%
cte_1   -5.83e-01 -4.08e-01 -3.22e-01 -2.37e-01 -7.93e-02
cte_2   -1.93e-01 -4.14e-02  2.16e-02  7.93e-02  1.93e-01
cte_3   -8.15e-01 -5.43e-01 -4.29e-01 -3.33e-01 -1.70e-01
NAS_1_1  8.58e-06  3.61e-05  4.95e-05  6.27e-05  8.88e-05
NAS_1_2 -3.24e-05 -7.04e-06  5.52e-06  1.93e-05  5.17e-05
NAS_1_3  3.36e-05  6.38e-05  8.08e-05  9.99e-05  1.45e-04
AS_1    -9.91e-03 -7.90e-03 -6.86e-03 -5.93e-03 -3.83e-03
AS_2     1.40e-01  2.25e-01  2.72e-01  3.28e-01  4.62e-01		\end{lstlisting}
	\end{VF}
\end{tcolorbox} 


\section{The multinomial logit model}\label{sec65}

The multinomial logit model is used to model mutually exclusive discrete outcomes or qualitative response variables. However, this model assumes the independence of irrelevant alternatives (IIA), meaning that the choice between two alternatives does not depend on a third alternative. We consider the multinomial mixed logit model (not to be confused with the random parameters logit model), which accounts for both alternative-varying regressors (conditional) and alternative-invariant regressors (multinomial) simultaneously.\footnote{The multinomial mixed logit model can be implemented as a conditional logit model.}

In this setting there are $L$ mutually exclusive alternatives, and the dependent variable $y_{il}$ is equal to 1 if the $l$th alternative is chosen by individual $i$, and 0 otherwise, $l=\left\{1,2,\dots,L\right\}$. The likelihood function is $p(\bm{\beta}|\bm{y},\bm{X})=\prod_{i=1}^{N}\prod_{l=1}^{L}p_{il}^{y_{il}}$, where the probability that individual $i$ chooses the alternative $l$ is given by $p_{il}:=p(y_i=l|\bm{\beta},\bm{X})=\frac{\exp\left\{\bm{x}_{il}^{\top}\bm{\beta}_l\right\}}{\sum_{j=1}^{L}\exp\left\{\bm{x}_{ij}^{\top}\bm{\beta}_j\right\}}$, $\bm{y}$ and $\bm{X}$ are the vector and matrix of the dependent variable and regressors, and $\bm{\beta}$ is the vector containing all the coefficients. Remember that coefficients associated with alternative-invariant regressors are set to 0 for the baseline category, and the coefficients associated with the alternative-varying regressors are the same for all the categories. In addition, we assume $\bm{\beta}\sim N(\bm{\beta}_0,\bm{B}_0)$ as prior distribution. Thus, the posterior distribution is $\pi(\bm{\beta}|\bm{y},\bm{X})\propto p(\bm{\beta}|\bm{y},\bm{X})\times \pi(\bm{\beta})$.

As the multinomial logit model does not have a standard posterior distribution, \cite{rossi2012bayesian} propose a ``tailored'' independent Metropolis--Hastings algorithm where the proposal distribution is a multivariate Student's $t$ distribution with $v$ degrees of freedom (tuning parameter), mean equal to the maximum likelihood estimator, and scale equal to the inverse of the Hessian matrix.\\

\textbf{Example: Simulation exercise}

Let's do a simulation exercise to check the performance of the Metropolis-Hastings algorithm to perform inference in the multinomial logit model. Assume a situation where there are three alternatives, one alternative-invariant regressor plus the intercept, and three alternative-varying regressors. The population parameters are $\bm{\beta}_1=[1 \ -2.5 \ 0.5 \ 0.8 \ -3]$, $\bm{\beta}_2=[1 \ -3.5 \ 0.5 \ 0.8 \ -3]$ and $\bm{\beta}_3=[0 \ 0 \ 0.5 \ 0.8 \ -3]$, the first two elements of the vectors are associated with the intercept and the alternative-invariant regressor, and the last three elements with the alternative-varying regressors. The sample size is 1000, and all regressors are simulated from standard normal distributions.

We can deploy our GUI using the command line at the beginning of this chapter. We should follow Algorithm \ref{alg:MultinomialLogit} to run multinomial logit models in our GUI (see Chapter \ref{chapGUI} for details):

\begin{algorithm}[h!]
	\caption{Multinomial logit models}\label{alg:MultinomialLogit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Multinomial Logit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Select the \textbf{Base Alternative}
		\State Select the \textbf{Number of choice categorical alternatives}
		\State Select the \textbf{Number of alternative specific variables}
		\State Select the \textbf{Number of Non-alternative specific variables} 
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax.
		\State Set the hyperparameters: mean vector and covariance matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameter for the Metropolis-Hastings algorithm, that is, the \textbf{Degrees of freedom: Multivariate Student's t distribution} 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following code in \textbf{R} shows how to implement the M-H algorithm from scratch. The first part simulates the dataset, the second part builds the loglikelihood function, and the third part implements the M-H algorithm. We use vague priors centered on zero, and covariance matrix $1000\bm{I}_{7}$. We observe that the posterior estimates closely match the population parameters, and all 95\% credible intervals contain the population parameters.  

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]		
remove(list = ls())
set.seed(12345)
# Simulation of data
N<-1000  # Sample Size
B<-c(0.5,0.8,-3); B1<-c(-2.5,-3.5,0); B2<-c(1,1,0)
# Alternative specific attributes of choice 1, for instance, price, quality and duration of choice 1
X1<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) 
# Alternative specific attributes of choice 2, for instance, price, quality and duration of choice 2
X2<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
# Alternative specific attributes of choice 3, for instance, price, quality and duration of choice 3
X3<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
X4<-matrix(rnorm(N,1,1),N,1)
V1<-B2[1]+X1%*%B+B1[1]*X4; V2<-B2[2]+X2%*%B+B1[2]*X4; V3<-B2[3]+X3%*%B+B1[3]*X4
suma<-exp(V1)+exp(V2)+exp(V3)
p1<-exp(V1)/suma; p2<-exp(V2)/suma; p3<-exp(V3)/suma
p<-cbind(p1,p2,p3)
y<- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE))
y1<-y==1; y2<-y==2; y3<-y==3
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Log likelihood
log.L<- function(Beta){
	V1<-Beta[1]+Beta[3]*X4+X1%*%Beta[5:7]
	V2<-Beta[2]+Beta[4]*X4+X2%*%Beta[5:7]
	V3<- X3%*%Beta[5:7]
	suma<-exp(V1)+exp(V2)+exp(V3)
	p11<-exp(V1)/suma; 	p22<-exp(V2)/suma; 	p33<-exp(V3)/suma
	suma2<-NULL
	for(i in 1:N){
		suma1<-y1[i]*log(p11[i])+y2[i]*log(p22[i])+y3[i]*log(p33[i])
		suma2<-c(suma2,suma1)}
	logL<-sum(suma2)
	return(-logL)
}
# Parameters: Proposal
k <- 7
res.optim<-optim(rep(0, k), log.L, method="BFGS", hessian=TRUE)
MeanT <- res.optim$par
ScaleT <- as.matrix(Matrix::forceSymmetric(solve(res.optim$hessian))) # Force this matrix to be symmetric
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
MHfunction <- function(iter, tuning){
	Beta <- rep(0, k); Acept <- NULL 
	BetasPost <- matrix(NA, iter, k)
	pb <- winProgressBar(title = "progress bar", min = 0, max = iter, width = 300)
	for(s in 1:iter){
		LogPostBeta <- -log.L(Beta) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(LaplacesDemon::rmvt(n=1, mu = MeanT, S = ScaleT, df = tuning))
		LogPostBetaC <- -log.L(BetaC) + mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
		alpha <- min(exp((LogPostBetaC-mvtnorm::dmvt(BetaC, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))-(LogPostBeta-mvtnorm::dmvt(Beta, delta = MeanT, sigma = ScaleT, df = tuning, log = TRUE))) ,1)
		u <- runif(1)
		if(u <= alpha){
			Acepti <- 1; Beta <- BetaC
		}else{
			Acepti <- 0; Beta <- Beta
		}
		BetasPost[s, ] <- Beta; Acept <- c(Acept, Acepti)
		setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),"% done"))
	}
	close(pb); AcepRate <- mean(Acept)
	Results <- list(AcepRate = AcepRate, BetasPost = BetasPost)
	return(Results)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# MCMC parameters
mcmc <- 10000; burnin <- 1000; thin <- 5; iter <- mcmc + burnin; keep <- seq(burnin, iter, thin); tuning <- 6 # Degrees of freedom
ResultsPost <- MHfunction(iter = iter, tuning = tuning)
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
Iterations = 1:2000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 2000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
			Mean      SD Naive SE Time-series SE
[1,]  0.9711 0.20162 0.004508       0.004508
[2,]  0.9742 0.20934 0.004681       0.004681
[3,] -2.4350 0.18950 0.004237       0.004137
[4,] -3.4195 0.24656 0.005513       0.005513
[5,]  0.5253 0.07396 0.001654       0.001654
[6,]  0.8061 0.08007 0.001790       0.001790
[7,] -3.0853 0.17689 0.003955       0.003955
2. Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
var1  0.5862  0.8367  0.9650  1.1017  1.3683
var2  0.5679  0.8310  0.9681  1.1151  1.3761
var3 -2.8239 -2.5607 -2.4291 -2.3050 -2.0812
var4 -3.9176 -3.5806 -3.4074 -3.2496 -2.9423
var5  0.3840  0.4761  0.5250  0.5759  0.6647
var6  0.6555  0.7494  0.8064  0.8616  0.9604
var7 -3.4476 -3.1991 -3.0777 -2.9641 -2.7500
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Ordered probit model}\label{sec66}

The ordered probit model is used when there is a natural order in the categorical response variable. In this case, there is a latent variable $y_i^*=\bm{x}_i^{\top}\bm{\beta}+\mu_i$, $\mu_i\stackrel{i.i.d.} {\thicksim} N(0,1)$ such that $y_i=l$ if and only if $\alpha_{l-1}<y_i^*\leq\alpha_l$, $l=\left\{1,2,\dots,L\right\}$, where $\alpha_0=-\infty$, $\alpha_1=0$ and $\alpha_L=\infty$.\footnote{Identification issues necessitate setting the variance in this model equal to 1 and $\alpha_1=0$. Observe that multiplying $y_i^*$ by a positive constant or adding a constant to all of the cut-offs and subtracting the same constant from the intercept does not affect $y_i$.} Then, $p(y_i=l)=\Phi(\alpha_l-\bm{x}_i^{\top}\bm{\beta})-\Phi(\alpha_{l-1}-\bm{x}_i^{\top}\bm{\beta})$, and the likelihood function is $p(\bm{\beta},\bm{\alpha}|\bm{y},\bm{X})=\prod_{i=1}^{N}p(y_i=l|\bm{\beta},\bm{\alpha},\bm{X})$.

There are independent priors of this model, $\pi(\bm{\beta},\bm{\gamma})=\pi(\bm{\beta})\times \pi(\bm{\gamma})$, where $\bm{\beta}\sim N(\bm{\beta}_0,\bm{B}_0)$ and $\bm{\gamma}\sim N(\bm{\gamma}_0,\bm{\Gamma}_0)$, $\bm{\gamma}=\left[\gamma_2,\gamma_3,\dots,\gamma_{L-1}\right]^{\top}$, such that $\bm{\alpha}=\left[\exp\left\{\gamma_2\right\},\sum_{l=2}^3\exp\left\{\gamma_l\right\},\dots,\sum_{l=2}^{L-1}\exp\left\{\gamma_l\right\}\right]^{\top}$. The latter structure imposes the ordinal condition in the cut-offs.

This model does not have a standard conditional posterior distribution for $\bm{\gamma}$ ($\bm{\alpha}$), but it does have a standard conditional distribution for $\bm{\beta}$ once data augmentation is used. Then, we can use a Metropolis-within-Gibbs sampling algorithm. In particular, we use Gibbs sampling algorithms to draw $\bm{\beta}$ and $\bm{y}^*$,
\begin{equation*}
	\bm{\beta}|\bm{y}^*, \bm{\alpha}, \bm{X} \sim N(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{\top}\bm{X})^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}\bm{y}^*)$, and $y_i^*|\bm{\beta},\bm{\alpha},\bm{y},\bm{X}\sim TN_{(\alpha_{y_i-1},\alpha_{y_i})}(\bm{x}_i^{\top}\bm{\beta},1)$.

We use a random-walk Metropolis--Hastings algorithm for $\bm{\gamma}$ that has as proposal a Gaussian distribution with mean equal to the current value, and covariance matrix $s^2(\bm{\Gamma}_0^{-1}+\hat{\bm{\Sigma}}_{\gamma}^{-1})^{-1}$, where $s>0$ is a tuning parameter, and $\hat{\bm{\Sigma}}_{\gamma}$ is the sample covariance matrix associated with $\gamma$ from the maximum likelihood estimation.\\

\textbf{Example: Determinants of preventive health care visits}

We used the file named \textit{2HealthMed.csv} in this applications. In particular, the dependent variable is \textit{MedVisPrevOr}, which is an ordered variable equal to 1 if the individual did not visit a physician for preventive reasons, 2 if the individual visited once in that year, and so on, until it is equal to 6 for visiting five or more times. The latter category is 1.6\% of the sample. Observe that the dependent variable has six categories.

In this example, the set of regressors is given by \textit{SHI}, which an indicator of being in the subsidized health care system (1 means being in the system), sex (\textit{Female}), age (linear and squared), socioeconomic conditions indicator (\textit{Est2} and \textit{Est3}), the lowest is tha baseline category, self perception of health status (\textit{Fair}, \textit{Good} and \textit{Excellent}), where \textit{Bad} is the baseline, and education level, primary (\textit{PriEd}), high school (\textit{HighEd}), vocational (\textit{VocEd}), and university (\textit{UnivEd}), \textit{no education} is the baseline category.

We ran this application with 50,000 MCMC iterations plus 10,000 as burn-in, and thinning parameter equal to 5. This setting means 10,000 effective posterior draws. We set $\bm{\beta}_0=\bm{0}_{11}$, $\bm{B}_0=1000\bm{I}_{11}$, $\bm{\gamma}_0=\bm{0}_4$, $\bm{\Gamma}_0=\bm{I}_4$, and the tuning parameter is 1.

We can run the ordered probit models in our GUI following the steps in the Algorithm \ref{alg:OrderedProbit}.
 
\begin{algorithm}[h!]
	\caption{Ordered probit models}\label{alg:OrderedProbit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Ordered Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax. Remember that this formula must have -1 to omit the intercept in the specification.
		\State Set the hyperparameters: mean vectors and covariance matrices. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameter for the Metropolis-Hastings algorithm 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code shows how to perform inference in this model using the command \textit{rordprobitGibbs} from the \textit{bayesm} library, which is the command that our GUI uses. We ask in exercise 7 to program from scratch a Metropolis-within-Gibbs sampling algorithm for this application. 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of preventive health care visits}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
Data <- read.csv("DataApplications/2HealthMed.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- MedVisPrevOr 
# MedVisPrevOr: Oredered variable for preventive visits to doctors in one year: 1 (none), 2 (once), ... 6 (five or more)
X <- cbind(SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent, PriEd, HighEd, VocEd, UnivEd)
k <- dim(X)[2]
L <- length(table(y))
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
gamma0 <- rep(0, L-2); Gamma0 <- diag(L-2)
# MCMC parameters
mcmc <- 60000+1; thin <- 5; tuningPar <- 1/(L-2)^0.5
DataApp <- list(y = y, X = X, k = L)
Prior <- list(betabar = b0, A = solve(B0), dstarbar = gamma0, Ad = Gamma0)
mcmcpar <- list(R = mcmc, keep = 5, s = tuningPar)
PostBeta <- bayesm::rordprobitGibbs(Data = DataApp, Prior = Prior, Mcmc = mcmcpar)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of preventive health care visits, results}
	\begin{VF}
		\begin{lstlisting}[language=R]
BetasPost <- coda::mcmc(PostBeta[["betadraw"]])
colnames(BetasPost) <- c("SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent", "PriEd", "HighEd", "VocEd", "UnivEd")
summary(BetasPost)		
Iterations = 1:12000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 12000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean        SD  Naive SE Time-series SE
SHI        0.0654824 2.281e-02 2.082e-04      3.357e-04
Female    -0.0374788 1.908e-02 1.742e-04      1.742e-04
Age        0.0190336 1.869e-03 1.706e-05      4.576e-05
Age2      -0.0002328 2.438e-05 2.225e-07      6.690e-07
Est2       0.0949445 2.226e-02 2.032e-04      4.659e-04
Est3      -0.1383965 3.411e-02 3.114e-04      3.459e-04
Fair       0.6451828 5.375e-02 4.907e-04      3.924e-03
Good       0.7343932 4.955e-02 4.523e-04      4.491e-03
Excellent  0.9826531 6.393e-02 5.836e-04      5.261e-03
PriEd      0.0309418 2.376e-02 2.169e-04      2.221e-04
HighEd    -0.1805753 2.910e-02 2.656e-04      3.456e-04
VocEd      0.1395760 9.640e-02 8.800e-04      9.291e-04
UnivEd    -0.2218120 1.189e-01 1.086e-03      1.086e-03
2. Quantiles for each variable:
				2.5%        25%        50%        75%      97.5%
SHI        0.02090  0.04995  0.06540  0.08085  0.11021
Female    -0.07463 -0.05042 -0.03777 -0.02456  0.00023
Age        0.01550  0.01781  0.01902  0.02023  0.02268
Age2      -0.00028 -0.00024 -0.00023 -0.00021 -0.00018
Est2       0.05149  0.08004  0.09482  0.10968  0.13933
Est3      -0.20559 -0.16144 -0.13815 -0.11563 -0.07179
Fair       0.55799  0.61295  0.64148  0.67268  0.74395
Good       0.66690  0.70808  0.73032  0.75406  0.81064
Excellent  0.88919  0.94770  0.97836  1.01026  1.08460
PriEd     -0.01584  0.01493  0.03101  0.04718  0.07732
HighEd    -0.23782 -0.20035 -0.18021 -0.16073 -0.12435
VocEd     -0.04911  0.07474  0.13811  0.20414  0.33331
UnivEd    -0.45381 -0.30239 -0.22193 -0.14148  0.00863
# Convergence diagnostics
coda::geweke.diag(BetasPost)
coda::raftery.diag(BetasPost,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(BetasPost)
# Cut offs
Cutoffs <- PostBeta[["cutdraw"]]
summary(Cutoffs)
coda::geweke.diag(Cutoffs)
coda::heidel.diag(Cutoffs)
coda::raftery.diag(Cutoffs[,-1],q=0.5,r=0.05,s = 0.95)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

The results suggest that older individuals (at decreasing rate) in the subsidized health program, characterized in the second socioeconomic status with increasing good self perception of health condition, and not having high school as their highest education degree, have a higher probability of visiting a physician for preventive health aims. Convergence diagnostics look well, except for the self health perception draws.

We also got the posterior estimates of the cutoffs in the ordered probit model.
These estimates are necessary to calculate the probability that an individual is in a specific category of visiting physicians. Due to identification restrictions, the first cutoff is set equal to 0. That is why we have \textit{NaN} values in \cite{Geweke1992} and \cite{Heidelberger1983} tests, and we observe only four values in the \cite{Raftery1992} test, which correspond to the remaining free cutoffs. It seems that these cutoff estimates have some convergence issues when taking as diagnostic tool the \cite{Raftery1992} test.
Their dependence factors are also very high.

\section{Negative binomial model}\label{sec67}

The dependent variable in the negative binomial model is a nonnegative integer or count. In contrast to the Poisson model, the negative binomial model takes into account over-dispersion. The Poisson model has equal mean and variance (equi-dispersion).

We assume that $y_i\stackrel{i.n.d.} {\thicksim}NB(\gamma,\theta_i)$, that is, the density function for individual $i$ is $\frac{\Gamma(y_i+\gamma)}{\Gamma(\gamma)y_i!}(1-\theta_i)^{y_i}\theta_i^{\gamma}$, where the success probability is $\theta_i=\frac{\gamma}{\lambda_i+\gamma}$, $\lambda_i=\exp\left\{\bm{x}_i^{\top}\bm{\beta}\right\}$ is the mean, and $\gamma=\exp\left\{\alpha \right\}$ is the target for number of successful trials, or dispersion parameter.

We assume independent priors for this model are $\bm{\beta} \sim N(\bm{\beta}_0,\bm{B}_0)$ and 
$\alpha \sim G(\alpha_0, \delta_0)$.\footnote{$G$ denotes a gamma density.}

This model does not have standard conditional posterior distributions, so \cite{rossi2012bayesian} use a random-walk Metropolis--Hastings algorithm where the proposal distribution for $\bm{\beta}$ is Gaussian centered at the current stage with covariance matrix $s_{\bm{\beta}}^2\hat{\bm{\Sigma}}_{\bm{\beta}}$ where $s_{\bm{\beta}}$ is a tuning parameter and $\hat{\bm{\Sigma}}_{\bm{\beta}}$ is the maximum likelihood covariance estimator. In addition, the proposal for $\alpha$ is normal centered at the current value, with variance $s_{\alpha}^2\hat{\sigma}_{\alpha}^2$ where $s_{\alpha}$ is a tuning parameter and $\hat{\sigma}_{\alpha}^2$ is the maximum likelihood variance estimator.\\

\textbf{Example: Simulation exercise}

Let's do a simulation exercise to check the performance of the M-H algorithms in the negative binomial model. There are two regressors, $x_{i1}\sim U(0,1)$ and $x_{i1}\sim N(0,1)$, and the intercept. The dispersion parameter is $\gamma=\exp\left\{1.2\right\})$, and $\bm{\beta}=\left[1 \ 1 \ 1\right]$. The sample size is 1,000.

We run this simulation using 10,000 MCMC iterations, a burn-in  equal to 1,000, and a thinning parameter equal to 5. We set vague priors for the location parameters, particularly,  $\bm{\beta}_0=\bm{0}_{3}$ and $\bm{B}_0=1000\bm{I}_{3}$, and $\alpha_0=0.5$ and $\delta_0=0.1$, which are the default values in the \textit{rnegbinRw} command from \textit{bayesm} package in \textbf{R}. In addition, the tuning parameters of the Metropolis--Hastings algorithms are $s_{\beta}=2.93/k^{1/2}$ and $s_{\alpha}=2.93$, which are also the default parameters in \textit{rnegbinRw}, $k$ is the number of location parameters.

We can run the negative binomial models in our GUI following the steps in the Algorithm \ref{alg:NegativeBinomial}.

\begin{algorithm}[h!]
	\caption{Negative binomial models}\label{alg:NegativeBinomial}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Univariate Models} on the top panel
		\State Select \textit{Negative Binomial (Poisson)} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend. You should see a preview of the dataset
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select dependent and independent variables using the \textit{Formula builder} table
		\State Click the \textit{Build formula} button to generate the formula in \textbf{R} syntax.
		\State Set the hyperparameters: mean vector, covariance matrix, shape and scale parameters. This step is not necessary as by default our GUI uses non-informative priors
		\State Select the tuning parameters for the Metropolis-Hastings algorithms 
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code shows how to perform inference in the negative binomial model programming the M-H algorithms from scratch. We ask to estimate this example using the \textit{rnegbinRw} command in exercise 8.

We observe from the results that all 95\% credible intervals encompass the population parameters, and the posterior means are very close to the population parameters.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(010101)
N <- 2000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2); k <- dim(X)[2]; B <- rep(1, k)
alpha <- 1.2; gamma <- exp(alpha); lambda <- exp(X%*%B)
y <- rnbinom(N, mu = lambda, size = gamma)
# log likelihood
logLik <- function(par){
	alpha <- par[1]; beta <- par[2:(k+1)]
	gamma <- exp(alpha)
	lambda <- exp(X%*%beta)
	logLikNB <- sum(sapply(1:N, function(i){dnbinom(y[i], size = gamma, mu = lambda[i], log = TRUE)}))
	return(-logLikNB)
}
# Parameters: Proposal
par0 <- rep(0.5, k+1)
res.optim <- optim(par0, logLik, method="BFGS", hessian=TRUE)
res.optim$par
res.optim$convergence
Covar <- solve(res.optim$hessian) 
CovarBetas <- Covar[2:(k+1),2:(k+1)]
VarAlpha <- Covar[1:1]
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
alpha0 <- 0.5; delta0 <- 0.1
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model, M-H algorithm}
	\begin{VF}
		\begin{lstlisting}[language=R]
# Metropolis-Hastings function 
MHfunction <- function(iter, sbeta, salpha){
	Beta <- rep(0, k); 	Acept1 <- NULL; Acept2 <- NULL
	BetasPost <- matrix(NA, iter, k); alpha <- 1
	alphaPost <- rep(NA, iter); par <- c(alpha, Beta)
	pb <- winProgressBar(title = "progress bar", min = 0, max = iter, width = 300)
	for(s in 1:iter){
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		BetaC <- c(MASS::mvrnorm(1, mu = Beta, Sigma = sbeta^2*CovarBetas))
		parC <- c(alpha, BetaC)
		LogPostBetaC <- -logLik(parC) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(BetaC, mean = b0, sigma = B0, log = TRUE)
		alpha1 <- min(exp((LogPostBetaC - mvtnorm::dmvnorm(BetaC, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))-(LogPostBeta - mvtnorm::dmvnorm(Beta, mean = Beta, sigma = sbeta^2*CovarBetas, log = TRUE))),1)
		u1 <- runif(1)
		if(u1 <= alpha1){Acept1i <- 1; Beta <- BetaC}else{
			Acept1i <- 0; Beta <- Beta
		}
		par <- c(alpha, Beta)
		LogPostBeta <- -logLik(par) + dgamma(alpha, shape = alpha0, scale = delta0, log = TRUE) + mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alphaC <- rnorm(1, mean = alpha, sd = salpha*VarAlpha^0.5)
		parC <- c(alphaC, Beta)
		LogPostBetaC <- -logLik(parC) + dgamma(alphaC, shape = alpha0, scale = delta0, log = TRUE) +  mvtnorm::dmvnorm(Beta, mean = b0, sigma = B0, log = TRUE)
		alpha2 <- min(exp((LogPostBetaC - dnorm(alphaC, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))-(LogPostBeta - dnorm(alpha, mean = alpha, sd = salpha*VarAlpha^0.5, log = TRUE))),1)
		u2 <- runif(1)
		if(u2 <= alpha2){Acept2i <- 1; alpha <- alphaC}else{
			Acept2i <- 0; alpha <- alpha
		}
		
		BetasPost[s, ] <- Beta; alphaPost[s] <- alpha
		Acept1 <- c(Acept1, Acept1i); Acept2 <- c(Acept2, Acept2i)
		setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),"% done"))
	}
	close(pb)
	AcepRateBeta <- mean(Acept1); AcepRateAlpha <- mean(Acept2)
	Results <- list(AcepRateBeta = AcepRateBeta, AcepRateAlpha = AcepRateAlpha, BetasPost = BetasPost, alphaPost = alphaPost)
	return(Results)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model, results}
	\begin{VF}
		\begin{lstlisting}[language=R]
# MCMC parameters
mcmc <- 10000
burnin <- 1000
thin <- 5
iter <- mcmc + burnin
keep <- seq(burnin, iter, thin)
sbeta <- 2.93/sqrt(k); salpha <- 2.93
# Run M-H
ResultsPost <- MHfunction(iter = iter, sbeta = sbeta, salpha = salpha)
ResultsPost$AcepRateBeta
ResultsPost$AcepRateAlpha
summary(coda::mcmc(ResultsPost$BetasPost[keep[-1], ]))
Iterations = 1:2000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 2000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
			Mean      SD  Naive SE Time-series SE
[1,] 1.0270 0.04799 0.0010730      0.0014727
[2,] 0.9981 0.07752 0.0017333      0.0024262
[3,] 0.9677 0.02343 0.0005239      0.0007182
2. Quantiles for each variable:
			2.5%    25%    50%    75% 97.5%
var1 0.9343 0.9943 1.0255 1.0592 1.122
var2 0.8445 0.9448 0.9980 1.0520 1.144
var3 0.9242 0.9512 0.9678 0.9839 1.013
summary(coda::mcmc(ResultsPost$alphaPost[keep[-1]]))
Iterations = 1:2000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 2000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
			Mean             SD       Naive SE Time-series SE 
1.282664       0.058769       0.001314       0.001427 
2. Quantiles for each variable:
2.5%   25%   50%   75% 97.5% 
1.173 1.242 1.282 1.320 1.407
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 


\section{Summary}\label{sec611}
We present ...

\section{Exercises}\label{sec612}

\begin{enumerate}
	\item Get the posterior conditional distributions of the Gaussian linear model assuming independent priors $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$.
	
	\item Show that the posterior conditional distributions of the Gaussian linear model with heteroskedasticity assuming independent priors $\pi(\bm{\beta},\sigma^2,\tau)=\pi(\bm{\beta})\times\pi(\sigma^2)\times\prod_{i=1}^N\pi(\tau_i)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$, $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$ and $\tau_i\sim G(v/2,v/2)$ are $\bm{\beta}|\sigma^2,\tau,{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$, $\sigma^2|\bm{\beta},\tau,{\bf{y}},{\bf{X}}\sim IG(\alpha_n,\delta_n)$ and $\tau_i|\bm{\beta},\sigma^2,{\bf{y}},{\bf{X}}\sim G(v_{1n},v_{2in})$, where $\tau=[\tau_1,\dots,\tau_n]^{\top}$, ${\bf{B}}_n=({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}\bm{\Psi}{\bf{X}})^{-1}$, $\bm{\beta}_n={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}\bm{\Psi}{\bf{y}})$, $\alpha_n=\alpha_0+N$, $\delta_n=\delta_0+({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\bm{\Psi}({\bf{y}}-{\bf{X}}\bm{\beta})$, $v_{1n}=v+1$, $v_{2in}=v+\sigma^{-2}(y_i-{\bf{x}}_i^{\top}\bm{\beta})^2$, and $\bm{\Psi}=\text{diagonal}\left\{\tau_i\right\}$.
	
	\item \textbf{The market value of soccer players in Europe continues}
	
	Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the the market value of soccer players in Europe setting $v=5$ and same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case?
	
	\item \textbf{Example: Determinants of hospitalization continues}
	
	Program a Gibbs sampling algorithm in the application of determinants of hospitalization.
	
	\item \textbf{Choice of the fishing mode continues} 
	
	Run the Algorithm \ref{alg:MultinomialProbit} of the book to show the results of the Geweke \cite{Geweke1992}, Raftery \cite{Raftery1992} and Heidelberger \cite{Heidelberger1983} tests using our GUI.
	
	\item \textbf{Simulation exercise of the multinomial logit model continues}
	
	Perform inference in the simulation of the multinomial logit model using the command \textit{rmnlIndepMetrop} from the \textit{bayesm} package of \textbf{R} and using our GUI.
	
	\item \textbf{Determinants of preventive health care visits continues}
	
	Program from scratch a Metropolis-within-Gibbs sampling algorithm for the application of determinants of preventive health care visits.
	
	\item \textbf{Simulation of the negative binomial model continues}
	
	Perform inference in the simulation of the negative binomial model using the \textit{bayesm} package in \textbf{R} software.   

\end{enumerate}
