\chapter{Conceptual differences of the Bayesian and Frequentist approaches}\label{chap2}

We give some of the conceptual differences between the Bayesian and Frequentist inferential approaches. We emphasize in the Bayesian concepts as most of the readers can be familiarized with the Frequentist statistical framework.

\section{The concept of probability}\label{sec21}

Let's begin with the following thought experiment: Assume that you are watching the international game show ``Who wants to be a millionaire?", the contestant is asked to answer a very simple question: \textbf{What is the last name of the brothers who are credited with inventing  the world's first successful motor-operated airplane?}

\begin{itemize}
	\item What is the probability that the contestant answers this question correctly? 
\end{itemize}

Unless you have: 

\begin{enumerate}
	\item  watched this particular contestant participating in this show many times,
	\item seen him asked this same question each time, 
	\item and computed the relative frequency with which he gives the correct answer,   
\end{enumerate}
 
you need to answer this question as a Bayesian!

Uncertainty about the event \textit{answer this question} needs to be expressed as a ``degree of belief" informed both by information coming from data on the skill of the particular participant, and how much he knows about inventors, and possibly prior knowledge on his performance in other game shows. Of course, your prior knowledge of the contestant may be minimal, or it may be very informed. Either way, your final answer remains a degree of belief held about an uncertain, and inherently unrepeatable state of nature.

The point of this hypothetical, light-hearted scenario is simply to highlight that a key distinction between the Frequentist and Bayesian approaches to inference is not the use (or nature) of prior information, but simply the manner in which probability is used. To the Bayesian, probability is the mathematical construct used to quantify uncertainty about an unknown state of nature, conditional on observed data and prior knowledge about the context in which that state of nature occurs. To the Frequentist, probability is linked intrinsically to the concept of a repeated experiment, and the relative frequency with which a particular outcome occurs, conditional on that unknown state. This distinction remains key whether the Bayesian chooses to be \textit{informative or subjective} in the specification of prior information, or chooses to be \textit{non-informative or objective}.

Frequentists consider probability as a physical phenomenon, like mass or wavelength, whereas Bayesians stipulate that probability lives in the mind of scientists as any scientific construct \cite{Parmigiani2008}.

It seems that the understanding of the concept of probability for the common human being is more associated with ``degrees of belief" rather than relative frequency. Peter Diggle, President of The Royal Statistical Society between 2014 and 2016, was asked in an interview ``A different trend which has surged upwards in statistics during Peter's career is the popularity of Bayesian statistics. Does Peter consider himself a Bayesian?", and he replied ``... you can't not believe in Bayes' theorem because it's true. But that doesn't make you a Bayesian in the philosophical sense. When people are making personal decisions -- even if they don't formally process Bayes' theorem in their mind -- they are adapting what they think they should believe in response to new evidence as it comes in. Bayes' theorem is just the formal mathematical machinery for doing that."

However, we should say that psychological experiments suggest that human beings suffer from \textit{anchoring}, that is, a cognitive bias that causes us to rely too heavily on the previous information (prior) such that the updating process (posterior) due to new information (likelihood) being low compared to the Bayes' rule \cite{daniel2017thinking}.

\section{Subjectivity is not the key}\label{sec22}

The concepts of \textit{subjectivity} and \textit{objectivity} indeed characterize both statistical paradigms in differing ways. Among Bayesians there are those who are immersed in \textit{subjective} rationality \cite{Ramsey1926, deFinetti1937, savage1954, Lindley2000}, but others who adopt \textit{objective} prior distributions such as Jeffreys', reference, empirical or robust \cite{Bayes1763, Laplace1812, Jeffreys1961, Berger2006} to operationalize Bayes' rule, and thereby weight quantitative (data-based) evidence. Among Frequentists, there are choices made about significance levels which, if not explicitly subjective, are typically not grounded in any objective and documented assessment of the relative losses of Type I and Type II errors.\footnote{Type I error is rejecting the null hypothesis when this is true, and the Type II error is not rejecting the null hypothesis when this is false.} In addition, both Frequentist and Bayesian statisticians make decisions about the form of the data generating process, or ``model", which -- if not subject to rigorous diagnostic assessment -- retains a subjective element that potentially influences the final inferential outcome. Although we all know that by definition a model is a schematic and simplified approximation to reality, 

``Since all models are wrong the scientist cannot obtain a \textit{correct} one by excessive elaboration. On the contrary following William of Occam he should seek an economical description of natural phenomena." \cite{Box1976}.


We also know that ``All models are wrong, but some are useful" \cite{box1979robustness}, that is why model diagnostics are important. This task can be performed in both approaches. Particularly, the Bayesian framework can use predictive \textit{p}--values for absolute testing \cite{Gelman1996,Bayarri2000} or posterior odds ratios for relative statements \cite{Jeffreys1935, Kass1995}. This is because the marginal likelihood, conditional on data, is interpreted as the evidence of the prior distribution \cite{berger93}.

In addition, what is objectivity in a Frequentist approach? For example, why should we use a 5\% or 1\% significance level rather than any other value? As someone said, the apparent objectivity is really a consensus \cite{Lindley2000}. In fact ``Student" (William Gosset) saw statistical significance at any level as being ``nearly valueless" in itself \cite{Ziliak2008}. But, this is not just a situation in the Frequentist approach. The cut--offs given to ``establish" scientific evidence against a null hypothesis in terms of $log_{10}$ scale \cite{Jeffreys1961} or $log_{e}$ scale \cite{Kass1995} in Table \ref{tab:guide} are also \textit{ad hoc}.

Although the true state of nature in Bayesian inference is expressed in ``degrees of belief", the distinction between the two paradigms does not reside in one being more, or less, \textit{subjective} than the other. Rather, the differences are philosophical, pedagogical, and methodological.

\section{Estimation, hypothesis testing and prediction}\label{sec23}

All what is required to perform estimation, hypothesis testing (model selection) and prediction in the Bayesian approach is to apply the Bayes' rule. This means coherence under a probabilistic view. But, there is no free lunch, coherence reduces flexibility. On the other hand, the Frequestist approach may be not coherent from a probabilistic point of view, but it is very flexible. This approach can be seen as a tool kit that offers inferential solutions under the umbrella of understanding probability as relative frequency. For instance, a point estimator in a Frequentist approach is found such that satisfies good sampling properties like unbiasness, efficiency, or a large sample property as consistency.

A remarkable difference is that optimal Bayesian decisions are calculated minimizing the expected value of the loss function with respect to the posterior distribution, that is, it is conditional on observed data. On the other hand, Frequentist ``optimal" actions are base on the expected values over the distribution of the estimator (a function of data) conditional on the unknown parameters, that is, it considers sampling variability.

The Bayesian approach allows to obtain the posterior distribution of any unknown object such as parameters, latent variables, future or unobserved variables or models. A nice advantage is that prediction can take into account estimation error, and predictive distributions (probabilistic forecasts) can be easily recovered. 

Hypothesis testing (model selection) is based on \textit{inductive logic} reasoning (\textit{inverse probability}); on the basis of what we see, we evaluate what hypothesis is most tenable, and is performed using posterior odds, which in turn are based on Bayes factors that evaluate evidence in favor of a null hypothesis taking explicitly the alternative \cite{Kass1995}, following the rules of probability \cite{Lindley2000}, comparing how well the hypothesis predicts data \cite{Goodman1999}, minimizing the weighted sum of type I and type II error probabilities \cite{DeGroot1975,Pericchip}, and taking the implicit balance of losses \cite{Jeffreys1961,Bernardo1994} into account. Posterior odds allows to use the same framework to analyze nested and non-nested models and perform model average. However, Bayes factors cannot be based on improper or vague priors \cite{koop2003bayesian}, the practical interplay between model selection and posterior distributions is not as easy as it maybe in the Frequentist approach, and the computational burden can be more demanding due to solving potentially difficult integrals.

On the other hand, the Frequentist approach establishes most of its estimators as the solution of a system of equations. Observe that optimization problems reduce to solve systems. We can potentially get the distribution of these estimators, but most of the time it is needed asymptotic arguments or resampling techniques. Hypothesis testing requires pivotal quantities and/or also resampling, and prediction most of the time is based on a \textit{plug-in approach}, which means not taking estimation error into account.\footnote{A pivot quantity is a function of unobserved parameters and observations whose probability distribution does not depend on the unknown parameters.} In addition, ancillary statistics can be used to build prediction intervals.\footnote{An ancillary statistic is a pivotal quantity that is also a statistic.} Comparing models depends on their structure, for instance, there are different Frequentist statistical approaches to compare nested and non-nested models. A nice feature in some situations is that there is a practical interplay between hypothesis testing and confidence intervals, for instance in the normal population mean hypothesis framework you cannot reject at $\alpha$ significance level (Type I error) any null hypothesis $H_0. \  \mu=\mu^0$ if $\mu^0$ is in the $1-\alpha$ confidence interval $P(\mu\in[\hat{\mu}-|t_{N-1}^{\alpha/2}|\times\hat \sigma_{\hat{\mu}},\hat{\mu}+|t_{N-1}|^{\alpha/2}\times \hat\sigma_{\hat{\mu}}])=1-\alpha$, where $\hat{\mu}$ and $\hat\sigma_{\hat{\mu}}$ are the maximum likelihood estimators of the mean and standard error, and $t_{N-1}^{\alpha/2}$ is the quantile value of the Student's t distribution at $\alpha/2$ probability and $N-1$ degrees of freedom, $N$ is the sample size.

A remarkable difference between the Bayesian and the Frequentist inferential frameworks is the interpretation of credible/confidence intervals. Observe that once we have estimates, such that for example the previous interval is $[0.2, 0.4]$ given a 95\% confidence level, we cannot say that $P(\mu\in [0.2, 0.4])=0.95$ in the Frequentist framework. In fact, this probability is 0 or 1 under this approach, as $\mu$ can be there or not, the problem is that we will never know in applied settings. This due to that $P(\mu\in[\hat{\mu}-|t_{N-1}^{0.025}|\hat\times \sigma_{\hat{\mu}},\hat{\mu}+|t_{N-1}^{0.025}|\times \hat\sigma_{\hat{\mu}}])=0.95$ being in the sense of repeated sampling. On the other hand, once we have the posterior distribution, we can say that $P(\mu\in [0.2, 0.4])=0.95$ under the Bayesian framework.      

Following common practice, most of researchers and practitioners do hypothesis testing based on the \textit{p}-value in the Frequentist framework. But, \textbf{what is a \textit{p}--value?} Most of the users do not know the answer due to many time statistical inference is not performed by statisticians \cite{Berger2006}.\footnote{https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/} A \textit{p}--value is the probability of obtaining a statistical summary of the data equal to or \textit{more extreme} than what was actually observed, assuming that the null hypothesis is true.

Therefore, \textit{p}--value calculations involve not just the observed data, but also more \textit{extreme} hypothetical observations. So,

``What the use of \textit{p} implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred."\cite{Jeffreys1961}

It seems that common Frequentist inferential practice intertwined two different logic reasoning arguments: the \textit{p}--value \cite{Fisher1958} and \textit{significance level} \cite{Neyman1933}. The former is an informal short--run criterion, whose philosophical foundation is \textit{reduction to absurdity}, which measures the discrepancy between the data and the null hypothesis. So, the \textit{p}--value is not a direct measure of the probability that the null hypothesis is false. The latter, whose philosophical foundations is \textit{deduction}, is based on a long--run performance such that controls the overall number of incorrect inferences in the repeated sampling without care of individual cases. The \textit{p}--value fallacy consists in interpreting the \textit{p}--value as the strength of evidence against the null hypothesis, and using it simultaneously with the frequency of type I error under the null hypothesis \cite{Goodman1999}.

The American Statistical Association has several concerns regarding the use of the \textit{p}--value as a cornerstone to perform hypothesis testing in science. This concern motivates the ASA's statement on \textit{p}--values \cite{Wasserstein2016}, which can be summarized in the following principles:

\begin{itemize}
	\item ``P--values can indicate how incompatible the data are with a specified statistical model." 
	\item ``P--values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone."
	\item ``Scientific conclusions and business or policy decisions should not be based only on whether a \textit{p}--value passes a specific threshold."
	\item ``Proper inference requires full reporting and transparency."
	\item ``A \textit{p}--value, or statistical significance, does not measure the size of an effect or the importance of a result."
	\item ``By itself, a \textit{p}--value does not provide a good measure of evidence regarding a model or hypothesis." 
\end{itemize}

To sum up, Fisher proposed the \textit{p}-value as a witness rather than a judge. So, a \textit{p}-value lower than the significance level means more inspection of the null hypothesis, but it is not a final conclusion about it. 

Another difference between the Frequentists and the Bayesians is the way how scientific hypothesis are tested. The former use the \textit{p}-value, whereas the latter use the Bayes factor. Observe that the \textit{p}--value is associated with the probability of the data given the hypothesis, whereas the Bayes factor is associated with the probability of the hypothesis given the data. However, there is an approximate link between the $t$ statistic and the Bayes factor for regression coefficients \cite{Raftery1995}. In particular, $|t|>(log(N)+6)^{1/2}$, corresponds to strong evidence in favor of rejecting the not relevance of a control in a regression. Observe that in this setting the threshold of the $t$ statistic, and as a consequence the significant level, depends on the sample size. Observe that this setting agrees with the idea in experimental designs of selecting the sample size such that we control Type I and Type II errors. In observational studies we cannot control the sample size, but we can select the significance level.

See also \cite{sellke2001calibration} and \cite{benjamin2018redefine} for nice exercises to reveal potential flaws of the \textit{p}--value ($p$) due to $p\sim U[0,1]$ under the null hypothesis,\footnote{https://joyeuserrance.wordpress.com/2011/04/22/proof-that-p-values-under-the-null-are-uniformly-distributed/ for a simple proof.} and calibrations of the \textit{p}-value to interpret them as the odds ratio and the error probability. In particular, $B(p)=-e\times p\times\log(p)$ when $p<e^{-1}$, and interpret this as the Bayes factor of $H_0$ to $H_1$, where $H_1$ denotes the unspecified alternative to $H_0$, and $\alpha(p)=(1+[-e\times p\times \log(p)]^{-1})^{-1}$ as the error probability $\alpha$ in rejecting $H_0$. Take into account that $B(p)$ and $\alpha(p)$ are lower bounds.

Logic of argumentation in the Frequentist approach is based on \textit{deductive logic}, this means that it starts from a statement about the true state of nature (null hypothesis), and predicts what should be seen if this statement were true. On the other hand, the Bayesian approach is based on \textit{inductive logic}, this means that it defines what hypothesis is more consistent with what is seen. The former inferential approach establishes that the true of the premises implies the true of the conclusion, that is why we reject or not reject hypothesis. The latter establishes that the premises supply some evidence, but not full assurance, of the true of the conclusion, that is why we get probabilistic statements.

Here, there is a difference between \textit{effects of causes} (forward causal inference) and \textit{causes of effects} (reverse causal inference) \cite{Gelman2013,Dawid2016}. To illustrate this point, imagine that a firm increases the price of a specific good, then economic theory would say that its demand decreases. The premise (null hypothesis) is a price increase, and the consequence is a demand reduction. Another view would be to observe a demand reduction, and try to identify which cause is more tenable. For instance, demand reduction can be caused by any positive supply shocks or any negative demand shocks. The Frequentist logic sees the first view, and the Bayesian reasoning gives the probability associated with possible causes.

\section{The likelihood principle}\label{sec24} 

The \textbf{likelihood principle} states that in making inference or decisions about the state of the nature all the relevant \textit{experimental} information is given by the \textit{likelihood function}. The Bayesian framework follows this statement, that is, it is conditional on observed data.

We follow \cite{berger93}, who in turns followed \cite{Lindley76}, to illustrate the likelihood principle. We are given a coin such that we are interested in the probability, $\theta$, of having it come up heads when flipped. It is desired to test $H_0. \ \theta=1/2$ versus $H_1. \ \theta>1/2$. An experiment is conducted by flipping the coin (independently) in a series of trials, the results of which is the observation of 9 heads and 3 tails.

This is not yet enough information to specify $p(y|\theta)$, since the series of trials was not explained. Two possibilities:

\begin{enumerate}
	\item The experiment consisted of a predetermine 12 flips, so that $Y=\left[Heads\right]$ would be ${B}(12,\theta)$, then $p_1(y|\theta)={\binom{{n}}{{y}}}\theta^y(1-\theta)^{n-y}=220\times\theta^9(1-\theta)^{3}.$
	\item The experiment consisted of flipping the coin until 3 tails were observed ($r=3$). Then, $Y$, the number of failures (heads) until getting 3 tails, is ${N}{B}(3,1-\theta)$. Then, $p_2(y|\theta)={\binom{{y+r-1}}{{r-1}}}(1-(1-\theta)^y(1-\theta)^{r}=55\times\theta^9(1-\theta)^{3}.$  
\end{enumerate}

Using a Frequentist approach, the significance level of $y=9$ using the Binomial model against $\theta=1/2$ would be:

\begin{equation*}
	\alpha_1=P_{1/2}(Y\geq
	9)=p_1(9|1/2)+p_1(10|1/2)+p_1(11|1/2)+p_1(12|1/2)=0.073.
\end{equation*}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The likelihood principle: Binomial model}
\begin{VF}
\begin{lstlisting}[ language=R]
success <- 9 
# Number of observed success in n trials
n <- 12 
# Number of trials
siglevel <- sum(sapply(9:n,function(y)dbinom(y,n,0.5)))
siglevel
0.073
\end{lstlisting}
\end{VF}
\end{tcolorbox}

For the Negative Binomial model, the significance level would be:

\begin{equation*}
	\alpha_2=P_{1/2}(Y\geq 9)=p_2(9|1/2)+p_2(10|1/2)+\ldots=0.0327.
\end{equation*}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The likelihood principle: Negative Binomial model}
\begin{VF}
\begin{lstlisting}[ language=R]
success <- 3 
# Number of target success (tails)
failures <- 9 
# Number of failures
siglevel <- 1 - pnbinom((failures - 1),success,0.5)
siglevel
0.0327
\end{lstlisting}
\end{VF}
\end{tcolorbox}

We arrive to different conclusions using a significance level equal to 5\%, whereas we obtain the same outcomes using a Bayesian approach because the kernels of both distributions are the same ($\theta^9\times(1-\theta)^3$).



\section{Why is not the Bayesian approach that popular?}\label{sec25}

At this stage, we may wonder why the Bayesian statistical framework is not the dominant inferential approach despite that it has its historical origin in 1763 \cite{bayes1763lii}, whereas the Frequentist statistical framework was largely developed in the early 20th century. The scientific battle over the Bayesian inferential approach lasted for 150 years, and this maybe explained by some of the following facts.

There is an issue regarding \textit{apparent subjectivity} as the Bayesian inferential approach runs counter the strong conviction that science demands objectivity, and Bayesian probability is a measure of degrees of belief, where the initial prior maybe just a guess; this was not accepted as objective and rigorous science. Initial critics said that Bayes was quantifying ignorance as he set equal probabilities to any potential result. As a consequence, prior distributions were damned \cite{mcgrayne2011theory}.

\textit{Bayes himself seemed not to have believed in his idea}. Although, it seems that Bayes achieved his breakthrough during the late 1740s, he did not send it off to the Royal Society for publication. It was his friend, Richard Price, another Presbyterian minister, who rediscovered Bayes' idea, polished it and published.           

However, it was Laplace who independently generalized Bayes' theorem  in 1781. He used it initially in gambling problems, and soon after in astronomy, mixing different sources of information in order to leverage research in specific situations where data was scarce. Then, he wanted to use his discovery to find the probability of causes, and thought that this required large data sets, and turned into demography. In this field, he had to perform large calculations that demanded to develop smart approximations, creating the Laplace's approximation and the central limit theorem \cite{Laplace1812}; although, at the cost of apparently leaving his research on Bayesian inference.

Once \textit{Laplace was gone in 1827}, the Bayes' rule disappeared from the scientific spectrum for almost a century. In part, personal attacks against Laplace made the rule be forgotten, and also, the old fashion thought that statistics does not have to say anything about causation, and that the prior is very subjective to be compatible with science. Although, practitioners used it to solve problems in astronomy, communication, medicine, military and social issues with remarkable results.

Thus, the concept of degrees of belief to operationalize probability was gone in name of scientific objectivity, and probability as the frequency an event occurs in many repeatable trials became the rule. Laplace critics argued that those concepts were diametric opposites, although, Laplace considered them as basically equivalent when large sample sizes are involved \cite{mcgrayne2011theory}.

\textit{The era of the Frequentists or sampling theorists began}, lead by Karl Pearson, and his nemesis, Ronald Fisher, both brilliant, against the inverse probability approach, persuasive and dominant characters that made impossible to argue against their ideas. Karl Pearson legacy was taken by his son Egon, and Egon's friend Neyman, both inherited the anti-Bayesian and anti-Fisher legacy.  

Despite the anti-Bayesian campaign among statisticians, there were some independent characters developing Bayesian ideas, Borel, Ramsey and de Fineti, all of them isolated in different countries, France, England and Italy. However, the anti-Bayesian trio of Fisher, Neyman and Egon Person got all the attention during the 1920s and 1930s. Only, a geophysicist, Harold Jeffreys, kept alive Bayesian inference in the 1930s and 1940s. Jeffreys was a very quiet, shy, uncommunicative gentleman working at Cambridge in the astronomy department. He was Fisher's friend thanks to his character, although they were diametric opposites regarding the Bayesian inferential approach, facing very high intellectual battles. Unfortunately for the Bayesian approach, \textit{Jeffreys lost}, he was very technical using confusing high level mathematics, worried about inference from scientific evidence, not guiding future actions based on decision theory, which was very important in that era for mathematical statistics due to the Second World War. On the other hand, Fisher was a very dominant character, persuasive in public and a master of practice, his techniques were written in a popular style with minimum mathematics.   
 
However, Bayes' rule achieved remarkable results in applied settings like the AT\&T company or the social security system in USA. Bayesian inference also had a relevant role during the second World War and the Cold War. Alan Turing used inverse probability at Bletchley Park to crack German messages called Enigma code used by U-boats, Andrei Kolmogorov used it to improved firing tables of Russia's artillery, Bernard Koopman applied it for searching targets in the open sea and the RAND Corporation used it in the Cold War. Unfortunately, \textit{these Bayesian developments were top secrets for almost 40 years} that keep classified the contribution of inverse probability in modern human history.

During 1950s and 1960s three mathematicians lead the rebirth of the Bayesian approach, Good, Savage and Lindley. However, it seems that they were unwilling to apply their theories to real problems, and despite that the Bayesian approach proved its worth, for instance, in business decisions, navy search, lung cancer, etc, it was applied to simple models due to its \textit{mathematical complexity and requirement of large computations}. But, there were some breakthrough that change this. First, hierarchical models introduced by Lindley and Smith, where a complex model is decomposed into many easy to solve models, and second, Markov chain Monte Carlo methods developed by Hastings in the 1970s \cite{hastings70} and the Geman brothers in the 1980s \cite{Geman1984}. These methods were introduced into the Bayesian inferential framework in the 1990s by Gelfand and Smith \cite{Gelfand1990}, and Tierney \cite{tierney1994markov}, when desktop computers got enough computational power to solve complex models. Since then, the Bayesian inferential framework has gained increasing popularity among practitioners and scientists.

\section{A simple working example}\label{sec26}

We will illustrate some conceptual differences between the Bayesian and Frequentist statistical approaches performing inference given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]$, where $y_i\stackrel{iid}{\sim} N(\mu, \sigma^2)$, $i=1,2,\dots,N$.

In particular, we set $\pi(\mu,\sigma)=\pi(\mu)\pi(\sigma)\propto \frac{1}{\sigma}$. This is a standard \textit{non-informative improper} prior (Jeffreys prior, see Chapter \ref{chap4}), that is, this prior is perfectly compatible with sample information. In addition, we are assuming independent priors for $\mu$ and $\sigma$. Then,

\begin{align*}
	\pi(\mu,\sigma|\mathbf{y})&\propto \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\\
	&= \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N ((y_i-\bar{y}) - (\mu-\bar{y}))^2\right\}\\
	&=  \frac{1}{\sigma}\exp\left\{-\frac{N}{2\sigma^2}(\mu-\bar{y})^2\right\}\times (\sigma)^{-N}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\bar{y})^2\right\}\\
	&=  \frac{1}{\sigma}\exp\left\{-\frac{N}{2\sigma^2}(\mu-\bar{y})^2\right\}\times (\sigma)^{-(\alpha_n+1)}\exp\left\{-\frac{\alpha_n\hat{\sigma}^2}{2\sigma^2}\right\},	
\end{align*}

where $\bar{y}=\frac{\sum_{i=1}^N}{N}$, $\alpha_n=N-1$ and $\hat{\sigma}^2=\frac{\sum_{i=1}^N (y_i-\bar{y})^2}{N-1}$.

The first term in the last expression is the kernel of a normal density, $\mu|\sigma,\mathbf{y}\sim N(\bar{y},\sigma^2/N)$. The second term is the kernel of an inverted gamma density \cite{zellner1996introduction}, $\sigma|\mathbf{y}\sim IG(\alpha_n,\hat{\sigma}^2)$. Therefore, $\pi(\mu|\sigma,\mathbf{y})=(2\pi\sigma^2/N)^{-1/2}\exp\left\{\frac{-N}{2\sigma^2}(\mu-\bar{y})^2\right\}$ and $\pi(\sigma|\mathbf{y})=\frac{2}{\Gamma(\alpha_n/2)}\left(\frac{\alpha_n\hat{\sigma}^2}{2}\right)^{\alpha_n/2}\frac{1}{\sigma^{\alpha_n+1}}\times\exp\left\{-\frac{\alpha_n\hat{\sigma}^2}{2\sigma^2}\right\}$.

Observe that $\mathbb{E}[\mu|\sigma,\mathbf{y}]=\bar{y}$, this is also the maximum likelihood (Frequentist) point estimate of $\mu$ in this setting. In addition, the Frequentist $(1-\alpha)\%$ confidence interval and the Bayesian $(1-\alpha)\%$ credible interval have exactly the same form, $\bar{y}\pm |z_{\alpha/2}|\frac{\sigma}{\sqrt{N}}$, where $z_{\alpha/2}$ is the $\alpha/2$ percentile of a standard normal distribution. However, the interpretations are totally different. The confidence interval has a probabilistic interpretation under sampling variability of $\bar{Y}$, that is, in repeated sampling $(1-\alpha)\%$ of the intervals $\bar{Y}\pm |z_{\alpha/2}|\frac{\sigma}{\sqrt{N}}$ would include $\mu$, but given an observed realization of $\bar{Y}$, say $\bar{y}$, the probability of $\bar{y}\pm |z_{\alpha/2}|\frac{\sigma}{\sqrt{N}}$ including $\mu$ is 1 or 0, that is why we say a $(1-\alpha)\%$ confidence interval. On the other hand, $\bar{y}\pm |z_{\alpha/2}|\frac{\sigma}{\sqrt{N}}$ has a simple probabilistic interpretation in the Bayesian framework, there is a $(1-\alpha)\%$ probability that $\mu$ lies in this interval.

If we want to get the marginal posterior density of $\mu$, 

\begin{align*}
	\pi(\mu|\mathbf{y})&=\int_{0}^{\infty} \pi(\mu,\sigma|\mathbf{y}) d\sigma\\
	&\propto \int_{0}^{\infty} \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\} d\sigma\\ 
	&= \int_{0}^{\infty} \left(\frac{1}{\sigma}\right)^{N+1} \exp\left\{-\frac{N}{2\sigma^2}\frac{\sum_{i=1}^N (y_i-\mu)^2}{N}\right\} d\sigma\\
	&=\left[\frac{2}{\Gamma(N/2)}\left(\frac{N\sum_{i=1}^N (y_i-\mu)^2}{2N}\right)^{N/2}\right]^{-1}\\
	&\propto \left[\sum_{i=1}^N (y_i-\mu)^2\right]^{-N/2}\\
	&=\left[\sum_{i=1}^N ((y_i-\bar{y})-(\mu-\bar{y}))^2\right]^{-N/2}\\
	&=[\alpha_n\hat{\sigma}^2+N(\mu-\bar{y})^2]^{-N/2}\\
	&\propto \left[1+\frac{1}{\alpha_n}\left(\frac{\mu-\bar{y}}{\hat{\sigma}/\sqrt{N}}\right)^2\right]^{-(\alpha_n+1)/2}.
\end{align*}

The fourth line is due to having the kernel of a inverted gamma density with $N$ degrees of freedom in the integral \cite{zellner1996introduction}.

The last expression is the kernel of a Student's t density function with $\alpha_n=N-1$ degrees of freedom, expected value equal to $\bar{y}$, and variance $\frac{\hat{\sigma}^2}{N}\left(\frac{\alpha_n}{\alpha_n-2}\right)$. Then, $\mu|\mathbf{y}\sim t\left(\bar{y},\frac{\hat{\sigma}^2}{N}\left(\frac{\alpha_n}{\alpha_n-2}\right),\alpha_n\right)$.

Observe that a $(1-\alpha)\%$ confidence interval and $(1-\alpha)\%$ credible interval have exactly the same expression, $\bar{y}\pm |t_{\alpha/2}^{\alpha_n}|\frac{\hat{\sigma}}{\sqrt{N}}$, where $t_{\alpha/2}^{\alpha_n}$ is the $\alpha/2$ percentile of a Student's t distribution. But again, the interpretations are totally different.

The mathematical similarity between the Frequentist and Bayesian expressions in this example is due to using an improper prior.

\subsection{Example: Math test}\label{sec261}

You have a random sample of math scores of size $N=50$ from a normal distribution, $Y_i\sim {N}(\mu, \sigma)$. The sample mean and variance are equal to $102$ and $10$, respectively. Assuming an improper prior equal to $1/\sigma$, 

\begin{itemize}
	\item Get 95\% confidence and credible intervals for $\mu$.
	\item What is the posterior probability that $\mu > 103$?  
\end{itemize}

Using $\mu|\mathbf{y}\sim t\left(\bar{y},\frac{\hat{\sigma}^2}{N}\left(\frac{\alpha_n}{\alpha_n-2}\right),\alpha_n\right)$, which implies that $\bar{y}\pm |t_{\alpha/2}^{\alpha_n}|\frac{\hat{\sigma}}{\sqrt{N}}$, where $\bar{y}=102$, $\hat{\sigma}^2=10$ and $\alpha_n=49$, the 95\% confidence and credible intervals for $\mu$ are the same (101.1, 102.9), and $P(\mu>103)=1.49\%$ given the sample information.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Example: Math test}
\begin{VF}
\begin{lstlisting}[ language=R]
N <- 50 # Sample size
y_bar <- 102 # Sample mean
s2 <- 10 # Sample variance
alpha <- N - 1
serror <- (s2/N)^0.5 
LimInf <- y_bar - abs(qt(0.025, alpha)) * serror
LimInf
101.101
# Lower bound
LimSup <- y_bar + abs(qt(0.025, alpha)) * serror
LimSup
102.898
# Upper bound
y.cut <- 103
P <- 1-metRology::pt.scaled(y.cut, df = alpha, mean = y_bar, sd = serror)
P
0.0149
# Probability of mu greater than y.cut
\end{lstlisting}
\end{VF}
\end{tcolorbox}



\section{Summary}\label{sec27}

The differences between the Bayesian and Frequentist inferential approaches are philosophical, including as pertains to the role of probability; pedagogical, in particular as relates to the use of inference to inform decision making; and methodological, as having differences in their mathematical and computational frameworks. Although at methodological level, the debate has become considerably muted, except for some aspects of inference, with the recognition that each approach has a great deal to contribute to statistical practice \cite{Good1992, Bayarri2004, Kass2011}. As Bradley Efron said ``Computer-age statistical inference at its most successful \textbf{combines} elements of the two philosophies" \cite{efron2016computer}.

\section{Exercises}\label{sec28}
\begin{enumerate}
\item \textbf{Jeffreys-Lindley's paradox}

The \textbf{Jeffreys-Lindley's paradox} \cite{Jeffreys1961,lindley1957statistical} is an apparent disagreement between the Bayesian and Frequentist frameworks to a hypothesis testing situation.

In particular, assume that in a city 49,581 boys and 48,870 girls have been born in 20 years. Assume that the male births is distributed Binomial with probability $\theta$. We want to test the null hypothesis $H_0. \ \theta=0.5$ versus $H_1. \ \theta\neq 0.5$.

\begin{itemize}
	\item Show that the posterior model probability for the model under the null is approximately 0.95. Assume $\pi(H_0)=\pi(H_1)=0.5$, and $\pi(\theta)$ equal to ${U}(0,1)$ under $H_1$.
	\item Show that the \textit{p}-value for this hypothesis test is equal to 0.0235 using the normal approximation, $Y\sim {N}(N\times \theta, N\times \theta \times (1-\theta))$. 
\end{itemize}

\item We want to test $H_0. \ \mu=\mu_0$ vs $H_1. \ \mu \neq \mu_0$ given $y_i\stackrel{iid}{\sim}N(\mu,\sigma^2)$.

Assume $\pi(H_0)=\pi(H_1)=0.5$, and $\pi(\mu,\sigma)\propto 1/\sigma$ under the alternative hypothesis.

Show that

$p(\mathbf{y}|\mathcal{M}_1)=\frac{\pi^{-N/2}}{2}\Gamma(N/2)2^{N/2}\left(\frac{1}{\alpha_n\hat{\sigma}^2}\right)^{N/2}\left(\frac{N}{\alpha_n\hat{\sigma}^2}\right)^{-1/2}\frac{\Gamma(1/2)\Gamma(\alpha_n/2)}{\Gamma((\alpha_n+1)/2)}$ and $p(\mathbf{y}|\mathcal{M}_0)=(2\pi)^{-N/2}\left[\frac{2}{\Gamma(N/2)}\left(\frac{N}{2}\frac{\sum_{i=1}^N(y_i-\mu_0)^2}{N}\right)^{N/2}\right]^{-1}$. Then,

\begin{align*}
	PO_{01}&=\frac{p(\mathbf{y}|\mathcal{M}_0)}{p(\mathbf{y}|\mathcal{M}_1)}\\
	& =\frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_n/2)}(\alpha_n\hat{\sigma}^2/N)^{-1/2}\left[1+\frac{(\mu_0-\bar{y})^2}{\alpha_n\hat{\sigma}^2/N}\right]^{-\left(\frac{\alpha_n+1}{2}\right)},
\end{align*}

where $\alpha_n=N-1$ and $\hat{\sigma}^2=\frac{\sum_{i=1}^N (y_i-\bar{y})^2}{N-1}$.

Find the relationship between the posterior odds and the classical test statistic for the null hypothesis.  

\item \textbf{Math test continues}

Using the setting of the \textbf{Example: Math test} in subsection \ref{sec261}, test $H_0. \ \mu=\mu_0$ vs $H_1. \ \mu \neq \mu_0$ where $\mu_0=\left\{100, 100.5, 101, 101.5, 102 \right\}$.

\begin{itemize}
	\item What is the \textit{p}-value for these hypothesis tests?
	\item Find the posterior model probability of the null model for each $\mu_0$.
\end{itemize} 

\end{enumerate}





