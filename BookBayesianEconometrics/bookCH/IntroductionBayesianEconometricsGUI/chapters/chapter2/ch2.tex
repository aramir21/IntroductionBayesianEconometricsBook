\chapter{Conceptual differences between the Bayesian and Frequentist approaches}\label{chap2}

We outline some of the conceptual differences between the Bayesian and Frequentist inferential approaches. We emphasize Bayesian concepts, as most readers may already be familiar with the Frequentist statistical framework. We illustrate the differences between these two inferential approaches using a simple example. In addition, we provide some potential explanations for why the Bayesian inferential framework is not well known at the introductory level among practitioners and applied researchers.

\section{The concept of probability}\label{sec21}

Let's begin with the following thought experiment: Assume that you are watching the international game show ``Who Wants to Be a Millionaire?". The contestant is asked to answer a very simple question: \textbf{What is the last name of the brothers who are credited with inventing the world's first successful motor-operated airplane?}

\begin{itemize}
	\item What is the probability that the contestant answers this question correctly? 
\end{itemize}

Unless you have: 

\begin{enumerate}
	\item watched this particular contestant participate in this show many times,
	\item seen her asked this same question each time, 
	\item and computed the relative frequency with which she gives the correct answer,   
\end{enumerate}

you need to answer this question as a Bayesian!

Uncertainty about the event \textit{answering this question} needs to be expressed as a ``degree of belief," informed by both data on the skill of the particular participant and how much she knows about inventors, as well as possibly prior knowledge of her performance in other game shows. Of course, your prior knowledge of the contestant may be minimal, or it may be well-informed. Either way, your final answer remains a degree of belief about an uncertain, and inherently unrepeatable, state of nature.

The point of this hypothetical, light-hearted scenario is simply to highlight that a key distinction between the Frequentist and Bayesian approaches to inference is not the use (or nature) of prior information, but the manner in which probability is used. To the Bayesian, probability is the mathematical construct used to quantify uncertainty about an unknown state of nature, conditional on observed data and prior knowledge about the context in which that state occurs. To the Frequentist, probability is intrinsically linked to the concept of a repeated experiment, and the relative frequency with which a particular outcome occurs, conditional on that unknown state. This distinction remains key whether the Bayesian chooses to be \textit{informative or subjective} in the specification of prior information, or chooses to be \textit{non-informative or objective}.

Frequentists consider probability to be a physical phenomenon, like mass or wavelength, whereas Bayesians stipulate that probability exists in the mind of scientists, as any scientific construct \cite{Parmigiani2008}.

It seems that the understanding of the concept of probability for the common human being is more associated with ``degrees of belief" rather than relative frequency. Peter Diggle, President of The Royal Statistical Society between 2014 and 2016, was asked in an interview, ``A different trend which has surged upwards in statistics during Peter's career is the popularity of Bayesian statistics. Does Peter consider himself a Bayesian?" He replied, ``... you can't not believe in Bayes' theorem because it's true. But that doesn't make you a Bayesian in the philosophical sense. When people are making personal decisions -- even if they don't formally process Bayes' theorem in their mind -- they are adapting what they think they should believe in response to new evidence as it comes in. Bayes' theorem is just the formal mathematical machinery for doing that."

However, we should mention that psychological experiments suggest that human beings suffer from \textit{anchoring}, a cognitive bias that causes us to rely too heavily on previous information (the prior), so that the updating process (posterior) due to new information (likelihood) is not as strong as Bayes' rule would suggest \cite{daniel2017thinking}.


\section{Subjectivity is not the key}\label{sec22}

The concepts of \textit{subjectivity} and \textit{objectivity} indeed characterize both statistical paradigms in differing ways. Among Bayesians, there are those who are immersed in \textit{subjective} rationality \cite{Ramsey1926, deFinetti1937, savage1954, Lindley2000}, but others who adopt \textit{objective} prior distributions such as Jeffreys', reference, empirical, or robust priors \cite{Bayes1763, Laplace1812, Jeffreys1961, Berger2006} to operationalize Bayes' rule and thereby weight quantitative (data-based) evidence. Among Frequentists, there are choices made about significance levels which, if not explicitly subjective, are typically not grounded in any objective and documented assessment of the relative losses of Type I and Type II errors.\footnote{Type I error is rejecting the null hypothesis when it is true, and Type II error is not rejecting the null hypothesis when it is false.} In addition, both Frequentist and Bayesian Econometrician/Statisticians make decisions about the form of the data generating process, or ``model", which -- if not subject to rigorous diagnostic assessment -- retains a subjective element that potentially influences the final inferential outcome. Although we all know that by definition, a model is a schematic and simplified approximation to reality,

``Since all models are wrong, the scientist cannot obtain a \textit{correct} one by excessive elaboration. On the contrary, following William of Occam, he should seek an economical description of natural phenomena." \cite{Box1976}.

We also know that ``All models are wrong, but some are useful" \cite{box1979robustness}, which is why model diagnostics are important. This task can be performed in both approaches. Particularly, the Bayesian framework can use predictive \textit{p}-values for absolute testing \cite{Gelman1996, Bayarri2000} or posterior odds ratios for relative statements \cite{Jeffreys1935, Kass1995}. This is because the marginal likelihood, conditional on data, is interpreted as the evidence for the prior distribution \cite{berger93}.

In addition, what does objectivity mean in a Frequentist approach? For example, why should we use a 5\% or 1\% significance level rather than any other value? As someone said, the apparent objectivity is really a consensus \cite{Lindley2000}. In fact, ``Student" (William Gosset) saw statistical significance at any level as being ``nearly valueless" in itself \cite{Ziliak2008}. But, this is not just a situation in the Frequentist approach. The cut-offs used to ``establish" scientific evidence against a null hypothesis, in terms of $log_{10}$ scale \cite{Jeffreys1961} or $log_{e}$ scale \cite{Kass1995} as shown in Table \ref{tab:guide}, are also \textit{ad hoc}.

Although the true state of nature in Bayesian inference is expressed in ``degrees of belief", the distinction between the two paradigms does not reside in one being more, or less, \textit{subjective} than the other. Rather, the differences are philosophical, pedagogical, and methodological.

\section{Estimation, hypothesis testing and prediction}\label{sec23}

All that is required to perform estimation, hypothesis testing (model selection), and prediction in the Bayesian approach is to apply Bayes' rule. This ensures coherence under a probabilistic view. However, there is no free lunch: coherence reduces flexibility. On the other hand, the Frequentist approach may not be coherent from a probabilistic point of view, but it is highly flexible. This approach can be seen as a toolkit that offers inferential solutions under the umbrella of understanding probability as relative frequency. For instance, a point estimator in a Frequentist approach is found such that it satisfies good sampling properties like unbiasedness, efficiency, or a large sample property such as consistency.

A notable difference is that optimal Bayesian decisions are calculated by minimizing the expected value of the loss function with respect to the posterior distribution, i.e., conditional on observed data. In contrast, Frequentist ``optimal" actions are based on the expected values over the distribution of the estimator (a function of data), conditional on the unknown parameters. This involves considering sampling variability.

The Bayesian approach allows for the derivation of the posterior distribution of any unknown object, such as parameters, latent variables, future or unobserved variables, or models. A major advantage is that predictions can account for estimation error, and predictive distributions (probabilistic forecasts) can be easily derived.

Hypothesis testing (model selection) in the Bayesian framework is based on \textit{inductive logic} reasoning (\textit{inverse probability}). Based on observed data, we evaluate which hypothesis is most tenable, performing this evaluation using posterior odds. These odds are in turn based on Bayes factors, which assess the evidence in favor of a null hypothesis while explicitly considering the alternative \cite{Kass1995}, following the rules of probability \cite{Lindley2000}. This approach compares how well hypotheses predict data \cite{Goodman1999}, minimizes the weighted sum of type I and type II error probabilities \cite{DeGroot1975,Pericchip}, and takes into account the implicit balance of losses \cite{Jeffreys1961,Bernardo1994}. Posterior odds allow for the use of the same framework to analyze nested and non-nested models and perform model averaging. 

However, Bayes factors cannot be based on improper or vague priors \cite{koop2003bayesian}, the practical interplay between model selection and posterior distributions is not as straightforward as it may be in the Frequentist approach, and the computational burden can be more demanding due to the need to solve potentially difficult integrals.

On the other hand, the Frequentist approach establishes most of its estimators as the solution to a system of equations. Observe that optimization problems often reduce to solving systems. We can potentially obtain the distribution of these estimators, but most of the time, asymptotic arguments or resampling techniques are required. Hypothesis testing relies on pivotal quantities and/or resampling, and prediction is typically based on a \textit{plug-in approach}, which means that estimation error is not taken into account.\footnote{A pivot quantity is a function of unobserved parameters and observations whose probability distribution does not depend on the unknown parameters.} 

Comparing models depends on their structure. For instance, there are different Frequentist statistical approaches to compare nested and non-nested models. A nice feature in some situations is that there is a practical interplay between hypothesis testing and confidence intervals. For example, in the normal population mean hypothesis framework, you cannot reject a null hypothesis $H_0: \mu = \mu^0$ at the $\alpha$ significance level (Type I error) if $\mu^0$ is in the $1-\alpha$ confidence interval. Specifically, 
\[
P\left( \mu \in \left[\hat{\mu} - |t_{N-1}^{\alpha/2}| \times \hat{\sigma}_{\hat{\mu}}, \hat{\mu} + |t_{N-1}^{\alpha/2}| \times \hat{\sigma}_{\hat{\mu}}\right] \right) = 1 - \alpha,
\]
where $\hat{\mu}$ and $\hat{\sigma}_{\hat{\mu}}$ are the maximum likelihood estimators of the mean and standard error, $t_{N-1}^{\alpha/2}$ is the quantile value of the Student's $t$-distribution at the $\alpha/2$ probability level with $N-1$ degrees of freedom, and $N$ is the sample size.

A remarkable difference between the Bayesian and Frequentist inferential frameworks is the interpretation of credible/confidence intervals. Observe that once we have estimates, such that for example the previous interval is $[0.2, 0.4]$ given a 95\% confidence level, we cannot say that $P(\mu \in [0.2, 0.4]) = 0.95$ in the Frequentist framework. In fact, this probability is either 0 or 1 in this approach, as $\mu$ is either in the interval or it is not. The problem is that we will never know for certain in applied settings. This is because $P(\mu \in [\hat{\mu} - |t_{N-1}^{0.025}| \times \hat{\sigma}_{\hat{\mu}}, \hat{\mu} + |t_{N-1}^{0.025}| \times \hat{\sigma}_{\hat{\mu}}]) = 0.95$ is interpreted in the context of repeated sampling. On the other hand, once we have the posterior distribution in the Bayesian framework, we can say that $P(\mu \in [0.2, 0.4]) = 0.95$.

Following common practice, most researchers and practitioners conduct hypothesis testing based on the \textit{p}-value in the Frequentist framework. But \textbf{what is a \textit{p}-value?} Most users do not know the answer, as statistical inference is often not performed by statisticians \cite{Berger2006}.\footnote{https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/} A \textit{p}-value is the probability of obtaining a statistical summary of the data equal to or \textit{more extreme} than what was actually observed, assuming that the null hypothesis is true.

Therefore, \textit{p}-value calculations involve not just the observed data, but also more \textit{extreme} hypothetical observations. Thus,

``What the use of \textit{p} implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred." \cite{Jeffreys1961}

Some researchers and practitioners using Frequentist inference often intertwines two distinct logical frameworks: Fisher’s \textit{p}-value approach \cite{Fisher1958} and the Neyman--Pearson significance testing framework \cite{Neyman1933}. The \textit{p}-value serves as an informal, data-dependent measure of evidence against the null hypothesis. It is rooted in \textit{reduction to absurdity} reasoning, where the extremeness of the observed data is assessed under the assumption that the null hypothesis is true. However, the \textit{p}-value is frequently misinterpreted as the probability that the null hypothesis is false—a misconception known as the \textit{p}-value fallacy \cite{Goodman1999}. 

In contrast, the Neyman--Pearson framework adopts a deductive, long-run perspective: it defines decision rules that control the frequency of Type I errors over repeated sampling, irrespective of the evidence in any particular case. Conflating these two frameworks leads to interpretational inconsistencies, especially when the \textit{p}-value is used both as a measure of evidence and as a decision-making threshold. A clearer separation of these paradigms is essential for coherent statistical reasoning.

The American Statistical Association has several concerns regarding the use of the \textit{p}-value as a cornerstone for hypothesis testing in science. This concern motivates the ASA's statement on \textit{p}-values \cite{Wasserstein2016}, which can be summarized in the following principles:

\begin{itemize}
	\item ``P-values can indicate how incompatible the data are with a specified statistical model."
	\item ``P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone."
	\item ``Scientific conclusions and business or policy decisions should not be based solely on whether a \textit{p}-value passes a specific threshold."
	\item ``Proper inference requires full reporting and transparency."
	\item ``A \textit{p}-value, or statistical significance, does not measure the size of an effect or the importance of a result."
	\item ``By itself, a \textit{p}-value does not provide a good measure of evidence regarding a model or hypothesis."
\end{itemize}

To sum up, Fisher proposed the \textit{p}-value as a witness rather than a judge. So, a \textit{p}-value lower than the significance level means more inspection of the null hypothesis, but it is not a final conclusion about it.

Another key distinction between frequentist and Bayesian approaches lies in how scientific hypotheses are evaluated. Users of the Frequentist approach rely on the \emph{p}-value, which quantifies the probability of observing data as extreme as—or more extreme than—the sample under the assumption that the null hypothesis is true. Bayesians, in contrast, use the Bayes factor, which compares the predictive performance of two competing hypotheses by evaluating the ratio of their marginal likelihoods. While the \emph{p}-value reflects \( P(\text{data} \mid \text{hypothesis}) \), the Bayes factor is more aligned with \( P(\text{hypothesis} \mid \text{data}) \), though not equivalent. Notably, there exists an approximate relationship between the \( t \)-statistic and the Bayes factor in the context of regression coefficients \cite{Raftery1995}, which offers some practical interpretability across paradigms. In particular, $|t|>(\log(N)+6)^{1/2}$ corresponds to strong evidence in favor of rejecting the null hypothesis of no relevance of a control in a regression. Observe that, in this setting, the threshold of the $t$ statistic, and as a consequence the significance level, depends on the sample size. This setting agrees with the idea in experimental designs of selecting the sample size such that we control Type I and Type II errors. In observational studies, we cannot control the sample size, but we can select the significance level.

See also \cite{sellke2001calibration} and \cite{benjamin2018redefine} for nice exercises that reveal potential flaws of the \textit{p}-value ($p$) due to $p \sim U[0,1]$ under the null hypothesis,\footnote{https://joyeuserrance.wordpress.com/2011/04/22/proof-that-p-values-under-the-null-are-uniformly-distributed/ for a simple proof.} and calibrations of the \textit{p}-value to interpret it as the odds ratio and the error probability. In particular, $B(p)=-e \times p \times \log(p)$ when $p < e^{-1}$, and interpret this as the Bayes factor of $H_0$ to $H_1$, where $H_1$ denotes the unspecified alternative to $H_0$, and $\alpha(p) = \left(1 + \left[-e \times p \times \log(p)\right]^{-1}\right)^{-1}$ as the error probability $\alpha$ in rejecting $H_0$. Take into account that $B(p)$ and $\alpha(p)$ are lower bounds.

The logic of argumentation in the Frequentist approach is based on \textit{deductive logic}, which means that it starts from a statement about the true state of nature (null hypothesis) and predicts what should be observed if this statement were true. On the other hand, the Bayesian approach is based on \textit{inductive logic}, which means that it defines which hypothesis is more consistent with what is observed. The former inferential approach establishes that the truth of the premises implies the truth of the conclusion, which is why we reject or fail to reject hypotheses. The latter establishes that the premises supply some evidence, but not full assurance, of the truth of the conclusion, which is why we get probabilistic statements.

Here, there is a distinction between the effects of causes (forward causal inference) and the causes of effects (reverse causal inference) \cite{Gelman2013, Dawid2016}. To illustrate this point, imagine that a firm increases the price of a specific good. Economic theory would suggest that, as a result, demand for the good decreases. In this case, the premise (null hypothesis) is the price increase, and the consequence is the decrease in the firm's demand.

Alternatively, one could observe a reduction in a firm's demand and attempt to identify the cause behind it. For example, a reduction in quantity could be due to a negative supply shock. The Frequentist approach typically follows the first view (effects of causes), while Bayesian reasoning focuses on determining the probability of potential causes (causes of effects).

\section{The likelihood principle}\label{sec24} 

The \textbf{likelihood principle} states that in making inferences or decisions about the state of nature, all the relevant \textit{experimental} information is given by the \textit{likelihood function}. The Bayesian framework follows this statement, i.e., it is conditional on observed data.

We follow \cite{berger93}, who in turn followed \cite{Lindley76}, to illustrate the likelihood principle. We are given a coin and are interested in the probability, $\theta$, of it landing heads when flipped. We wish to test $H_0: \theta = 1/2$ versus $H_1: \theta > 1/2$. An experiment is conducted by flipping the coin (independently) in a series of trials, with the result being the observation of 9 heads and 3 tails.

This is not yet enough information to specify $p(y|\theta)$, since the series of trials has not been explained. Two possibilities arise:

\begin{enumerate}
	\item The experiment consisted of a predetermined 12 flips, so that $Y = \left[ \text{Heads} \right]$ follows a ${B}(12, \theta)$ distribution. In this case, $p_1(y|\theta) = \binom{12}{y} \theta^y (1 - \theta)^{12 - y} = 220 \times \theta^9 (1 - \theta)^3$.
	\item The experiment consisted of flipping the coin until 3 tails were observed ($r = 3$). In this case, $Y$, the number of heads (failures) before obtaining 3 tails, follows a ${NB}(3, 1 - \theta)$ distribution. Here, $p_2(y|\theta) = \binom{y + r - 1}{r - 1} (1 - (1 - \theta)^y)(1 - \theta)^r = 55 \times \theta^9 (1 - \theta)^3$.
\end{enumerate}

Using a Frequentist approach, the significance level of $y=9$ using the Binomial model against $\theta=1/2$ would be:

\begin{equation*}
	\alpha_1=P_{1/2}(Y\geq
	9)=p_1(9|1/2)+p_1(10|1/2)+p_1(11|1/2)+p_1(12|1/2)=0.073.
\end{equation*}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The likelihood principle: Binomial model}
\begin{VF}
\begin{lstlisting}[ language=R]
success <- 9 
# Number of observed success in n trials
n <- 12 
# Number of trials
siglevel <- sum(sapply(9:n,function(y)dbinom(y,n,0.5)))
siglevel
0.073
\end{lstlisting}
\end{VF}
\end{tcolorbox}

For the Negative Binomial model, the significance level would be:

\begin{equation*}
	\alpha_2=P_{1/2}(Y\geq 9)=p_2(9|1/2)+p_2(10|1/2)+\ldots=0.0327.
\end{equation*}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The likelihood principle: Negative Binomial model}
\begin{VF}
\begin{lstlisting}[ language=R]
success <- 3 
# Number of target success (tails)
failures <- 9 
# Number of failures
siglevel <- 1 - pnbinom((failures - 1),success,0.5)
siglevel
0.0327
\end{lstlisting}
\end{VF}
\end{tcolorbox}

We arrive at different conclusions using a significance level of 5\%, whereas we obtain the same outcomes using a Bayesian approach because the kernels of both distributions are identical ($\theta^9 \times (1 - \theta)^3$).

\section{Why is not the Bayesian approach that popular?}\label{sec25}

At this stage, one might wonder why the Bayesian statistical framework is not the dominant inferential approach, despite its historical origin in 1763 \cite{bayes1763lii}, whereas the Frequentist statistical framework was largely developed in the early 20th century. The scientific debate over the Bayesian inferential approach lasted for 150 years, and this may be explained by some of the following factors.

One issue is the \textit{apparent subjectivity} of the Bayesian approach, which runs counter to the strong conviction that science demands objectivity. Bayesian probability is considered a measure of degrees of belief, where the initial prior may be just a guess. This was not accepted as objective and rigorous science. Initial critics argued that Bayes was quantifying ignorance by assigning equal probabilities to all potential outcomes. As a consequence, prior distributions were dismissed \cite{mcgrayne2011theory}.

\textit{Bayes himself seemed not to have believed in his idea}. Although it seems that Bayes made his breakthrough in the late 1740s, he did not submit it for publication to the Royal Society. It was his friend, Richard Price, another Presbyterian minister, who rediscovered Bayes' idea, polished it, and published it.

However, it was Laplace who independently generalized Bayes' theorem in 1781. Initially, he applied it to gambling problems and soon thereafter to astronomy, combining various sources of information to advance research in situations where data was scarce. He later sought to apply his discovery to finding the probability of causes, which he thought required large datasets, thus turning to demography. In this field, he had to perform large-scale calculations, leading to the development of Laplace’s approximation and the central limit theorem \cite{Laplace1812}. Unfortunately, this came at the cost of abandoning his research on Bayesian inference.

Once \textit{Laplace passed away in 1827}, Bayes' rule disappeared from the scientific discourse for almost a century. In part, personal attacks against Laplace led to the rule being forgotten. Moreover, there was a prevailing belief that statistics should not address causation, and that the prior was too subjective to be compatible with science. Nonetheless, practitioners continued to use Bayes' rule to solve problems in astronomy, communication, medicine, military affairs, and social issues with remarkable results.

Thus, the concept of degrees of belief to operationalize probability was abandoned in favor of scientific objectivity. Probability was then defined as the frequency with which an event occurs in many repeatable trials, which became the accepted norm. Critics of Laplace argued that these two concepts were diametrically opposed, although Laplace considered them to be basically equivalent when large sample sizes are involved \cite{mcgrayne2011theory}.

\textit{The era of Frequentists, or sampling theorists, began}, led by Karl Pearson and his nemesis, Ronald Fisher. Both were brilliant and persuasive characters, opposing the inverse probability (Bayesian) approach and making it nearly impossible to argue against their ideas. Pearson's legacy was carried on by his son, Egon, and Egon’s friend, Jerzy Neyman. Both inherited the anti-Bayesian and anti-Fisher sentiments.

Despite the anti-Bayesian campaign among statisticians, some independent thinkers continued to develop Bayesian ideas, including Borel, Ramsey, and de Finetti, who were isolated in different countries: France, England, and Italy. However, the anti-Bayesian trio of Fisher, Neyman, and Egon Pearson dominated the spotlight in the 1920s and 1930s. Only a geophysicist, Harold Jeffreys, kept Bayesian inference alive during the 1930s and 1940s. Jeffreys was a quiet, reserved gentleman working in the astronomy department at Cambridge. He was Fisher’s friend due to their shared character, although they were intellectual opposites when it came to Bayesian inference, leading to intense intellectual battles. Unfortunately for the Bayesian approach, \textit{Jeffreys lost}. His work was highly technical, using confusing high-level mathematics. He focused on inference from scientific evidence, rather than guiding future actions based on decision theory, which was crucial in that era for mathematical statistics, especially during the Second World War. In contrast, Fisher was a dominant figure, persuasive in public and a master of practical applications, with his techniques written in a popular style with minimal mathematics.

Nevertheless, Bayes' rule achieved remarkable results in applied settings such as at AT\&T and the U.S. Social Security system. Bayesian inference also played a significant role during the Second World War and the Cold War. Alan Turing used inverse probability at Bletchley Park to crack German messages encoded using the Enigma machine, which was employed by U-boats. Andrei Kolmogorov used Bayesian methods to improve firing tables for Russian artillery. Bernard Koopman applied it for searching targets at sea, and the RAND Corporation used it during the Cold War. Unfortunately, \textit{these Bayesian developments remained top secret for almost 40 years}, keeping the contribution of inverse probability hidden from modern history.

In the 1950s and 1960s, three mathematicians led the resurgence of the Bayesian approach: Good, Savage, and Lindley. However, it seems that they were reluctant to apply their theories to real-world problems. Despite the fact that the Bayesian approach proved its worth in various areas such as business decisions, naval searches, and lung cancer detection, it was largely applied to simple models due to its \textit{mathematical complexity and requirement for large computations}. However, some breakthroughs changed this. First, hierarchical models were introduced by Lindley and Smith, where a complex model is decomposed into many smaller, easier-to-solve models. Second, Markov chain Monte Carlo (MCMC) methods were developed by Hastings in the 1970s \cite{hastings70} and the Geman brothers in the 1980s \cite{Geman1984}. These methods were incorporated into the Bayesian inferential framework in the 1990s by Gelfand and Smith \cite{Gelfand1990}, and Tierney \cite{tierney1994markov}, when desktop computers gained sufficient computational power to solve complex models. Since then, the Bayesian inferential framework has gained increasing popularity among both practitioners and scientists.

\section{A simple working example}\label{sec26}

We will illustrate some conceptual differences between the Bayesian and Frequentist statistical approaches by performing inference on a random sample $\mathbf{Y} = [Y_1, Y_2, \dots, Y_N]$, where $Y_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$ for $i = 1, 2, \dots, N$.

In particular, we set $\pi(\mu, \sigma) = \pi(\mu) \pi(\sigma) \propto \frac{1}{\sigma}$. This is a standard \textit{non-informative improper} prior (Jeffreys prior, see Chapter \ref{chap4}). That is, this prior is perfectly compatible with the sample information. Thus, the posterior distribution is

\begin{align*}
	\pi(\mu,\sigma|\mathbf{y})&\propto \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\\
	&= \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N ((y_i-\bar{y}) - (\mu-\bar{y}))^2\right\}\\
	&=  \frac{1}{\sigma}\exp\left\{-\frac{N}{2\sigma^2}(\mu-\bar{y})^2\right\}\times (\sigma)^{-N}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\bar{y})^2\right\}\\
	&=  \frac{1}{\sigma}\exp\left\{-\frac{N}{2\sigma^2}(\mu-\bar{y})^2\right\}\times (\sigma)^{-(\alpha_n+1)}\exp\left\{-\frac{\alpha_n\hat{\sigma}^2}{2\sigma^2}\right\},	
\end{align*}

where $\bar{y}=\frac{\sum_{i=1}^N}{N}$, $\alpha_n=N-1$ and $\hat{\sigma}^2=\frac{\sum_{i=1}^N (y_i-\bar{y})^2}{N-1}$.

The first term in the last expression is the kernel of a normal density, $\mu|\sigma,\mathbf{y}\sim N(\bar{y},\sigma^2/N)$. The second term is the kernel of an inverted gamma density \cite[p.~371]{zellner1996introduction}, $\sigma|\mathbf{y}\sim IG(\alpha_n,\hat{\sigma}^2)$. Therefore, $\pi(\mu|\sigma,\mathbf{y})=(2\pi\sigma^2/N)^{-1/2}\exp\left\{\frac{-N}{2\sigma^2}(\mu-\bar{y})^2\right\}$ and $\pi(\sigma|\mathbf{y})=\frac{2}{\Gamma(\alpha_n/2)}\left(\frac{\alpha_n\hat{\sigma}^2}{2}\right)^{\alpha_n/2}\frac{1}{\sigma^{\alpha_n+1}}\times\exp\left\{-\frac{\alpha_n\hat{\sigma}^2}{2\sigma^2}\right\}$.

Observe that $\mathbb{E}[\mu | \sigma, \mathbf{y}] = \bar{y}$; this is also the maximum likelihood (Frequentist) point estimate of $\mu$ in this setting. In addition, the Frequentist $(1-\alpha)\%$ confidence interval and the Bayesian $(1-\alpha)\%$ credible interval have exactly the same form, $\bar{y} \pm |z_{\alpha/2}| \frac{\sigma}{\sqrt{N}}$, where $z_{\alpha/2}$ is the $\alpha/2$ percentile of a standard normal distribution. However, the interpretations are entirely different. The confidence interval has a probabilistic interpretation under sampling variability of $\bar{Y}$: in repeated sampling, $(1-\alpha)\%$ of the intervals $\bar{Y} \pm |z_{\alpha/2}| \frac{\sigma}{\sqrt{N}}$ would include $\mu$. However, given an observed realization of $\bar{Y}$, say $\bar{y}$, the probability of $\bar{y} \pm |z_{\alpha/2}| \frac{\sigma}{\sqrt{N}}$ including $\mu$ is either 1 or 0. This is why we refer to it as a $(1-\alpha)\%$ confidence interval. On the other hand, $\bar{y} \pm |z_{\alpha/2}| \frac{\sigma}{\sqrt{N}}$ has a straightforward probabilistic interpretation in the Bayesian framework: there is a $(1-\alpha)\%$ probability that $\mu$ lies within this interval.

If we want to get the marginal posterior density of $\mu$, 
\begin{align*}
	\pi(\mu|\mathbf{y})&=\int_{0}^{\infty} \pi(\mu,\sigma|\mathbf{y}) d\sigma\\
	&\propto \int_{0}^{\infty} \frac{1}{\sigma}\times (\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\} d\sigma\\ 
	&= \int_{0}^{\infty} \left(\frac{1}{\sigma}\right)^{N+1} \exp\left\{-\frac{N}{2\sigma^2}\frac{\sum_{i=1}^N (y_i-\mu)^2}{N}\right\} d\sigma\\
	&=\left[\frac{2}{\Gamma(N/2)}\left(\frac{N\sum_{i=1}^N (y_i-\mu)^2}{2N}\right)^{N/2}\right]^{-1}\\
	&\propto \left[\sum_{i=1}^N (y_i-\mu)^2\right]^{-N/2}\\
	&=\left[\sum_{i=1}^N ((y_i-\bar{y})-(\mu-\bar{y}))^2\right]^{-N/2}\\
	&=[\alpha_n\hat{\sigma}^2+N(\mu-\bar{y})^2]^{-N/2}\\
	&\propto \left[1+\frac{1}{\alpha_n}\left(\frac{\mu-\bar{y}}{\hat{\sigma}/\sqrt{N}}\right)^2\right]^{-(\alpha_n+1)/2}.
\end{align*}

The fourth line arises from the kernel of an inverted gamma density with $N$ degrees of freedom in the integral \cite{zellner1996introduction}.

The last expression represents the kernel of a Student's $t$ density function with $\alpha_n = N - 1$ degrees of freedom, expected value equal to $\bar{y}$, and variance $\frac{\hat{\sigma}^2}{N} \left( \frac{\alpha_n}{\alpha_n - 2} \right)$. Therefore, $\mu | \mathbf{y} \sim t \left( \bar{y}, \frac{\hat{\sigma}^2}{N} \left( \frac{\alpha_n}{\alpha_n - 2} \right), \alpha_n \right)$.

Observe that a $(1-\alpha)\%$ confidence interval and a $(1-\alpha)\%$ credible interval have the same form, $\bar{y} \pm |t_{\alpha/2}^{\alpha_n}| \frac{\hat{\sigma}}{\sqrt{N}}$, where $t_{\alpha/2}^{\alpha_n}$ is the $\alpha/2$ percentile of a Student's $t$ distribution. However, the interpretations are entirely different.

The mathematical similarity between the Frequentist and Bayesian expressions in this example arises from the use of an improper prior.

\subsection{Example: Math test}\label{sec261}

You have a random sample of math scores of size $N = 50$ from a normal distribution, $Y_i \sim N(\mu, \sigma^2)$. The sample mean and variance are equal to $102$ and $10$, respectively. Assuming an improper prior equal to $1/\sigma$, we proceed with the following tasks:

\begin{itemize}
	\item Compute the 95\% confidence and credible intervals for $\mu$.
	\item Determine the posterior probability that $\mu > 103$.
\end{itemize}

Using the fact that $\mu | \mathbf{y} \sim t\left(\bar{y}, \frac{\hat{\sigma}^2}{N} \left( \frac{\alpha_n}{\alpha_n - 2} \right), \alpha_n \right)$, which implies that the confidence and credible intervals for $\mu$ are given by 
\[
\bar{y} \pm |t_{\alpha/2}^{\alpha_n}| \frac{\hat{\sigma}}{\sqrt{N}},
\]
where $\bar{y} = 102$, $\hat{\sigma}^2 = 10$, and $\alpha_n = 49$. Thus, the 95\% confidence and credible intervals for $\mu$ are the same, namely $(101.1, 102.9)$, and the posterior probability that $\mu > 103$ is $1.49\%$ given the sample information.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Example: Math test}
\begin{VF}
\begin{lstlisting}[ language=R]
N <- 50 # Sample size
y_bar <- 102 # Sample mean
s2 <- 10 # Sample variance
alpha <- N - 1
serror <- (s2/N)^0.5 
LimInf <- y_bar - abs(qt(0.025, alpha)) * serror
LimInf
101.101
# Lower bound
LimSup <- y_bar + abs(qt(0.025, alpha)) * serror
LimSup
102.898
# Upper bound
y.cut <- 103
P <- 1-metRology::pt.scaled(y.cut, df = alpha, mean = y_bar, sd = serror)
P
0.0149
# Probability of mu greater than y.cut
\end{lstlisting}
\end{VF}
\end{tcolorbox}



\section{Summary}\label{sec27}

The differences between the Bayesian and Frequentist inferential approaches are philosophical, particularly with regard to the role of probability; pedagogical, especially in relation to the use of inference for decision-making; and methodological, due to differences in their mathematical and computational frameworks. Although, at the methodological level, the debate has become considerably muted—except for certain aspects of inference—there is widespread recognition that each approach has much to contribute to statistical practice \cite{Good1992, Bayarri2004, Kass2011}. As Bradley Efron stated, ``Computer-age statistical inference at its most successful \textbf{combines} elements of the two philosophies" \cite{efron2016computer}.

\section{Exercises}\label{sec28}
\begin{enumerate}
	\item \textbf{Jeffreys-Lindley's Paradox}
	
	The \textbf{Jeffreys-Lindley's paradox} \cite{Jeffreys1961,lindley1957statistical} represents an apparent disagreement between the Bayesian and Frequentist frameworks in a hypothesis testing scenario.
	
	In particular, assume that in a city, 49,581 boys and 48,870 girls have been born over 20 years. Assume that the male births follow a Binomial distribution with probability $\theta$. We wish to test the null hypothesis $H_0: \ \theta = 0.5$ versus the alternative hypothesis $H_1: \ \theta \neq 0.5$.
	
	\begin{itemize}
		\item Show that the posterior model probability for the null model is approximately 0.95. Assume $\pi(H_0) = \pi(H_1) = 0.5$, and that $\pi(\theta)$ follows a uniform distribution, i.e., ${U}(0,1)$, under $H_1$.
		\item Show that the \textit{p}-value for this hypothesis test is 0.0235 using the normal approximation, $Y \sim N(N \times \theta, N \times \theta \times (1 - \theta))$.
	\end{itemize}
	
	\item We want to test $H_0: \ \mu = \mu_0$ versus $H_1: \ \mu \neq \mu_0$ given $Y_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$.
	
	Assume $\pi(H_0) = \pi(H_1) = 0.5$, and that $\pi(\mu, \sigma) \propto 1/\sigma$ under the alternative hypothesis.
	
	Show that	
	\[
	p(\mathbf{y}|\mathcal{M}_1) = \frac{\pi^{-N/2}}{2} \Gamma(N/2) \left( \frac{1}{\alpha_n \hat{\sigma}^2} \right)^{N/2} \left( \frac{N}{\alpha_n \hat{\sigma}^2} \right)^{-1/2} \frac{\Gamma(1/2) \Gamma(\alpha_n/2)}{\Gamma((\alpha_n+1)/2)}
	\]
	and 
	\[
	p(\mathbf{y}|\mathcal{M}_0) = (2\pi)^{-N/2} \left[ \frac{2}{\Gamma(N/2)} \left( \frac{N}{2} \frac{\sum_{i=1}^N (y_i - \mu_0)^2}{N} \right)^{N/2} \right]^{-1}.
	\]
	
	Then, the posterior odds ratio is:	
	\begin{align*}
		PO_{01} &= \frac{p(\mathbf{y}|\mathcal{M}_0)}{p(\mathbf{y}|\mathcal{M}_1)} \\
		&= \frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_n/2)} (\alpha_n \hat{\sigma}^2 / N)^{-1/2} \left[ 1 + \frac{(\mu_0 - \bar{y})^2}{\alpha_n \hat{\sigma}^2 / N} \right]^{-\left(\frac{\alpha_n + 1}{2}\right)},
	\end{align*}
	
	where $\alpha_n = N - 1$ and $\hat{\sigma}^2 = \frac{\sum_{i=1}^N (y_i - \bar{y})^2}{N-1}$.
	
	Find the relationship between the posterior odds ratio and the classical test statistic for the null hypothesis.
	
	\item \textbf{Math Test Continues}
	
	Using the setting of the \textbf{Example: Math Test} in subsection \ref{sec261}, test $H_0: \ \mu = \mu_0$ versus $H_1: \ \mu \neq \mu_0$ where $\mu_0 = \left\{ 100, 100.5, 101, 101.5, 102 \right\}$.
	
	\begin{itemize}
		\item What is the \textit{p}-value for these hypothesis tests?
		\item Find the posterior model probability of the null model for each $\mu_0$.
	\end{itemize}
	
\end{enumerate}





