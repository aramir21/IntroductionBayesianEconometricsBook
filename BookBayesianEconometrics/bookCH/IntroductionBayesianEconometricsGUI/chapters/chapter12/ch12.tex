\chapter{Causal inference}\label{chap12}

In this chapter, we present some Bayesian methods to perform inference on \textit{causal} effects. The point of departure is \textit{identification conditions} which are the set of assumptions that allow us to express the \textit{estimand} —the causal or statistical quantity of interest— as a unique function of the observable data distribution. These identification conditions are conceptually distinct from the econometric or statistical framework used to perform inference once those conditions are satisfied. In other words, the assumptions necessary for identification do not constrain whether we apply a Frequentist or a Bayesian approach for estimation and inference.

Identification addresses the question: \textit{Can the causal effect be expressed as a function of the observable data distribution under certain assumptions?} Once the causal effect is identified in terms of the observed data distribution, we can proceed with statistical inference using either Frequentist methods or Bayesian methods. These inferential frameworks differ in how they estimate and quantify uncertainty but operate on the same identified causal effect. Thus, the identification assumptions are logically prior and independent of whether we adopt a Frequentist or a Bayesian inferential paradigm.

Thus, we present the underlying identification assumptions employed in popular strategies such as randomized controlled trials, conditional independence, and instrumental variables, among others, as well as the Bayesian inferential framework used to estimate causal effects in these settings. 

We should be clear that this is only an introduction to causal inference, and we do not aim to provide an in-depth discussion. There are excellent texts on causal inference, such as \cite{angrist2009mostly,angrist2014mastering,imbens2015causal,hernan2020causal,cunningham2021causal,chernozhukov2024applied}. We recommend that readers consult these references for a deeper understanding of some of the concepts and tools introduced in this chapter.

\section{Identification setting}

The aim is to identify the \textit{causal} or \textit{treatment effect} of a particular regressor or treatment on an outcome. For example, this could involve estimating the causal effect of a labor training program on individuals’ earnings or unemployment, or the price elasticity of demand. The starting point is the \textit{potential outcomes} framework \cite{rubin1974}, which defines a \textit{counterfactual scenario} describing what would happen to the outcome variable if the treatment status or level were different (commonly referred to as the \textit{Rubin causal model} \cite{holland1986statistics}).

In this context, \textit{treatment status} refers to a binary treatment case with two potential outcomes; for instance, what would my earnings be if I had participated in the training program? Conversely, \textit{treatment level} applies to cases where there is a potential outcome for each level of the treatment; for example, what would demand be if the price had increased by 10\%?

Following the potential outcomes notation in the binary treatment case, let $D_i = 1$ and $D_i = 0$ indicate the treatment status, corresponding to \textit{treated} and \textit{control} units, respectively. The potential outcomes $Y_i(1)$ and $Y_i(0)$ represent the outcome for unit $i = 1, 2, \dots, N$ under treatment and control, respectively. For instance, $Y_i(1)$ denotes the employment status an individual would have if she/he would have participated in the training program, regardless of their actual participation, whereas $Y_i(0)$ denotes the employment status if the individual would not have attended the program. The individual-level treatment effect is then defined as
\begin{align*}
	\tau_i = Y_i(1) - Y_i(0).
\end{align*}

The potential outcomes framework can also be extended to situations where the treatment is continuous \cite{imbens2014ivperspective}. In this case, let $Y_i(x_s)$ denote the outcome for unit $i$ under the counterfactual scenario where the treatment variable $X_s$ takes the value $x_s$. The ``treatment effect'' of changing $X_s$ from $x_s$ to $x_s'$, for example, the effect of increasing the price by 10\% on demand, is given by
\begin{align*}
	\beta_{si} = Y_i(x_s') - Y_i(x_s).
\end{align*}
When the change is infinitesimal, the causal effect can be expressed as a marginal effect:
\begin{align*}
	\beta_{si} =\frac{\partial Y_i(x_s)}{\partial x_s}.
\end{align*}
This setting is more complex than the binary treatment case because there is a potential outcome for each possible value of $x_s$ \cite{gill2001causal}. Therefore, we begin by considering the binary treatment case. However, \textit{the fundamental problem of causal inference} \cite{holland1986statistics} remains: it is impossible to observe the same unit under different \textit{treatment statuses} simultaneously. Consequently, we must learn about causal effects by comparing the outcomes of treated and untreated units.

Bayesian inference for causal effects \cite{rubin1978bayesian} is more direct than procedures based on Fisher's \textit{p}-value approach, which relies on the logic of stochastic proof by contradiction, or Neyman's randomization-based inference, which is grounded in the idea of repeated sampling and the construction of confidence intervals for treatment effects \cite{rubin2004teaching}.

Bayesian inference for causal effects treats the potential outcomes as random variables and involves computing the posterior predictive distribution to evaluate treatments not received, conditional on the observed responses to treatments actually received. This approach yields the posterior distribution of the causal estimands as a function of both the observed outcomes and the unobserved potential outcomes, which are treated as missing data and handled through data augmentation methods \cite{Tanner1987}. 

In addition, it is important to note that the likelihood function does not contain information about the correlation between potential outcomes due to \textit{the fundamental problem of causal inference}. Consequently, most of the classical literature on treatment effects has focused on average treatment effects, which require only the marginal distributions of the potential outcomes $Y(1)$ and $Y(0)$ \cite{heckman2014treatment}. Although, it is also possible to estimate \textit{quantile treatment effects} \cite{AbadieAngristImbens2002,Chernozhukov2005}.

By contrast, a Bayesian inferential approach allows us to estimate \textit{distributional treatment effects} with uncertainty quantification, as formally defined by \cite{aakvik2005estimating,heckman2014treatment}. These effects characterize the entire distribution of potential outcomes under treatment and control, rather than focusing solely on the average treatment effect, and thus capture heterogeneity. This is particularly relevant for policy, as it enables estimating the proportion of the population that benefits from a social program or the probability that a treated individual experiences a positive effect \cite{Ramirez2019a}.

Furthermore, under a Bayesian framework, the impact of adding or relaxing identification assumptions can be assessed by examining how the distributional treatment effects change \cite{imbens1997bayesian}.

\section{Randomized controlled trial (RCT)}

Two of the most relevant estimands in the causal inference literature are the average treatment effect (ATE),
\[
\text{ATE} = \mathbb{E}[Y_i(1) - Y_i(0)],
\]
and the average treatment effect on the treated (ATT),
\[
\text{ATT} = \mathbb{E}[Y_i(1) - Y_i(0) \mid D_i = 1].
\]

However, these estimands are not directly identifiable without additional assumptions because we never observe the same unit under both treatment states simultaneously. Therefore, identification restrictions are required to express these causal quantities in terms of the observed data.

Note that the observed mean difference between treated and untreated units can be decomposed as:
\begin{align*}
\mathbb{E}[Y_i \mid D_i = 1] - \mathbb{E}[Y_i \mid D_i = 0] 
& = \underbrace{\mathbb{E}[Y_i(1) - Y_i(0) \mid D_i = 1]}_{\text{ATT}}\\
& + \underbrace{\Big( \mathbb{E}[Y_i(0) \mid D_i = 1] - \mathbb{E}[Y_i(0) \mid D_i = 0]\Big)}_{\textit{Selection Bias}},
\end{align*}

that is, the observed mean difference equals the ATT plus the \textit{selection bias}, which captures the difference between the expected potential outcome of treated individuals had they not been treated (unobserved) and the expected outcome of untreated individuals (observed). 

The sign of the selection bias depends on the nature of self-selection into treatment. Two illustrative examples are:

\begin{table}[H]
	\caption{Illustrative examples of selection bias}
	\begin{minipage}{0.5\textwidth} % Adjust width as needed
		
		\begin{tabular}{lccc}
			\hline
			\textbf{Scenario} & $\mathbb{E}[Y(0)\mid D=1]$ & $\mathbb{E}[Y(0)\mid D=0]$ & Selection Bias \\
			\hline
			\textit{Training Program} & 80 & 70 & $+10$ \\
			\textit{Remedial Education} & 60 & 70 & $-10$ \\
			\hline
		\end{tabular}
	\end{minipage}
\end{table}

In the first scenario, motivated individuals self-select into a training program and would have higher earnings even without participation, producing a positive selection bias and causing the observed mean difference to overestimate the ATT. In the second scenario, disadvantaged individuals are more likely to enroll in a remedial education program and would have lower outcomes without the program, resulting in a negative selection bias and causing the observed mean difference to underestimate the ATT.

Note that if we assume that \textit{assignment to treatment is random},
\[
\{ Y_i(1), Y_i(0) \} \perp D_i,
\]
then the selection bias becomes zero,
\[
\mathbb{E}[Y_i(0) \mid D_i = 1] - \mathbb{E}[Y_i(0) \mid D_i = 0] = 0,
\]
which means there is no selection bias under random assignment.

Moreover, under random assignment, ATT equals ATE:
\[
\tau = \underbrace{\mathbb{E}[Y_i(1)\mid D_i=1]-\mathbb{E}[Y_i(0)\mid D_i=1]}_{\text{ATT}} 
= \underbrace{\mathbb{E}[Y_i(1)-Y_i(0)]}_{\text{ATE}}.
\]
Consequently,
\begin{equation}
	\tau=\mathbb{E}[Y_i \mid D_i = 1] - \mathbb{E}[Y_i \mid D_i = 0],
\end{equation}

that is, the average causal effect is function of the data under random assignment to treatment.
 
\textit{Randomized controlled trials} (RCTs) are characterized by \textit{random assignment to treatment}. The identification of causal effects in RCTs additionally relies on two key assumptions: \textit{overlap}, which requires that every unit has a positive probability of being assigned to both treatment and control groups ($0 < P(D_i = 1) < 1$), and the \textit{Stable Unit Treatment Value Assumption} (SUTVA). The latter consists of two components: (i) \textit{no interference}, meaning one unit’s potential outcome is unaffected by another unit’s treatment, and (ii) \textit{consistency}, meaning the observed outcome equals the potential outcome corresponding to the received treatment.

We can represent the causal mechanism in an RCT using causal diagrams \cite{pearl1995causal,pearl2018book}. These diagrams provide a powerful framework for visualizing and analyzing the underlying structures that enable causal identification. It is important to note that causal diagrams do not impose specific functional forms, such as those assumed in linear regression models, and are closely related to structural equation models (SEMs). SEMs offer an alternative perspective on causal inference in econometrics, which is closely connected to the potential outcomes approach \cite{haavelmo1943statistical,marschak1944random,imbens2014ivperspective}. See below.

In particular, we can depict the causal mechanism using a \textit{Directed Acyclic Graph (DAG)}, which is a graphical representation of a set of variables and their assumed causal relationships. A DAG consists of \textit{nodes}, representing variables, and \textit{directed edges (arrows)}, representing direct causal effects from one variable to another. The graph is \textit{acyclic}, meaning it contains no directed cycles: starting from any node and following the arrows, it is impossible to return to the same node. A defining property of causal DAGs
is that, conditional on its direct causes, any variable on the DAG is independent
of any other variable for which it is not a cause (\textit{causal Markov assumption}). See \cite{hernan2020causal} for a nice introduction. Figure~\ref{DAG1} illustrates the causal structure underlying RCTs.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		\node[main node] (D) {$D$};
		\node[main node] (Y) [right of=D] {$Y$};
		
		\path (D) edge (Y)
		(D) edge (Y);
	\end{tikzpicture}
	\caption{Directed Acyclic Graph (DAG) implied by a Randomized Controlled Trial (RCT).}
	\label{DAG1}
\end{figure} 

The counterfactual representation of the DAG, known as the \textit{Single-World Intervention Graph} (SWIG), is shown in Figure~\ref{SWIG1}. In this figure, the treatment variable $D$ is split to reflect the intervention: we fix $D$ at a specific value $d$ (the intervention), and replace the outcome $Y$ with its counterfactual representation $Y(d)$, which denotes the value of $Y$ if $D$ were set to $d$. The natural value of $D$ is also shown but becomes irrelevant for the outcome once the intervention is imposed.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		% Nodes
		\node[main node] (Dfix) {$D = d$};    % Fixed intervention
		\node[main node] (Y) [right of=Dfix] {$Y(d)$}; % Counterfactual outcome
		\node[main node] (Dnat) [below of=Dfix, node distance=1.5cm] {$D$}; % Natural value (irrelevant)
		
		% Edges
		\path (Dfix) edge[dashed] (Y); % Dashed arrow
	\end{tikzpicture}
	\caption{Single-World Intervention Graph (SWIG) for intervention $do(D=d)$: $Y$ is replaced by the counterfactual $Y(d)$, and $D$ is split into the fixed value $D=d$ and its natural value $D$. The dashed arrow indicates the fixed causal assignment.}
	\label{SWIG1}
\end{figure}

Note that we can express the observed outcome in terms of the potential outcomes as:
\[
Y_i = [Y_i(1) - Y_i(0)] D_i + Y_i(0),
\]
which shows that the observed outcome equals the control potential outcome plus the treatment effect if treated.

Under the assumption of a constant treatment effect and a linear \textit{Conditional Expectation Function} (CEF) $\mathbb{E}[Y_i\mid D_i]$, this relationship can be represented as a linear regression model \cite{angrist2009mostly}:
\[
Y_i = \underbrace{\beta_0}_{\mathbb{E}[Y_i(0)]} 
+ \underbrace{\tau}_{\text{constant treatment effect}} D_i 
+ \underbrace{\mu_i}_{Y_i(0) - \mathbb{E}[Y_i(0)]},
\]
where $\tau$ represents the common treatment effect across all units.

Under random assignment, we have
\begin{equation}\label{eq:12_1}
	\mathbb{E}[\mu_i \mid D_i] = \mathbb{E}[Y_i(0) \mid D_i] - \mathbb{E}[Y_i(0)] = \mathbb{E}[Y_i(0)] - \mathbb{E}[Y_i(0)] = 0,
\end{equation}
that is, the error term is \textit{mean-independent of treatment status}. This implies that a regression of $Y_i$ on $D_i$ consistently estimates the causal effect under random assignment.\\

\textbf{Example: Simple example}

Bayesian inference for causal effects in a completely randomized experiment without covariates can be illustrated using the normal model in \cite{rubin1990neyman}. Assume that $Y_i(d) \sim N(\mu_d, \sigma_d^2)$ for $d \in \{1,0\}$. Then, the likelihood function given a random sample and independent treatment assignment is:
\begin{align*}
	p(\mathbf{y} \mid \mathbf{d}; \mu_1,\mu_0,\sigma^2_1,\sigma^2_0)
	&= \prod_{i:d_i=1}\phi(y_i \mid \mu_1,\sigma_1^2) \prod_{i:d_i=0}\phi(y_i \mid \mu_0,\sigma_0^2) \\
	&= \prod_{i}\phi(y_i \mid \mu_1,\sigma_1^2)^{d_i} \, \phi(y_i \mid \mu_0,\sigma_0^2)^{1-d_i},
\end{align*}
where $\mathbf{y}$ and $\mathbf{d}$ collect the observations of the outcomes and treatments, and $\phi(\cdot)$ denotes the normal density function.

Assume conjugate priors: 
\[
\mu_d \mid \sigma^2_d \sim N\left(\mu_{0d}, \frac{\sigma^2_d}{\beta_{0d}}\right), 
\qquad 
\sigma^2_d \sim IG\left(\frac{\alpha_{0d}}{2}, \frac{\delta_{0d}}{2}\right).
\]
From the normal/inverse-gamma model in Chapter~\ref{chap4}, the posterior conditional distributions are:
\[
\mu_d \mid \sigma^2_d, \{y_i,d_i\}_{i:d_i=d} \sim N \left(\mu_{nd}, \frac{\sigma^2_d}{\beta_{nd}}\right), 
\]
where
\[
\mu_{nd} = \frac{\beta_{0d}\mu_{0d} + N_d \bar{y}_d}{\beta_{0d} + N_d}, 
\qquad 
\beta_{nd} = \beta_{0d} + N_d,
\]
and $\bar{y}_d$, $N_d$ denote the sample mean and sample size of group $d$. For the variance:
\[
\sigma^2_d \mid \{y_i,d_i\}_{i:d_i=d} \sim IG\left(\frac{\alpha_{nd}}{2}, \frac{\delta_{nd}}{2}\right),
\]
where
\[
\alpha_{nd} = \alpha_{0d} + N_d,
\qquad 
\delta_{nd} = \sum_{i:d_i=d} (y_i-\bar{y}_d)^2 + \delta_{0d} + \frac{\beta_{0d}N_d}{\beta_{0d}+N_d}(\bar{y}_d-\mu_{0d})^2.
\]

In addition, the predictive distribution is given by
\[
Y(d)\mid \mathbf{y}\sim t\left(\mu_{nd},\frac{(\beta_{nd}+1)\delta_{nd}}{\beta_{nd}\alpha_{nd}},\alpha_{nd}\right).
\]
Therefore, given the definition of the average treatment effect, 
\begin{align*}
 \text{ATE} & = \mathbb{E}[Y(1) - Y(0)]\\
 & = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\\
 & = \int_{\mathcal{R}} y(1) f_{Y(1)}(y(1))dy(1) - \int_{\mathcal{R}} y(0) f_{Y(0)}(y(0))dy(0)\\
 & = \mu_{1} - \mu_{0}.
\end{align*}

Note that this expectation is taken with respect to the population distribution of the potential outcomes, not with respect to the posterior distribution.\footnote{Other functions of the potential outcomes, such as $P(Y(1)\geq Y(0))$, require the joint distribution of the potential outcomes rather than just the marginal distributions, as is the case for the ATE.} Thus, the posterior distribution of the ATE is given by 
\[
\pi(\text{ATE}\mid \mathbf{y}) = \pi(\mu_{1} - \mu_{0}\mid \mathbf{y}).
\]

We can obtain this posterior distribution by simulation, because the difference of two independent Student's $t$-distributed random variables does not follow a Student's $t$ distribution. However, as the degrees of freedom increase, each posterior distribution of $\mu_{d}$ converges to a normal distribution, and the difference of two normals is also normal. Consequently, an approximate point estimate of the ATE is $\mu_{n1} - \mu_{n0}$, which asymptotically equals $\bar{y}_1 - \bar{y}_0$ under non-informative priors. This is the classical \textit{estimator} of the ATE.

Finally, note that we can also obtain the posterior distribution of the ATE using the simple linear regression framework by assuming non-informative prior distributions, in which case the posterior mean of the slope parameter coincides with the maximum likelihood estimator (see Exercise~1).

Let's assume that $\mu_1 = 15$, $\mu_0 = 10$, $\sigma_1^2 = 4$, $\sigma_0^2 = 2$, and $N_1=N_0=100$, and compute the posterior distribution of the ATE. The following code illustrates this procedure, while Figures \ref{fig12_0} and \ref{fig12_0a} display the predictive densities of the potential outcomes and the posterior distribution of the ATE. The posterior mean is 4.60, and the 95\% credible interval is (4.10, 5.12). Note that the population value is $\mu_1-\mu_0=5$.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code0_chap12}
	\textit{R code. Simple example: Average treatment effects}
	\begin{VF}
		\begin{lstlisting}[language=R]		
set.seed(10101)
# Parameters
mu1 <- 15; mu0 <- 10
sigma1 <- sqrt(4); sigma0 <- sqrt(2)
N1 <- 100; N0 <- 100
# Simulate data
y1 <- rnorm(N1, mu1, sigma1); y0 <- rnorm(N0, mu0, sigma0)
# Prior hyperparameters
alpha0 <- 0.01; delta0 <- 0.01
simulate_t <- function(y) {
	N <- length(y); ybar <- mean(y)
	sse <- sum((y - ybar)^2)
	alpha_n <- alpha0 + N; delta_n <- sse + delta0
	scale2 <- ((N + 1) * delta_n) / (N * alpha_n)
	df <- alpha_n
	loc <- ybar; scale <- sqrt(scale2)
	rt(1, df = df) * scale + loc
}
# Posterior predictive draws
ppd1 <- replicate(1000, simulate_t(y1))
ppd0 <- replicate(1000, simulate_t(y0))
# Plot
hist(ppd1, col = rgb(1, 0, 0, 0.4), freq = FALSE, main = "Posterior Predictive",
xlab = "Y", xlim = c(5, 25))
hist(ppd0, col = rgb(0, 0, 1, 0.4), freq = FALSE, add = TRUE)
legend("topright", legend = c("Treatment", "Control"),
fill = c(rgb(1, 0, 0, 0.4), rgb(0, 0, 1, 0.4)))
# Posterior distribution of ATE
posterior_mu <- function(y) {
	N <- length(y); ybar <- mean(y)
	sse <- sum((y - ybar)^2)
	alpha_n <- alpha0 + N
	delta_n <- sse + delta0
	# Posterior variance for mean
	var_mu <- delta_n / (alpha_n * N)
	df <- alpha_n
	loc <- ybar; scale <- sqrt(var_mu)
	rt(1, df = df) * scale + loc
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code0_chap12}
	\textit{R code. Simple example: Average treatment effects}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# Posterior draws for parameters
n_draws <- 10000
mu1_draws <- replicate(n_draws, posterior_mu(y1))
mu0_draws <- replicate(n_draws, posterior_mu(y0))
# Parameter uncertainty: difference of means
ate_draws <- mu1_draws - mu0_draws
summary(coda::mcmc(ate_draws))
# Summaries
ate_mean <- mean(ate_draws)
ci <- quantile(ate_draws, c(0.025, 0.975))
cat("Posterior mean of ATE:", ate_mean, "\n")
cat("95% Credible Interval:", ci, "\n")

# Plot posterior distribution of ATE
hist(ate_draws, breaks = 50, freq = FALSE,
main = "Posterior Distribution of ATE",
xlab = "ATE (Y(1) - Y(0))", col = "lightblue", border = "white")
abline(v = ate_mean, col = "red", lwd = 2)
abline(v = ci, col = "darkgreen", lty = 2, lwd = 2)

legend("topright", legend = c("Posterior Mean", "95% Credible Interval"), col = c("red", "darkgreen"), lwd = 2, lty = c(1, 2), bty = "n", cex = 0.8)  # Smaller legend using cex
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  
 

\begin{figure}
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/BasicYd.png}
	\caption[List of figure caption goes here]{Predictive distributions: Potential outcomes.}\label{fig12_0}
\end{figure}


\begin{figure}
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/BasicATE.png}
	\caption[List of figure caption goes here]{Posterior distribution: Average treatment effects.}\label{fig12_0a}
\end{figure}


An RCT is considered the gold standard for identifying causal effects because it provides the strongest basis for satisfying the key identification assumption in causal inference: \textit{independence between treatment assignment and potential outcomes}. However, RCTs may face challenges such as \textit{non-compliance}, which occurs when individuals do not adhere to their assigned treatment in an experimental study. This is important because, in many real-world settings, some individuals do not take the treatment they were assigned, leading to deviations from the ideal randomized controlled trial design. In addition, RCTs are sometimes infeasible due to ethical, logistical, or financial constraints. Moreover, they can be too narrowly focused or too localized to provide general conclusions about what works \cite{deaton2010instruments}.

It is also important to note that RCTs primarily identify the mean of the treatment effect distribution but do not capture other features, such as the median or higher-order moments. These additional aspects of the distribution of treatment effects can be highly relevant for policymakers and stakeholders. See \cite{deaton2010instruments} for a detailed discussion of other potential shortcomings of RCTs.


\section{Conditional independence assumption (CIA)}

In practice, researchers often work with \textit{observational data}, where the treatment status is not randomly assigned because units actively choose the treatment they receive. In an RCT, the assignment mechanism is determined by chance, whereas in observational studies it is driven by choice. In these situations, we can identify the causal effect if the \textit{conditional independence assumption} (CIA) holds. This assumption states that the potential outcomes are independent of the treatment status, conditional on a set of observed  \textit{pre-treatment variables}:
\[
\{ Y_i(1), Y_i(0) \} \perp D_i \mid \mathbf{X}_i.
\]
This means that, conditional on the \textit{pre-treatment variables} $\mathbf{X}_i$, treatment assignment is as good as random. This property is known as \textit{unconfoundedness given} $\mathbf{X}_i$, or equivalently, the \textit{no unmeasured confounders} assumption. When this condition is combined with the requirement that, for all possible values of $\mathbf{X}_i$, there is a positive probability of receiving each treatment level ($0 < P(D_i = 1 \mid \mathbf{X}_i) < 1$), the joint condition is referred to as \textit{strong ignorability}. Identification of average causal effects in this setting also requires the SUTVA.

Note that under the CIA,
\begin{align*}
	\text{ATE} &= \mathbb{E}[Y_i(1)] - \mathbb{E}[Y_i(0)] \\
	&= \mathbb{E}_{\mathbf{X}}\left\{ \mathbb{E}[Y_i(1)\mid \mathbf{X}_i] - \mathbb{E}[Y_i(0)\mid \mathbf{X}_i] \right\} \\
	&= \mathbb{E}_{\mathbf{X}}\left\{ \mathbb{E}[Y_i(1)\mid \mathbf{X}_i, D_i=1] - \mathbb{E}[Y_i(0)\mid \mathbf{X}_i, D_i=0] \right\} \\
	&= \mathbb{E}_{\mathbf{X}}\left\{ \mathbb{E}[Y_i\mid \mathbf{X}_i, D_i=1] - \mathbb{E}[Y_i\mid \mathbf{X}_i, D_i=0] \right\} \\
	&= \mathbb{E}_{\mathbf{X}}\left\{ \tau(\mathbf{X}_i) \right\},
\end{align*}
where the second equality follows from the law of iterated expectations, the third uses CIA, and
\[
\tau(\mathbf{X}_i) = \mathbb{E}[Y_i \mid \mathbf{X}_i, D_i=1] - \mathbb{E}[Y_i \mid \mathbf{X}_i, D_i=0]
\]
is the \textit{conditional average treatment effect} (CATE) at covariate value $\mathbf{X}_i$, which captures treatment effect heterogeneity across different values of $\mathbf{X}$. Therefore, the ATE is the expectation of the CATE over the distribution of $\mathbf{X}$.

Figure~\ref{DAG2} illustrates the causal structure underlying the CIA. We follow the convention that time flows from left to right. Here, $\mathbf{X}$ -a vector of pre-treatment variables or \textit{confounders}- influences both the treatment ($D$) and the outcome ($Y$). Under the CIA, we can identify the causal effect of $D$ on $Y$ by adjusting for $\mathbf{X}$ because this causal structure satisfies the \textit{back-door criterion}. This criterion states that a set of variables $\mathbf{X}$ satisfies the condition for identifying the effect of $D$ on $Y$ if no variable in $\mathbf{X}$ is a descendant of $D$ and $\mathbf{X}$ blocks every back-door path from $D$ to $Y$. A \textit{back-door path} is any path from $D$ to $Y$ that begins with an arrow pointing into $D$. In addition, Figure \ref{SWIG2} displays the SWIG. 

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		\node[main node] (X) {$\mathbf{X}$};
		\node[main node] (D) [right of=X] {$D$};
		\node[main node] (Y) [right of=D] {$Y$};
		
		\path (X) edge (D)
		(X) edge[bend left=20] (Y)
		(D) edge (Y);
	\end{tikzpicture}
	\caption{Directed Acyclic Graph (DAG) implied by the Conditional Independence Assumption (CIA).}
	\label{DAG2}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		% Nodes
		\node[main node] (X) {$\mathbf{X}$};
		\node[main node] (Dfix) [right of=X] {$D = d$}; % Intervention
		\node[main node] (Y) [right of=Dfix] {$Y(d)$};  % Counterfactual outcome
		\node[main node] (Dnat) [below of=Dfix, node distance=1.5cm] {$D$}; % Natural value
		
		% Edges
		\path (Dfix) edge[dashed] (Y)
		(X) edge[bend left=25] (Y)
		(X) edge (Dnat);
	\end{tikzpicture}
	\caption{Single-World Intervention Graph (SWIG) for $do(D=d)$: The treatment $D$ is split into the fixed intervention $D=d$ and its natural value. The outcome is replaced by $Y(d)$. $\mathbf{X}$ still influences $Y(d)$, so adjustment for $\mathbf{X}$ is needed for identification.}
	\label{SWIG2}
\end{figure}
 
The simple regression framework of the previous section can be extended to a multiple linear regression model by including the \textit{pre-treatment variables}; therefore, the CEF is linear in the treatment and pre-treatment variables:
\begin{align*}
	Y_i = \beta_0 + \tau D_i + \mathbf{X}_i^{\top}\boldsymbol{\beta} + \mu_i.
\end{align*}
Note that, by assumption in RCTs, $D_i$ and $\mathbf{X}_i$ are independent. Thus, the identification of $\tau$ is not affected under random assignment; however, including $\mathbf{X}_i$ helps explain part of the variability in $Y_i$, thereby improving the precision of the estimates.

On the other hand, if the CIA is satisfied, the error term in the linear regression is defined as
\[
\mu_i = Y_i(0) - \mathbb{E}[Y_i(0) \mid \mathbf{X}_i].
\]

Thus,
\[
\mathbb{E}[\mu_i \mid D_i, \mathbf{X}_i] = \mathbb{E}[Y_i(0) - \mathbb{E}[Y_i(0) \mid \mathbf{X}_i] \mid D_i, \mathbf{X}_i] 
= \mathbb{E}[Y_i(0) \mid D_i, \mathbf{X}_i] - \mathbb{E}[Y_i(0) \mid \mathbf{X}_i].
\]

By the CIA, $Y_i(0) \perp D_i \mid \mathbf{X}_i$, so:
\[
\mathbb{E}[Y_i(0) \mid D_i, \mathbf{X}_i] = \mathbb{E}[Y_i(0) \mid \mathbf{X}_i].
\]

Therefore:
\begin{equation}\label{eq:12_2}
	\mathbb{E}[\mu_i \mid D_i, \mathbf{X}_i] = 0.
\end{equation}

This condition is known as \textit{conditional mean independence}: after controlling for $\mathbf{X}_i$, treatment assignment is independent of unobserved determinants of the outcome. It justifies unbiased estimation of $\tau$ by regression adjustment in observational studies.\\

\textbf{Example: Treatment effect of 401(k) eligibility on net financial assets}

We study the average treatment effect of eligibility (\textit{e401}) for participation in the 401(k) retirement savings plan in the United States on net financial assets (\textit{net\_tfa}) using the dataset \textit{401k.csv}. The rationale for the exogeneity of 401(k) eligibility is that it becomes exogenous after conditioning on observable characteristics related to job choice, which may correlate with whether a firm offers this retirement plan \cite{chernozhukov2024applied}.

Accordingly, we control for the following covariates: age (\textit{age}), income (\textit{inc}), family size (\textit{fsize}), years of education (\textit{educ}), a marital status indicator (\textit{marr}), a two-earner status indicator (\textit{twoearn}), a defined benefit pension status indicator (\textit{db}), an IRA participation indicator (\textit{pira}), and a home ownership indicator (\textit{hown}). Under this specification, the key assumption is that eligibility is conditionally independent of net financial assets given these covariates \cite{chernozhukov2024applied}.

We can use the framework in Section \ref{sec61} to estimate the average treatment effect of 401k eligibility on net financial assets. The code \ref{code1_chap12} shows how to get the posterior distribution of this effect. Figure \ref{fig12_1} displays the posterior distribution of the treatment effect, the 95\% credible interval is (USD 3,473, USD 8,371), and the posterior mean is USD 5,903.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code1_chap12}
	\textit{R code. Treatment effect: 401(k) eligibility on net financial assets}
	\begin{VF}
		\begin{lstlisting}[language=R]		
# 401k: Treatment effects
rm(list = ls())
set.seed(10101)

library(MCMCpack)
library(coda)
library(ggplot2)

mydata <- read.csv("https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv", sep = ",", header = TRUE, quote = "")
attach(mydata )
y <- net_tfa 
# net_tfa: net financial assets
# Regressors quantity including intercept
X <- cbind(e401, age, inc, fsize, educ, marr, twoearn, db, pira, hown, 1)
# e401: 401k eligibility 
# age, income, family size, years of education, marital status indicator, two-earner status indicator, defined benefit pension status indicator, IRA participation indicator, and home ownership indicator.

# Posterior distributions using packages: MCMCpack sets the model in terms of the precision matrix. We use default parameters
posterior  <- MCMCpack::MCMCregress(y~X-1)
summary(coda::mcmc(posterior))

# Extract posterior samples for e401 (first coefficient)
beta_e401 <- posterior[, 1]

# Convert to data frame for ggplot
df_posterior <- data.frame(beta_e401)

# Plot with ggplot
ggplot(df_posterior, aes(x = beta_e401)) +
geom_density(fill = "steelblue", alpha = 0.6) +
geom_vline(xintercept = mean(beta_e401), color = "red", linetype = "dashed") +
labs(
title = "Posterior Distribution of Treatment Effect (e401)",
x = expression(beta[e401]),
y = "Density"
) +
theme_minimal(base_size = 14)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  

\begin{figure}
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/TE401k.png}
	\caption[List of figure caption goes here]{Posterior distribution: Treatment effect 401k eligibility on net financial assets.}\label{fig12_1}
\end{figure}

Observe that the identification strategy relies on conditioning on $\mathbf{X}$. However, adding more controls is not always safe because some variables, known as \textit{bad controls}, can introduce bias if included in the adjustment set \cite{angrist2009mostly}. One important type of bad control is a \textit{collider}, a variable that is caused by (or is a common effect of) two or more other variables in a DAG. Colliders play a critical role because conditioning on them can create spurious associations between their causes, leading to what is known as \textit{collider bias} (or selection bias).

Figure~\ref{DAG3} illustrates this situation. Here, $C$ is a common effect of $D$ and $\mathbf{X}$. Conditioning on $C$ opens an additional path between $D$ and $Y$, creating a spurious association that violates the back-door criterion. In particular, the variable $C$ is a collider on the path $D \to C \leftarrow \mathbf{X}$. By default, a collider \textit{blocks} the flow of association along the path on which it lies, so this path is closed when $C$ is not conditioned on. However, conditioning on $C$ (or on any descendant of $C$) opens this path, creating a spurious association between $D$ and $\mathbf{X}$. Since $\mathbf{X}$ also affects $Y$, this induces bias in estimating the causal effect of $D$ on $Y$.\\ 

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		% Nodes
		\node[main node] (X) {$\mathbf{X}$};
		\node[main node] (D) [right of=X] {$D$};
		\node[main node] (Y) [right of=D] {$Y$};
		\node[main node] (C) [below of=Y, node distance=1cm] {$C$}; % Collider
		
		% Edges
		\path (X) edge (D)
		(X) edge[bend left=20] (Y)
		(D) edge (Y)
		(D) edge (C)
		(X) edge (C);
	\end{tikzpicture}
	\caption{Directed Acyclic Graph (DAG): The additional collider $C$ caused by both $D$ and $\mathbf{X}$ would open a path between $D$ and $\mathbf{X}$, inducing bias.}
	\label{DAG3}
\end{figure}

\textbf{Example: Birth-weight ``paradox", collider bias}

The collider bias illustrated in DAG \ref{fig:coll}, based on \cite{chernozhukov2024applied}, reflects the well-known Birth-weight ``paradox". This phenomenon arises when conditioning on an intermediate variable (birth weight, $B$) induces a spurious association between the binary indicator of smoking ($S$) and infant mortality ($Y$). In this setting, $U$ represents unobserved factors that affect both birth weight and infant mortality. Conditioning on $B$ creates collider bias because it opens a back-door path through the unobserved factors $U$ (dashed).

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[
		node distance=2cm,
		thick,
		main/.style={circle, draw, minimum size=8mm},
		>={Stealth}
		]
		
		% Nodes
		\node[main] (S) {$S$};
		\node[main] (B) [below right=1.2cm and 1.8cm of S] {$B$};
		\node[main, dashed] (U) [below left=1.2cm and 1.8cm of B] {$U$};
		\node[main] (Y) [right=1cm of B] {$Y$};
		
		% Arrows
		\draw[->] (S) -- (B);
		\draw[->] (S) -- (Y);
		\draw[->] (B) -- (Y);
		\draw[->] (U) -- (B);
		\draw[->] (U) -- (Y);
		
	\end{tikzpicture}
	\caption{DAG illustrating the birth-weight paradox: $S$ (smoking), $B$ (birth weight), $Y$ (infant mortality), $U$ (other unobserved health factors).}
	\label{fig:coll}
\end{figure}	

The code in \ref{code2_chap12} illustrates the bias by performing 100 replications, and Figure \ref{fig12_2} shows the distribution of posterior means across these replications. In this setting, the total effect of smoking on infant mortality is 2, consisting of a direct effect of 1 and an indirect effect through birth weight, also equal to 1.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code2_chap12}
	\textit{R code. Collider bias: Birth weight ``paradox"}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(10101)
library(MCMCpack); library(dplyr); library(ggplot2)
# Parameters
n <- 1000; true_effect <- 2; replications <- 100
# Store results
results <- data.frame(rep = 1:replications, correct = numeric(replications), biased = numeric(replications))

for (r in 1:replications) {
	# Simulate data under DAG: S -> B -> Y, S -> Y, U -> B, U -> Y
	U <- rnorm(n, 0, 1); S <- rbinom(n, prob = 0.7, size = 1)
	B <- S + U + rnorm(n)
	Y <- S + B + 1.5*U + rnorm(n)
	# Correct model: does NOT condition on collider B
	model_correct <- MCMCregress(Y ~ S, burnin = 1000, mcmc = 3000, verbose = FALSE)
	# Biased model: conditions on collider B
	model_biased <- MCMCregress(Y ~ S + B, burnin = 1000, mcmc = 3000, verbose = FALSE)
	# Posterior means for S
	results$correct[r] <- mean(as.matrix(model_correct)[, "S"])
	results$biased[r] <- mean(as.matrix(model_biased)[, "S"])
}
# Compute bias
results <- results %>% mutate(
bias_correct = correct - true_effect,
bias_biased = biased - true_effect
)
# Average bias and SD
avg_bias <- results %>%
summarise(
mean_correct = mean(bias_correct),
mean_biased = mean(bias_biased),
sd_correct = sd(bias_correct),
sd_biased = sd(bias_biased)
)
print(avg_bias)
# Visualization: distribution of posterior means across 100 simulations
df_long <- results %>%
select(rep, correct, biased) %>%
tidyr::pivot_longer(cols = c(correct, biased), names_to = "model", values_to = "estimate")

ggplot(df_long, aes(x = estimate, fill = model)) + geom_density(alpha = 0.5) + geom_vline(xintercept = true_effect, color = "black", linetype = "dashed", linewidth = 1) + labs(title = "Posterior Means Across 100 Simulations", x = expression(paste("Posterior Mean of ", beta[S])), y = "Density") + theme_minimal(base_size = 14)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/FigColliderBias.png}
	\caption[List of figure caption goes here]{Posterior distribution: Posterior means of the collider bias example.}\label{fig12_2}
\end{figure}

Collider (selection) bias is one possible source of bias in the identification of causal effects. The well-known Heckman’s sample selection problem in econometrics \cite{heckman1979sample} can be interpreted as a form of collider bias because restricting the analysis to selected observations (e.g., those with positive wages) conditions on a variable that is a common effect of observed and unobserved factors, thereby opening a non-causal path and creating bias. In other words, the sample is no longer representative of the population, which undermines the identification of the causal effect. 

Other well-known sources of bias in econometrics include the omission of common causes affecting both the treatment and the outcome, that is, \textit{omission of correlated relevant regressors}, measurement error in regressors leading to \textit{attenuation bias} (where the estimated causal effect is biased toward zero), and \textit{reverse or simultaneous causality}, which often arises in systems of equations (see \cite{wooldridge2010econometric} for details). We will discuss these additional sources of bias later.

\section{Instrumental variables (IV)}

In many real-world situations, we face biases such as those described in the previous paragraph, arising from measurement errors, omission of relevant variables, and similar issues. Even in these cases, it is still possible to identify the causal effect. One common strategy is to use a set of \textit{instrumental variables} ($\mathbf{Z}$) that satisfy two key conditions:
\begin{enumerate}
	\item \textit{Relevance}, meaning the instruments are correlated with the treatment:
	\[
	\mathbf{Z}_i \not\perp D_i.
	\]
	\item \textit{Exogeneity}, which in the linear model requires:
	\begin{equation}\label{eq:12_3}
		\mathbb{E}[\mu_i \mid \mathbf{Z}_i] = 0,
	\end{equation}
	and, in terms of potential outcomes, entails:
	\begin{itemize}
		\item \textit{Exclusion restriction}: instruments affect the outcome only through the treatment:
		\[
		Y_i(D_i = d, \mathbf{Z}_i = \mathbf{z}) = Y_i(D_i = d, \mathbf{Z}_i = \mathbf{z}') = Y_i(D_i = d), \quad d \in \{0,1\},
		\]
		\item \textit{Marginal exchangeability}: instruments are independent of the potential outcomes:
		\[
		\mathbf{Z}_i \perp \{ Y_i(1), Y_i(0) \}.
		\]
	\end{itemize}
\end{enumerate}

Relevance is testable, typically by checking whether instruments significantly predict the endogenous variable \cite{staiger1997instrumental,cragg1993testing,kleibergen2006generalized,stock2005asymptotic}. However, \textit{weak instruments}, those that exhibit only a weak association with the treatment, can lead to serious consequences, including a high level of uncertainty and the potential amplification of even small biases when estimating the causal effect.

Exogeneity is fundamentally untestable. However, when the number of instruments exceeds the number of endogenous variables, the Sargan test (or its robust version, the Hansen $J$-test) can be used to assess the validity of overidentifying restrictions. However, it should not be misinterpreted as a direct test of the exclusion restriction. Instead, the test evaluates whether the overidentifying restrictions implied by the IV model hold. Specifically, the null hypothesis states that all instruments are jointly uncorrelated with the structural error term: $\mathbb{E}[\mathbf{Z}^\top \mu] = \mathbf{0}$ \cite{sargan1958econometric,hansen1982large}. If the test rejects, it indicates that at least one instrument is invalid, but it does not reveal whether the violation results from a failure of the exclusion restriction, correlation with unobserved confounders, or model misspecification. Conversely, if the test fails to reject, it suggests that the sample moment conditions do not provide evidence against instrument validity under the maintained model, but this does not prove that the exclusion restrictions hold. Therefore, while the Sargan test is informative about overall instrument validity, it is not a separate or definitive test of the exclusion restriction \cite{wooldridge2010econometric,hayashi2000econometrics}.

Figure~\ref{DAG4} illustrates a situation where pre-treatment variables that influence both the treatment and the outcome are partially observed or measured with error. In such cases, an instrument can help identify the causal effect of the treatment on the outcome.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,node distance=2.5cm,
		thick,main node/.style={circle,draw,minimum size=7mm}]
		% Nodes
		\node[main node, dashed] (U) {$\mathbf{U}$};
		\node[main node] (D) [right of=X] {$D$};
		\node[main node] (Y) [right of=D] {$Y$};
		\node[main node] (Z) [below of=D, node distance=2cm] {$Z$};
		
		% Edges
		\path (Z) edge (D)
		(U) edge[bend left=20] (Y)
		(D) edge (Y)
		(U) edge (D);
	\end{tikzpicture}
	\caption{Directed Acyclic Graph (DAG) showing an instrumental variable $Z$ used to identify the causal effect of $D$ on $Y$ when some confounders $\mathbf{U}$ (dashed) are unobserved or measured with error.}
	\label{DAG4}
\end{figure}

However, the two conditions of the instruments only imply partial identification of the treatment effect \cite{manski1990nonparametric,manski1995identification,manski2003partial}. Thus, it is necessary to add a third condition to get point identification of a particular treatment effect using IV. This condition is \textit{monotonicity} which asserts that the instrument moves all units’ treatment decisions in the same direction, that is,
\[
D_i(1)\geq D_i(0), i=1,2,\dots,N,
\]
with equality for at least one unit $i$, and assuming a binary instrument $Z_i=\left\{0,1\right\}$. 

In the case where $Z_i$ denotes the assignment to treatment and $D_i$ the actual treatment status, there are four potential groups: \textit{always-takers} ($D = 1 \mid Z = 1$ and $D = 1 \mid Z = 0$), \textit{never-takers} ($D = 0 \mid Z = 1$ and $D = 0 \mid Z = 0$), \textit{compliers} ($D = 1 \mid Z = 1$ and $D = 0 \mid Z = 0$), and \textit{defiers} ($D = 0 \mid Z = 1$ and $D = 1 \mid Z = 0$). Note that these groups are not identified; for instance, we do not know whether an individual with $Z_i = 1$ and $D_i = 1$ is a complier or an always taker. The monotonicity assumption rules out defiers, meaning that the instrument (assignment to treatment) either does not change treatment status or only increases it.

Therefore, under relevance, exogeneity, monotonicity, and SUTVA, IV methods allow identification of the causal effect for the subgroup of \textit{compliers}, that is, individuals who take the treatment when encouraged by the instrument and do not take it otherwise \cite{imbens1994identification,angrist1996identification}. This estimand is known as the \textit{Local Average Treatment Effect} (LATE) because it applies to a specific subpopulation rather than the entire population \cite{hernan2020causal}:

\begin{align*}
	\tau_{LATE} 
	&= \mathbb{E}[Y_i(1)-Y_i(0)\mid D_i(1)=1, D_i(0)=0].
\end{align*}

We define $Y_i(z_i,D_i(z))$ to be the outcome for unit $i$ if exposed to treatment $D_i(z)$ after being assigned to treatment $z$. Therefore, the \textit{Intention-to-Treat} (ITT) causal effect of $Z_i$ on $Y_i$ is:
{\scriptsize{
\begin{align*}
	\mathbb{E}[Y_i(1,D_i(1))-Y_i(0,D_i(0))]&=\underbrace{\mathbb{E}[Y_i(1,D_i(1))-Y_i(0,D_i(0))\mid D_i(1)=1,D_i(0)=1]\times P(D_i(1)=1,D_i(0)=1)}_{always-takers}\\
	&+\underbrace{\mathbb{E}[Y_i(1,D_i(1))-Y_i(0,D_i(0))\mid D_i(1)=0,D_i(0)=0]\times P(D_i(1)=0,D_i(0)=0)}_{never-takers}\\
	&+\underbrace{\mathbb{E}[Y_i(1,D_i(1))-Y_i(0,D_i(0))\mid D_i(1)=1,D_i(0)=0]\times P(D_i(1)=1,D_i(0)=0)}_{compliers}\\
	&+\underbrace{\mathbb{E}[Y_i(1,D_i(1))-Y_i(0,D_i(0))\mid D_i(1)=0,D_i(0)=1]\times P(D_i(1)=0,D_i(0)=1)}_{defiers},\\
\end{align*}
}}
that is, ITT is a mixture over the four potential groups.

We know that the intention-to-treat (ITT) effect of $Z_i$ on $Y_i$ is zero for always-takers and never-takers due to the exclusion restriction, i.e., $Z_i$ affects $Y_i$ only through $D_i$, and $D_i$ does not vary with $Z_i$ for always-takers and never-takers. Moreover, the monotonicity assumption rules out the existence of defiers. Hence, only compliers contribute to the effect of $Z_i$ on $Y_i$:

{\scriptsize
	\begin{align*}
		\mathbb{E}[Y_i(1,D_i(1)) - Y_i(0,D_i(0))] 
		&= \mathbb{E}[Y_i(1,D_i(1)) - Y_i(0,D_i(0)) \mid D_i(1)=1, D_i(0)=0] \cdot P(D_i(1)=1, D_i(0)=0).
	\end{align*}
}

For compliers, $D_i = Z_i$, so we can simplify $Y_i(1,D_i(1)) = Y_i(1,1) = Y_i(1)$ and $Y_i(0,D_i(0)) = Y_i(0,0) = Y_i(0)$. Thus,

\begin{align*}
	\mathbb{E}[Y_i(1) - Y_i(0)] 
	&= \mathbb{E}[Y_i(1) - Y_i(0) \mid D_i(1) = 1, D_i(0) = 0] \cdot P(D_i(1) = 1, D_i(0) = 0),
\end{align*}

which implies that

\begin{align*}
	\tau_{LATE} = \mathbb{E}[Y_i(1) - Y_i(0) \mid D_i(1) = 1, D_i(0) = 0] 
	&= \frac{\mathbb{E}[Y_i(1) - Y_i(0)]}{P(D_i(1) = 1, D_i(0) = 0)}.
\end{align*}

Under the assumption that the instrument \( Z_i \) is independent of the potential outcomes (i.e., random assignment), we have
\[
\tau_{LATE} = \mathbb{E}[Y_i(1) - Y_i(0) \mid D_i(1) = 1, D_i(0) = 0] 
= \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{P(D_i(1) = 1, D_i(0) = 0)}.
\]

Note that \( P(D_i = 1 \mid Z_i = 1) = P(\text{compliers}) + P(\text{always-takers}) \), because only these groups can receive treatment when \( Z_i = 1 \). Also, under monotonicity (which rules out defiers), \( P(D_i = 1 \mid Z_i = 0) = P(\text{always-takers}) \). Thus,
\[
P(D_i = 1 \mid Z_i = 1) - P(D_i = 1 \mid Z_i = 0) = P(\text{compliers}) = P(D_i(1) = 1, D_i(0) = 0).
\]

Finally, taking into account that \( \mathbb{E}[D_i \mid Z_i] = P(D_i = 1 \mid Z_i) \), we obtain
\[
\tau_{LATE} = \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0]}.
\]

This is the standard instrumental variables (IV) estimand, often referred to as the \textit{Wald estimator}. Note that the \textit{relevance condition} is required to ensure identification, i.e.,
\[
\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0] \neq 0.
\]

The IV estimand represents the ratio of the intention-to-treat effect on the outcome to the intention-to-treat effect on the treatment (i.e., the proportion of compliers). The instrument induces random variation in treatment: the numerator captures how outcomes respond to the instrument, and the denominator captures how treatment responds to the instrument. Dividing the two isolates the causal effect for compliers.

Note that the LATE is not the global average causal effect, and has been subject to several criticisms. First, the proportion of compliers in the population may be small, raising concerns about the policy relevance of this estimand. Observe that while we cannot identify all four principal strata (compliers, never-takers, always-takers, and defiers), the proportion of compliers is identifiable. Second, the monotonicity assumption is not always plausible, particularly in observational studies where instruments may induce heterogeneous behavioral responses. Third, partitioning the population into four strata can become ill-defined in settings where instruments arise from different mechanisms or individual-specific criteria \cite{deaton2010instruments,hernan2020causal}. For arguments in defense of LATE against these critiques, see \cite{angrist2010better}.\\

\textbf{Example: Effects of vitamin A supplements on children's survival}

This example is taken from \cite{imbens1997bayesian}. The authors perform Bayesian inference for causal effects in a completely randomized experiment with no covariates and partial compliance. The experiment was conducted in Indonesia, where children were randomly assigned to receive vitamin A supplements. No individual assigned to the control group ($Z_i = 0$) actually took the vitamin, meaning that $D_i(0) = 1$ was never observed. This rules out always-takers and defiers. However, some individuals assigned to the treatment group ($Z_i = 1$) did not take the vitamin, so $D_i(1) = 0$ occurred for some units. Consequently, $D_i(1) \geq D_i(0)$ holds for all individuals (there are not defiers), satisfying the monotonicity assumption. Under this setting, the population consists only of compliers and never-takers.

\begin{table}[h!]
	%\centering % This ensures the table is centered on the page
	\caption{Effects of vitamin A supplements on children's survival \cite{imbens1997bayesian}}
	\label{tab:observed_data}
	\begin{minipage}{0.5\textwidth}
		%\centering % Centers the content inside minipage
		\begin{tabular}{lcccr}
			\hline
			\multirow{2}{*}{\textbf{Type}} & \textbf{Assignment} & \textbf{Treatment} & \textbf{Survival}  & \textbf{Units} \\
			& $Z_{\text{obs},i}$ & $D_{\text{obs},i}$ & $Y_{\text{obs},i}$ & 23,682 \\
			\hline
			Complier or never-taker & 0 & 0 & 0 & 74 \\
			Complier or never-taker & 0 & 0 & 1 & 11,514 \\
			Never-taker & 1 & 0 & 0 & 34 \\
			Never-taker & 1 & 0 & 1 & 2,385 \\
			Complier & 1 & 1 & 0 & 12 \\
			Complier & 1 & 1 & 1 & 9,663 \\
			\hline
		\end{tabular}
	\end{minipage}
\end{table}

Table \ref{tab:observed_data} summarizes the data. Using this table, we can illustrate key concepts in the identification of causal effects with noncompliance. Let $C_i$ denote the compliance type of individual $i$, which in this case can be either a complier ($c$) or a never-taker ($n$). The probability of being a complier, $\omega := P(C_i = c)$, is given by
\begin{align*}
	P(C_i = c) 
	&= \underbrace{P(D_i = 1 \mid Z_i = 1)}_{P(\text{compliers}) + P(\text{always-takers})} 
	- \underbrace{P(D_i = 1 \mid Z_i = 0)}_{P(\text{always-takers}) = 0} \\
	&= \frac{P(D_i = 1, Z_i = 1)}{P(Z_i = 1)}.
\end{align*}
Thus, the ML estimate is
\[
\hat{\omega} = \frac{12 + 9{,}663}{12 + 9{,}663 + 34 + 2{,}385}= 0.8.
\]
Note that $P(C_i = n) = 1 - P(C_i = c)$ in this example; thus, the ML estimate is $1-\hat{\omega} = 0.2$.

The probability of survival for compliers assigned to treatment, $\eta_{c1} := P(Y_i = 1 \mid C_i = c, Z_i = 1)$, is
\begin{align*}
	P(Y_i = 1 \mid C_i = c, Z_i = 1) 
	&= \frac{P(Y_i = 1, C_i = c \mid Z_i = 1) \times P(Z_i = 1)}{P(C_i = c \mid Z_i = 1) \times P(Z_i = 1)}.
\end{align*}
Thus, the ML estimate is $\hat{\eta}_{c1}= \frac{9{,}663}{9{,}663 + 12}= 0.999.$

Similarly, the probability of survival for never-takers assigned to treatment, $\eta_{n1} := P(Y_i = 1 \mid C_i = n, Z_i = 1)$, is
\begin{align*}
	P(Y_i = 1 \mid C_i = n, Z_i = 1) 
	&= \frac{P(Y_i = 1, C_i = n \mid Z_i = 1) \times P(Z_i = 1)}{P(C_i = n \mid Z_i = 1) \times P(Z_i = 1)}.
\end{align*}
Consequently, the ML estimate is $\hat{\eta}_{n1} = \frac{2{,}385}{2{,}385 + 34} = 0.986.$

Note that $\eta_{c0} = P(Y_i = 1 \mid C_i = c, Z_i = 0)$ and $\eta_{n0} = P(Y_i = 1 \mid C_i = n, Z_i = 0)$ cannot be \textit{point-identified}. However, we can obtain \emph{set identification} because
\begin{align*}
	\eta_0:=P(Y_i = 1 \mid Z_i = 0) 
	&= P(Y_i = 1 \mid Z_i = 0, C_i = c) \times P(C_i = c) \\
	&\quad + P(Y_i = 1 \mid Z_i = 0, C_i = n) \times P(C_i = n) \\
	&= \frac{P(Y_i = 1, Z_i = 0)}{P(Z_i = 0)},
\end{align*} where the ML estimate is $\hat{\eta}_0= \frac{11{,}514}{11{,}514 + 74}= 0.9936.$

The first equality holds because the probability of survival given no treatment assignment is expressed as a mixture by the law of total probability, and the second equality follows from Bayes' rule.

This implies that 
\[
0.9936 = \hat{\omega} \hat{\eta}_{c0} + (1-\hat{\omega})\hat{\eta}_{n0},
\]
which leads to 
\[
\hat{\eta}_{n0} = 4.968 - 4\hat{\eta}_{c0}.
\]
Given that probabilities must lie in the unit interval, such that $0 \leq 4.968 - 4\hat{\eta}_{c0} \leq 1$, we obtain:  
(i) $\hat{\eta}_{c0} \leq 1 \leq 1.242$ (automatically satisfied), and  
(ii) $4\hat{\eta}_{c0} \geq 4.968 - 1 = 3.968$, which implies $\hat{\eta}_{c0} \geq 0.992$.  

Consequently, the ML estimates $\hat{\eta}_{c0} \in [0.992, 1]$ and $\hat{\eta}_{n0} \in [0.968, 1]$.

These results imply that the \textit{complier average causal effect} (CACE) is
\begin{align*}
	\tau_{CACE} &= \mathbb{E}[Y_i(1)-Y_i(0)\mid D_i(1)=1,D_i(0)=0] \\
	&= P[Y_i = 1 \mid C_i = c, Z_i = 1] - P[Y_i = 1 \mid C_i = c, Z_i = 0] \\
	&= \eta_{c1} - \eta_{c0},
\end{align*}
where the first line is the formal definition, the second equality uses random assignment to express the estimand in terms of observed probabilities, and the third relies on the model parameterization.

The CACE is only set-identified, yielding the interval
\[
\hat{\tau}_{CACE} \in [-0.001, 0.007],
\]
meaning that the survival rate among compliers can vary between 1 and 7 per 1,000 children due to vitamin A supplementation.

Point identification can be achieved by imposing the exclusion restriction, which states that treatment assignment is unrelated to potential outcomes for never-takers and always-takers. Under this assumption,
\[
P(Y_i = y \mid C_i = n, Z_i = z) = P(Y_i = y \mid C_i = n),
\]
so that $\hat{\eta}_{n1} = \hat{\eta}_{n0} = \hat{\eta}_n = 0.986$. Substituting into the mixture equation yields $\hat{\eta}_{c0} = 0.9955$, and consequently,
\[
\hat{\tau}_{CACE} = 0.0035,
\]
indicating that survival increases by approximately 3.5 per 1,000 children due to vitamin A supplementation.\footnote{CACE and LATE refer to the same estimand -the average causal effect for compliers- under monotonicity and the exclusion restriction. If the exclusion restriction fails, they differ because LATE (as identified by IV) no longer equals the complier-specific causal effect (CACE).}
 
We use conjugate families to perform Bayesian inference in this example. Specifically, we adopt the Bernoulli-Beta model (see Section \ref{sec42}) with independent non-informative Beta priors, each having parameters equal to 1. This corresponds to a uniform distribution on the interval $(0,1)$.  

An advantage of the Bayesian formulation is that, after applying data augmentation with the compliance type $C_i$, all the conditional posterior distributions are Beta (See \cite{imbens1997bayesian} for details in derivations). In particular:
\[
\omega \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + N_c,\, 1 + N_n),
\]
where $N_c$ and $N_n$ denote the number of compliers and never-takers, respectively.
\[
\eta_{c1} \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + 9{,}663,\, 1 + 12),
\]
\[
\eta_{n1} \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + 2{,}385,\, 1 + 34),
\]
\[
\eta_{c0} \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + N_{c01},\, 1 + N_{c00}),
\]
where $N_{c01}$ and $N_{c00}$ are the numbers of compliers in the control group who survived and did not survive, respectively.
\[
\eta_{n0} \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + N_{n01},\, 1 + N_{n00}),
\]
where $N_{n01}$ and $N_{n00}$ are the numbers of never-takers in the control group who survived and did not survive, respectively.

In addition, the conditional probability of being a complier is given by:
\[
P(C_i = c \mid Z_{\text{obs},i}, D_{\text{obs},i}, Y_{\text{obs},i}) =
\begin{cases}
	0, & i \in \{Z_i = 1, D_i = 0\},\\
	1, & i \in \{Z_i = 1, D_i = 1\},\\
	\dfrac{\omega g_{c0,i}}{\omega g_{c0,i} + (1-\omega) g_{n0,i}}, & i \in \{Z_i = 0\},
\end{cases}
\]
where $g_{c0,i} = \eta_{c0}^{Y_{\text{obs},i}} (1 - \eta_{c0})^{1 - Y_{\text{obs},i}}$ and $g_{n0,i} = \eta_{n0}^{Y_{\text{obs},i}} (1 - \eta_{n0})^{1 - Y_{\text{obs},i}}$. Note that $Y_{\text{obs},i} = 1$ implies $g_{c0,i} = \eta_{c0}$ and $g_{n0,i} = \eta_{n0}$, while $Y_{\text{obs},i} = 0$ implies $g_{c0,i} = (1 - \eta_{c0})$ and $g_{n0,i} = (1 - \eta_{n0})$.

The following code shows the implementation, and the corresponding figure displays the posterior distribution of the CACE, the mean is 0.0024, and the 95\% credible intervals is (-0.0012, 0.0071).

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{codea_chap12}
	\textit{R code. Effects of vitamin A supplements on children's survival}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101)
library(dplyr)
# Simulate data
Nc111 <- 9663
c111 <- cbind(rep(1, Nc111), rep(1, Nc111), rep(1, Nc111)) 
Nc110 <- 12
c110 <- cbind(rep(1, Nc110), rep(1, Nc110), rep(0, Nc110)) 
Nn101 <- 2385
n101 <- cbind(rep(1, Nn101), rep(0, Nn101), rep(1, Nn101)) 
Nn100 <- 34
n100 <- cbind(rep(1, Nn100), rep(0, Nn100), rep(0, Nn100)) 
Ncn001 <- 11514
cn001 <- cbind(rep(0, Ncn001), rep(0, Ncn001), rep(1, Ncn001)) 
Ncn000 <- 74
cn000 <- cbind(rep(0, Ncn000), rep(0, Ncn000), rep(0, Ncn000)) 

mydata <- rbind(c111, c110, n101, n100, cn001, cn000)
mydata <- data.frame(Z = mydata[,1], D = mydata[,2], Y = mydata[,3])
N <- dim(mydata)[1]
attach(mydata)

# Sampling function C (type)
SampleType <- function(z, d, y, wc, nc0, nn0){
	if(z == 1 & d == 0){
		pc <- 0
	}else{
		if(z == 1 & d == 1){
			pc <- 1
		}else{
			if(y == 1){
				pc <- (wc * nc0) / (wc * nc0 +  (1 - wc) * nn0)
			}else{
				pc <- (wc * (1 - nc0)) / (wc * (1 - nc0) +  (1 - wc) * (1 - nn0))
			}
		}
	}
	rbinom(1, 1, prob = pc) # 1: Complier/ 0: Never taker
}
z = 0; d = 0; y = 0; wc = 0.8; nc0 = 0.9; nn0 = 0.05
SampleType(z = z, d = d, y = y, wc = wc, nc0 = nc0, nn0 = nn0)
Clat <- sapply(1:N, function(i){SampleType(z = Z[i], d = D[i], y = Y[i], wc = wc, nc0 = nc0, nn0 = nn0)})
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Effects of vitamin A supplements on children's survival}
	\begin{VF}
		\begin{lstlisting}[language=R]
# Gibbs sampler
a0 <- 1; b0 <- 1 # Hyperparameters beta priors
burnin <- 500; S <- 2000; tot <- S + burnin 
PosteriorDraws <- matrix(NA, tot, 5)
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)

for(s in 1:tot){
	dataLat <- cbind(mydata, Clat)
	Nc011 <- sum(dataLat$Z == 0 & dataLat$Clat == 1 & dataLat$Y == 1)
	Nc010 <- sum(dataLat$Z == 0 & dataLat$Clat == 1 & dataLat$Y == 0)
	Nn001 <- sum(dataLat$Z == 0 & dataLat$Clat == 0 & dataLat$Y == 1)
	Nn000 <- sum(dataLat$Z == 0 & dataLat$Clat == 0 & dataLat$Y == 0)
	Nc <- sum(Clat == 1)
	Nn <- sum(Clat == 0)
	wc <- rbeta(1, 1 + Nc, 1 + Nn)
	nc1 <- rbeta(1, 1 + Nc111, 1 + Nc110)
	nn1 <- rbeta(1, 1 + Nn101, 1 + Nn100)
	nc0 <- rbeta(1, 1 + Nc011, 1 + Nc010)
	nn0 <- rbeta(1, 1 + Nn001, 1 + Nn000)
	Clat <- sapply(1:N, function(i){SampleType(z = Z[i], d = D[i], y = Y[i], wc = wc, nc0 = nc0, nn0 = nn0)})
	PosteriorDraws[s, ] <- c(wc, nc1, nc0, nn1, nn0)
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot)
LATE <- PosteriorDraws[keep, 2] - PosteriorDraws[keep, 3]
LATEmean <- mean(LATE)
LATEci <- quantile(LATE, c(0.025, 0.975))

# Plot posterior distribution of CATE
hist(LATE, breaks = 40, freq = FALSE,
main = "Posterior Distribution of CACE",
xlab = "CACE", col = "lightblue", border = "white")
abline(v = LATEmean, col = "red", lwd = 2)
abline(v = LATEci, col = "darkgreen", lty = 2, lwd = 2)

legend("topright", legend = c("Posterior Mean", "95% Credible Interval"),
col = c("red", "darkgreen"), lwd = 2, lty = c(1, 2),
bty = "n", cex = 0.8)  # Smaller legend using cex
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/CACE.png}
	\caption[List of figure caption goes here]{Posterior distribution: Complier average causal effect of vitamin A supplements.}\label{fig12_CACE}
\end{figure}

We can perform inference by imposing the exclusion restriction, setting $\eta_{n} = \eta_{n0} = \eta_{n1}$, so that the posterior distribution is
\[
\eta_{n} \mid \mathbf{C}, \mathbf{Z}_{\text{obs}}, \mathbf{D}_{\text{obs}}, \mathbf{Y}_{\text{obs}} \sim \text{Beta}(1 + N_{n1},\, 1 + N_{n0}),
\]
where $N_{n1}$ and $N_{n0}$ denote the numbers of never-takers who survived and did not survive, respectively. This is the only modification to the Gibbs sampler.  

The following code shows the implementation, and Figure~\ref{fig12_LATE} displays the posterior distribution of the CACE (which equals the LATE under the exclusion restriction). The posterior mean is 0.0030, and the 95\% credible interval is (0.0008, 0.0054). Therefore, imposing the exclusion restriction leaves the posterior mean approximately unchanged but increases the precision and informativeness of the posterior distribution. Without the restriction, the posterior was relatively flat within the 95\% credible interval, whereas under the exclusion restriction, the posterior distribution is approximately normal, resulting in a narrower 95\% credible interval.

Thus, this example illustrates how the Bayesian framework provides a way to quantify uncertainty and conduct sensitivity analysis regarding the exclusion restriction. See \cite{hirano2000assessing} for an extension controlling for covariates.\\

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Effects of vitamin A supplements on children's survival}
	\begin{VF}
		\begin{lstlisting}[language=R]
PosteriorDrawsER <- matrix(NA, tot, 4)
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)

for(s in 1:tot){
	dataLat <- cbind(mydata, Clat)
	Nc011 <- sum(dataLat$Z == 0 & dataLat$Clat == 1 & dataLat$Y == 1)
	Nc010 <- sum(dataLat$Z == 0 & dataLat$Clat == 1 & dataLat$Y == 0)
	Nn01 <- sum(dataLat$Clat == 0 & dataLat$Y == 1)
	Nn00 <- sum(dataLat$Clat == 0 & dataLat$Y == 0)
	Nc <- sum(Clat == 1)
	Nn <- sum(Clat == 0)
	wc <- rbeta(1, 1 + Nc, 1 + Nn)
	nc1 <- rbeta(1, 1 + Nc111, 1 + Nc110)
	nn <- rbeta(1, 1 + Nn01, 1 + Nn00)
	nc0 <- rbeta(1, 1 + Nc011, 1 + Nc010)
	Clat <- sapply(1:N, function(i){SampleType(z = Z[i], d = D[i], y = Y[i], wc = wc, nc0 = nc0, nn0 = nn)})
	PosteriorDrawsER[s, ] <- c(wc, nc1, nc0, nn)
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot)
LATEER <- PosteriorDrawsER[keep, 2] - PosteriorDrawsER[keep, 3]
LATEERmean <- mean(LATEER)
LATEERci <- quantile(LATEER, c(0.025, 0.975))
# Plot posterior distribution of ATE
hist(LATE, breaks = 40, freq = FALSE,
main = "Posterior Distribution of CACE",
xlab = "CACE", col = "lightblue", border = "white")
abline(v = LATEmean, col = "red", lwd = 2)
abline(v = LATEci, col = "darkgreen", lty = 2, lwd = 2)
legend("topright", legend = c("Posterior Mean", "95% Credible Interval"),
col = c("red", "darkgreen"), lwd = 2, lty = c(1, 2),
bty = "n", cex = 0.8)  # Smaller legend using cex
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/LATE.png}
	\caption[List of figure caption goes here]{Posterior distribution: Local average average atreatment effect vitamin A supplements.}\label{fig12_LATE}
\end{figure}


\textbf{Example: Treatment effect of 401(k) participation on net financial assets}

In the example of the effect of \textit{eligibility} on 401(k) net financial assets, we calculate the intention-to-treat (ITT) effect. However, following \cite{chernozhukov2004effects}, we use eligibility as an instrument for \textit{participation} and perform inference on the (local) average treatment effect. We adopt the framework described in Section \ref{sec73} for this example. The following code illustrates the procedure, and Figure \ref{fig12_3} displays the posterior distribution of the (local) average treatment effect of participation. The 95\% credible interval is (USD 5,102, USD 12,010), and the posterior mean is USD 8,520, which is higher than the intention-to-treat effect associated with eligibility (USD 5,903). 

Exercise~3 asks how to recover the ITT from the LATE and the effect of eligibility on participation. See \cite{Conley2012} for practical methods to conduct sensitivity analysis of posterior results when relaxing the exclusion restriction in IV settings. In addition, \cite{conley2008semi} present a Bayesian inferential framework using a semi-parametric approach in which stochastic errors are modeled using a Dirichlet process (see Exercise 4), and \cite{ramirez2024welfare} extend this approach to systems of equations.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code3n_chap12}
	\textit{R code. Treatment effect: 401(k) participation on net financial assets}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(10101)
library(coda); library(ggplot2)
mydata <- read.csv("https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv", sep = ",", header = TRUE, quote = "")
# Attach variables
attach(mydata)
y <- net_tfa/1000  # Outcome: net financial assets
x <- as.vector(p401) # Endogenous regressor: participation
w <- as.matrix(cbind(1, age, inc, fsize, educ, marr, twoearn, db, pira, hown))  # Exogenous regressors with intercept
z <- as.matrix(e401)  # Instrument: eligibility (NO intercept here)
X <- cbind(x, w); Z <- cbind(z, w)
# Dimensions
k <- ncol(X); kz <- ncol(Z)  
# Priors
b0 <- rep(0, k); B0i <- diag(1e-5, k)
g0 <- rep(0, kz); G0i <- diag(1e-5, kz)
nu <- 3; Psi0 <- nu * 1000 * diag(2); Psi0i <- solve(Psi0)
# MCMC parameters
mcmc <- 5000; burnin <- 1000
tot <- mcmc + burnin; thin <- 1
# Auxiliary elements
XtX <- t(X)%*%X; ZtZ <- t(Z)%*%Z; nun <- nu + length(y)
# Gibbs sampling
PostBeta <- function(Sigma, Gamma){
	w1 <- Sigma[1,1] - Sigma[1,2]^2/Sigma[2,2]
	Bn <- solve(w1^(-1)*XtX + B0i)
	yaux <- y - (Sigma[1,2]/Sigma[2,2])*(x - Z%*%Gamma)
	bn <- Bn%*%(B0i%*%b0 + w1^(-1)*t(X)%*%yaux)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostGamma <- function(Sigma, Beta){
	w2 <- Sigma[2,2] - Sigma[1,2]^2/Sigma[1,1]
	Gn <- solve(w2^(-1)*ZtZ + G0i)
	xaux <- x - (Sigma[1,2]/Sigma[1,1])*(y - X%*%Beta)
	gn <- Gn%*%(G0i%*%g0 + w2^(-1)*t(Z)%*%xaux)
	Gamma <- MASS::mvrnorm(1, gn, Gn)
	return(Gamma)
}
PostSigma <- function(Beta, Gamma){
	Uy <- y - X%*%Beta; Ux <- x - Z%*%Gamma
	U <- cbind(Uy, Ux)
	Psin <- solve(Psi0i + t(U)%*%U)
	Sigmai <- rWishart::rWishart(1, df = nun, Sigma = Psin)
	Sigma <- solve(Sigmai[,,1]) 
	return(Sigma)
}
	\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code3_chap12}
	\textit{R code. Treatment effect: 401(k) participation on net financial assets}
	\begin{VF}
		\begin{lstlisting}[language=R]		
PostBetas <- matrix(0, tot, k)
PostGammas <- matrix(0, tot, kz)
PostSigmas <- matrix(0, tot, 2*(2+1)/2)
Beta <- rep(0, k); Gamma <- rep(0, kz)
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	Sigma <- PostSigma(Beta = Beta, Gamma = Gamma)
	Beta <- PostBeta(Sigma = Sigma, Gamma = Gamma)
	Gamma <- PostGamma(Sigma = Sigma, Beta = Beta)
	PostBetas[s,] <- Beta
	PostGammas[s,] <- Gamma
	PostSigmas[s,] <- matrixcalc::vech(Sigma)
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]
Gs <- PostGammas[keep,]
Sigmas <- PostSigmas[keep,]
summary(coda::mcmc(Bs))
summary(coda::mcmc(Gs))
summary(coda::mcmc(Sigmas))
# Extract posterior draws for the treatment effect (participation = p401)
beta_draws <- Bs[,1]
# Plot posterior distribution of treatment effect
df_beta <- data.frame(effect = as.vector(beta_draws))
ggplot(df_beta, aes(x = effect)) + geom_density(fill = "steelblue", alpha = 0.6) +
geom_vline(xintercept = mean(beta_draws), color = "red", linetype = "dashed", linewidth = 1) + labs( title = "Posterior Distribution of 401(k) Participation Effect", x = expression(beta["p401"]), y = "Density") + theme_minimal(base_size = 14)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  
 
\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/FigP401k.png}
	\caption[List of figure caption goes here]{Posterior distribution: Local average treatment effect 401k participation on net financial assets.}\label{fig12_3}
\end{figure}

\section{Difference-in-differences (DiD)}\label{sec12_5}

Following \cite{roth2023whats}, we consider a setting with two time periods, \( t = 1, 2 \), and two groups: a treated group (\( D_i = 1 \)) that receives the treatment between periods \( t = 1 \) and \( t = 2 \), and a control group that never receives the treatment (\( D_i = 0 \)). We observe outcomes \( Y_{it} \) and treatment status \( D_i \) for units \( i = 1, 2, \dots, N \), assuming a balanced panel data structure.

Let \( Y_{it}(0,1) \) denote the potential outcome for unit \( i \) in period \( t \) if it is untreated in the first period and treated in the second, while \( Y_{it}(0,0) \) denotes the potential outcome if it is never treated. In this framework, known as \textit{difference-in-differences} (DiD), the primary estimand of interest is the average treatment effect on the treated (ATT),
\[
\tau_{2} = \mathbb{E}[Y_{i2}(0,1) - Y_{i2}(0,0) \mid D_i = 1].
\]

Note that we do not observe \( Y_{i2}(0,0) \mid D_i = 1 \); that is, the outcome in the second period for treated units had they not received the treatment. Thus, the identification conditions in DiD are designed to express this counterfactual outcome as a function of the data.

There are two key identification assumptions:

\textit{1. Parallel trends:}
\begin{equation}
	\mathbb{E}[Y_{i2}(0,0) - Y_{i1}(0,0) \mid D_i = 1] = \mathbb{E}[Y_{i2}(0,0) - Y_{i1}(0,0) \mid D_i = 0],
\end{equation}
which states that, in the absence of treatment, the average change in outcomes for the treated and control groups would have evolved similarly over time.

\textit{2. No anticipation effects:}
\begin{equation}
	Y_{i1}(0,0) = Y_{i1}(0,1), \quad \text{for all } D_i = 1,
\end{equation}
meaning that the treatment has no causal effect prior to its implementation.

Thus, we can use the parallel trends assumption to express \( \mathbb{E}[Y_{i2}(0,0) \mid D_i = 1] \) as
\begin{align*}
	\mathbb{E}[Y_{i2}(0,0)\mid D_i = 1] 
	&= \mathbb{E}[Y_{i1}(0,0) \mid D_i = 1] + \mathbb{E}[Y_{i2}(0,0) - Y_{i1}(0,0) \mid D_i = 0] \\
	&= \mathbb{E}[Y_{i1}(0,1) \mid D_i = 1] + \mathbb{E}[Y_{i2}(0,0) - Y_{i1}(0,0) \mid D_i = 0] \\
	&= \mathbb{E}[Y_{i1} \mid D_i = 1] + \mathbb{E}[Y_{i2} - Y_{i1} \mid D_i = 0],
\end{align*}
where the second equality follows from the no anticipation effects assumption.

Therefore,
\[
\tau_2 = \mathbb{E}[Y_{i2} - Y_{i1} \mid D_i = 1] - \mathbb{E}[Y_{i2} - Y_{i1} \mid D_i = 0],
\]
that is, the average treatment effect on the treated (ATT) is the difference in outcome differences between treated and control units.

Note that we can express the observed outcome in terms of potential outcomes as
\[
Y_{it} = Y_{it}(0,0) + \big[\,Y_{it}(0,1) - Y_{it}(0,0)\,\big] D_i.
\]
Therefore,
\[
Y_{i1} = Y_{i1}(0,0) + \big[\,Y_{i1}(0,1) - Y_{i1}(0,0)\,\big] D_i,
\]
and
\[
Y_{i2} = Y_{i2}(0,0) + \big[\,Y_{i2}(0,1) - Y_{i2}(0,0)\,\big] D_i.
\]
Subtracting the first equation from the second yields
\[
Y_{i2} - Y_{i1} = \underbrace{Y_{i2}(0,0) - Y_{i1}(0,0)}_{\text{common trend}} 
+ \underbrace{\big[(Y_{i2}(0,1) - Y_{i1}(0,1)) - (Y_{i2}(0,0) - Y_{i1}(0,0))\,\big]}_{\text{treatment effect in changes}} D_i.
\]

Under the \emph{parallel trends} assumption, the common trend is the same for treated and control units. This assumption is compatible with the linear CEF function
\[
Y_{it} = \alpha_i + \phi_t + \tau D_i + \mu_{it},
\]
which implies
\[
Y_{i2} - Y_{i1} = (\phi_2 - \phi_1) + \tau D_i + (\mu_{i2} - \mu_{i1}).
\]
Comparing this expression with the one derived from the potential outcomes framework, it follows that regressing the time difference $Y_{i2} - Y_{i1}$ on a constant and the treatment indicator $D_i$ identifies $\tau$, the ATT.

Another formulation in the linear regression setting that recovers the ATT is
\[
Y_{it} = \alpha_i + \phi_t + \tau \,\big[ D_i \cdot \mathbbm{1}(t = 2) \big] + \epsilon_{it},
\]
where $\mathbbm{1}(t = 2)$ is an indicator for the post-treatment period \cite{roth2023whats}.


 







\cite{normington2019bayesian,normington2022bayesian,breunig2024semiparametric}






In this framework, there is a model for the assignment mechanism $P(D_i\mid Y_i(1), Y_i(0), \mathbf{X}_i)$, like randomization or propensity score matching \cite{rosenbaum1983central}, supplemented by a model for the data $P(Y_i(1), Y_i(0)\mid \mathbf{X}_i)$. A causal inference is delivered by conditioning in what is observed to calculate the posterior distribution of the causal estimand given also the models for the assignment mechanisms and data. Therefore, there is a model to input the potential outcome missing value that allows to get the predictive distribution, and then, the posterior predictive distribution of the treatment effect ($Y_i(1)-Y_i(0)$) is calculated. We should take into account that the parameters of the models for predicting the missing potential outcomes are generally not causal effects. Due to using models for the data based on simulation, the Bayesian approach is by far the most direct and flexible of the modes of inference for causal effects. However, it relies on assumptions on the assignment mechanism and data.

Given pretreatment variables, it improves the predictive power of the potential outcome missing data



There, no compliance is explicitly treated as missing data. A nice advantage of the Bayesian approach is to relax the exclusion restriction \cite{rubin2004teaching}.

Propensity matching should be used to create ``assigned treatment" and ``assigned control" groups that are well balanced on all covariates. The covariate modeling should be applied to estimate the effects of ``received treatment" for the subgroup of true compliers \cite{rubin2004teaching}.

Simultaneous causality is conceptually challenging because causality is often associated with chronological order, cause preceding effect. In simultaneous systems, such as supply and demand, outcomes are jointly determined in equilibrium, so this temporal order is not explicit in the observed data. Nevertheless, the underlying structural equations still encode causal relationships, even though their effects are realized simultaneously.

In general, association does not imply causation, although causation does imply association. Two variables can be associated for several reasons: one may cause the other, they may share common causes, or they may share a common effect when the analysis is restricted to a specific level of that effect (or one of its descendants) \cite{hernan2020causal}. For this reason, causal graphs are valuable tools for explicitly representing the assumptions about the causal relationships under study and should be among the first steps in establishing an identification strategy.

Introduce maximum entropy using identification conditions to build likelihood functions.

Instrumental variables change the level of the treatment, without affecting the potential outcomes associated with these treatment levels \cite{imbens2014ivperspective}.

 
 
The gold standard in identification of causal effects is \textit{randomized controlled trial (RCT)}, where assignment to treatment is random, independent of the outcome and other potential exogenous determinants of the outcome. However, most of the time practitioners use \textit{observational data}, where the treatment status/level is \textit{endogenous} due to units actively defining the treatment that they receive \cite{imbens2014ivperspective}. Thus the assignment mechanism in RCTs are given by chance, whereas in observational studies by choice. This makes a subtantial methodological difference such that the latter implies looking a source of \textit{exogenous variation} that influences the \textit{treatment status/level}.

Two critical aspects in the identification of causal effects are: (i) the presence of a strong source of \textit{exogenous variation} that influences the \textit{endogenous regressors}, which are often the primary variables of interest to researchers, as they may directly affect the outcome or response variables and can be influenced by policy decisions, for example, identifying the causal effects of social programs on income or education, or evaluating strategic interventions in industry, such as estimating the price elasticity of demand for a specific product; and (ii) the effective \textit{control of other relevant exogenous covariates}, such as pre-treatment characteristics or external factors in a demand model.

In this context, the use of \textit{non-parametric models} (see Chapters~\ref{chap12} and~\ref{chap13}) is valuable due to their flexibility and weaker structural assumptions. Therefore, non-parametric and machine learning approaches serve as \textit{powerful tools} that can be combined with strong exogenous variation to robustly identify causal effects \cite{chernozhukov2018double,chernozhukov2024applied}.


Read this reference \cite{iacovone2023bayesian} before begin working in this chapter! Also read \cite{imbens1997bayesian}.



\section{Sample selection}\label{sec12_2}
\cite{greenberg2012introduction}

\section{Regression discontinuity design}\label{sec12_3}
\cite{chib2016bayesian,chib2023nonparametric,kowalska2024bayesian}

\section{Regression kink design}\label{sec12_4}
\cite{chan2025minimum}

\section{Synthetic control}\label{sec12_5a}
\cite{amjad2018robust,kim2020bayesian}



\section{Event Analysis}\label{sec12_7}

\section{Bayesian exponential tilted empirical likelihood}\label{sec12_8}
Bayesian parametric approaches are often criticized on the basis that they require arbitrary distribution assumptions which often are not examined. Partial information approaches are based only on certain moment assumptions without making specific distributional assumptions. However, there are no free lunch, as these methods imply efficiency losses.

The point of departure of Bayesian exponential tilted empirical likelihood (BETEL) are moment conditions that are used to build the likelihood function.

\section{A general framework for updating belief distributions}\label{sec12_9}
Introduce \cite{bissiri2016general} as a generalization of \cite{chernozhukov2003mcmc}.

\section{Bayesian model averaging}\label{sec12_10}
We can use BMA as a sensible way to perform robustness analysis regarding model specification (regressors uncertainty) in performing inference of treatment effects.

%\section{Double-Debiased machine learning causal effects}\label{sec12_11a}

\section{Summary}

\section{Exercises}

\begin{enumerate}
	\item Show that the Average Treatment Effect (ATE) in the simple linear regression framework
	\[
	Y_i = \beta_0 + \tau D_i + \mu_i,
	\]
	assuming non-informative prior distributions, so that the posterior mean of the location parameter coincides with the maximum likelihood estimator, is equal to
	\[
	\bar{y}_1 - \bar{y}_0.
	\]
	
	\item Some readers may question the assumption that potential outcomes are normally distributed. However, it is important to note that the normal distribution is the \textit{maximum entropy} continuous distribution given a specified mean $\mu$ and finite variance $\sigma^2$. In other words, among all distributions with the same mean and variance, the normal distribution represents the one with the greatest level of uncertainty or unpredictability \cite{cover2006elements}.
	
	Show that the normal distribution is the \textit{maximum entropy} continuous distribution given a specified mean $\mu$ and finite variance $\sigma^2$ by considering the formal definition of entropy:
	\[
	H(f) = - \int_{-\infty}^{\infty} f(y) \log f(y) \, dy,
	\]
	where $f(y)$ is a probability density function.
	
	\item \textbf{401(k) participation on net financial assets continues I}  
	
	Apply the framework from this example to compute the intention-to-treat effect, the local average treatment effect, and the effect of eligibility on participation.
	
	\item \textbf{401(k) participation on net financial assets continues II} 
	
	Use the function \textit{rivDP} from the \textit{bayesm} package to perform inference on 401(k) participation and its effect on net financial assets using the same specification as in the main text, and plot the LATE.
	  
	 
	
	
\end{enumerate}
