\chapter{Multivariate models}\label{chap7}

We describe how to perform Bayesian inference in multivariate response models, including multivariate regression, seemingly unrelated regression, instrumental variable, and multivariate probit models. In particular, we present the posterior distributions of the parameters and illustrate their use through selected applications and simulations. We also demonstrate how to perform inference in these models using three levels of programming engagement: through the GUI, via \textbf{R} packages, and by directly programming the algorithms from scratch. Finally, we provide a set of mathematical and computational exercises.

Remember that we can run our GUI typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
		\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor. However, users should see Chapter \ref{chapGUI} for seeing other options.

\section{Multivariate regression}\label{sec71}

A complete presentation of this model is provided in Section~\ref{sec44}. Here, we summarize the setting and posterior distributions for ease of exposition. There are $M$ jointly dependent variables that share the same set of regressors, and their stochastic errors are contemporaneously correlated. Specifically, let $\bm{Y} = \left[ \bm{y}_1 \ \bm{y}_2 \ \ldots \ \bm{y}_M \right]$ denote an $N \times M$ matrix generated by
\[
\bm{Y} = \bm{X}\bm{B} + \bm{U},
\]
where $\bm{X}$ is an $N \times K$ matrix of regressors, $\bm{B} = \left[ \bm{\beta}_1 \ \bm{\beta}_2 \ \ldots \ \bm{\beta}_M \right]$ is a $K \times M$ matrix of coefficients, and $\bm{U} = \left[ \bm{\mu}_1 \ \bm{\mu}_2 \ \ldots \ \bm{\mu}_M \right]$ is an $N \times M$ matrix of stochastic errors such that each row $\bm{\mu}_i \stackrel{i.i.d.}{\sim} N(\bm{0}, \bm{\Sigma})$ for $i = 1, 2, \dots, N$.

The prior distributions are specified as $\bm{B} \mid \bm{\Sigma} \sim N(\bm{B}_0, \bm{V}_0, \bm{\Sigma})$ and $\bm{\Sigma} \sim IW(\bm{\Psi}_0, \alpha_0)$. The conditional posterior distributions are then given by
\begin{equation*}
	\bm{B} \mid \bm{\Sigma}, \bm{Y}, \bm{X} \sim N(\bm{B}_n, \bm{V}_n, \bm{\Sigma}),
\end{equation*}
\begin{equation*}
	\bm{\Sigma} \mid \bm{Y}, \bm{X} \sim IW(\bm{\Psi}_n, \alpha_n),
\end{equation*}
where $\bm{V}_n = (\bm{X}^{\top}\bm{X} + \bm{V}_0^{-1})^{-1}$, $\bm{B}_n = \bm{V}_n(\bm{V}_0^{-1}\bm{B}_0 + \bm{X}^{\top}\bm{X}\hat{\bm{B}})$, $\hat{\bm{B}} = (\bm{X}^{\top}\bm{X})^{-1}\bm{X}^{\top}\bm{Y}$, ${\bm{S}} = (\bm{Y} - \bm{X}\hat{\bm{B}})^{\top}(\bm{Y} - \bm{X}\hat{\bm{B}})$, $\bm{\Psi}_n = \bm{\Psi}_0 + \bm{S} + \bm{B}_0^{\top}\bm{V}_0^{-1}\bm{B}_0 + \hat{\bm{B}}^{\top}\bm{X}^{\top}\bm{X}\hat{\bm{B}} - \bm{B}_n^{\top}\bm{V}_n^{-1}\bm{B}_n$, and $\alpha_n = \alpha_0 + N$.

Since the conditional posterior distributions are standard, inference in this model can be carried out using a Gibbs sampling algorithm.\\

\textbf{Example: The effect of institutions on per capita gross domestic product}

To illustrate multivariate regression models, we use the dataset from \cite{Acemoglu2001}, who analyzed the effect of property rights on economic growth.

We begin with the following \textit{simultaneous structural} economic model:\footnote{This model captures the potential underlying economic relationships among the variables.}
\begin{align}\label{eq:str1}
	\log(\text{pcGDP95}_i) &= \beta_1 + \beta_2 \text{PAER}_i + \beta_3 \text{Africa}_i + \beta_4 \text{Asia}_i + \beta_5 \text{Other}_i + u_{1i},\\
	\text{PAER}_i &= \alpha_1 + \alpha_2 \log(\text{pcGDP95}_i) + \alpha_3 \log(\text{Mort}_i) + u_{2i}. \label{eq:str2}
\end{align}
Here, \textit{pcGDP95}, \textit{PAER}, and \textit{Mort} denote, respectively, the per capita gross domestic product in 1995, the average index of protection against expropriation between 1985 and 1995, and the settler mortality rate during the period of colonization. \textit{Africa}, \textit{Asia}, and \textit{Other} are continent indicators, with \textit{America} serving as the baseline category.

In this model, there is \textit{simultaneous causality} because \textit{GDP} affects \textit{PAER}, and \textit{PAER} simultaneously affects \textit{GDP}. Estimating Equations~\ref{eq:str1} and~\ref{eq:str2} without accounting for this simultaneity leads to posterior mean estimates that are \textit{biased} and \textit{inconsistent} from a frequentist perspective.\footnote{Specifically, $\mathbb{E}[u_{1i}\text{PAER}_i] \neq 0$, violating a key condition for obtaining unbiased and consistent estimators of $\bm{\beta}$. See Exercise~1.} 

A standard approach to address this issue is to estimate the \textit{reduced-form} model—one without simultaneous causality—in which all \textit{endogenous variables} are expressed as functions of the \textit{exogenous variables}. The former are determined within the model (e.g., $\log(\text{pcGDP95}_i)$ and \textit{PAER} in this example), while the latter are determined outside the model (e.g., $\log(\text{Mort}_i)$, \textit{Africa}, \textit{Asia}, and \textit{Other}).

Substituting Equation~\ref{eq:str2} into Equation~\ref{eq:str1} and solving for $\log(\text{pcGDP95}_i)$ yields:
\begin{align}\label{eq:red1}
	\log(\text{pcGDP95}_i) = \pi_1 + \pi_2 \log(\text{Mort}_i) + \pi_3 \text{Africa}_i + \pi_4 \text{Asia}_i + \pi_5 \text{Other}_i + e_{1i}.
\end{align}
Then, substituting Equation~\ref{eq:red1} into Equation~\ref{eq:str2} and solving for \textit{PAER} gives:
\begin{align}\label{eq:red2}
	\text{PAER}_i = \gamma_1 + \gamma_2 \log(\text{Mort}_i) + \gamma_3 \text{Africa}_i + \gamma_4 \text{Asia}_i + \gamma_5 \text{Other}_i + e_{2i},
\end{align}
where $\pi_2 = \frac{\beta_2 \alpha_3}{1 - \beta_2 \alpha_2}$ and $\gamma_2 = \frac{\alpha_3}{1 - \beta_2 \alpha_2}$, assuming $\beta_2 \alpha_2 \neq 1$, which ensures model identification (see Exercise~2).

Observe that Equations~\ref{eq:red1} and~\ref{eq:red2} take the form of a multivariate regression model, where the common set of regressors is $\bm{X} = \left[\log(\text{Mort}) \ \text{Africa} \ \text{Asia} \ \text{Other}\right]$, and the common set of dependent variables is $\bm{Y} = \left[\log(\text{pcGDP95}) \ \text{PAER}\right]$. Therefore, we can estimate this model using the framework described in this section.

In the first stage, we estimate the parameters of the \textit{reduced-form} model (Equations~\ref{eq:red1} and~\ref{eq:red2}); however, our primary interest lies in the parameters of the \textit{structural} model (Equations~\ref{eq:str1} and~\ref{eq:str2}). A natural question arises as to whether the \textit{structural} parameters can be recovered (identified) from the \textit{reduced-form} parameters. There are two key criteria to address this question: the \textit{order condition}, which is necessary, and the \textit{rank condition}, which is both necessary and sufficient.\footnote{The posterior distribution of the structural parameters, $\pi(\boldsymbol{\beta}, \boldsymbol{\alpha} \mid \boldsymbol{\gamma}, \boldsymbol{\pi}, \mathbf{Y}, \mathbf{X})$, is proportional to the prior distribution of the structural parameters conditional on the reduced-form parameters, $\pi(\boldsymbol{\beta}, \boldsymbol{\alpha} \mid \boldsymbol{\gamma}, \boldsymbol{\pi})$. In other words, it is the prior distribution of the reduced-form parameters that is updated by the sample information, while the updating of the structural parameters occurs solely through the reduced-form parameters. See Section~9.3 in \cite{zellner1996introduction} for further details.}\\
 
\textbf{The order condition}

Given a system of equations with $M$ endogenous variables and $K$ exogenous variables (including the intercept), there are two ways to assess the order condition:
\begin{itemize}
	\item The parameters of an equation in the system are identified if there are at least $M - 1$ variables excluded from that equation (\textit{exclusion restrictions}). The equation is \textit{exactly identified} if the number of excluded variables equals $M - 1$, and \textit{over-identified} if the number of excluded variables is greater than $M - 1$.
	\item The parameters of equation $m$ in the system are identified if $K - K_m \geq M_m - 1$, where $K_m$ and $M_m$ denote the number of exogenous and endogenous variables in equation $m$, respectively. The $m$th equation is \textit{exactly identified} if $K - K_m = M_m - 1$, and \textit{over-identified} if $K - K_m > M_m - 1$.
\end{itemize}

Otherwise, the structural parameters are said to be \textit{under-identified}.

From Equations~\ref{eq:str1} and~\ref{eq:str2} in this example, we have $K = 5$, $M = 2$, $K_1 = 4$, $K_2 = 2$, $M_1 = 2$, and $M_2 = 2$. Therefore, $K - K_1 = 1 = M - 1$ and $K - K_2 = 3 > M - 1 = 1$. According to the order condition, both equations satisfy the necessary condition for identification: the first equation is \textit{exactly identified}, and the second equation is \textit{over-identified}. Note that one variable is excluded from the first equation, while three variables are excluded from the second equation.\\

\textbf{The rank condition}

The rank condition (necessary and sufficient) states that, given a \textit{structural} model with $M$ equations (and $M$ endogenous variables), an equation is identified if and only if there exists at least one nonzero determinant of an $(M - 1) \times (M - 1)$ matrix constructed from the variables that are excluded from the equation under analysis but included in at least one other equation of the system. Otherwise, the structural parameters are said to be \textit{under-identified}.

To verify this condition, it is useful to construct the \textit{identification matrix}. Table~\ref{tab:71} presents this matrix for the current example.

\begin{table}[!h]
	%\noautomaticrules
	\tabletitle{Identification matrix.}\label{tab:71}%
	\begin{tabular}{ccccccc}
		$\log(\text{pcGDP95})$ & PAER & Constant & $\log(\text{Mort})$ & Africa & Asia & Other \\
		\hline
		1 & -$\beta_2$ & -$\beta_1$ & 0 & -$\beta_3$ & -$\beta_4$ & -$\beta_5$\\
		-$\alpha_2$ & 1 & $-\alpha_1$ & -$\alpha_3$ & 0 & 0 & 0 \\
	\end{tabular}
\end{table}

The only excluded variable in the $\log(\text{pcGDP95})$ equation is $\log(\text{Mort})$. Therefore, only one matrix can be constructed using the excluded variables from this equation, namely $[-\alpha_3]$ (see column~4 in Table~\ref{tab:71}). The determinant of this matrix is $-\alpha_3$, and as long as this coefficient is nonzero (i.e., $\alpha_3 \neq 0$)—meaning that the mortality rate is relevant in the PAER equation—the coefficients in the $\log(\text{pcGDP95})$ equation are \textit{exactly identified}. For example, $\beta_2 = \pi_2 / \gamma_2$, which represents the effect of property rights on GDP, is exactly identified.

It is important to note the role of excluding $\log(\text{Mort})$ from the $\log(\text{pcGDP95})$ equation while including it in the PAER equation. This exclusion is known as an \textit{exclusion restriction}, which requires an exogenous source of variation in the PAER equation to identify the $\log(\text{pcGDP95})$ equation. The presence of relevant exogenous variation is essential for the identification, estimation, and inference of \textit{structural} parameters.

Regarding the identification of the \textit{structural} parameters in the PAER equation, three possible matrices can be constructed: $[-\beta_3]$, $[-\beta_4]$, and $[-\beta_5]$ (see columns~5–7 in Table~\ref{tab:71}). As long as any of these parameters are relevant in the $\log(\text{pcGDP95})$ equation, the PAER equation is identified. In this case, the PAER equation is \textit{over-identified}, meaning that multiple exclusion restrictions are available to estimate its parameters. For instance, $\alpha_2 = \gamma_3 / \pi_3 = \gamma_4 / \pi_4 = \gamma_5 / \pi_5$ (see Exercise~2).

In general, recovering the \textit{structural} parameters from the \textit{reduced-form} parameters can be challenging, as it requires relevant and credible identification restrictions, which may be difficult to justify or find in some empirical applications.\footnote{For accessible introductions to identification in linear systems, see \cite[Chap.~19]{gujarati2009basic} and \cite[Chap.~16]{wooldridge2016introductory}.}

For this example, we set noninformative priors: $\bm{B}_0 = \left[\bm{0}_5 \ \bm{0}_5\right]$, $\bm{V}_0 = 100 \bm{I}_K$, $\bm{\Psi}_0 = 5 \bm{I}_2$, and $\alpha_0 = 5$.\footnote{Note that these priors are specified for the \textit{reduced-form} model. This choice may have unintended consequences for the posterior distributions of the \textit{structural} parameters, which are ultimately of interest to researchers. For further discussion, see \cite[p.~302]{koop2003bayesian}.} Once the GUI is displayed (see the beginning of this chapter), follow Algorithm~\ref{alg:MultReg} to estimate multivariate linear models in the GUI (see Chapter~\ref{chapGUI} for details, particularly on data setup).

\begin{algorithm}[h!]
	\caption{Multivariate linear model}\label{alg:MultReg}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Multivariate Models} on the top panel
		\State Select \textit{Simple Multivariate} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select the number of dependent variables in the box \textbf{Number of endogenous variables: m}
		\State Select the number of independent variables (including the intercept) in the box \textbf{Number of exogenous variables: k}
		\State Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following \textbf{R} code shows how to perform the Gibss sampling algorithm in this example using the dataset \textit{4Institutions.csv}. We ask to run this example using the \textit{rmultireg} command from the \textit{bayesm} package as an exercise. We find that the posterior mean \textit{structural} effect of property rights on GDP is 0.98, and the 95\% credible interval is (0.56, 2.87). This means that there is evidence supporting a positive effect of property rights on gross domestic product. 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The effect of institutions on per capita GDP}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
DataInst <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/4Institutions.csv", sep = ",", header = TRUE, quote = "")
attach(DataInst)
Y <- cbind(logpcGDP95, PAER)
X <- cbind(1, logMort, Africa, Asia, Other)
M <- dim(Y)[2]
K <- dim(X)[2]
N <- dim(Y)[1]
# Hyperparameters
B0 <- matrix(0, K, M)
c0 <- 100
V0 <- c0*diag(K)
Psi0 <- 5*diag(M)
a0 <- 5
# Posterior parameters
Bhat <- solve(t(X)%*%X)%*%t(X)%*%Y 
S <- t(Y - X%*%Bhat)%*%(Y - X%*%Bhat)
Vn <- solve(solve(V0) + t(X)%*%X) 
Bn <- Vn%*%(solve(V0)%*%B0 + t(X)%*%X%*%Bhat)
Psin <- Psi0 + S + t(B0)%*%solve(V0)%*%B0 + t(Bhat)%*%t(X)%*%X%*%Bhat - t(Bn)%*%solve(Vn)%*%Bn
an <- a0 + N
#Posterior draws
s <- 10000 #Number of posterior draws
SIGs <- replicate(s, LaplacesDemon::rinvwishart(an, Psin))
BsCond <- sapply(1:s, function(s) {MixMatrix::rmatrixnorm(n = 1, mean=Bn, U = Vn,V = SIGs[,,s])})
summary(coda::mcmc(t(BsCond)))
SIGMs <- t(sapply(1:s, function(l) {gdata::lowerTriangle(SIGs[,,l], diag=TRUE, byrow=FALSE)}))
summary(coda::mcmc(SIGMs))
hdiBs <- HDInterval::hdi(t(BsCond), credMass = 0.95) # Highest posterior density credible interval
hdiBs
hdiSIG <- HDInterval::hdi(SIGMs, credMass = 0.95) # Highest posterior density credible interval
hdiSIG
beta2 <- BsCond[2,]/BsCond[7,] 
summary(coda::mcmc(beta2)) # Effect of property rights on GDP
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\section{Seemingly unrelated regression}\label{sec72}

In seemingly unrelated regression (SUR) models, there are $M$ dependent variables, each with potentially different sets of regressors, and their stochastic errors are contemporaneously correlated. The model is given by
\[
\bm{y}_m = \bm{X}_m \bm{\beta}_m + \bm{\mu}_m,
\]
where $\bm{y}_m$ is an $N$-dimensional vector of observations, $\bm{X}_m$ is an $N \times K_m$ matrix of regressors, $\bm{\beta}_m$ is a $K_m$-dimensional vector of coefficients, and $\bm{\mu}_m$ is an $N$-dimensional vector of stochastic errors, for $m = 1, 2, \dots, M$.

Let $\bm{\mu}_i = \left[\mu_{i1} \ \mu_{i2} \ \dots \ \mu_{iM}\right]^{\top}$, where $\bm{\mu}_i \sim {N}(\bm{0}, \bm{\Sigma})$. Stacking the $M$ equations, the system can be written as
\[
\bm{y} = \bm{X} \bm{\beta} + \bm{\mu},
\]
where $\bm{y} = \left[\bm{y}_1^{\top} \ \bm{y}_2^{\top} \ \dots \ \bm{y}_M^{\top}\right]^{\top}$ is an $MN$-dimensional vector, $\bm{\beta} = \left[\bm{\beta}_1^{\top} \ \bm{\beta}_2^{\top} \ \dots \ \bm{\beta}_M^{\top}\right]^{\top}$ is a $K$-dimensional vector with $K = \sum_{m=1}^M K_m$, and $\bm{X}$ is an $MN \times K$ block-diagonal matrix composed of the individual $\bm{X}_m$ matrices:
\[
\bm{X} =
\begin{bmatrix}
	\bm{X}_1 & \bm{0} & \dots & \bm{0} \\
	\bm{0} & \bm{X}_2 & \dots & \bm{0} \\
	\vdots & \vdots & \ddots & \vdots \\
	\bm{0} & \bm{0} & \dots & \bm{X}_M
\end{bmatrix}.
\]
Similarly, the error vector is $\bm{\mu} = \left[\bm{\mu}_1^{\top} \ \bm{\mu}_2^{\top} \ \dots \ \bm{\mu}_M^{\top}\right]^{\top}$, an $MN$-dimensional vector with $\bm{\mu} \sim {N}(\bm{0}, \bm{\Sigma} \otimes \bm{I}_N)$.

The likelihood function for the parameters is
\[
p(\bm{\beta}, \bm{\Sigma} \mid \bm{y}, \bm{X}) \propto |\bm{\Sigma}|^{-N/2}
\exp\left\{ -\frac{1}{2} (\bm{y} - \bm{X} \bm{\beta})^{\top}
(\bm{\Sigma}^{-1} \otimes \bm{I}_N) (\bm{y} - \bm{X} \bm{\beta}) \right\}.
\]

Assuming independent priors, $\pi(\bm{\beta}) \sim {N}(\bm{\beta}_0, \bm{B}_0)$ and
$\pi(\bm{\Sigma}^{-1}) \sim {W}(\alpha_0, \bm{\Psi}_0)$, the posterior distributions are
\begin{equation*}
	\bm{\beta} \mid \bm{\Sigma}, \bm{y}, \bm{X} \sim {N}(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\Sigma}^{-1} \mid \bm{\beta}, \bm{y}, \bm{X} \sim {W}(\alpha_n, \bm{\Psi}_n),
\end{equation*}
where
\[
\bm{B}_n = \left(\bm{X}^{\top} (\bm{\Sigma}^{-1} \otimes \bm{I}_N) \bm{X} + \bm{B}_0^{-1}\right)^{-1}, \quad
\bm{\beta}_n = \bm{B}_n \left(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}(\bm{\Sigma}^{-1} \otimes \bm{I}_N)\bm{y}\right),
\]
\[
\alpha_n = \alpha_0 + N, \quad \text{and} \quad
\bm{\Psi}_n = \left(\bm{\Psi}_0^{-1} + \bm{U}^{\top}\bm{U}\right)^{-1},
\]
where $\bm{U}$ is an $N \times M$ matrix whose columns are given by
$\bm{y}_m - \bm{X}_m \bm{\beta}_m$.

We can show, through straightforward but tedious algebra, that by defining $\bm{y}_i = [y_{i1} \ y_{i2} \ \dots \ y_{iM}]^{\top}$ and
\begin{align*}
	\bm{X}_i =
	\begin{bmatrix}
		\mathbf{x}_{1i}^{\top} & \bm{0} & \dots & \bm{0} \\
		\bm{0} & \mathbf{x}_{2i}^{\top} & \dots & \bm{0} \\
		\vdots & \vdots & \ddots & \vdots \\
		\bm{0} & \bm{0} & \dots & \mathbf{x}_{Mi}^{\top}
	\end{bmatrix},
\end{align*}
we can alternatively express
\[
\bm{B}_n = \left(\bm{B}_0^{-1} + \sum_{i=1}^N \bm{X}_i^{\top} \bm{\Sigma}^{-1} \bm{X}_i \right)^{-1}, \quad
\bm{\beta}_n = \bm{B}_n \left(\bm{B}_0^{-1} \bm{\beta}_0 + \sum_{i=1}^N \bm{X}_i^{\top} \bm{\Sigma}^{-1} \bm{y}_i \right),
\]
and
\[
\bm{\Psi}_n = \left(\bm{\Psi}_0^{-1} + \sum_{i=1}^N (\bm{y}_i - \bm{X}_i\bm{\beta})(\bm{y}_i - \bm{X}_i\bm{\beta})^{\top}\right)^{-1}.
\]

Since the conditional posteriors are standard, we can use a Gibbs sampling algorithm to obtain the posterior draws.\\

\textbf{Example: Utility demand}

Let's use the dataset \textit{Utilities.csv} to estimate a seemingly unrelated regression (SUR) model for utility demand. We adopt the same setting as in Exercise~14 of Chapter~\ref{chap4}, where a multivariate regression model is estimated after omitting households with zero consumption in any utility. In this exercise, we observe that not all regressors are relevant for the demand for electricity, water, and gas. Therefore, we estimate the following model:

\begin{align*}
	\log(\text{electricity}_i) & = \beta_1 + \beta_2\log(\text{electricity price}_i)+\beta_3\log(\text{water price}_i)\\
	&+\beta_4\log(\text{gas price}_i)+\beta_5\text{IndSocio1}_i+\beta_6\text{IndSocio2}_i+\beta_7\text{Altitude}_i\\
	&+\beta_8\text{Nrooms}_i+\beta_9\text{HouseholdMem}_i+\beta_{10}\log(\text{Income}_i)+\mu_{i1}\\
	\log(\text{water}_i) & = \alpha_1 + \alpha_2\log(\text{electricity price}_i)+\alpha_3\log(\text{water price}_i)\\
	&+\alpha_4\log(\text{gas price}_i)+\alpha_5\text{IndSocio1}_i+\alpha_6\text{IndSocio2}_i\\
	&+\alpha_7\text{Nrooms}_i+\alpha_8\text{HouseholdMem}_i+\mu_{i2}\\
	\log(\text{gas}_i) & = \gamma_1 + \gamma_2\log(\text{electricity price}_i)+\gamma_3\log(\text{water price}_i)\\
	&+\gamma_4\log(\text{gas price}_i)+\gamma_5\text{IndSocio1}_i+\gamma_6\text{IndSocio2}_i+\gamma_7\text{Altitude}_i\\
	&+\gamma_8\text{Nrooms}_i+\gamma_9\text{HouseholdMem}_i+\mu_{i3},
\end{align*} 
where electricity, water, and gas represent the monthly consumption of electricity (kWh), water (m$^3$), and gas (m$^3$) by Colombian households. The dataset includes information for 2,103 households, with details on the average prices of electricity (USD/kWh), water (USD/m$^3$), and gas (USD/m$^3$), as well as indicators of the socioeconomic conditions of the neighborhoods where the households are located (\textit{IndSocio1} being the lowest and \textit{IndSocio3} the highest). Additionally, the dataset contains information on whether the household is located in a municipality situated above 1,000 meters above sea level, the number of rooms in the dwelling, the number of household members, and monthly income (USD).

Because each equation includes a different set of regressors and we suspect correlation among the stochastic errors of the three equations, a SUR model is appropriate. Unobserved correlation across equations is expected since we are modeling utilities, and in many cases, a single provider manages all three services and issues a single bill.

Algorithm~\ref{alg:SUR} illustrates how to estimate SUR models using our GUI. The GUI implements the command \textit{rsurGibbs} from the \textit{bayesm} package in \textbf{R}. See Chapter~\ref{chapGUI} for additional details, including instructions on how to set up the dataset, and consult the templates available in our GitHub repository (\textbf{https://github.com/besmarter/BSTApp}) under the \textbf{DataApp} and \textbf{DataSim} folders.

\begin{algorithm}[h!]
	\caption{Seemingly unrelated regression}\label{alg:SUR}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Multivariate Models} on the top panel
		\State Select \textit{Seemingly Unrelated Regression} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Select the number of dependent variables in the box \textbf{Number of endogenous variables: m}
		\State Select the number of independent variables in the box \textbf{TOTAL number Exogenous Variables: k}. This is the sum of all exogenous variables over all equations including intercepts. In the example of \textbf{Utility demand}, it is equal to 27
		\State Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

The following code shows how to program this application using the \textit{bayesm} package. We use 10,000 MCMC iterations, with $\bm{\beta}_0 = \bm{0}_{27}$, $\bm{B}_0 = 100\bm{I}_{27}$, $\alpha_0 = 5$, and $\bm{\Psi}_0 = 5\bm{I}_3$.

The posterior median estimates of the own-price elasticities of demand for electricity, water, and gas are $-1.88$, $-0.36$, and $-0.62$, respectively, and none of the 95\% credible intervals include zero. This implies that a 1\% increase in the prices of electricity, water, and gas results in a 1.88\%, 0.36\%, and 0.62\% decrease in monthly consumption of these utilities, respectively.\footnote{This is an example where concerns about \textit{biased} and \textit{inconsistent} posterior mean estimates may arise, for instance, due to \textit{simultaneous causality} between quantity and price. These concerns are valid; however, we use micro-level data, which implies no price-quantity simultaneity. Moreover, the utility providers operate in regulated natural monopoly markets, mitigating potential endogeneity arising from searching provider strategies. Finally, prices were obtained directly from provider records, reducing possible measurement errors \cite{ramirez2024welfare}.} Overall, the results provide evidence supporting the relevance of most regressors in these equations and indicate the presence of unobserved correlation in the demand for these services, further justifying the use of the SUR model in this application.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Utility demand in Colombia}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101); library(dplyr)
DataUt <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/Utilities.csv", sep = ",", header = TRUE, quote = "")
DataUtEst <- DataUt %>% filter(Electricity != 0 & Water !=0 & Gas != 0)
attach(DataUtEst)
y1 <- log(Electricity); y2 <- log(Water); y3 <- log(Gas)
X1 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem, Lnincome)
X2 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Nrooms, HouseholdMem)
X3 <- cbind(1, LnPriceElect, LnPriceWater, LnPriceGas, IndSocio1, IndSocio2, Altitude, Nrooms, HouseholdMem)
regdata <- NULL
regdata[[1]] <- list(y = y1, X = X1); regdata[[2]] <- list(y = y2, X = X2); regdata[[3]] <- list(y = y3, X = X3)
M <- length(regdata); K1 <- dim(X1)[2]; K2 <- dim(X2)[2]; K3 <- dim(X3)[2] 
K <- K1 + K2 + K3
# Hyperparameters
b0 <- rep(0, K); c0 <- 100; B0 <- c0*diag(K); V <- 5*diag(M); a0 <- M
Prior <- list(betabar = b0, A = solve(B0), nu = a0, V = V)
#Posterior draws
S <- 10000; keep <- 1; Mcmc <- list(R = S, keep = keep)
PosteriorDraws <- bayesm::rsurGibbs(Data = list(regdata = regdata), Mcmc = Mcmc, Prior = Prior)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Utility demand in Colombia, results}
	\begin{VF}
		\begin{lstlisting}[language=R]
Bs <- PosteriorDraws[["betadraw"]]
Names <- c("Const", "LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", "Altitude", "Nrooms", "HouseholdMem", "Lnincome", "Const", "LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", 
"Nrooms", "HouseholdMem","Const", "LnPriceElect", "LnPriceWater", "LnPriceGas", "IndSocio1", "IndSocio2", "Altitude", "Nrooms", "HouseholdMem")
colnames(Bs) <- Names
summary(coda::mcmc(Bs))
summary(coda::mcmc(PosteriorDraws[["Sigmadraw"]]))
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

In Exercise~5, we ask you to run this application using our GUI and the dataset \textit{Utilities.csv}. Note that this file must be modified to match the structure required by our GUI (see the dataset \textit{5Institutions.csv} in the \textit{DataApp} folder of our GitHub repository — \textbf{https://github.com/besmarter/BSTApp} — for a template). In addition, we ask you to program the Gibbs sampling algorithm for this application from scratch.

\section{Instrumental variable}\label{sec73}

This inferential approach is used when there are \textit{endogeneity} issues, that is, when the stochastic error term is not independent of the regressors. This dependence generates \textit{bias} in posterior mean estimates when using an inferential approach that does not account for it. \textit{Endogeneity} can arise from \textit{simultaneous causality}, \textit{omitted relevant correlated variables}, or \textit{measurement error} in the regressors.\footnote{See \cite[Chap.~15]{wooldridge2016introductory} for an introductory treatment of instrumental variables under the frequentist inferential approach.}

Let the dependent variable be modeled as a linear function of one endogenous regressor and a set of exogenous regressors:
\[
y_{i} = \bm{x}_{ei}^{\top}\bm{\beta}_1 + \beta_s x_{si} + \mu_{i},
\]
where
\[
x_{si} = \bm{x}_{ei}^{\top}\bm{\gamma}_1 + \bm{z}_i^{\top}\bm{\gamma}_2 + v_{i}.
\]
Here, \(x_{si}\) is the variable that generates endogeneity (\(\mathbb{E}[\mu \mid x_s] \neq 0\)), \(\bm{x}_e\) is a vector of \(K_1\) exogenous regressors (\(\mathbb{E}[\mu \mid \bm{x}_e] = \bm{0}\)), and \(\bm{z}\) is a vector of \(K_2\) instruments. The instruments influence \(x_s\) (\(\mathbb{E}[x_s \bm{z}] \neq \bm{0}\)) but do not directly affect \(y\) (\(\mathbb{E}[y \bm{z} \mid x_s] = \bm{0}\)). The equation for \(y\) is known as the \textit{structural equation}, which is the main object of inference.

Assuming $(\mu_i, v_i)^{\top} \stackrel{i.i.d.}{\sim} N(\bm{0}, \bm{\Sigma})$, with $\bm{\Sigma} = [\sigma_{lm}]$, $l,m = 1,2$, the likelihood function is
\begin{align*}
	p(\bm{\beta}, \bm{\gamma}, \bm{\Sigma} \mid \bm{y}, \bm{X}, \bm{Z}) 
	&= \frac{1}{(2\pi)^{N/2} |\bm{\Sigma}|^{N/2}}\\
	&\times
	\exp\left\{
	-\frac{1}{2} \sum_{i=1}^N 
	(y_i - \bm{x}_i^{\top}\bm{\beta},\, x_{si} - \bm{w}_i^{\top}\bm{\gamma})
	\bm{\Sigma}^{-1}
	\begin{pmatrix}
		y_i - \bm{x}_i^{\top}\bm{\beta} \\
		x_{si} - \bm{w}_i^{\top}\bm{\gamma}
	\end{pmatrix}
	\right\},
\end{align*}
where $\bm{\beta} = [\bm{\beta}_1^{\top} \ \beta_s]^{\top}$, $\bm{\gamma} = [\bm{\gamma}_1^{\top} \ \bm{\gamma}_2^{\top}]^{\top}$, $\bm{x}_i = [\bm{x}_{ei}^{\top} \ x_{si}]^{\top}$, and $\bm{w}_i = [\bm{x}_{ei}^{\top} \ \bm{z}_{i}^{\top}]^{\top}$.

We obtain standard conditional posterior densities by specifying independent priors:
\[
\bm{\gamma} \sim N(\bm{\gamma}_0, \bm{G}_0), 
\quad 
\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0), 
\quad 
\bm{\Sigma}^{-1} \sim W(\alpha_0, \bm{\Psi}_0).
\]
The conditional posterior distributions are:
\begin{equation*}
	\bm{\beta} \mid \bm{\gamma}, \bm{\Sigma}, \bm{y}, \bm{X}, \bm{Z} \sim N(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\gamma} \mid \bm{\beta}, \bm{\Sigma}, \bm{y}, \bm{X}, \bm{Z} \sim N(\bm{\gamma}_n, \bm{G}_n),
\end{equation*}
\begin{equation*}
	\bm{\Sigma}^{-1} \mid \bm{\beta}, \bm{\gamma}, \bm{y}, \bm{X}, \bm{Z} \sim W(\alpha_n, \bm{\Psi}_n),
\end{equation*}
where
\(
\bm{\beta}_n = \bm{B}_n \left( \bm{B}_0^{-1} \bm{\beta}_0 
+ \omega_1^{-1} \sum_{i=1}^{N} 
\bm{x}_i \left( y_i - \frac{\sigma_{12}(x_{si} - \bm{w}_i^{\top}\bm{\gamma})}{\sigma_{22}} \right) \right),
\)
\(
\bm{B}_n = \left( \omega_1^{-1} \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^{\top} + \bm{B}_0^{-1} \right)^{-1}, \omega_1 = \sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}},
\)
\(
\bm{G}_n = \left( \omega_2^{-1} \sum_{i=1}^{N} \bm{w}_i \bm{w}_i^{\top} + \bm{G}_0^{-1} \right)^{-1},\) 
\(\bm{\gamma}_n = \bm{G}_n \left( \bm{G}_0^{-1} \bm{\gamma}_0 
+ \omega_2^{-1} \sum_{i=1}^{N} 
\bm{w}_i \left( x_{si} - \frac{\sigma_{12}(y_i - \bm{x}_i^{\top}\bm{\beta})}{\sigma_{11}} \right) \right),
\)
\(
\omega_2 = \sigma_{22} - \frac{\sigma_{12}^2}{\sigma_{11}}, 
\quad 
\bm{\Psi}_n = \left[ \bm{\Psi}_0^{-1} 
+ \sum_{i=1}^N 
\begin{pmatrix} 
	y_i - \bm{x}_i^{\top}\bm{\beta} \\ 
	x_{si} - \bm{w}_i^{\top}\bm{\gamma} 
\end{pmatrix} 
(y_i - \bm{x}_i^{\top}\bm{\beta},\, x_{si} - \bm{w}_i^{\top}\bm{\gamma}) 
\right]^{-1},
\)
\(
\alpha_n = \alpha_0 + N,
\) and $\sigma_{lm}$ are the elements of $\bm{\Sigma}$.

We also use a Gibbs sampling algorithm in this model since we have standard conditional posterior distributions.\\

\textbf{Example: Simulation exercise}

Let's simulate the simple process $y_i=\beta_1+\beta_2x_{si}+\mu_i$ and $x_{si}=\gamma_1+\gamma_2z_i+v_i$ where $[\mu_i \ v_i]^{\top}\sim N(\bm{0},\bm{\Sigma})$, $\bm{\Sigma}=[\sigma_{lj}]$ such that $\sigma_{12}\neq 0$, $i=1,2,\dots,100$.

Observe that $\mu \mid v \sim N\left(\frac{\sigma_{12}}{\sigma_{22}}v,\ \sigma_{11} - \frac{\sigma_{21}^2}{\sigma_{22}}\right)$. This implies that $\mathbb{E}[\mu \mid x_s] = \mathbb{E}[\mu \mid v] = \frac{\sigma_{12}}{\sigma_{22}}v \neq 0$ when $\sigma_{12} \neq 0$, while $\mathbb{E}[\mu \mid z] = 0$.

Let all location parameters be equal to 1, and set $\sigma_{11} = \sigma_{22} = 1$, $\sigma_{12} = 0.8$, and $z \sim N(0,1)$. From the large-sample properties of the posterior mean, we know that it converges to the maximum likelihood estimator (see Section~\ref{sec11} and \cite{Lehmann2003,van2000asymptotic}). In this setting, the estimator is 
\[
\hat{\beta}_2 = \frac{\widehat{\mathrm{Cov}}(x_s, y)}{\widehat{\mathrm{Var}}(x_s)},
\]
which converges in probability to 
\[
\beta_2 + \frac{\sigma_{12}}{\sigma_{22}\mathrm{Var}(x_s)} 
= \beta_2 + \frac{\sigma_{12}}{\sigma_{22}(\gamma_2^2 \mathrm{Var}(z) + \sigma_{22})} 
= 1.4.
\]
Thus, the asymptotic bias when using the posterior mean of a linear regression without accounting for endogeneity is 0.4 in this example.

We assess the sampling performance of the Bayesian estimators by simulating this setting 100 times. The following code demonstrates how to perform this exercise using a linear model that does not account for the \textit{endogeneity} problem (see Section~\ref{sec61}), as well as how to implement the instrumental variable model using the function \textit{rivGibbs} from the \textit{bayesm} package.\footnote{It appears that this function does not explicitly account for the effects of exogenous regressors in the equation for the endogenous regressor.} 

In this setup, we use \( \bm{B}_0 = 1000 \bm{I}_2 \), \( \bm{\beta}_0 = \bm{0}_2 \), and set the parameters of the inverse-gamma distribution to 0.0005. For the instrumental variable model, we additionally specify \( \bm{\gamma}_0 = \bm{0}_2 \), \( \bm{G}_0 = 1000 \bm{I}_2 \), \( \alpha_0 = 3 \), and \( \bm{\Psi}_0 = 3 \bm{I}_2 \).

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise, sampling properties ordinary and instrumental models}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101)
N <- 100; k <- 2
B <- rep(1, k); G <- rep(1, 2); s12 <- 0.8
SIGMA <- matrix(c(1, s12, s12, 1), 2, 2)
z <- rnorm(N); Z <- cbind(1, z); w <- matrix(1,N,1); S <- 100
U <- replicate(S, MASS::mvrnorm(n = N, mu = rep(0, 2), SIGMA))
x <- G[1] + G[2]*z + U[,2,]; y <- B[1] + B[2]*x + U[,1,]
# Hyperparameters
d0 <- 0.001/2; a0 <- 0.001/2
b0 <- rep(0, k); c0 <- 1000; B0 <- c0*diag(k)
B0i <- solve(B0); g0 <- rep(0, 2)
G0 <- 1000*diag(2); G0i <- solve(G0)
nu <- 3; Psi0 <- nu*diag(2)
# MCMC parameters
mcmc <- 5000; burnin <- 1000
tot <- mcmc + burnin; thin <- 1
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise, sampling properties ordinary and instrumental models}
	\begin{VF}
		\begin{lstlisting}[language=R]
# Gibbs sampling
Gibbs <- function(x, y){
	Data <- list(y = y, x = x, w = w, z = Z)
	Mcmc <- list(R = mcmc, keep = thin, nprint = 0)
	Prior <- list(md = g0, Ad = G0i, mbg = b0, Abg = B0i, nu = nu, V = Psi0)
	RestIV <- bayesm::rivGibbs(Data = Data, Mcmc = Mcmc, Prior = Prior)
	PostBIV <- mean(RestIV[["betadraw"]])
	ResLM <- MCMCpack::MCMCregress(y ~ x + w - 1, b0 = b0, B0 = B0i, c0 = a0, d0 = d0)
	PostB <- mean(ResLM[,1]); Res <- c(PostB,PostBIV)
	return(Res)
}
PosteriorMeans <- sapply(1:S, function(s) {Gibbs(x = x[,s], y = y[,s])})
rowMeans(PosteriorMeans)
Model <- c(replicate(S, "Ordinary"), replicate(S, "Instrumental"))
postmeans <- c(t(PosteriorMeans))
df <- data.frame(postmeans, Model, stringsAsFactors = FALSE)
library(ggplot2); library(latex2exp)
histExo <- ggplot(df, aes(x = postmeans, fill = Model)) + geom_histogram(bins = 40, position = "identity", color = "black", alpha = 0.5) + labs(title = "Overlayed Histograms", x = "Value", y = "Count") + scale_fill_manual(values = c("black", "grey")) + geom_vline(aes(xintercept = mean(postmeans[1:S])), color = "black", linewidth = 1, linetype = "dashed") + geom_vline(aes(xintercept = mean(postmeans[101:200])), color = "black", linewidth = 1, linetype = "dotted") + geom_vline(aes(xintercept = B[2]), color = "black", linewidth = 1) + xlab(TeX("$E[\\beta_2]$")) + ylab("Frequency") + ggtitle("Histogram: Posterior means simulating 100 samples") 
histExo 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

Figure~\ref{fig71} displays the histograms of the posterior means of \( \beta_2 \) obtained from the ordinary model, which does not account for endogeneity, and from the instrumental variable model. On one hand, the mean of the posterior means for the ordinary model is 1.41 (black dashed line in the gray histogram), implying a bias of 0.41, very close to the theoretical bias of 0.40. On the other hand, the mean of the posterior means for the instrumental variable model is 1.04 (black dotted line in the black histogram), which is close to the true population value of \( \beta_2 = 1 \) (black line).

We also observe that the histogram of the posterior means for the ordinary model is less dispersed. In other words, this estimator is more efficient, a well-known result in the frequentist inferential framework when comparing ordinary least squares (OLS) and two-stage least squares (2SLS) estimators (see \cite[Chap.~5]{wooldridge2010econometric}).

Two key aspects in the instrumental variables literature are the \textit{weakness} and \textit{exogeneity} of the instruments. The former refers to the strength of the relationship between the instruments and the endogenous regressors, while the latter concerns the independence of the instruments from the stochastic error in the \textit{structural equation}. In Exercise~6, you are asked to use the previous code as a baseline to study these two aspects. 

Observe the link between the \textit{weakness} and \textit{exogeneity} of the instruments and the \textit{exclusion restrictions} (\( \mathbb{E}[x_s \bm{z}] \neq \bm{0} \) and \( \mathbb{E}[y \bm{z} \mid x_s] = \bm{0} \)). This is the point of departure for \cite{Conley2012}, who propose assessing the plausibility of the \textit{exclusion restrictions} by defining \textit{plausible exogeneity} as having prior information suggesting that the effect of the instrument in the \textit{structural equation} is close to zero, though perhaps not exactly zero. See Chapter~\ref{chap12}, particularly Section~\ref{chap12_4}, for further discussion of instrumental variables.
 
\begin{figure}
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter7/figures/Fig71.png}
	\caption[List of figure caption goes here]{Histogram of posterior means: Ordinary and instrumental models.}\label{fig71}
\end{figure}

Algorithm \ref{alg:IVReg} can be used to estimate the instrumental variable model using our GUI. We ask in Exercise 8 to replicate the example of the effect of institutions on per capita GDP using our GUI.  

\begin{algorithm}[h!]
	\caption{Instrumental variable model}\label{alg:IVReg}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Multivariate Models} on the top panel
		\State Select \textit{Variable instrumental (two equations)} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the formula of the structural equation in the \textbf{Main Equation} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Write down the formula of the endogenous regressor in the \textbf{Instrumental Equation} box. This formula must be written using the syntax of the \textit{formula} command of \textbf{R} software. This equation includes intercept by default, do not include it in the equation
		\State Set the hyperparameters: mean vectors, covariance matrices, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

\section{Multivariate probit model}\label{sec74}

In the multivariate probit model \cite{Edwards2003}, the response variable \( y_{il} \in \{0, 1\} \) indicates whether individual \( i \) selects option \( l \) among \( L \) mutually exclusive binary choices, where \( l = 1, 2, \dots, L \) and \( i = 1, 2, \dots, N \). Specifically,
\[
y_{il} = 
\begin{cases}
	0, & \text{if } y_{il}^* \leq 0, \\[3pt]
	1, & \text{if } y_{il}^* > 0,
\end{cases}
\]
where \( \bm{y}_i^* = \bm{X}_i \bm{\beta} + \bm{\mu}_i \stackrel{iid}{\sim}N(\bm{0}, \bm{\Sigma}) \). Here, \( \bm{y}_i^* \) is an unobserved latent \( L \)-dimensional vector, \( \bm{X}_i =   \bm{x}_i^\top \otimes \bm{I}_L \) is an \( L \times K \) design matrix of regressors with \( K = L \times k \), and \( k \) is the number of regressors (i.e., the dimension of \( \bm{x}_i \)). In addition, \( \bm{\beta} = [\bm{\beta}_1^\top \ \bm{\beta}_2^\top \dots \bm{\beta}_k^\top]^\top \), where each \( \bm{\beta}_j \) is an \( L \)-dimensional vector of coefficients for \( j = 1, 2, \dots, k \).

The likelihood function for this model is given by
\[
p(\bm{\beta}, \bm{\Sigma} \mid \bm{y}, \bm{X}) = \prod_{i=1}^N \prod_{l=1}^L p_{il}^{y_{il}},
\]
where \( p_{il} = p(y_{il}^* \geq 0) \).

Note that \( p(y_{il}^* \geq 0) = p(\lambda_{ll} y_{il}^* \geq 0) \) for any \( \lambda_{ll} > 0 \). This leads to identification issues because only the correlation matrix can be identified, similar to the univariate probit model, where the variance is fixed to 1 for identification. We follow the post-processing strategy proposed by \cite{Edwards2003} to obtain identified parameters, namely,
\[
\tilde{\bm{\beta}} = \mathrm{vec}\{\bm{\Lambda}\bm{B}\}
\quad \text{and} \quad 
\bm{R} = \bm{\Lambda}\bm{\Sigma}\bm{\Lambda},
\]
where \( \bm{\Lambda} = \mathrm{diag}\{\sigma_{ll}\}^{-1/2} \) and \( \bm{B} = [\bm{\beta}_1 \ \bm{\beta}_2 \dots \bm{\beta}_k] \).\footnote{In a Bayesian framework, a model can be non-identified; however, the posterior distribution of the parameters exists as long as proper prior distributions are specified \cite{Edwards2003}.}

We assume independent priors \( \bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0) \) and \( \bm{\Sigma}^{-1} \sim W(\alpha_0, \bm{\Psi}_0) \). Using data augmentation for \( \bm{y}^* \), we can apply Gibbs sampling, since this becomes a standard Bayesian linear regression model.

The posterior conditional distributions are:
\begin{equation*}
	\bm{\beta} \mid \bm{\Sigma}, \bm{w} \sim N(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\Sigma}^{-1} \mid \bm{\beta}, \bm{w} \sim W(\alpha_n, \bm{\Psi}_n),
\end{equation*}
\begin{equation*}
	y_{il}^* \mid \bm{y}_{i,-l}^*, \bm{\beta}, \bm{\Sigma}^{-1}, \bm{y}_i 
	\sim TN_{I_{il}}(m_{il}, \tau_{ll}^2),
\end{equation*}
where
\[
\bm{B}_n = (\bm{B}_0^{-1} + \bm{X}^{*\top}\bm{X}^*)^{-1}, 
\quad
\bm{\beta}_n = \bm{B}_n (\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{*\top}\bm{y}^{**}),
\]
\[
\bm{\Sigma}^{-1} = \bm{C}^\top \bm{C}, 
\quad
\bm{X}_i^{*} = \bm{C}^\top \bm{X}_i,
\quad
\bm{y}_i^{**} = \bm{C}^\top \bm{y}_i^*,
\]
\[
\alpha_n = \alpha_0 + N, 
\quad 
\bm{\Psi}_n = \left(\bm{\Psi}_0 + \sum_{i=1}^N (\bm{y}_i^* - \bm{X}_i\bm{\beta})(\bm{y}_i^* - \bm{X}_i\bm{\beta})^\top\right)^{-1},
\]
\[
m_{il} = \bm{x}_{il}^\top \bm{\beta} + \bm{f}_l^\top(\bm{y}_{i,-l}^* - \bm{X}_{i,-l}\bm{\beta}),
\quad
\tau_{ll}^2 = \sigma_{ll} - \bm{\omega}_{l,-l}^\top \bm{\Sigma}_{-l,-l}^{-1} \bm{\omega}_{-l,l},
\]
and
\[
\bm{f}_l^\top = \bm{\omega}_{l,-l}^\top \bm{\Sigma}_{-l,-l}^{-1}, 
\quad
\bm{y}_{i,-l}^* \text{ is the } (L-1)\text{-dimensional subvector of } \bm{y}_i^* \text{ excluding } y_{il}^*,
\]
\[
\bm{x}_{il}^\top \text{ is the } l\text{-th row of } \bm{X}_i, 
\quad
\bm{X}_{i,-l} \text{ is } \bm{X}_i \text{ with the } l\text{-th row removed},
\]
\[
\bm{X}^* = 
\begin{bmatrix}
	\bm{X}_1^* \\[2pt]
	\bm{X}_2^* \\[2pt]
	\vdots \\[2pt]
	\bm{X}_N^*
\end{bmatrix},
\quad
I_{il} =
\begin{cases}
	y_{il}^* > 0, & \text{if } y_{il} = 1,\\
	y_{il}^* \leq 0, & \text{if } y_{il} = 0,
\end{cases}
\quad \text{and} \quad
\bm{\Sigma} =
\begin{bmatrix}
	\bm{\omega}_1^\top \\
	\bm{\omega}_2^\top \\
	\vdots \\
	\bm{\omega}_L^\top
\end{bmatrix}.
\]

The setting in our GUI has same regressors in each binary decision. However, we can see that the multivariate probit model is similar to a SUR model in latent variables. We ask in Exercise 9 to implement a Gibbs sampling algorithm for a multivariate probit model with different regressors in each equation.\\

\textbf{Example: Self selection in hospitalization due to a subsidized health care program}

We use the dataset \textit{7HealthMed.csv}, where the dependent variable is \( y = [\text{Hosp} \ \text{SHI}]^{\top} \), with \(\text{Hosp} = 1\) if an individual was hospitalized in the year prior to the survey (0 otherwise), and \(\text{SHI} = 1\) if the individual was enrolled in a subsidized health insurance program (0 otherwise).

Recall that in our previous application of binary response models, we examined the determinants of hospitalization in Medellín (Colombia), where one of the regressors was a binary indicator of participation in a subsidized health care program (see Section~\ref{sec63}). If we suspect dependence between the decisions related to these two variables, a bivariate probit model is appropriate. 

A priori, we would expect that being enrolled in a subsidized health care program increases the probability of hospitalization \textit{ceteris paribus}, since it reduces the patient’s out-of-pocket cost. However, if an individual anticipates a future hospitalization and the factors influencing this expectation are unobserved by the modeler, a feedback effect may arise from hospitalization to enrollment in the subsidized health care program.

\begin{algorithm}[h!]
	\caption{Multivariate probit model}\label{alg:MtultProbit}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Multivariate Models} on the top panel
		\State Select \textit{Multivariate Probit} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Write down the number of cross-sectional units in the \textbf{Number of individuals: n} box
		\State Write down the number of exogenous variables in the \textbf{Number of exogenous variables: k} box
		\State Write down the number of choices in the \textbf{Number of choices: l} box
		\State Set the hyperparameters: mean vectors, covariance matrix, degrees of freedom, and the scale matrix. This step is not necessary as by default our GUI uses non-informative priors
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}

We consider seven regressors: a constant, gender (female), age, self-perceived health status (with categories fair, good, and excellent, using bad as the reference category), and the proportion of the individual’s age spent living in their neighborhood. The latter variable serves as a proxy for social capital, which may influence enrollment in the subsidized health insurance program, as the target population is identified by local government authorities \cite{Ramirez2019a}. The dataset comprises 12,975 individuals who can ``choose'' between two outcomes: hospitalization and enrollment in the subsidized health insurance regime.

Algorithm \ref{alg:MtultProbit} shows how to run a multivariate probit model using our GUI.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Self selection in hospitalization}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/7HealthMed.csv", sep = ",", header = TRUE, quote = "")
attach(Data); str(Data)
p <- 2; nd <- 7; N <- length(y)/p; y <- y
Xd <- as.matrix(Data[seq(1, p*N, 2),3:9])
XcreateMP<-function(p,nxs,nind,Data){
	pandterm = function(message) {
		stop(message, call. = FALSE)
	}
	if (missing(nxs)) 
	pandterm("requires number of regressors: include intercept if required")
	if (missing(nind)) 
	pandterm("requires number of units (individuals)")
	if (missing(Data)) 
	pandterm("requires dataset")
	if (nrow(Data)!=nind*2)
	pandterm("check dataset! number of units times number alternatives should be equal to dataset rows")
	XXDat<-array(0,c(p,1+nxs,nind))
	XX<-array(0,c(p,nxs*p,nind))
	YY<-array(0,c(p,1,nind))
	is<- seq(p,nind*p,p)
	cis<- seq(nxs,nxs*p+1,nxs)
	for(i in is){
		j<-which(i==is)
		XXDat[,,j]<-as.matrix(Data[c((i-(p-1)):i),-1])
		YY[,,j]<-XXDat[,1,j]
		for(l in 1:p){
			XX[l,((cis[l]-(nxs-1)):cis[l]),j]<-XXDat[l,-1,j]
		}
	}
	return(list(y=YY,X=XX))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Self selection in hospitalization}
	\begin{VF}
		\begin{lstlisting}[language=R]
Dat <- XcreateMP(p = p, nxs = nd, nind = N, Data = Data)
y<-NULL; X<-NULL
for(i in 1:dim(Dat$y)[3]){
	y<-c(y,Dat$y[,,i])
	X<-rbind(X,Dat$X[,,i])
}
DataMP = list(p=p, y=y, X=X)
# Hyperparameters
k <- dim(X)[2]; b0 <- rep(0, k); c0 <- 1000
B0 <- c0*diag(k); B0i <- solve(B0)
a0 <- p - 1 + 3; Psi0 <- a0*diag(p)
Prior <- list(betabar = b0, A = B0i, nu = a0, V = Psi0)
# MCMC parameters
mcmc <- 20000; thin <- 5; 
Mcmc <- list(R = mcmc, keep = thin)
Results <- bayesm::rmvpGibbs(Data = DataMP, Mcmc = Mcmc, Prior = Prior)
betatilde1 <- Results$betadraw[,1:7] / sqrt(Results$sigmadraw[,1])
summary(coda::mcmc(betatilde1))
betatilde2 <- Results$betadraw[,8:14] / sqrt(Results$sigmadraw[,4])
summary(coda::mcmc(betatilde2))
sigmadraw12 <-  Results$sigmadraw[,3] / (Results$sigmadraw[,1]*Results$sigmadraw[,4])^0.5
summary(coda::mcmc(sigmadraw12))
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

We set 20,000 MCMC iterations with a thinning parameter of 5. The hyperparameters are specified as $\bm{\beta}_0 = \bm{0}_{14}$, $\bm{B}_0 = 100\bm{I}_{14}$, $\alpha_0 = 4$, and $\bm{\Psi}_0 = 4\bm{I}_2$.\footnote{Note that the order of the location coefficients in our GUI follows the order of the equations rather than the regressors, as in the theoretical framework presented in this section. This distinction is important for correctly specifying the hyperparameters and interpreting the estimated location parameters.}

The previous \textbf{R} code illustrates how to obtain posterior draws using the \textit{rmvpGibbs} command from the \textit{bayesm} package. The results suggest that females, older individuals, and those who perceive their health as poor are more likely to be hospitalized. Moreover, females, older individuals, and individuals with poor or fair self-perceived health who have spent a larger proportion of their lives in their current neighborhoods are more likely to be enrolled in the subsidized health care system. Finally, the results indicate no evidence of unobserved correlation between the two equations, as the 95\% credible interval for the correlation parameter is (-0.07, 0.06).

\section{Summary}\label{sec75}
In this chapter, we present the setting and posterior distributions of several common multivariate models. The multivariate framework enables us to address \textit{endogeneity} issues by exploiting the conditional distribution of a multivariate normal vector. Moreover, in these models, the posterior conditional distributions belong to standard families, multivariate normal, Wishart, and truncated normal. This property allows the straightforward implementation of the Gibbs sampling algorithm across all these models.


\section{Exercises}\label{sec76}
\begin{enumerate}
	
	\item Show that $\mathbb{E}[u_1\text{PAER}] = \frac{\alpha_1}{1 - \beta_1\alpha_1} \sigma_1^2$, assuming that $\mathbb{E}[u_1 u_2] = 0$ and $\text{Var}(u_1) = \sigma_1^2$, in the example on the effect of institutions on per capita GDP.
	
	\item Show that $\beta_1 = \pi_1 / \gamma_1$ in the example on the effect of institutions on per capita GDP.
	
	\item \textbf{The effect of institutions on per capita GDP continues I}
	
	Use the \textit{rmultireg} command from the \textit{bayesm} package to perform inference in the example of the effect of institutions on per capita GDP.
	
	\item \textbf{Demand and supply simulation}
	
	Consider the structural demand–supply model:
	\begin{align*}
		q_i^d &= \beta_1 + \beta_2 p_i + \beta_3 y_i + \beta_4 pc_i + \beta_5 ps_i + u_{i1}, \\
		q_i^s &= \alpha_1 + \alpha_2 p_i + \alpha_3 er_i + u_{i2},
	\end{align*}
	where $q^d$ and $q^s$ represent demand and supply, respectively; $p$, $y$, $pc$, $ps$, and $er$ denote price, income, complementary price, substitute price, and exchange rate. The variables $pc$ and $ps$ refer to the prices of complementary and substitute goods for $q$. Assume that $\bm{\beta} = [5 \ -0.5 \ 0.8 \ -0.4 \ 0.7]^{\top}$, $\bm{\alpha} = [-2 \ 0.5 \ -0.4]^{\top}$, $u_1 \sim N(0, 0.5^2)$, and $u_2 \sim N(0, 0.5^2)$. Additionally, assume that $y \sim N(10, 1)$, $pc \sim N(5, 1)$, $ps \sim N(5, 1)$, and $er \sim N(15, 1)$.
	
	\begin{itemize}
		\item Derive the \textit{reduced-form} model under the equilibrium condition $q^d = q^s$, which defines the observable quantity $q$.
		\item Simulate $p$ and $q$ from the \textit{reduced-form} equations.
		\item Perform inference for the \textit{reduced-form} model using the \textit{rmultireg} command from the \textit{bayesm} package.
		\item Use the posterior draws of the \textit{reduced-form} parameters to perform inference for the \textit{structural} parameters. Discuss any potential issues. \textit{Hint:} Are all structural parameters exactly identified?
	\end{itemize}
	
	\item \textbf{Utility demand continues}
	
	\begin{itemize}
		\item Run the \textbf{Utility demand} application using our GUI and the dataset \textit{Utilities.csv}. \textit{Hint:} This file must be modified to match the structure required by our GUI (see the dataset \textit{5Institutions.csv} in the \textit{DataApp} folder of our GitHub repository—\textbf{https://github.com/besmarter/BSTApp}—as a template).
		\item Program the Gibbs sampling algorithm for this application from scratch.
	\end{itemize}
	
	\item \textbf{Simulation exercise of instrumental variables continues I}
	
	\begin{itemize}
		\item Use the setting of the simulation exercise with instrumental variables to analyze the impact of a weak instrument. For example, set $\gamma_2 = 0.2$ and compare the posterior mean estimates of the ordinary and instrumental variable models.
		\item Perform a simulation to analyze how the degree of exogeneity of the instrument affects the performance of the posterior mean in the instrumental variable model.
	\end{itemize}
	
	\item \textbf{Simulation exercise of instrumental variables continues II}
	
	Program the Gibbs sampling algorithm for the instrumental variable model from scratch.
	
	\item \textbf{The effect of institutions on per capita GDP continues II}
	
	Estimate the structural Equation \ref{eq:str1} using the instrumental variable model, where $\log(\textit{Mort})$ serves as the instrument for PAER. Compare the estimated effect of property rights on per capita GDP with that obtained in the example on the effect of institutions on per capita GDP. Use the file \textit{6Institutions.csv} to conduct this exercise in our GUI, setting $\bm{B}_0 = 100\bm{I}_5$, $\bm{\beta}_0 = \bm{0}_5$, $\bm{\gamma}_0 = \bm{0}_2$, $\bm{G}_0 = 100\bm{I}_2$, $\alpha_0 = 3$, and $\bm{\Psi}_0 = 3\bm{I}_2$. Set the MCMC parameters as follows: 50,000 iterations, 1,000 burn-in, and a thinning interval of 5.
	
	\item \textbf{Multivariate probit with different regressors}
	
	Simulate the model
	\[
	y_{i1}^* = 0.5 - 1.2x_{i11} + 0.7x_{i12} + 0.8x_{i3} + \mu_{i1}, \quad
	y_{i2}^* = 1.5 - 0.8x_{i21} + 0.5x_{i22} + \mu_{i2},
	\]
	with
	\[
	\bm{\Sigma} =
	\begin{bmatrix}
		1 & 0.5 \\
		0.5 & 1
	\end{bmatrix},
	\]
	where all regressors follow a standard normal distribution and $N = 5000$. Use $\bm{\beta}_0 = \bm{0}$, $\bm{B}_0 = 1000\bm{I}$, $\alpha_0 = 4$, and $\bm{\Psi}_0 = 4\bm{I}_2$. Set the number of iterations to 2,000 and the thinning parameter to 5.
	
	\begin{itemize}
		\item Perform inference under the setting of Section \ref{sec74}, assuming that $x_{i3}$ may affect $y_{i2}$.
		\item Program a Gibbs sampling algorithm that accounts for different regressors in each binary decision, that is, where $x_{i3}$ does not affect $y_{i2}$.
	\end{itemize}
	
\end{enumerate}



