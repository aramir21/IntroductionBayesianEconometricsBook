\chapter{Bayesian model average}\label{chap10}

We describe in this chapter how to introduce model uncertainty and average over different models in a probabilistic consistent way. We describe .... 

Remember that we can run our GUI typing

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. How to display our graphical user interface}
	\begin{VF}
		\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

in the \textbf{R} package console or any \textbf{R} code editor, and once our GUI is deployed, select \textit{Bayesian Model Averaging}. However, users should see Chapter \ref{chapGUI} for other options and details.

\section{Foundation}\label{sec10_1}
Remember from Chapter \ref{chap1} that Bayesian model averaging (BMA) is an approach which takes into account model uncertainty. In particular, we consider uncertainty in the regressors (variable selection) in a regression framework where there are $K$ possible explanatory variables.\footnote{Take into account that $K$ can increase when interaction terms and/or polynomial terms of the original control variables are included.} This implies $2^K$ potential models indexed by parameters $\bm{\theta}_m$, $m=1,2,\dots,2^K$.

Following \cite{Simmons2010}, the posterior model probability is
\begin{equation*}
	\pi(\mathcal{M}_j |\bm{y})=\frac{p(\bm{y} | \mathcal{M}_j)\pi(\mathcal{M}_j)}{\sum_{m=1}^{2^K}p(\bm{y} | \mathcal{M}_m)\pi(\mathcal{M}_m)},
\end{equation*}
where $\pi(\mathcal{M}_j)$ is the prior model probability,\footnote{We attach equal prior probabilities to each model in our GUI. However, this choice gives more prior probability to the set of models of medium size (think about the $k$-th row of Pascal's triangle). An interesting alternative is to use the Beta-Binomial prior proposed by \cite{ley2009effect}.} 
\begin{equation*}
	p(\bm{y} | \mathcal{M}_j)=\int_{\bm{\Theta}_j} p(\bm{y}| \bm{\theta}_j,\mathcal{M}_j)\pi(\bm{\theta}_j | \mathcal{M}_j) d\bm{\theta}_{j}
\end{equation*}
is the marginal likelihood, and $\pi(\bm{\theta}_j | \mathcal{M}_j)$ is the prior distribution of $\bm{\theta}_j$ conditional on model $\mathcal{M}_j$.

Following \cite{Raftery93}, the posterior distribution of $\bm{\theta}$ is 
\begin{equation*}
	\pi(\bm{\theta}|\bm{y})= \sum_{m=1}^{2^K}\pi(\bm{\theta}_m|\bm{y},\mathcal{M}_m) \pi(\mathcal{M}_m|\bm{y})
\end{equation*}
where $\pi(\bm{\theta}_m|\bm{y},\mathcal{M}_m)$ is the posterior distribution of $\bm{\theta}$ under model $\mathcal{M}_m$, $\mathbb{E}[\bm{\theta}|\bm{y}]=\sum_{m=1}^{2^K}\hat{\bm{\theta}}_m \pi(\mathcal{M}_m|\bm{y})$, $Var({\theta}_{km}|\bm{y})= \sum_{m=1}^{2^K}\pi(\mathcal{M}_m|\bm{y}) \widehat{Var} ({\theta}_{km}|\bm{y},\mathcal{M}_m)+\sum_{m=1}^{2^K} \pi(\mathcal{M}_m|\bm{y}) (\hat{{\theta}}_{km}-\mathbb{E}[{\theta}_{km}|\bm{y}])^2$, $\hat{\bm{\theta}}_m$ is the posterior mean and $\widehat{Var}({\theta}_{km}|\bm{y},\mathcal{M}_m)$ is the posterior variance of the element $k$-th of $\bm{\theta}$ under model $\mathcal{M}_m$.

The posterior variance highlights how the BMA method takes into account
model uncertainty. The first term is the weighted variance of each model, averaged over all potential models, and the second term indicates how stable the estimates are across models. The more the estimates differ between models, the greater is the posterior variance.

The posterior predictive distribution is
\begin{equation*}
	\pi(\bm{Y}_0|\bm{y})= \sum_{m=1}^{2^K}p_m(\bm{Y}_0|\bm{y},\mathcal{M}_m) \pi(M_m|\bm{y})
\end{equation*}

where $p_m(\bm{Y}_0|\bm{y},\mathcal{M}_m)=\int_{\bm{\Theta}_m} p(\bm{Y}_0|\bm{y},\bm{\theta}_m,\mathcal{M}_m)\pi(\bm{\theta}_m |\bm{y}, \mathcal{M}_m) d\bm{\theta}_{m}$ is the posterior predictive distribution under model $\mathcal{M}_m$. 

Another important statistic in BMA is the posterior inclusion probability associated with variable $\bm{x}_k$, $k=1,2,\dots,K$, which is

\begin{equation*}
	PIP(\bm{x}_k)=\sum_{m=1}^{2^K}\pi(\mathcal{M}_m|\bm{y})\times \mathbbm{1}_{k,m},
\end{equation*}
where
$\mathbbm{1}_{k,m}= \left\{ \begin{array}{lcc}
	1&   if  & \bm{x}_{k}\in \mathcal{M}_m \\
	\\ 0 &  if & \bm{x}_{k}\not \in \mathcal{M}_m
\end{array}
\right\}.$\\

\cite{Kass1995} suggest that posterior inclusion probabilities (PIP) less than 0.5 are evidence against the regressor, $0.5\leq PIP<0.75$ is weak evidence, $0.75\leq PIP<0.95$ is positive evidence, $0.95\leq PIP<0.99$ is strong evidence, and $PIP\geq 0.99$ is very strong evidence.

There are two main computational issues in implementing BMA based on variable selection. First, the number of models in the model space is $2^K$, which sometimes can be enormous. For instance, three regressors imply just eight models, see Table \ref{tab:chap10}, but 40 regressors implies approximately  1.1e+12 models. Take into account that models always include the intercept, and all regressors should be standardized to avoid scale issues.\footnote{Scaling variables is always an important step in variable selection.} The second computational issue is calculating the marginal likelihood $p(\bm{y} | \mathcal{M}_j)=\int_{\bm{\Theta}_j} p(\bm{y}| \bm{\theta}_j,\mathcal{M}_j)\pi(\bm{\theta}_j | \mathcal{M}_j) d\bm{\theta}_{j}$, which most of the time does not have an analytic solution. 

\begin{table}[!ht]
	\tabletitle{Space of models: Three regressors.}\label{tab:chap10}
%	\begin{threeparttable}
		\resizebox{1\textwidth}{!}{\begin{minipage}{\textwidth}
				\begin{tabular}{ccccccccc}
					 Regressor & \multicolumn{8}{c}{Inclusion}\\
					\hline
					$x_1$ & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\
					$x_2$ & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\
					$x_3$ & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\ 			
				\end{tabular}
				\begin{tablenotes}[para,flushleft]
					\footnotesize \textit{Notes}: ``1" indicates inclusion of the regressor, and ``0" indicates no inclusion. The space of models is composed by 8 models. The model always includes intercept.\\
				\end{tablenotes}
		\end{minipage}}
%	\end{threeparttable}
\end{table}
The first computational issue is basically a problem of ranking models. This can be tackled using different approaches, such as Occam's window criterion \cite{Madigan1994,Raftery1997}, reversible jump Markov chain Monte Carlo computation \cite{Green1995}, Markov chain Monte Carlo model composition \cite{madigan95}, and multiple testing using intrinsic priors \cite{Casella2006} or nonlocal prior densities \cite{Johnson2012}. We focus on Occam's window and Markov chain Monte Carlo model composition in our GUI.\footnote{Variable selection (model selection or regularization) is a topic related to model uncertainty. Approaches such as stochastic search variable selection (spike and slab) \cite{George1993,George1997} and Bayesian Lasso \cite{Park2008} are good examples of how to tackle this issue. See Chapter \ref{chap13}.}

In Occam's window, a model is discarded if its predictive performance is much worse than that of the best model  \cite{Madigan1994,Raftery1997}.
Thus, models not belonging to $\mathcal{M}'=\left\{\mathcal{M}_j:\frac{\max_m {\pi(\mathcal{M}_m|\bm{y})}}{\pi(\mathcal{M}_j|\bm{y})}\leq c\right\}$ should be discarded, where $c$ is chosen by the user (\cite{Madigan1994} propose $c=20$).
In addition, complicated models than are less supported by the data than simpler models are also discarded, that is, $\mathcal{M}''=\left\{\mathcal{M}_j:\exists \mathcal{M}_m\in\mathcal{M}',\mathcal{M}_m\subset \mathcal{M}_j,\frac{\pi(\mathcal{M}_m|\bm{y})}{\pi(\mathcal{M}_j|\bm{y})}>1\right\}$. Then, the set of models used in BMA is $\mathcal{M}^*=\mathcal{M}'\cap \mathcal{M}''^c\in\mathcal{M}$. \cite{Raftery1997} find that the number of models in $\mathcal{M}^*$ is normally less than 25.

However, the previous theoretical framework requires finding the model with the maximum a posteriori model probability ($\max_m {\pi(\mathcal{M}_m|\bm{y})}$), which implies calculating all possible models in $\mathcal{M}$. This is computationally burdensome. Hence, a heuristic approach is proposed by \cite{Raftery2012} based on ideas of \cite{Madigan1994}. The search strategy is based on a series of nested comparisons of ratios of posterior model probabilities. Let $\mathcal{M}_0$ be a model with one regressor less than model $\mathcal{M}_1$, then:
\begin{itemize}
	\item If $\log(\pi(\mathcal{M}_0|\bm{y})/\pi(\mathcal{M}_1|\bm{y}))>\log(O_R)$, then $\mathcal{M}_1$ is rejected and $\mathcal{M}_0$ is considered.
	\item If $\log(\pi(\mathcal{M}_0|\bm{y})/\pi(\mathcal{M}_1|\bm{y}))\leq -\log(O_L)$, then $\mathcal{M}_0$ is rejected, and $\mathcal{M}_1$ is considered.
	 \item If $\log(O_L)<\log(\pi(\mathcal{M}_0|\bm{y})/\pi(\mathcal{M}_1|\bm{y}))\leq \log(O_R$), $\mathcal{M}_0$ and $\mathcal{M}_1$ are considered.
\end{itemize} 
Here $O_R$ is a number specifying the maximum ratio for excluding models in Occam's window, and $O_L=1/O_R^{2}$ is defined by default in \cite{Raftery2012}. The search strategy can be ``up,'' adding one regressor, or ``down,'' dropping one regressor (see \cite{Madigan1994} for details about the down and up algorithms). The leaps and bounds algorithm \cite{Furnival1974} is implemented to improve the computational efficiency of this search strategy \cite{Raftery2012}. Once the set of potentially acceptable models is defined, we discard all the models that are not in $\mathcal{M}'$, and the models that are in $\mathcal{M}''$ where 1 is replaced by $\exp\left\{O_R\right\}$ due to the leaps and bounds algorithm giving an approximation to BIC, so as to ensure that no good models are discarded.

The second approach that we consider in our GUI to tackle the model space size issue is Markov chain Monte Carlo model composition (MC3) \cite{madigan1995bayesian1}.
In particular, given the space of models $\mathcal{M}_m$, we simulate a chain of $\mathcal{M}_s$ models, $s = 1, 2, ..., S<<2^K$, where the algorithm randomly extracts a candidate model $\mathcal{M}_c$ from a neighborhood of models ($nbd(\mathcal{M}_m)$) that consists of the actual model itself and the set of models with either one variable more or one variable less \cite{Raftery1997}. Therefore, there is a transition kernel in the space of models $q(\mathcal{M}_m\rightarrow \mathcal{M}_c)$, such that $q(\mathcal{M}_m\rightarrow \mathcal{M}_{c})=0 \ \forall \mathcal{M}_{c}\notin nbd(\mathcal{M}_m)$ and $q(\mathcal{M}_m\rightarrow \mathcal{M}_{c})=\frac{1}{|nbd(\mathcal{M}_m)|} \ \forall \mathcal{M}_m\in nbd(\mathcal{M}_m)$, $|nbd(\mathcal{M}_m)|$ being the number of neighbors of $\mathcal{M}_m$. This candidate model is accepted with probability

\begin{equation*}
	\alpha (\mathcal{M}_{s-1},\mathcal{M}_{c})=\min \bigg \{ \frac{|nbd(\mathcal{M}_m)|p(\bm{y} | \mathcal{M}_c)\pi(\mathcal{M}_c)}{|nbd(\mathcal{M}^{c})|p(\bm{y}| \mathcal{M}_{(s-1)})\pi(\mathcal{M}_{(s-1)})},1 \bigg \}.
\end{equation*}

Observe that by construction $|nbd(\mathcal{M}_m)|=|nbd(\mathcal{M}_c)|=k$, except in extreme cases where a model has only one regressor or has all regressors.

The Bayesian information criterion is a possible solution for the second computational issue in BMA, that is, calculating the marginal likelihood when there is no an analytic solution. Defining $h(\bm{\theta}|\mathcal{M}_j)=-\frac{\log(p(\bm{y}| \bm{\theta}_j,\mathcal{M}_j)\pi(\bm{\theta}_j | \mathcal{M}_j))}{N}$, then $p(\bm{y} | \mathcal{M}_j)=\int_{\bm{\Theta}_j} \exp\left\{-N h(\bm{\theta}|\mathcal{M}_j)\right\}  d\bm{\theta}_{j}$. If $N$ is sufficiently large (technically $N\to \infty$), we can make the following assumptions \cite{Hoeting1999}:

\begin{itemize}
	\item We can use the Laplace method for approximating integrals \cite{Tierney1986}.
	\item The posterior mode is reached at the same point as the maximum likelihood estimator (MLE), denoted by $\hat{\bm{\theta}}_{MLE}$.
\end{itemize}

We get the following results under these assumptions:
\begin{align*}
	p(\bm{y} | \mathcal{M}_j)\approx&\left( \frac{2\pi}{N}\right)^{K_j/2}|\bm{\Sigma}|^{-1/2} \exp\left\{-N h(\bm{\hat{\theta}}_j^{MLE}|\mathcal{M}_j)\right\}, \ N\rightarrow\infty,
\end{align*}
where $\bm{\Sigma}$ is the Hessian matrix of $h(\bm{\hat{\theta}}_j^{MLE}|\mathcal{M}_j)$, and $K_j=dim\left\{\bm{\theta}_j\right\}$.

This implies
\begin{align*}
	\log\left(p(\bm{y} | \mathcal{M}_j)\right)\approx& \frac{K_j}{2}\log(2\pi)- \frac{K_j}{2}\log(N) -\frac{1}{2}\log(|\bm{\Sigma}|) + \log(p(\bm{y}| \bm{\hat{\theta}}_j^{MLE},\mathcal{M}_j))\\
	&+\log(\pi(\bm{\hat{\theta}}_j^{MLE} | \mathcal{M}_j)), \ N\rightarrow\infty.
\end{align*}

Since $\frac{K_j}{2}\log(2\pi)$ and $\log(\pi(\bm{\hat{\theta}}_j^{MLE} | \mathcal{M}_j))$ are constants as functions of $\bm{y}$, and $|\bm{\Sigma}|$ is bounded by a finite constant, we have
\begin{align*}
	log\left(p(\bm{y} | \mathcal{M}_j)\right)\approx& -\frac{K_j}{2}\log(N)+\log(p(\bm{y}| \bm{\hat{\theta}}_j^{MLE},\mathcal{M}_j))= -\frac{BIC}{2}, \ N \rightarrow \infty.
\end{align*}

The marginal likelihood thus asymptotically converges to a linear transformation of the Bayesian Information Criterion (BIC), significantly simplifying its calculation.

\section{The Gaussian linear model}\label{sec10_2}

The Gaussian linear model specifies $\bf{y}=\alpha\bm{i}_N+\bm{X}_m\bm{\beta}_m+\bm{\mu}_m$ such that $\bm{\mu}_m\sim{N}(\bm{0},\sigma^2\bm{I}_n)$, and $\bm{X}_m$ does not have the column of ones. Following \cite{koop2003bayesian}, the conjugate prior for the location parameters is $\bm{\beta}_m|\sigma^2 \sim {N}(\bm{\beta}_{m0}, \sigma^2 \bm{B}_{m0})$, and the priors for $\sigma^2$ and $\alpha$ can be improper, as these parameters are common to all models $\mathcal{M}_m$. Particularly, $\pi(\sigma^2)\propto 1/\sqrt{\sigma^2}$ and $\pi(\alpha)\propto 1$.

The selection of the hyperparameters of $\bm{\beta}_m$ is more critical, as these parameters are not common to all models. A very common prior for the location parameters in the BMA literature is the Zellner's prior \cite{zellner1986assessing}, where $\bm{\beta}_{m0}=\bm{0}_m$ and $\bm{B}_{m0}=(g_m\bm{X}_m^{\top}\bm{X}_m)^{-1}$. Observe that this covariance matrix is similar to the covariance matrix of ordinary least squares estimator of the location parameters. This suggests that there is compatibility between the prior information and the sample information, and the only parameter to elicit is $g_m\geq 0$, which facilitates the elicitation process, as eliciting covariance matrices is a very hard endeavor.

Following same steps as in Section \ref{sec43}, the posterior conditional distribution of $\bm{\beta}_m$ has covariance matrix $\sigma^2\bm{B}_{mn}$, where $\bm{B}_{mn}=((1+g_m)\bm{X}_m^{\top}\bm{X}_m)^{-1}$ (Exercise 1), which means that $g_m=0$ implies a non-informative prior, whereas $g_m=1$ implies that prior and data information have same weights. We follow \cite{fernandez2001benchmark}, who recommend
\begin{align*}
	g_m & =
	\begin{Bmatrix}
		1/K^2, & N \leq K^2\\
		1/N, & N>K^2 
	\end{Bmatrix}.
\end{align*}  
 
Given the likelihood function, 
\begin{equation*}
	p(\bm{\beta}_m, \sigma^2|\bm{y}, \bm{X}_m) = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\bm{y} - \alpha\bm{i}_N - \bm{X}_m\bm{\beta}_m)^{\top}(\bm{y} - \alpha\bm{i}_N - \bm{X}_m\bm{\beta}_m) \right\},
\end{equation*}
the marginal likelihood associated with model $\mathcal{M}_m$ is proportional to (Exercise 1) 
\begin{align*}
	p(\bm{y}|\mathcal{M}_m)&\propto \left(\frac{g_m}{1+g_m}\right)^{k_m/2}\\
	&\times \left[\frac{1}{1+g_m}(\bm{y}-\bm{X}_m\hat{\bm{\beta}}_m)^{\top}(\bm{y}-\bm{X}_m\hat{\bm{\beta}}_m)+\frac{g_m}{1+g_m}(\bm{y}-\bar{y}\bm{i}_N)^{\top}(\bm{y}-\bar{y}\bm{i}_N)\right]^{-(N-1)/2},
\end{align*}
where all parameter are indexed to model $\mathcal{M}_m$, $\hat{\bm{\beta}}_m$ is the maximum likelihood estimator, and $\bar{y}$ is the sample mean of $\bm{y}$.

We implement in our GUI three approaches to perform BMA in the Gaussian linear model: the BIC approximation using the Occam's window approach, the MC3 algorithm using the analytical expression for calculating the marginal likelihood, and an instrumental variable approach based on conditional likelihoods.\\

%\subsection{The BIC using the Occam's window}\label{sec10_21}

%We can show that the Bayesian information criterion is $N\log(\sum_{i=1}^N(y_i-\bm{x}_{mi}^{\top}\hat{\bm{\beta}}_m)^2/N)+K_m\log(N)$ in the particular case of the Gaussian linear model (see Exercise 1). Thus, we use this approximation with the Occam's window approach to decrease the computational burden of Bayesian model averaging.\\

\textbf{Example: Simulation exercises}

Let's perform some simulation exercises to assess the performance of the BIC approximation using the Occam's window, and the Markov chain Monte Carlo model composition approaches. Let's begin with a model where the computational burden is low and we know the data generating process. In particular, we set 10 regressors such that $x_k\sim N(1, 1)$, $k =1,\dots,6$, and $x_k\sim B(0.5)$, $k=7,\dots,10$. We set $\bm{\beta}=[1 \ 0 \ 0 \ 0 \ 0.5 \ 0, 0, 0, 0, -0.7]^{\top}$ such that just $x_1$, $x_5$ and $x_{10}$ are relevant to drive $y_i=1+\bm{x}^{\top}\bm{\beta}+\mu_i$, $\mu_i\sim N(0,0.5^2)$. Observe that we just have $2^{10}=1024$ models in this setting, thus, we can calculate the posterior model probability for each model. 

Our GUI uses the commands \textit{bic.glm} and \textit{MC3.REG} from the package \textit{BMA} to perform Bayesian model average in the linear regression model using the BIC approximation and MC3, respectively. These commands in turn are based on \cite{Raftery1995} and \cite{Raftery1997}. The following code shows how to perform the simulation and get the posterior mean and standard deviation using these commands with the default values of hyperparameters and tuning parameters.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Bayesian model average, small setting}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
N <- 1000
K1 <- 6; K2 <- 4; K <- K1 + K2
X1 <- matrix(rnorm(N*K1,1 ,1), N, K1)
X2 <- matrix(rbinom(N*K2, 1, 0.5), N, K2)
X <- cbind(X1, X2); e <- rnorm(N, 0, 0.5)
B <- c(1,0,0,0,0.5,0,0,0,0,-0.7)
y <- 1 + X%*%B + e
df <- as.data.frame(cbind(y, X)) 
colnames(df) <- c("y", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "x10")
#### BIC approximation
BMAglm <- BMA::bic.glm(y ~ X, data = df, glm.family = gaussian()) 
summary(BMAglm)
           p!=0    EV         SD        model 1     model 2   
Intercept  100     1.03306  0.0305   1.032e+00   1.047e+00
X1.x       100.0   1.00178  0.0153   1.002e+00   1.002e+00
X2.x         0.0   0.00000  0.0000       .           .    
X3.x         0.0   0.00000  0.0000       .           .    
X4.x         0.0   0.00000  0.0000       .           .    
X5.x       100.0   0.49762  0.0157   4.976e-01   4.975e-01
X6.x         4.8  -0.00072  0.0047       .      -1.509e-02
X7.x         0.0   0.00000  0.0000       .           .    
X8.x         0.0   0.00000  0.0000       .           .    
X9.x         0.0   0.00000  0.0000       .           .    
X10.x      100.0  -0.70355  0.0310  -7.036e-01  -7.036e-01

nVar                                       3           4      
BIC                                     -5.8e+03  -5.8e+03
post prob                                0.952       0.048
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

We can see from the results that the BIC approximation with the Occam's window, and the MC3 algorithm perform a good job finding the relevant regressors, and their posterior BMA means are very close to the population values. We also see that the BMA results are very similar in the two approaches.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Bayesian model average, small setting}
	\begin{VF}
		\begin{lstlisting}[language=R]
#### Markov chain Monte Carlo model composition
BMAreg <- BMA::MC3.REG(y, X, num.its=500)
Models <- unique(BMAreg[["variables"]])
nModels <- dim(Models)[1]
nVistModels <- dim(BMAreg[["variables"]])[1]
PMP <- NULL
for(m in 1:nModels){
	idModm <- NULL
	for(j in 1:nVistModels){
		if(sum(Models[m,] == BMAreg[["variables"]][j,]) == K){
			idModm <- c(idModm, j)
		}else{
			idModm <- idModm
		} 
	}
	PMPm <- sum(BMAreg[["post.prob"]][idModm])
	PMP <- c(PMP, PMPm)
}
PIP <- NULL
for(k in 1:K){
	PIPk <- sum(PMP[which(Models[,k] == 1)])
	PIP <- c(PIP, PIPk)
}
plot(PIP)
Means <- matrix(0, nModels, K)
Vars <- matrix(0, nModels, K)
for(m in 1:nModels){
	idXs <- which(Models[m,] == 1)
	if(length(idXs) == 0){
		Regm <- lm(y ~ 1)
	}else{
		Xm <- X[, idXs]
		Regm <- lm(y ~ Xm)
		SumRegm <- summary(Regm)
		Means[m, idXs] <- SumRegm[["coefficients"]][-1,1]
		Vars[m, idXs] <- SumRegm[["coefficients"]][-1,2]^2 
	} 
}
BMAmeans <- colSums(Means*PMP)
BMAsd <- (colSums(PMP*Vars)  + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5
BMAmeans
[1]  1.001771e+00 -5.322016e-05  6.635422e-06  3.721457e-07  4.976335e-01
[6] -1.271339e-04  1.000932e-08  2.107441e-05  6.578654e-06 -7.035557e-01 
BMAsd
[1] 1.527261e-02 1.353624e-03 5.936816e-04 1.163947e-04 1.566698e-02 1.987360e-03
[7] 2.778896e-05 1.270579e-03 6.997305e-04 3.093389e-02
BMAmeans/BMAsd
[1]  6.559266e+01 -3.931680e-02  1.117673e-02  3.197272e-03  3.176320e+01
[6] -6.397124e-02  3.601905e-04  1.658647e-02  9.401697e-03 -2.274385e+01
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

We can perform Bayesian model averaging in our GUI using Algorithm \ref{alg:BMAnormal}.  

\begin{algorithm}[h!]
	\caption{Bayesian model average in linear Gaussian models}\label{alg:BMAnormal}
	\begin{algorithmic}[1]  		 			
		\State Select \textit{Hierarchical Longitudinal Model} on the top panel
		\State Select \textit{Poisson} model using the left radio button
		\State Upload the dataset selecting first if there is header in the file, and the kind of separator in the \textit{csv} file of the dataset (comma, semicolon, or tab). Then, use the \textit{Browse} button under the \textbf{Choose File} legend
		\State Select MCMC iterations, burn-in and thinning parameters using the \textit{Range sliders}
		\State Click the \textit{Go!} button
		\State Analyze results
		\State Download posterior chains and diagnostic plots using the \textit{Download Posterior Chains} and \textit{Download Posterior Graphs} buttons
	\end{algorithmic} 
\end{algorithm}


We show in the following code how to program a MC3 algorithm from scratch to perform BMA using the setting from Section \ref{sec10_2}. The first part of the code is the function to calculate the log marginal likelihood. This is a small simulation setting, thus we can calculate the marginal likelihood for all 1024 models, and then calculate the posterior model probability standardizing using the model with the largest log marginal likelihood. We see from the results that this model is the data generating process. We also find that the posterior inclusion probabilities for $x_{1}$, $x_{5}$ and $x_{10}$ are 1, whereas the PIP for the other variables are less than 0.05. Although BMA allows incorporating model uncertainty in a regression framework, sometimes it is desirable to select just one model. Two compelling alternatives are the model with the largest posterior model probability, and the median probability model. The latter is the model which includes every predictor that has posterior inclusion probability higher than 0.5. The first model is the best alternative for prediction in the case of a 0--1 loss function \cite{Clyde2004}, whereas the second is the best alternative when there is a quadratic loss function in prediction \cite{Barbieri2004}. In this simulation, the two criteria indicate selection of the data generating process.

We also show how to estimate the posterior mean and standard deviation based on BMA. We see that the posterior means are very close to the population parameters.  

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Bayesian model average, small setting from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
LogMLfunt <- function(Model){
	indr <- Model == 1
	kr <- sum(indr)
	if(kr > 0){
		gr <- ifelse(N > kr^2, 1/N, kr^(-2))
		Xr <- matrix(Xnew[ , indr], ncol = kr)
		PX <- Xr%*%solve(t(Xr)%*%Xr)%*%t(Xr)
		s2pos <- c((t(y - mean(y))%*%(y - mean(y))) - t(y)%*%PX%*%y/(1 + gr))
		mllMod <- (kr/2)*log(gr/(1+gr))-(N-1)/2*log(s2pos)
	}else{
		gr <- ifelse(N > kr^2, 1/N, kr^(-2))
		s2pos <- c((t(y - mean(y))%*%(y - mean(y))))
		mllMod <- (kr/2)*log(gr/(1+gr))-(N-1)/2*log(s2pos)
	}
	return(mllMod)
}
combs <- expand.grid(c(0,1), c(0,1), c(0,1), c(0,1), c(0,1),c(0,1), c(0,1), c(0,1), c(0,1), c(0,1))
Xnew <- apply(X, 2, scale)
mll <- sapply(1:2^K, function(s){LogMLfunt(matrix(combs[s,], 1, K))})
MaxPMP <- which.max(mll); StMarLik <- exp(mll-max(mll))
PMP <- StMarLik/sum(StMarLik)
PMP[MaxPMP]
combs[MaxPMP,]
    Var1 Var2 Var3 Var4 Var5 Var6 Var7 Var8 Var9 Var10
530    1    0    0    0    1    0    0    0    0     1
PIP <- NULL
for(k in 1:K){
	PIPk <- sum(PMP[which(combs[,k] == 1)]); PIP <- c(PIP, PIPk)
}
PIP
[1] 1.00000000 0.03617574 0.03208369 0.03516743 1.00000000 0.04795509 0.03457102 0.03468819 0.03510209 1.00000000
nModels <- dim(combs)[1]; Means <- matrix(0, nModels, K)
Vars <- matrix(0, nModels, K)
for(m in 1:nModels){
	idXs <- which(combs[m,] == 1)
	if(length(idXs) == 0){
		Regm <- lm(y ~ 1)
	}else{
		Xm <- X[, idXs]; Regm <- lm(y ~ Xm)
		SumRegm <- summary(Regm)
		Means[m, idXs] <- SumRegm[["coefficients"]][-1,1]
		Vars[m, idXs] <- SumRegm[["coefficients"]][-1,2]^2 
	}
}
BMAmeans <- colSums(Means*PMP)
1.0018105888 -0.0003196423  0.0001489711  0.0002853524  0.4976225353 -0.0007229563  0.0005342718  0.0005441905  0.0005758708 -0.7035206822
BMAsd <- (colSums(PMP*Vars)  + colSums(PMP*(Means-matrix(rep(BMAmeans, each = nModels), nModels, K))^2))^0.5
0.015274980 0.003304115 0.002814491 0.003214722 0.015668278 0.004694003 0.006400541 0.006435695 0.006528471 0.030940753 
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

The following part of the code shows how to perform the MC3 algorithm. This algorithm is not necessary in this case due to being a small dimensional problem, but it helps as a pedagogical exercise. The point of departure is to set $S=100$ random models, and order their log marginal likelihoods. Thus, the logic of the algorithm is to pick the worse model among the $S$ models, and propose a candidate model to compete against it. We repeat this MC3 iterations (1000 in the code). Observe that 1000 iterations is less than the number of potential models (1024). This is the idea of the MC3 algorithm, that is, performing less iterations than the number of elements of the space of models. 

In our algorithm, we analyze all model scenarios using different conditionals and reasonably assume the same prior model probability for all models and the same cardinality for both the actual and candidate models. We can calculate the posterior model probability (PMP) in different ways. One way is to recover the unique models from the final set of $S$ models, calculate the log marginal likelihood for these models, and standardize using the best model among them. Another way is to calculate the PMP using the complete set of $S$ final models, accounting for the fact that the same model can appear multiple times in this set, thus requiring us to sum the PMPs of repeated models. An additional way is to calculate the PMP using the relative frequency with which a model appears in the final set of $S$ models. These three methods can yield different PMP, particularly when the number of MC3 iterations is small. In our setting using 1000 MC3 iterations, the data generating process got the largest PMP in the three ways to calculate the PMP. 

A remarkable point in this algorithm is that we can get just one model after substantially increasing the number of iterations (try this code using 10000 iterations). This can be a good feature if we require just one model. However, this neglects model uncertainty, which can be a desirable characteristic. We ask to program an algorithm where we end up with $S$ different models after finishing the MC3 iterations (Exercise 2).

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Bayesian model average, small setting from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
M <- 100
Models <- matrix(rbinom(K*M, 1, p = 0.5), ncol=K, nrow = M)
mllnew <- sapply(1:M,function(s){LogMLfunt(matrix(Models[s,], 1, K))})
oind <- order(mllnew, decreasing = TRUE)
mllnew <- mllnew[oind]; Models <- Models[oind, ]; iter <- 1000
pb <- winProgressBar(title = "progress bar", min = 0, max = iter, width = 300); s <- 1
while(s <= iter){
	ActModel <- Models[M,]; idK <- which(ActModel == 1)
	Kact <- length(idK)
	if(Kact < K & Kact > 1){
		CardMol <- K; opt <- sample(1:3, 1)
		if(opt == 1){ # Same
			CandModel <- ActModel
		}else{
			if(opt == 2){ # Add
				All <- 1:K; NewX <- sample(All[-idK], 1)
				CandModel <- ActModel; CandModel[NewX] <- 1
			}else{ # Subtract
				LessX <- sample(idK, 1); CandModel <- ActModel
				CandModel[LessX] <- 0
			}
		}
	}else{
		CardMol <- K + 1
		if(Kact == K){
			opt <- sample(1:2, 1)
			if(opt == 1){ # Same
				CandModel <- ActModel
			}else{ # Subtract
				LessX <- sample(1:K, 1); CandModel <- ActModel
				CandModel[LessX] <- 0
			}
		}else{
			if(K == 1){
				opt <- sample(1:3, 1)
				if(opt == 1){ # Same
					CandModel <- ActModel
				}else{
					if(opt == 2){ # Add
						All <- 1:K; NewX <- sample(All[-idK], 1)
						CandModel <- ActModel; CandModel[NewX] <- 1
					}else{ # Subtract
						LessX <- sample(idK, 1); CandModel <- ActModel
						CandModel[LessX] <- 0
					}
				}
			}else{ # Add
				NewX <- sample(1:K, 1); CandModel <- ActModel
				CandModel[NewX] <- 1
			}
		}
	}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Bayesian model average, small setting from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
	LogMLact <- LogMLfunt(matrix(ActModel, 1, K))
	LogMLcand <- LogMLfunt(matrix(CandModel, 1, K))
	alpha <- min(1, exp(LogMLcand-LogMLact))
	u <- runif(1)
	if(u <= alpha){
		mllnew[M] <- LogMLcand; Models[M, ] <- CandModel
		oind <- order(mllnew, decreasing = TRUE)
		mllnew <- mllnew[oind]; Models <- Models[oind, ]
	}else{
		mllnew <- mllnew; Models <- Models
	}
	s <- s + 1
	setWinProgressBar(pb, s, title=paste( round(s/iter*100, 0),"% done"))
}
close(pb)
ModelsUni <- unique(Models)
mllnewUni <- sapply(1:dim(ModelsUni)[1], function(s){LogMLfunt(matrix(ModelsUni[s,], 1, K))})
StMarLik <- exp(mllnewUni-mllnewUni[1])
PMP <- StMarLik/sum(StMarLik) # PMP based on unique selected models
plot(PMP)
# PMP using number of visits
nModels <- dim(ModelsUni)[1]
StMarLik <- exp(mllnew-mllnew[1])
PMPold <- StMarLik/sum(StMarLik) # PMP all selected models
PMPot <- NULL
PMPap <- NULL
FreqMod <- NULL
for(m in 1:nModels){
	idModm <- NULL
	for(j in 1:M){
		if(sum(ModelsUni[m,] == Models[j,]) == K){
			idModm <- c(idModm, j)
		}else{
			idModm <- idModm
		}
	}
	PMPm <- sum(PMPold[idModm]) # PMP unique models using sum of all selected models
	PMPot <- c(PMPot, PMPm)
	PMPapm <- length(idModm)/M # PMP using relative frequency in all selected models
	PMPap <- c(PMPap, PMPapm)
	FreqMod <- c(FreqMod, length(idModm))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 
 


\textbf{Determinants of export diversification I}


%\subsection{The Markov chain Monte Carlo model composition}\label{sec10_22}
%The second approach that we implement in our GUI is using the Markov chain Monte Carlo model composition (MC3) using the analytical expression of the marginal likelihood of the Gaussian linear model.\\

%\textbf{Example: Simulation exercise}
%Let's perform the same simulation exercise of the previous section to assess the performance of this approach. 

%\textbf{Determinants of export diversification II}

\subsection{Instrumental variable }\label{sec10_21}

We also implement the instrumental variable approach of Section \ref{sec73} to tackle potential endogeneity issues in BMA. We assume that $\bm{\gamma}\sim {N}(\bm{0},\bm{I})$, $\bm{\beta}\sim {N}(\bm{0},\bm{I})$, and $\bm{\Sigma}^{-1} \sim {W}(3,\bm{I})$ \cite{Karl2012}.

\cite{Lenkoski2013} propose an algorithm based on conditional Bayes factors \cite{Dickey1978} that allows embedding MC3 within a Gibbs sampling algorithm. Given the candidate ($M_{c}^{2nd}$) and actual ($M_{s-1}^{2nd}$) models for the iteration $s$ in the second stage, the conditional Bayes factor is 
\begin{equation*}
	CBF^{2nd}=\frac{p(\bm{y}|M_{c}^{2nd},\bm{\gamma},\bm{\Sigma})}{p(\bm{y}|M_{s-1}^{2nd},\bm{\gamma},\bm{\Sigma})},
\end{equation*}
where 
\begin{equation*}
	p(\bm{y}|M_{c}^{2nd},\bm{\gamma},\bm{\Sigma})=\int_{\mathcal{M}^{2nd}}p(\bm{y}|\bm{\beta},\bm{\gamma},\bm{\Sigma})\pi(\bm{\beta}|M_{c}^{2nd})d\bm{\beta}\propto |\bm{B}_n|^{1/2} \exp\left\{\frac{1}{2}{\bm{\beta}_n}^{\top}\bm{B}_n^{-1}\bm{\beta}_n\right\}
	.
\end{equation*}

In the first stage,
\begin{equation*}
	CBF^{1st}=\frac{p(\bm{y}|M_{c}^{1st},\bm{\beta},\bm{\Sigma})}{p(\bm{y}|M_{s-1}^{1st},\bm{\beta},\bm{\Sigma})},
\end{equation*}
where \begin{equation*}
	p(\bm{y}|M_{c}^{1st},\bm{\beta},\bm{\Sigma})=\int_{\mathcal{M}^{1st}}p(\bm{y}|\bm{\gamma},\bm{\beta},\bm{\Sigma})\pi(\bm{\gamma}|M_{c}^{1st})d\bm{\gamma}\propto |\bm{G}_n|^{1/2} \exp\left\{\frac{1}{2}{\bm{\gamma}_n}^{\top}\bm{G}_n^{-1}\bm{\gamma}_n\right\}.
\end{equation*}
In the case that $\beta_k=0$, the update is based on the seemingly unrelated regressions framework. These conditional Bayes factors assume $\pi(M^{1st},M^{2sd})\propto 1$. See \cite{Lenkoski2013} for more details of the instrumental variable BMA algorithm.\footnote{\cite{Koop12} and \cite{Lenkoski2014} propose other frameworks for BMA taking into account endogeneity.}\\

\textbf{Example: Simulation exercise}
Let's perform the simulation exercise of Subsections \ref{sec10_21} and \ref{sec10_21} but introducing endogeneity. 

\textbf{Determinants of export diversification III} 


The Gaussian linear model is an example of a generalized linear model.
A GLM is characterized by a distribution function that is in the exponential family, that is, $p_i(y_i|\theta_i,\phi)=h(y_i,\phi)Exp\left\{(\theta_iy_i-b(\theta_i))/a(\phi)\right\}$ (canonical representation), $y_i\stackrel{i.n.d.} {\thicksim}p_i$, $i=1,2,\dots,n$.
It also has a linear predictor $\theta_i=\bm{x}_i^{\top}\bm{\beta}$, and a link function $g$ such that $E(Y_i|x_i)\equiv \mu_i=b'(\theta_i)=g^{-1}(\bm{x}_i^{\top}\bm{\beta})$ ($g$ is monotonic and differentiable), and $V(Y_i)=b''(\theta_i)a(\phi)$ \cite{McCullagh1989}.
The identity function $\mu_i=\bm{x}_i^{\top}\bm{\beta}$ is the canonical link function in the case of the Gaussian model.\footnote{A canonical link functions is characterized by the existence of a sufficient statistic ($\bm{X}^{\top}\bm{y}$) equal in dimension to $\bm{\beta}$.} This statistical framework can help us to characterize:

\section{The logit model}\label{sec10_3}

The logit model is also a GLM, where the link function is $\bm{x}_i^{\top}\bm{\beta}=log\left(\frac{\mu_i}{1-\mu_i}\right)$.
We carry out BMA using the BIC approximation and the Occam's window approach in the logit model.

\section{The gamma model}\label{sec10_4}
The gamma model is also a GLM, where the link function is $\bm{x}_i^{\top}\bm{\beta}=\mu_i^{-1}$.
We carry out BMA using the BIC approximation and the Occam's window approach in the gamma model.

\section{The Poisson model}\label{sec10_5}
The logit model is also a GLM, where the link function is $\bm{x}_i^{\top}\bm{\beta}=\log(\mu_i)$.
We carry out BMA using the BIC approximation and the Occam's window approach in the Poisson model.


\section{Calculating the marginal likelihood}\label{sec10_6}

The BIC approximation to the marginal likelihood is an asymptotic approximation. This implies limitations. Thus, there are other strategies that can be used to calculate the marginal likelihood when there is no analytic solution.

\subsection{Savage-Dickey density ratio}\label{sec10_11}

\subsection{Gelfand-Dey method}\label{sec10_12}

\subsection{Chib's methods}\label{sec10_13}

\section{Summary}\label{sec10_7}

\section{Exercises}\label{sec10_8}

\begin{enumerate}
	\item The Gaussian linear model specifies $\bf{y}=\alpha\bm{i}_N+\bm{X}_m\bm{\beta}_m+\bm{\mu}_m$ such that $\bm{\mu}_m\sim{N}(\bm{0},\sigma^2\bm{I}_n)$, and $\bm{X}_m$ does not have the column of ones. Assuming that $\pi(\sigma^2)\propto 1/\sqrt{\sigma^2}$, $\pi(\alpha)\propto 1$, and $\bm{\beta}_m|\sigma^2 \sim {N}(\bm{0}_{k_m}, \sigma^2 (g_m\bm{X}_m^{\top}\bm{X}_m)^{-1})$.
	\begin{itemize}
		\item Show that the posterior conditional distribution of $\bm{\beta}_m$ is $N(\bm{\beta}_{mn},\sigma^2\bm{B}_{mn})$, where $\bm{\beta}_{mn}=\bm{B}_{mn}\bm{X}_m^{\top}(\bm{y}-\alpha\bm{i}_N)$ and $\bm{B}_{mn}=((1+g_m)\bm{X}_m^{\top}\bm{X}_m)^{-1}$.
		\item Show that the marginal the marginal likelihood associated with model $\mathcal{M}_m$ is proportional to
		\begin{align*}
			p(\bm{y}|\mathcal{M}_m)&\propto \left(\frac{g_m}{1+g_m}\right)^{k_m/2}\\
			&\times \left[\frac{1}{1+g_m}(\bm{y}-\bm{X}_m\hat{\bm{\beta}}_m)^{\top}(\bm{y}-\bm{X}_m\hat{\bm{\beta}}_m)+\frac{g_m}{1+g_m}(\bm{y}-\bar{y}\bm{i}_N)^{\top}(\bm{y}-\bar{y}\bm{i}_N)\right]^{-(N-1)/2},
		\end{align*}
		where all parameter are indexed to model $\mathcal{M}_m$, $\hat{\bm{\beta}}_m$ is the maximum likelihood estimator, and $\bar{y}$ is the sample mean of $\bm{y}$. Take into account that $\bm{i}_N^{\top}\bm{X}_m=\bm{0}_{k_m}$ due to all columns being centered with respect to their means.   
	\end{itemize}

\item \textbf{Simulation exercise of the Markov chain Monte Carlo model composition continues}

Program an algorithm to perform MC3 where the final $S$ models are unique. Use the simulation setting of Section \ref{sec10_2} increasing the number of regressors to 40, this implies approximately 1.1e+12 models.    
	
\end{enumerate}