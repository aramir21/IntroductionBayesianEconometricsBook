\chapter*{Foreword}

The guiding principle of Bayesian analysis is elegant in its simplicity: the goal of a Bayesian analyst is to describe the distribution of an unknown state of nature conditional on observed data using the laws of conditional probability and a subjective assessment about the marginal distribution of the unknown state of interest. Almost from its inception, however, Bayesianism has faced many detractors; some reasons for this were political, others practical, but many were due to the particulars of the scientific minds whose ideas held sway at the time. Nonetheless, and much to the chagrin of its many detractors, the idea of Bayesian inference simply refused to die. It clung to life, often by a thread, for nearly two hundred years before it would eventually rise to prominence and be viewed, in modern terms, on an equal footing with the ``frequentist" notions of probability, which are often taught in early higher-education not for their practicality or effectiveness but because of their simplicity and broad applicability.

It is my firm belief that the subjective assessment of nature which sits at the core of Bayesianism is precisely what allowed the idea to persist within the larger scientific zeitgeist in spite of the fact that it was largely impractical as a method of scientific enquiry until the late 1990s: Bayesianism, as a philosophy, persisted not because of its novelty, and certainly not because of its practicality, but because it is an intrinsically human way of expressing uncertainty. Each human being experiences the world differently, and thousands of times a day we use these differential experiences to subjectively  assess the probability of events and outcomes within our lives. This subjectivity, and its intrinsic link to human experience and decision making, kept Bayesianism afloat within the larger scientific culture even though many prominent critics repeatedly resigned the idea of Bayesian inference to the historical waste bin. 

The key issue that hindered the adoption of Bayesian analysis, and which largely relegated it to the realm of theoretical analysis and select applications based on simple models, is the stubborn fact that Thomas Bayes first encountered in the very development of Bayesian probability, i.e., the law of inverse conditional probability: the distribution for the state of nature given the observed data is most often not expressible analytically, and can only be represented as a non-normalized probability distribution, except for very specific subjective marginal beliefs. This key sticking point in the Bayesian view of probability consigned Bayesianism to the fringes of scientific enquiry for over two hundred years. 

The view of Bayesian inference as a fringe statistical doctrine, however, changed dramatically at the turn of the 21st century, witnessing an exponential rise in the interest in and application of Bayesian methods to novel problems and settings previously inaccessible to practitioners of Bayesian methods. The dual advances that unlocked the potential for Bayesian inference were: i) the proliferation of personal computational resources; ii) historic advances in Markov chain Monte Carlo methods. Together, these advances ensured, for the first time, that practitioners could access draws from the cornerstone of Bayesian inference – the posterior distribution – in general settings and with relative ease. 

These dual advances, in computational and the application of Bayesian methods, have therefore inextricably linked the two ideas in the minds of most academics and practitioners, so much so that advances in sampling techniques are now viewed as a sub-field of Bayesian inference, under the guise of ``Bayesian Computation”. This is to say, modern Bayesian inference cannot be understood without also understanding how one draws samples from the key target in Bayesian inference, the posterior distribution. 

The rise in popularity of Bayesian methods has led several authors to write useful and practically helpful texts that introduce the keen reader to the Bayesian philosophy of statistical inference. While all such texts have their areas of excellence, the current text is one of the best at making the crucial link between Bayesian methods and the algorithms necessary to implement them both transparent and insightful. This has been achieved using a novel route: an intuitive graphical user interface that accompanies this book helps those interested in applying Bayesian methods understand the computational challenges required to implement Bayesian methods, while not requiring that they invest significant time and resources in coding such algorithms by hand. With knowledge of a few simple commands in R, the readers of the book can learn how to model data from a Bayesian standpoint, while not having to simultaneously labour over the coding requirements necessary to implement such models.  I see this book as a great starting point for econometricians and statisticians who are curious about Bayesian methods, and their application, but who do not wish to devote the hours necessary to become familiar with the inner-workings of the algorithms necessary to deploy them to their problems of interest. 

Those curious about Bayes who find their way to this book will gain a foundational understanding of Bayesian inference and keen insight into its practical applicability across a range of problems. Armed with the accessible computational tools developed in this text, practitioners will discover the modeling flexibility that Bayesian methods afford while developing the confidence to deploy these techniques in commonly encountered situations. This book represents an important step forward in making Bayesian methods accessible to the broader community of econometricians and applied statisticians who stand to benefit most from them.

\begin{flushright}
	David Frazier, Professor of Econometrics\\
	Monash University, Melbourne, Australia
\end{flushright}




