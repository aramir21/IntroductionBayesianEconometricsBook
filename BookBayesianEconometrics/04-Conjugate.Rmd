---
output:
  pdf_document: default
  html_document: default
---
# Basic statistical models: Conjugate families {#conjfam}

We will introduce conjugate families in basic statistical models with examples, solving them analytically and computationally using R. We will have some mathematical, and computational exercises in R.

## Motivation of conjugate families {#sec41}

Observing three fundamental pieces of Bayesian analysis: the posterior distribution (parameter inference), the marginal likelihood (hypothesis testing), and the predictive distribution (prediction), equations \@ref(eq:411), \@ref(eq:412) and \@ref(eq:413), respectively, 

\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&=\frac{p(\mathbf{y}|\mathbf{\theta}) \times \pi(\mathbf{\theta})}{p(\mathbf{y})},
  (\#eq:411)
\end{align}

\begin{equation}
p(\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{y}|\mathbf{\theta})\pi(\mathbf{\theta})d\mathbf{\theta},
(\#eq:412)
\end{equation}

and 

\begin{equation}
p(\mathbf{Y}_0|\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{Y}_0|\mathbf{\theta})\pi(\mathbf{\theta}|\mathbf{y})d\mathbf{\theta},
(\#eq:413)
\end{equation}

we can understand that some of the initial limitations of the application of the Bayesian analysis were associated with the ausence of algorithms to draw from non-standard posterior distributions (equation \@ref(eq:411)), and the lack of analytical solutions of the marginal likelihood (equation \@ref(eq:412)) and the predictive distribution (equation \@ref(eq:413)). Both issues requiring computational power.

Although there were algorithms to sample from non-standard posterior distributions since the second half of the last century [@metropolis53][@hastings70],[@Geman1984], their particular application in the Bayesian framework emerged later [@Gelfand1990],[@tierney1994markov], maybe until increasing computational power of desktop computers. However, it is also common practice nowadays to use models that have standard conditional posterior distributions to mitigate computational requirements. In addition, nice mathematical tricks plus computational algorithms [@gelfand1994bayesian], [@chib1995marginal],[@chib2001marginal] and approximations [@Tierney1986][@Jordan1999] are used to obtain the marginal likelihood (prior predictive).

Despite these advances, there are two potentially conflicting desirable model specification features that we can see from equations \@ref(eq:411), \@ref(eq:412) and \@ref(eq:413): analytical solutions and the posterior distribution in the same family as the prior distribution for a given likelihood. The latter is called *conjugate priors*, a family of priors that is closed under sampling [@schlaifer1961applied, p. 43-57].

These features are desirable as the former implies facility to perform hypothesis testing and predictive analysis, and the latter means invariance of the prior-to-posterior updating. Both feautures imply less computational burden.

We can easily achieve each of these features independenly, for instance using improper priors for analytical tractability, and defining in a broad sense the family of prior distributions for prior conjugacy. However, these are in conflict. 


Fortunately, we can achieve these two nice features if we assume that the data generating process is given by a distribution function in the *exponential family*. That is, given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$, a probability density function $p(\mathbf{y}|\mathbf{\theta})$ belongs to the exponential family if it has the form

\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&=\prod_{i=1}^N h(y_i) C(\mathbf{\theta}) \exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(y_i)\right\}\\ 
  &=h(\mathbf{y}) C(\mathbf{\theta})^N\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\} (\#eq:414)\\
  &=h(\mathbf{y})\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})-A(\mathbf{\theta})\right\},
\end{align}

where $h(\mathbf{y})=\prod_{i=1}^N h(y_i)$ is a non-negative function, $\eta(\mathbf{\theta})$ is known function of the parameters, $A(\mathbf{\theta})=\log\left\{\int_{\mathbf{Y}}h(\mathbf{y})\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\}d\mathbf{y}\right\}=-N\log(C(\mathbf{\theta}))$ is a normalization factor, and $\mathbf{T}(\mathbf{y})=\sum_{i=1}^N\mathbf{T}(y_i)$ is the vector of sufficient statistics of the distribution (by the factorization theorem).  

If the support of $\mathbf{y}$ is independent of $\mathbf{\theta}$, then the family is said to be *regular*, and otherwise it is *irregular*. In addition, if we set $\eta=\eta(\mathbf{\theta})$, then the exponential family is said to be in the *canonical form* 

\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&=h(\mathbf{y})D(\mathbf{\eta})^N\exp\left\{\eta^{\top}\mathbf{T}(\mathbf{y})\right\}\\
  &=h(\mathbf{y})\exp\left\{\eta^{\top}\mathbf{T}(\mathbf{y})-B(\mathbf{\eta})\right\}.
\end{align}

A nice feature of this representation is that $\mathbb{E}[\mathbf{T}(\mathbf{y})|\mathbf{\eta}]=\nabla B(\mathbf{\eta})$ and $Var[\mathbf{T}(\mathbf{y})|\mathbf{\eta}]=\nabla^2 B(\mathbf{\eta})$. 

**Examples of exponential family distributions**

1. Discrete distributions

* Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **Poisson distribution** show that $p(\mathbf{y}|\lambda)$ is in the exponential family.

\begin{align}
  p(\mathbf{y}|\lambda)&=\prod_{i=1}^N \frac{\lambda^{y_i} \exp(-\lambda)}{y_i!}\\
  &=\frac{\lambda^{\sum_{i=1}^N y_i}\exp(-N\lambda)}{\prod_{i=1}^N y_i!}\\
  &=\frac{\exp(-N\lambda)\exp(\sum_{i=1}^Ny_i\log(\lambda))}{\prod_{i=1}^N y_i!},
\end{align}

then $h(\mathbf{y})=\left[\prod_{i=1}^N y_i!\right]^{-1}$, $\eta(\lambda)=\log(\lambda)$, $T(\mathbf{y})=\sum_{i=1}^N y_i$ (sufficient statistic) and $C(\lambda)=\exp(-\lambda)$.

If we set $\eta=\log(\lambda)$, then 

\begin{align}
  p(\mathbf{y}|\eta)&=\frac{\exp(\sum_{i=1}^Ny_i\eta-N\exp(\eta))}{\prod_{i=1}^N y_i!},
\end{align}

such that $B(\eta)=N\exp(\eta)$, then $\nabla(B(\eta))=N\exp(\eta)=N\lambda=\mathbb{E}\left[\sum_{i=1}^N y_i\biggr\rvert\lambda\right]$, that is, $\mathbb{E}\left[\frac{\sum_{i=1}^N y_i}{N}\biggr\rvert\lambda\right]=\mathbb{E}[\bar{y}|\lambda]=\lambda$, and $\nabla^2(B(\eta))=N\exp(\eta)=N\lambda=Var\left[\sum_{i=1}^N y_i\biggr\rvert\lambda\right]=N^2 \times Var\left[\bar{y}\rvert\lambda\right]$, then $Var\left[\bar{y}\rvert\lambda\right]=\frac{\lambda}{N}$. 

* Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **Bernoulli distribution** show that $p(\mathbf{y}|\theta)$ is in the exponential family.


\begin{align}
  p(\mathbf{y}|\theta)&=\prod_{i=1}^N \theta^{y_i}(1-\theta)^{1-y_i}\\
  &=\theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}\\
  &=(1-\theta)^N\exp\left\{\sum_{i=1}^N y_i\log\left(\frac{\theta}{1-\theta}\right)\right\},
\end{align}

then $h(\mathbf{y})=\mathbb{I}[y_i\in\left\{0,1\right\}]$, $\eta(\theta)=\log\left(\frac{\theta}{1-\theta}\right)$, $T(\mathbf{y})=\sum_{i=1}^N y_i$ and $C(\theta)=1-\theta$.

Write this distribution in the canonical form, and find the mean and variance of the sufficient statistic (exercise 1). 

* Given a random sample $\mathbf{y}=[\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_N]$ from a **m-dimensional multinomial distribution**, where $\mathbf{y}_i=\left[y_{i1},\dots,y_{im}\right]$, $\sum_{l=1}^m y_{il}=n$, $n$ independent trials each of which leads to a success for exactly one of $m$ categories with probabilities $\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_m]$, $\sum_{l=1}^m \theta_l=1$. Show that $p(\mathbf{y}|\mathbf{\theta})$ is in the exponential family.


\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&=\prod_{i=1}^N \frac{n!}{\prod_{l=1}^m y_{il}!} \prod_{l=1}^m\theta_l^{y_il}\\
  &=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\exp\left\{\sum_{i=1}^N\sum_{l=1}^m y_{il}\log(\theta_l)\right\}\\
  &=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\exp\left\{\left(N\times n-\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\right)\log(\theta_m)+\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\log(\theta_l)\right\}\\
  &=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\theta_m^{N\times n}\exp\left\{\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\log(\theta_l/\theta_m)\right\},
\end{align}

then $h(\mathbf{y})=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}$, $\eta(\mathbf{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right)\dots \log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]$, $T(\mathbf{y})=\left[\sum_{i=1}^N y_{i1}\dots \sum_{i=1}^N y_{im-1}\right]$ and $C(\mathbf{\theta})=\theta_m^n$.

2. Continuos distributions

* Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **normal distribution** show that $p(\mathbf{y}|\mu,\sigma^2)$ is in the exponential family.

\begin{align}
  p(\mathbf{y}|\mu,\sigma^2)&=\prod_{i=1}^N \frac{1}{2\pi\sigma^2}\exp\left\{-\frac{1}{2\sigma^2}\left(y_i-\mu\right)^2\right\}\\
  &= (2\pi)^{-N/2}(\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N\left(y_i-\mu\right)^2\right\}\\
  &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^Ny_i^2+\frac{\mu}{\sigma^2}\sum_{i=1}^N y_i-N\frac{\mu^2}{2\sigma^2}-\frac{N}{2}\log(\sigma^2)\right\}

\end{align}

then $h(\mathbf{y})=(2\pi)^{-N/2}$, $\eta(\mu,\sigma^2)=\left[\frac{\mu}{\sigma^2} \ \frac{-1}{\sigma^2}\right]$, $T(\mathbf{y})=\left[\sum_{i=1}^N y_i \ \sum_{i=1}^N y_i^2\right]$ and $C(\mu,\sigma^2)=\exp\left\{-\frac{\mu^2}{2\sigma^2}-\frac{\log(\sigma^2)}{2}\right\}$.

Observe that 

\begin{align}
  p(\mathbf{y}|\mu,\sigma^2)&= (2\pi)^{-N/2}\exp\left\{\eta_1\sum_{i=1}^N y_i+\eta_2\sum_{i=1}^Ny_i^2-\frac{N}{2}\log(-2\eta_2)+\frac{N}{4}\frac{\eta_1^2}{\eta_2}\right\},
\end{align}

where $B(\mathbf{\eta})=\frac{N}{2}\log(-2\eta_2)-\frac{N}{4}\frac{\eta_1^2}{\eta_2}$. Then,

\begin{align}
  \nabla B(\mathbf{\eta}) & = \begin{bmatrix}
    -\frac{N}{2}\frac{\eta_1}{\eta_2}\\
    -\frac{N}{2}\frac{1}{\eta_2}+\frac{N}{4}\frac{\eta_1^2}{\eta_2^2}
  \end{bmatrix}
   =
  \begin{bmatrix}
    N\times\mu\\
    N\times(\mu^2+\sigma^2)
  \end{bmatrix}  = \begin{bmatrix}
    \mathbb{E}\left[\sum_{i=1}^N y_i\bigr\rvert \mu,\sigma^2\right]\\
    \mathbb{E}\left[\sum_{i=1}^N y_i^2\bigr\rvert \mu,\sigma^2\right]
  \end{bmatrix}. 
\end{align}

* Given $\mathbf{y}\sim N_N(\mathbf{\mu},\mathbf{\Sigma})$, that is, a **multivariate normal distribution** show that $p(\mathbf{y}|\mathbf{\mu},\mathbf{\Sigma})$ is in the exponential family.

\begin{align}
  p(\mathbf{y}|\mu,\sigma^2)&= (2\pi)^{-N/2}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}\left(\mathbf{y}-\mathbf{\mu}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{y}-\mathbf{\mu}\right)\right\}\\
  &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{y}-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}\\
  &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(tr\left\{\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{y}\right\}-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}\\
  &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(vec\left(\mathbf{y}\mathbf{y}^{\top}\right)^{\top}vec\left(\mathbf{\Sigma}^{-1}\right)-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\},

\end{align}

where $tr$ and $vec$ are the trace and vectorization operators, respectively.

Then $h(\mathbf{y})=(2\pi)^{-N/2}$, $\eta(\mathbf{\mu},\mathbf{\Sigma})=\left[\mathbf{\Sigma}^{-1}\mathbf{\mu} \ \ vec\left(\mathbf{\Sigma}^{-1}\right)\right]$, $T(\mathbf{y})=\left[\mathbf{y} \ \ -\frac{1}{2}vec(\mathbf{y}\mathbf{y}^{\top})\right]$ and $C(\mathbf{\mu},\mathbf{\Sigma})=\exp\left\{-\frac{1}{2N}\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}$.



## Conjugate prior to exponential family {#sec42}

**Theorem 4.2.1**

The prior distribution $\pi(\mathbf{\theta})\propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}$ is conjugate to the exponential family (equation \@ref(eq:414)).

**Proof**

\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})& \propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\} \times h(\mathbf{y}) C(\mathbf{\theta})^N\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\}\\
  & \propto C(\mathbf{\theta})^{N+b_0} \exp\left\{\eta(\mathbf{\theta})^{\top}(\mathbf{T}(\mathbf{y})+\mathbf{a}_0\right\}. 
\end{align}

Observe that the posterior is in the exponential family, $\pi(\mathbf{\theta}|\mathbf{y})\propto C(\mathbf{\theta})^{\beta_n} \exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{\alpha}_n\right\}$, $\beta_n=N+b_0$ and $\mathbf{\alpha}_n=\mathbf{T}(\mathbf{y})+\mathbf{a}_0$.

*Remarks*

We see comparing the prior and the likelihood that $b_0$ plays the role of a hypothetical sample size, and $\mathbf{a}_0$ plays the role of hypothetical sufficient statistics. This view helps the elicitation process.

In addition, we stablished the result in the *standard form* of the exponential family. We can also stablish this result in the *canonical form* of the exponential family. Observe that given $\mathbf{\eta}=\mathbf{\eta}(\mathbf{\theta})$ another way to get a prior for $\mathbf{\eta}$ is to use the change of variables theorem given a bijective function.

In the setting where there is a prior regular conjugate prior [@diaconis1979conjugate] show that we obtain a posterior expectation of the sufficient statistics that is a weighted average between the prior expectation and the likelihood estimate. 

**Examples: Theorem 4.2.1**

1. Likelihood functions from discrete distributions

* **The Poisson-gamma model**

Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **Poisson distribution** then a conjugate prior density for $\lambda$ has the form 

\begin{align}
  \pi(\lambda)&\propto \left(\exp(-\lambda)\right)^{b_0} \exp\left\{a_0\log(\lambda)\right\}\\
 & = \exp(-\lambda b_0) \lambda^{a_0}\\
 & = \exp(-\lambda \beta_0) \lambda^{\alpha_0-1}.
\end{align}

This is the kernel of a gamma density in the *rate parametrization*, $G(\alpha_0,\beta_0)$, $\alpha_0=a_0+1$ and $\beta_0=b_0$.^[Another parametrization of the gamma density is the *scale parametrization* where $\kappa_0=1/\beta_0$. See the health insurance example in Chapter \@ref(basics).] Then, a prior conjugate distribution for the Poisson likelihood is a gamma distribution.   

Taking into account that $\sum_{i=1}^N y_i$ is a sufficient statistic for the Poisson distribution, then we can think about $a_0$ as the number of occurrences in $b_0$ experiments. 
Observe that

\begin{align}
  \pi(\lambda|\mathbf{y})&\propto \exp(-\lambda \beta_0) \lambda^{\alpha_0-1} \times \exp(-N\lambda)\lambda^{\sum_{i=1}^Ny_i}\\
  &= \exp(-\lambda(N+\beta_0)) \lambda^{\sum_{i=1}^Ny_i+\alpha_0-1}. 
\end{align}

As expected, this is the kernel of a gamma distribution, which means $\lambda|\mathbf{y}\sim G(\alpha_n,\beta_n)$, $\alpha_n=\sum_{i=1}^Ny_i+\alpha_0$ and $\beta_n=N+\beta_0$.

Observe that $\alpha_0/\beta_0$ is the prior mean, and $\alpha_0/\beta_0^2$ is the prior variance. Then, $\alpha_0\rightarrow 0$ and $\beta_0\rightarrow 0$ imply a non-informative prior such that the posterior mean converges to the maximum likelihood estimator $\bar{y}=\frac{\sum_{i=1}^N y_i}{N}$,

\begin{align}
  \mathbb{E}\left[\lambda|\mathbf{y}\right]&=\frac{\alpha_n}{\beta_n}\\
  &=\frac{\sum_{i=1}^Ny_i+\alpha_0}{N+\beta_0}\\
  &=\frac{N\bar{y}}{N+\beta_0}+\frac{\alpha_0}{N+\beta_0}
\end{align}

The posterior mean is a weighted average between sample and prior information. This is a general result from regular conjugate priors [@diaconis1979conjugate]. Observe that $\mathbb{E}\left[\lambda|\mathbf{y}\right]=\bar{y}, \lim N\rightarrow\infty$. 

In addition, $\alpha_0\rightarrow 0$ and $\beta_0\rightarrow 0$ corresponds to $\pi(\lambda)\propto \frac{1}{\lambda}$, which is an improper prior. Improper priors have bad consequences on Bayes factors (hypothesis testing). In this setting, we can get analytical solutions for the marginal likelihood and the predictive distribution (see the health insurance example and exercise 3 in Chapter \@ref(basics)). 

* **The Bernoulli-beta model**

Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **Bernoulli distribution** then a conjugate prior density for $\lambda$ has the form 

\begin{align}
  \pi(\theta)&\propto (1-\theta)^{b_0} \exp\left\{a_0\log\left(\frac{\theta}{1-\theta}\right)\right\}\\
 & = (1-\theta)^{b_0-a_0}\theta^{a_0}\\
 & = \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}.
\end{align}

This is the kernel of a beta density, $B(\alpha_0,\beta_0)$, $\alpha_0=a_0+1$ and $\beta_0=b_0-a_0+1$. A prior conjugate distribution for the Bernoulli likelihood is a beta distribution. Given that $b_0$ is the hypothetical sample size, and $a_0$ is the hypothetical sufficient statistic, which is the number of successes, then $b_0-a_0$ is the number of failures. This implies that $\alpha_0$ is the number of prior successes plus one, and $\beta_0$ is the number of prior failures plus one. Given that the mode of a beta distribuited random variable is $\frac{\alpha_0-1}{\alpha_0+\beta_0-2}=\frac{a_0}{b_0}$, then we have the a priori probability of success. Setting $\alpha_0=1$ and $\beta_0=1$, which implies a 0-1 uniform distribution, corresponds to a setting with 0 successes (and 0 failures) in 0 experiments.   

Observe that

\begin{align}
  \pi(\lambda|\mathbf{y})&\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^Ny_i}\\
  &= \theta^{\alpha_0+\sum_{i=1}^N y_i-1}(1-\theta)^{\beta_0+N-\sum_{i=1}^Ny_i-1}. 
\end{align}

The posterior distribution is beta, $\theta|\mathbf{y}\sim B(\alpha_n,\beta_n)$, $\alpha_n=\alpha_0+\sum_{i=1}^N y_i$ and $\beta_n=\beta_0+N-\sum_{i=1}^Ny_i$, where the posterior mean $\mathbf{E}[\theta|\mathbf{y}]=\frac{\alpha_n}{\alpha_n+\beta_n}=\frac{\alpha_0+N\bar{y}}{\alpha_0+\beta_0+N}=\frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+N}\frac{\alpha_0}{\alpha_0+\beta_0}+\frac{N}{\alpha_0+\beta_0+N}\bar{y}$. The posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.

El marginal likelihood in this setting is

\begin{align}
  p(\mathbf{y})=&\int_{0}^1 \frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0,\beta_0)}\times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}d\theta\\
  =& \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)},
\end{align}

where $B(\cdot ,\cdot)$ is the beta function.

In addition, the predictive density is

\begin{align}
  p(Y_0|\mathbf{y})&=\int_0^1 \theta^{y_0}(1-\theta)^{1-y_0}\times \frac{\theta^{\alpha_n-1}(1-\theta)^{\beta_n-1}}{B(\alpha_n,\beta_n)}d\theta\\
  &=\frac{B(\alpha_n+y_0,\beta_n+1-y_0)}{B(\alpha_n,\beta_n)}\\
  &=\frac{\Gamma(\alpha_n+\beta_n)\Gamma(\alpha_n+y_0)\Gamma(\beta_n+y_0)}{\Gamma(\alpha_n+\beta_n+1\Gamma(\alpha_n)\Gamma(\beta_n)}\\
  &=\begin{Bmatrix}
  \frac{\alpha_n}{\alpha_n+\beta_n}, & y_0=1\\
  \frac{\beta_n}{\alpha_n+\beta_n}, & y_0=0\\
  \end{Bmatrix}.
\end{align}

This is a Bernoulli distribution with probability of success equal to $\frac{\alpha_n}{\alpha_n+\beta_n}$. 

* **The multinomial-Dirichlet model**

Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **multinomial distribution** then a conjugate prior density for $\mathbf{\theta}=\left[\theta_1,\theta_2,\dots,\theta_m\right]$ has the form 

\begin{align}
  \pi(\mathbf{\theta})&\propto \theta_m^{b_0} \exp\left\{\mathbf{\eta}(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}\\
 & = \prod_{l=1}^{m-1}\theta_l^{a_{0l}}\theta_m^{b_0-\sum_{l=1}^{m-1}a_{0l}}\\
 & = \prod_{l=1}^{m}\theta_l^{\alpha_{0l}-1},
\end{align}

where $\mathbf{\eta}(\mathbf{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right),\dots,\log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]$, $\mathbf{a}_0=\left[a_{01},\dots,a_{am-1}\right]^{\top}$, $\mathbf{\alpha}_0=\left[\alpha_{01},\alpha_{02},\dots,\alpha_{0m}\right]$, $\alpha_{0l}=a_{0l}+1$, $l=1,2,\dots,m-1$ and $\alpha_{0m}=b_0-\sum_{l=1}^{m-1} a_{0l}+1$. 

This is the kernel of a Dirichlet distribution, that is, the prior distribution is $D(\mathbf{\alpha}_0)$.

Observe that $a_{0l}$ is the number of hypothetical number of times outcome $l$ is observed over the hypothetical $b_0$ trials. Setting $\alpha_{0l}=1$, that is a uniform distribution over the open standard simplex, implicitly we set $a_{0l}=0$, which means that there are 0 occurrences of category $l$ in $b_0=0$ experiments.    

The posterior distribution of the multinomial-Dirichlet model is given by

\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&\propto \prod_{l=1}^m \theta_l^{\alpha_{0l}-1}\times\prod_{l=1}^m \theta_l^{\sum_{i=1}^{N} y_{il}}\\
  &=\prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^{N} y_{il}-1}
\end{align}

This is the kernel of a Dirichlet distribution $D(\mathbf{\alpha}_n)$, $\mathbf{\alpha}_n=\left[\alpha_{n1},\alpha_{n2},\dots,\alpha_{nm}\right]$, $\alpha_{nl}=\alpha_{0l}+\sum_{i=1}^{N}y_{il}$, $l=1,2,\dots,m$. Observe that

\begin{align}

\mathbb{E}[\theta_{j}|\mathbf{y}]&=\frac{\alpha_{nj}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\\
&=\frac{\sum_{l=1}^m \alpha_{0l}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\alpha_{0j}}{\sum_{l=1}^m \alpha_{0l}}+\frac{\sum_{l=1}^m\sum_{i=1}^N y_{il}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\sum_{i=1}^N y_{ij}}{\sum_{l=1}^m\sum_{i=1}^N y_{il}}.
\end{align}

We have again that the posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.

The marginal likelihood is

\begin{align}
  p(\mathbf{y})&=\int_{\mathbf{\Theta}}\frac{\prod_{l=1}^m \theta_l^{\alpha_{0l}-1}}{B(\mathbf{\alpha}_0)}\times \prod_{i=1}^N\frac{n!}{\prod_{l=1}^m y_{il}}\prod_{l=1}^m \theta_{l}^{y_{il}}d\mathbf{\theta}\\
  &=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\int_{\mathbf{\Theta}} \prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^N y_{il}-1} d\mathbf{\theta}\\
  &=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}B(\mathbf{\alpha}_n)\\
  &=\frac{N\times n! \Gamma\left(\sum_{l=1}^n \alpha_{0l}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{0l}+N\times n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}\right)}{\Gamma\left(\alpha_{0l}\right)\prod_{i=1}^N y_{il}!},
\end{align}

where $B(\mathbf{\alpha})=\frac{\prod_{l=1}^m\Gamma(\alpha_l)}{\Gamma\left(\sum_{l=1}^m \alpha_l\right)}$.

Following similar steps we get the predictive density

\begin{align}
  p(Y_0|\mathbf{y})&=\frac{ n! \Gamma\left(\sum_{l=1}^n \alpha_{nl}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{nl}+ n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}+y_{0l}\right)}{\Gamma\left(\alpha_{nl}\right) y_{0l}!}.
\end{align}

This is a Dirichlet-multinomial distribution with parameters $\mathbf{\alpha}_n$.

2. Likelihood functions from continuos distributions

* **The normal-normal/gamma model**

What can the prior distribution be of a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a normal distribution?

Find the posterior distribution of the normal-normal/gamma model.

What is the posterior distribution of the multivariate normal likelihood with a normal-Wishart prior?


## Computational examples

* ¿What is the probability that the Sun will rise tomorrow?

This is the most famaous Ricard Price's example developed in the Appendix of the Bayes' theorem paper [@bayes1763lii]. Here, we implicitly use *Laplace's Rule of Succession* to solve this question. In perticular, if we were *a priori* uncertain about the probability the Sun will on a specified day rise, that is, a prior uniform distribution over (0,1), that is, a beta (1,1) distribution...

* Mixture of normals using multinomial-Dirichlet model

## Summary: Chapter 4

## Exercises: Chapter 4

1. Write in the canonical form the distribution of the Bernoulli example, and find the mean and variance of the sufficient statistic.

2. Given a random sample $\mathbf{y}=[\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_N]^{\top}$ from a **binomial distribution** where the number of trials ($n$) is known. Show that $p(\mathbf{y}|\theta)$ is in the exponential family, and find the posterior distribution, the marginal likelihood and the predictive distribution of the binomial-beta model assuming the number of trials is known.

3. Given a random sample $\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}$ from a **exponential distribution**. Show that $p(\mathbf{y}|\alpha,\beta)$ is in the exponential family, and find the posterior distribution, marginal likelihood and predictive distribution of the exponential-gamma model.

