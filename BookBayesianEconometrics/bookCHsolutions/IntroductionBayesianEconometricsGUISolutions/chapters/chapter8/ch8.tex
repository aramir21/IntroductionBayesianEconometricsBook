\chapter{Time series models}\label{chap8}

\section{Solutions of Exercises}\label{sec81}
\begin{enumerate}[leftmargin=*]
	
	\item Show that in the $AR(2)$ stationary process, $y_t=\mu+\phi_1y_{t-1}+\phi_2y_{t-2}+e_t$, where $e_t\sim N(0,\sigma^2)$, $\mathbb{E}[y_t]=\frac{\mu}{1-\phi_1-\phi_2}$, and $Var[y_t]=\frac{\sigma^2(1-\phi_2)}{1-\phi_2-\phi_1^2-\phi_1^2\phi_2-\phi_2^2+\phi_2^3}$.
	
	\textbf{Answer}
	
	A necessary condition to achieve second-order stationarity in the $AR(2)$ process is that roots of of the polynomial $1-\phi_1L-\phi_2L^2=0$ lie outside the unit circle. This is achieved if $|\phi_2|<1$, $\phi_1+\phi_2<1$ and $\phi_2-\phi_1<1$.
	
	Let's calculate the expected value:
	\begin{align*}
		\mathbb{E}[y_t]&=\bar{\mu}\\
			&=\mathbb{E}[\mu+\phi_1y_{t-1}+\phi_2y_{t-2}+e_t]\\
			&=\mu+\phi_1\mathbb{E}[y_{t-1}]+\phi_2\mathbb{E}[y_{t-2}]+\mathbb{E}[e_{t}]\\
			&=\mu+\phi_1\bar{\mu}+\phi_2\bar{\mu},
	\end{align*}
then, $\bar{\mu}(1-\phi_1-\phi_2)=\mu$, then, $\mathbb{E}[y_t]=\bar{\mu}=\frac{\mu}{1-\phi_1-\phi_2}$.

See that $y_t=\mu+\phi_1y_{t-1}+\phi_2y_{t-2}+e_t$ implies that $y_t=\bar{\mu}(1-\phi_1-\phi_2)+\phi_1y_{t-1}+\phi_2y_{t-2}+e_t$, which means that $y_t-\bar{\mu}=\phi_1(y_{t-1}-\bar{\mu})+\phi_2(y_{t-2}-\bar{\mu})+e_t$, and setting $z_t=y_t-\bar{\mu}$, we have $z_t=\phi_1z_{t-1}+\phi_2z_{t-2}+e_t$ where $\mathbb{E}[z_t]=0$. Then, we can calculate the variance of $y_t$ using $z_t$, given that both have same variance.  

Let's define $\gamma_s=Cov(z_t,z_{t-s})=\mathbb{E}[z_tz_{t-s}]$, then the Yule-Walker equations are:
\begin{align*}
	\gamma_0&=\phi_1\gamma_1+\phi_2\gamma_2+\sigma^2\\
	\gamma_1&=\phi_1\gamma_0+\phi_2\gamma_1\\
	\gamma_2&=\phi_1\gamma_1+\phi_2\gamma_0.
\end{align*}  
Solving this system of equations for $\gamma_0$, $\gamma_1$ and $\gamma_2$, we find that $\gamma_0=\frac{\sigma^2(1-\phi_2)}{1-\phi_2-\phi_1^2-\phi_1^2\phi_2-\phi_2^2+\phi_2^3}$. 
	

\item Show that the posterior distribution of $\bm{\phi}|\bm{\beta},\sigma^2,\bm{y},\bm{X}$ in the model $y_t=\bm{x}_t^{\top}\bm{\beta}+\mu_t$ where $\phi(L)\mu_t=e_t$ and $e_t\stackrel{iid}{\sim}N(0,\sigma^2)$ is $N(\bm{\phi}_n, \bm{\Phi}_n)\mathbbm{1}[\bm{\phi}\in S_{\bm{\phi}}]$, where $\bm{\Phi}_n=(\bm{\Phi}_0^{-1}+\sigma^{-2}\bm{U}^{\top}\bm{U})$, $\bm{\phi}_n=\bm{\Phi}_n(\bm{\Phi}_0^{-1}\bm{\phi}_0+\sigma^{-2}\bm{U}^{\top}\bm{\mu})$, and $S_{\phi}$ is the stationary region of $\bm{\phi}$.
	
\textbf{Answer}
	
The conditional posterior distribution can be got as follows:
	\begin{align*}
		\pi(\bm{\phi}|\bm{\beta},\sigma^2,\bm{y})&\propto \exp\left\{-\frac{1}{2\sigma^2}(\bm{\mu}-\bm{U}\bm{\phi})^{\top}(\bm{\mu}-\bm{U}\bm{\phi})\right\}\\
		&\times \exp\left\{-\frac{1}{2}(\bm{\phi}-\bm{\phi}_0)^{\top}\bm{\Phi}_0^{-1}(\bm{\phi}-\bm{\phi}_0)\right\}\\
		&\propto\exp\left\{-\frac{1}{2}[\bm{\phi}^{\top}(\sigma^{-2}\bm{U}^{\top}\bm{U}+\bm{\Phi}_0^{-1})\bm{\phi}-2\bm{\phi}^{\top}(\sigma^{-2}\bm{U}^{\top}\bm{\mu}+\bm{\Phi}_0^{-1}\bm{\phi}_0)\right.\\
		&\left.+\bm{\phi}_n^{\top}\bm{\Phi}_n^{-1}\bm{\phi}_n-\bm{\phi}_n^{\top}\bm{\Phi}_n^{-1}\bm{\phi}_n]\right\}\\
		&\propto\exp\left\{-\frac{1}{2}[(\bm{\phi}-\bm{\phi}_n)^{\top}\bm{\Phi}_n^{-1}(\bm{\phi}-\bm{\phi}_n)]\right\}, 
	\end{align*}
where $\bm{\Phi}_n=(\sigma^{-2}\bm{U}^{\top}\bm{U}+\bm{\Phi}_0^{-1})^{-1}$ and $\bm{\phi}_n=\bm{\Phi}_n(\sigma^{-2}\bm{U}^{\top}\bm{\mu}+\bm{\Phi}_0^{-1}\bm{\phi}_0)$. The last expression is the kernel of a multivariate normal distribution with mean $\bm{\phi}_n$ and variance matrix $\bm{\Phi}_n$.   
	
	\begin{tcolorbox}[enhanced,width=4.67in,center upper,
		fontupper=\large\bfseries,drop shadow southwest,sharp corners]
		\textit{R code. Name}
		\begin{VF}
			\begin{lstlisting}[language=R]
	shiny::runGitHub("besmarter/BSTApp", launch.browser = T)\end{lstlisting}
		\end{VF}
	\end{tcolorbox} 
	
\end{enumerate}