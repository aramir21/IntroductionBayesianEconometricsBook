\chapter{Causal inference}\label{chap12}

\section*{Solutions of Exercises}\label{sec12_1}
\begin{enumerate}[leftmargin=*]
\item Show that the Average Treatment Effect (ATE) in the simple linear regression framework
\[
Y_i = \beta_0 + \tau D_i + \mu_i,
\]
assuming non-informative prior distributions, so that the posterior mean of the location parameter coincides with the maximum likelihood estimator, is equal to
\[
\bar{y}_1 - \bar{y}_0.
\]

\textbf{Answer}

Consider the simple linear regression model
\[
Y_i = \beta_0 + \tau D_i + \mu_i, \qquad i = 1,\dots,N,
\]
where $D_i \in \{0,1\}$ is a treatment indicator. In matrix notation:
\[
\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\mu}, \qquad
X = \begin{pmatrix}
	1 & D_1 \\
	\vdots & \vdots \\
	1 & D_n
\end{pmatrix}, \quad
\boldsymbol{\beta} = \begin{pmatrix}\beta_0 \\ \tau \end{pmatrix}.
\]

Under the usual Gaussian likelihood and a non-informative prior $\pi(\boldsymbol{\beta},\sigma^2) \propto \sigma^{-2}$, the posterior mean of $\boldsymbol{\beta}$ coincides with the maximum likelihood estimator (MLE), which is the ordinary least squares (OLS) estimator:
\[
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
\]

Compute $\mathbf{X}^\top \mathbf{X}$ and $\mathbf{X}^\top \mathbf{y}$:
\[
\mathbf{X}^\top \mathbf{X} =
\begin{pmatrix}
	N & N_1 \\
	N_1 & N_1
\end{pmatrix}, \qquad
\mathbf{X}^\top \mathbf{y} =
\begin{pmatrix}
	\sum_{i=1}^N y_i \\
	\sum_{i:D_i=1} y_i
\end{pmatrix},
\]
where $N_1 = \sum_i D_i$ and $N_0 = N - N_1$. Let $\bar{y}_1 = \frac{1}{N_1}\sum_{i:D_i=1} y_i$ and $\bar{y}_0 = \frac{1}{N_0}\sum_{i:D_i=0} y_i$.

Compute the inverse:
\[
(\mathbf{X}^\top \mathbf{X})^{-1} =
\frac{1}{N_0 N_1}
\begin{pmatrix}
	N_1 & -N_1 \\
	-N_1 & N
\end{pmatrix}.
\]

Thus:
\[
\widehat{\boldsymbol{\beta}} =
\frac{1}{N_0 N_1}
\begin{pmatrix}
	N_1 & -N_1 \\
	-N_1 & N
\end{pmatrix}
\begin{pmatrix}
	\sum_{i=1}^N y_i \\
	\sum_{i:D_i=1} y_i
\end{pmatrix}
=
\begin{pmatrix}
	\bar{y}_0 \\
	\bar{y}_1 - \bar{y}_0
\end{pmatrix}.
\]

Therefore, the intercept equals the mean outcome for the control group ($\bar{y}_0$), and the slope equals the difference in means between treated and control groups:
\[
\widehat{\tau} = \bar{y}_1 - \bar{y}_0.
\]

Since under a non-informative prior the posterior mean equals the ML estimator, the posterior mean of $\tau$ coincides with the sample difference in means:
\[
\text{ATE} = \mathbb{E}[\tau \mid \mathbf{y}] = \bar{y}_1 - \bar{y}_0.
\]

\item Some readers may question the assumption that potential outcomes are normally distributed. However, it is important to note that the normal distribution is the \textit{maximum entropy} continuous distribution given a specified mean $\mu$ and finite variance $\sigma^2$. In other words, among all distributions with the same mean and variance, the normal distribution represents the one with the greatest level of uncertainty or unpredictability \cite{cover2006elements}.
 
Show that the normal distribution is the \textit{maximum entropy} continuous distribution given a specified mean $\mu$ and finite variance $\sigma^2$ by considering the formal definition of entropy:
\[
H(f) = - \int_{-\infty}^{\infty} f(y) \log f(y) \, dy,
\]
where $f(y)$ is a probability density function.

\textbf{Answer}
  
We want to maximize $H(f)$ subject to the constraints:
\[
\int_{-\infty}^{\infty} f(y)\, dy = 1, \qquad
\int_{-\infty}^{\infty} y f(y)\, dy = \mu, \qquad
\int_{-\infty}^{\infty} (y-\mu)^2 f(y)\, dy = \sigma^2.
\]

Form the Lagrangian:
\[
\mathcal{L}(f) = - \int f(y)\log f(y) \, dy 
+ \lambda_0 \left( \int f(y)\, dy - 1\right)
+ \lambda_1 \left( \int y f(y)\, dy - \mu \right)
+ \lambda_2 \left( \int (y-\mu)^2 f(y)\, dy - \sigma^2 \right).
\]

Taking the functional derivative with respect to $f(y)$:
\[
\frac{\delta \mathcal{L}}{\delta f} = -(\log f(y) + 1) + \lambda_0 + \lambda_1 y + \lambda_2 (y-\mu)^2 = 0.
\]

Thus:
\[
\log f(y) = \lambda_0' + \lambda_1 y + \lambda_2 (y-\mu)^2,
\]
where $\lambda_0'$ absorbs constants. Exponentiating:
\[
f(y) \propto \exp\big( \lambda_1 y + \lambda_2 (y-\mu)^2 \big).
\]

For integrability, $\lambda_2 < 0$. Completing the square:
\[
f(y) \propto \exp\left( -\frac{(y-\mu)^2}{2\sigma^2} \right),
\]
which is the kernel of a normal density. Normalizing:
\[
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y-\mu)^2}{2\sigma^2} \right).
\]

Therefore, the maximum entropy distribution for a given mean $\mu$ and variance $\sigma^2$ is
\[
Y \sim N(\mu, \sigma^2).
\]

\item Use the package \textit{dagity} to construct the DAG in Figure~13.5, verify that it is acyclic, and check whether the causal effect of \(D\) on \(Y\) is identifiable by controlling for \(\mathbf{X}\).

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simple exercise in \textit{dagity} package: Conditional independence assumption}
	\begin{VF}
		\begin{lstlisting}[language=R]		
library(dagitty)
library(ggdag)

Gd = dagitty('dag{
	X [pos="-1,1"]
	D [exposure, pos="0,0"]
	Y [outcome, pos="1,1"]
	X -> D
	X -> Y
	D -> Y
}')

ggdag(Gd) +  theme_dag()

isAcyclic(Gd)

for( n in names(Gd) ){
	for( m in children(Gd,n) ){
		a <- adjustmentSets(Gd, n, m, effect = c("direct"), type = c("minimal"))
		if( length(a) > 0 ){
			cat("The effect ",n,"->",m,
			" is identifiable by controlling for:\n",sep="")
			print( a, prefix=" * " )
		}
	}
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\item Use the package \textit{dagitty} to construct the DAG in Figure~13.8, verify that it is acyclic, and check that the causal effect of \(D\) on \(Y\) is identifiable by controlling for \(\mathbf{X}\) but not for \(C\).


\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simple exercise in \textit{dagity} package: Collider bias}
	\begin{VF}
		\begin{lstlisting}[language=R]		
#### DAG: Collider bias  ####
library(dagitty)
library(ggdag)

Gd = dagitty('dag{
	X [pos="-1,1"]
	D [exposure, pos="0,0"]
	Y [outcome, pos="1,1"]
	C [pos="0,0.5"]
	X -> D
	X -> Y
	X -> C
	D -> Y
	D -> C
}')

ggdag(Gd) +  theme_dag()

isAcyclic(Gd)

adjustmentSets(Gd, exposure = "D", outcome = "Y")

for( n in names(Gd) ){
	for( m in children(Gd,n) ){
		a <- adjustmentSets(Gd, n, m, effect = c("direct"), type = c("minimal"))
		if( length(a) > 0 ){
			cat("The effect ",n,"->",m,
			" is identifiable by controlling for:\n",sep="")
			print( a, prefix=" * " )
		}
	}
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\item Use the package \textit{dagitty} to construct the DAG in Figure~13.11, taking into account that $\mathbf{U}$ is unobserved (latent). Verify that it is acyclic, check that $Z$ is a valid instrument, and determine whether the causal effect of \(D\) on \(Y\) is identifiable.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simple exercise in \textit{dagity} package: Instrumental variable}
	\begin{VF}
		\begin{lstlisting}[language=R]		
#### DAG: Instruments  ####
library(dagitty)
library(ggdag)

Gd = dagitty('dag{
	U [latent, pos="-1,1"]
	D [exposure, pos="0,0"]
	Y [outcome, pos="1,1"]
	Z [pos="0,0.5"]
	U -> D
	U -> Y
	D -> Y
	Z -> D
}')

ggdag(Gd) +  theme_dag()

isAcyclic(Gd)

instrumentalVariables(Gd)

adjustmentSets(Gd, exposure = "D", outcome = "Y")
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\item \textbf{401(k) participation on net financial assets continues I}  

Apply the framework from this example to compute the intention-to-treat effect, the local average treatment effect, and the effect of eligibility on participation.

\textbf{Answer}

Taking into account that
\begin{align*}
	\tau_{LATE} &= \mathbb{E}[Y_i(1)-Y_i(0)\mid D(1)=1, D(0)=0] \\
	&= \frac{\mathbb{E}[Y_i \mid Z_i=1] - \mathbb{E}[Y_i \mid Z_i=0]}{\mathbb{E}[D_i \mid Z_i=1] - \mathbb{E}[D_i \mid Z_i=0]},
\end{align*}
we can recover the ITT effect by multiplying the LATE by the effect of eligibility on participation. The following code illustrates this procedure and plots the ITT. The posterior distribution of the ITT is shown in Figure \ref{fig12_1}, which closely resembles the posterior distribution of the effect of eligibility reported in the main text.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code1_chap12}
	\textit{R code. Treatment effect: 401(k) participation on net financial assets}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(10101)
library(coda); library(ggplot2)
mydata <- read.csv("https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv", sep = ",", header = TRUE, quote = "")
# Attach variables
attach(mydata)
y <- net_tfa/1000  # Outcome: net financial assets
x <- as.vector(p401) # Endogenous regressor: participation
w <- as.matrix(cbind(1, age, inc, fsize, educ, marr, twoearn, db, pira, hown))  # Exogenous regressors with intercept
z <- as.matrix(e401)  # Instrument: eligibility (NO intercept here)
X <- cbind(x, w); Z <- cbind(z, w)
# Dimensions
k <- ncol(X); kz <- ncol(Z)  
# Priors
b0 <- rep(0, k); B0i <- diag(1e-5, k)
g0 <- rep(0, kz); G0i <- diag(1e-5, kz)
nu <- 3; Psi0 <- nu * 1000 * diag(2); Psi0i <- solve(Psi0)
# MCMC parameters
mcmc <- 5000; burnin <- 1000
tot <- mcmc + burnin; thin <- 1
# Auxiliary elements
XtX <- t(X)%*%X; ZtZ <- t(Z)%*%Z; nun <- nu + length(y)
# Gibbs sampling
PostBeta <- function(Sigma, Gamma){
	w1 <- Sigma[1,1] - Sigma[1,2]^2/Sigma[2,2]
	Bn <- solve(w1^(-1)*XtX + B0i)
	yaux <- y - (Sigma[1,2]/Sigma[2,2])*(x - Z%*%Gamma)
	bn <- Bn%*%(B0i%*%b0 + w1^(-1)*t(X)%*%yaux)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostGamma <- function(Sigma, Beta){
	w2 <- Sigma[2,2] - Sigma[1,2]^2/Sigma[1,1]
	Gn <- solve(w2^(-1)*ZtZ + G0i)
	xaux <- x - (Sigma[1,2]/Sigma[1,1])*(y - X%*%Beta)
	gn <- Gn%*%(G0i%*%g0 + w2^(-1)*t(Z)%*%xaux)
	Gamma <- MASS::mvrnorm(1, gn, Gn)
	return(Gamma)
}
PostSigma <- function(Beta, Gamma){
	Uy <- y - X%*%Beta; Ux <- x - Z%*%Gamma
	U <- cbind(Uy, Ux)
	Psin <- solve(Psi0i + t(U)%*%U)
	Sigmai <- rWishart::rWishart(1, df = nun, Sigma = Psin)
	Sigma <- solve(Sigmai[,,1]) 
	return(Sigma)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]\label{code1a_chap12}
	\textit{R code. Treatment effect: 401(k) participation on net financial assets}
	\begin{VF}
		\begin{lstlisting}[language=R]		
ITT <- Bs[,1]*Gs[,1]
summary(coda::mcmc(ITT))
# Convert to data frame for ggplot
df_ITT <- data.frame(ITT = as.vector(ITT))

# Plot posterior distribution of treatment effect
ggplot(df_ITT, aes(x = ITT)) + geom_density(fill = "steelblue", alpha = 0.6) +
geom_vline(xintercept = mean(ITT), color = "red", linetype = "dashed", linewidth = 1) + labs(title = "Posterior Distribution of 401(k) ITT", x = expression(beta["ITT"]), y = "Density") + theme_minimal(base_size = 14)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/FigP401k.png}
	\caption[List of figure caption goes here]{Posterior distribution: Intention-to-treat effect 401k participation on net financial assets.}\label{fig12_1}
\end{figure}

\item \textbf{401(k) participation on net financial assets continues II} 

Use the function \textit{rivDP} from the \textit{bayesm} package to perform inference on 401(k) participation and its effect on net financial assets using the same specification as in the main text, and plot the LATE.

\textbf{Answer}

The following code implements the exercise, and Figure \ref{fig12_2} display the posterior distribution of the LATE. The results are very similar to the ones in the main text assuming a normal distribution; however, the degree of dispersion of higher using a Dirichlet process for the stochastic errors.  

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Treatment effect: 401(k) participation on net financial assets using Dirichlet process}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(10101)
library(ggplot2); library(bayesm)
# Load data
mydata <- read.csv("https://raw.githubusercontent.com/BEsmarter-consultancy/BSTApp/refs/heads/master/DataApp/401k.csv",
sep = ",", header = TRUE, quote = "")
# Attach variables
attach(mydata)
y <- net_tfa/1000     # Outcome: net financial assets
x <- as.vector(p401)  # Endogenous regressor: participation
w <- as.matrix(cbind(age, inc, fsize, educ, marr, twoearn, db, pira, hown))  # Exogenous regressors with intercept
z <- as.matrix(e401)  # Instrument: eligibility (NO intercept here)

# specify data input and mcmc parameters
Data = list(); 
Data$z <- z
Data$x <- x
Data$y <- y
Data$w <- w

Mcmc = list()
Mcmc$maxuniq <- 100
Mcmc$R <- 5000
keep <- seq((1000+1), Mcmc$R)

out <- rivDP(Data=Data, Mcmc=Mcmc)

# Plot posterior distribution of treatment effect
df_ITT <- data.frame(ITT = out[["betadraw"]][kepp])
ggplot(df_ITT, aes(x = ITT)) + geom_density(fill = "steelblue", alpha = 0.6) + geom_vline(xintercept = mean(ITT), color = "red", linetype = "dashed", linewidth = 1) + labs(title = "Posterior Distribution of 401(k) ITT: Dirichlet process", x = expression(beta["ITT"]), y = "Density") + theme_minimal(base_size = 14)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/FigP401k.png}
	\caption[List of figure caption goes here]{Posterior distribution: Local average treatment effect 401k participation on net financial assets using a Dirichlet process.}\label{fig12_2}
\end{figure}

	\item \textbf{Difference-in-Differences simulation continues}

Perform the simulation of the DiD example, and perform inference using the specification:

\[
Y_{it} = \alpha + \alpha_i + \phi_t + \tau_2 \,\big[ D_i \cdot \mathbbm{1}(t = 2) \big] + \epsilon_{it}.
\]

\textbf{Answer}

You can check that we get same results as in example in the main book.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Difference-in-Differences: Simulation exercise}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls()); set.seed(10101)
library(ggplot2); library(dplyr); library(fastDummies)
# Parameters
N_per_group <- 200          # units per group
T_periods    <- 2           # keep 2x2 for clarity
tau_true     <- 1         # ATT
sigma_eps    <- 0.5         # noise SD
# Panel index
id  <- rep(1:(2*N_per_group), each = T_periods)
t   <- rep(1:T_periods, times = 2*N_per_group)
# Group: treated (D=1) vs control (D=0)
D   <- rep(c(rep(0, N_per_group), rep(1, N_per_group)), each = T_periods)
# Post indicator (t=2 is post)
post <- as.integer(t == 2)
# Unit fixed effects (random heterogeneity)
alpha_i <- rnorm(2*N_per_group, 0, 0.8)
alpha   <- alpha_i[id]
# Time effects
phi_t <- c(0, -1.8)  # common decline from t=1 to t=2
phi   <- phi_t[t]
treat_effect <- tau_true * (D * post)
# Outcome
eps <- rnorm(length(id), 0, sigma_eps)
Y   <- alpha + phi + treat_effect + eps
did <- data.frame(id, t, D, post, Y)
plot_data <- did %>%
group_by(D, t) %>%
summarise(meanY = mean(Y), .groups = "drop") %>%
mutate(Group = ifelse(D == 1, "Treated", "Control"))
ggp <- ggplot(plot_data, aes(x = t, y = meanY, group = Group, linetype = Group)) +
geom_line(linewidth = 1) + geom_point() + scale_x_continuous(breaks = c(1, 2), labels = c("t = 1 (pre)", "t = 2 (post)")) + labs(x = "Time", y = "Mean outcome", title = "Synthetic DiD: Parallel Trends & No Anticipation") + theme_minimal(base_size = 12)
print(ggp)
# Bayesian inference: Model with interaction treatment x post period
post_fit1 <- MCMCpack::MCMCregress(Y ~ 1 + factor(id) + factor(t) + I(D * post), data = did, burnin = 100, mcmc = 1000)
tau_draws1 <- post_fit1[, "I(D * post)"]
quantile(tau_draws1, c(.025,.5,.975))
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  

\item \textbf{Difference-in-Differences simulation continues II}

Note that another strategy to perform inference on the ATT is to estimate the saturated model
\[
Y_{it} = \sum_{t,d} \mu_{tl} \big[ D_{il} \cdot \mathbbm{1}(t = t) \big] + \epsilon_{it}, \quad t = 1, 2, \ d = 1, 0,
\]
and then use the posterior draws to compute
\[
\tau_2 = (\mu_{21} - \mu_{11}) - (\mu_{20} - \mu_{10}).
\]
Inference on $\tau_2$ using this approach in the simulation setting shows that the posterior mean is similar to that from the previous approaches, but the level of uncertainty is higher.

\textbf{Answer}

There is more uncertainty because performing inference on $\tau_2$ uses four parameters in this setting, whereas in the previous we use only one parameter.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Difference-in-Differences: Simulation exercise}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101)
library(ggplot2); library(dplyr); library(fastDummies)
# Parameters
N_per_group <- 200          # units per group
T_periods    <- 2           # keep 2x2 for clarity
tau_true     <- 1           # ATT
sigma_eps    <- 0.5         # noise SD
# Panel index
id  <- rep(1:(2*N_per_group), each = T_periods)
t   <- rep(1:T_periods, times = 2*N_per_group)
# Group: treated (D=1) vs control (D=0)
D   <- rep(c(rep(0, N_per_group), rep(1, N_per_group)), each = T_periods)
# Post indicator (t=2 is post)
post <- as.integer(t == 2)
# Unit fixed effects (random heterogeneity)
alpha_i <- rnorm(2*N_per_group, 0, 0.8)
alpha   <- alpha_i[id]
# Time effects
phi_t <- c(0, -1.8)  # common decline from t=1 to t=2
phi   <- phi_t[t]
# No anticipation: effect is zero in pre, equals tau in post *only for treated*
treat_effect <- tau_true * (D * post)
# Outcome: Y_it(0) = alpha_i + phi_t + linear trend if you want; here just FE + shock
eps <- rnorm(length(id), 0, sigma_eps)
Y   <- alpha + phi + treat_effect + eps
did <- data.frame(id, t, D, post, Y)
# Calculating the means at each level, and then ATT
# ---- Build 2x2 cell indicators (no intercept model) ----
did$c_pre  <- as.integer(did$D == 0 & did$t == 1)
did$c_post <- as.integer(did$D == 0 & did$t == 2)
did$t_pre  <- as.integer(did$D == 1 & did$t == 1)
did$t_post <- as.integer(did$D == 1 & did$t == 2)
# Sanity check: each row must belong to exactly one cell
stopifnot(all(did$c_pre + did$c_post + did$t_pre + did$t_post == 1))
# ---- Bayesian saturated cell-mean model (compatible priors) ----
# No intercept: each coefficient IS a cell mean.
fit_cells <- MCMCpack::MCMCregress(
Y ~ 0 + c_pre + c_post + t_pre + t_post, data  = did, burnin = 1000, mcmc = 10000, thin = 5)
draws_cells <- as.matrix(fit_cells)
colnames(draws_cells)  # should be c("c_pre","c_post","t_pre","t_post", "sigma2")
# Posterior of the four cell means
mu_c_pre_draw  <- draws_cells[, "c_pre"]; mu_c_post_draw <- draws_cells[, "c_post"]
mu_t_pre_draw  <- draws_cells[, "t_pre"]; mu_t_post_draw <- draws_cells[, "t_post"]
# ATT = (mu_t,post - mu_t,pre) - (mu_c,post - mu_c,pre)
ATT_cells_draw <- (mu_t_post_draw - mu_t_pre_draw) - (mu_c_post_draw - mu_c_pre_draw)
quantile(ATT_cells_draw, c(.025, .5, .975))
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  

\item Perform a simulation exercise to assess the ability of BETEL to identify the causal effect when an instrument is used to address the omission of relevant regressors. Illustrate the consequences of varying the dependence between the omitted regressor and the observed regressor.

\textbf{Answer}

We simulate the observed process $X_i = 0.5Z_i + e_i$, with the unobserved regressor defined as $W_i = H_i + \nu_i$, where 
\[
\begin{bmatrix}
	e_i \\
	\nu_i
\end{bmatrix}\sim N\left(\begin{bmatrix}
0\\
0
\end{bmatrix},\begin{bmatrix}
1 & \rho\\
\rho & 1
\end{bmatrix}\right).
\]
Thus, $\rho$ defines the degree of correlation between $X_i$ and $W_i$, and we perform two exercises with $\rho \in \{0.0, 0.7\}$. The variables $Z_i$ and $H_i$ are standard normal. The dependent variable is specified as 
\[
Y_i = 1 + 1.2X_i - 0.7W_i + \mu_i,
\]
where $\mu_i$ follows a mixture of two normal distributions with means $0.5$ and $-0.5$, and standard deviations $0.5$ and $1.2$. The sample size is 2{,}000, the burn-in is 1{,}000, and the number of MCMC draws retained after burn-in is 10{,}000. We adopt the default hyperparameter values provided in the \textit{betel} package. For comparison, we also perform the analysis under the assumption of no endogeneity using the package \textit{MCMCpack} using default values for the hyperparameters. 

The following code shows the implementation, and Figure~\ref{fig12_Omission} displays the posterior distributions. When $\rho = 0$, implying no association between the regressors, there are no endogeneity issues, and assuming exogeneity yields more efficient posterior estimates. In contrast, when $\rho = 0.7$, endogeneity arises, and assuming exogeneity produces a biased posterior distribution. BETEL performs relatively well in both settings, although it is less efficient.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. BETEL: Omission of a relevant regressor}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(10101)
library(betel); library(ucminf)
# Simulate data
N <- 2000; d <- 2; k <- 2
gamma <- 0.5; beta <- c(1, 1.2, -0.7); rho <- 0.7
# Mixture
mum1 <- 1/2; mum2 <- -1/2
mu1 <- rnorm(N, mum1, 0.5); mu2 <- rnorm(N, mum2, 1.2)
mu <- sapply(1:N, function(i){sample(c(mu1[i], mu2[i]), 1, prob = c(0.5, 0.5))})
z <- rnorm(N) # Instrument
E <- MASS::mvrnorm(n = N, mu = c(0, 0), Sigma = matrix(c(1,rho,rho,1),2,2))
x <- gamma*z + E[,1] # Observed regressor
w <- rnorm(N) + E[,2] # Unobserved correlated regressor
X <- cbind(1, x, w)
y <- X%*%beta + mu
dat <- cbind(1, x, z) # Data
# Function g_i by row in BETEL
gfunc <- function(psi = psi, y = y, dat = dat) {
	X <- dat[,1:2]
	e <- y - X %*% psi
	E <- e %*% rep(1,d)
	Z <- dat[,c(1,3)]
	G <- E * Z;
	return(G)
}
nt <- round(N * 0.1, 0); # training sample size for prior
psi0 <- lm(y[1:nt]~x[1:nt])$coefficients # Starting value of psi = (theta, v), v is the slack parameter in CSS (2018)
names(psi0) <- c("alpha","beta")
psi0_ <- as.matrix(psi0) # Prior mean of psi 
Psi0_ <- 5*rep(1,k) # Prior dispersions of psi
lam0 <- .5*rnorm(d) # Starting value of lambda
nu <- 2.5 # df of the prior student-t
nuprop <- 15 # df of the student-t proposal
n0 <- 1000 # burn-in
m <- 10000 # iterations beyond burn-in
# MCMC ESTIMATION BY THE CSS (2018) method
psim2 <- betel::bayesetel(gfunc = gfunc, y = y[-(1:nt)], dat = dat[-(1:nt),], psi0 = psi0, lam0 = lam0, psi0_ = psi0_, Psi0_ = Psi0_, nu = nu, nuprop = nuprop,
controlpsi = list(maxiterpsi = 50, mingrpsi = 1.0e-8), #  list of parameters in maximizing likelihood over psi
controllam = list(maxiterlam = 50, # list of parameters in minimizing dual over lambda
mingrlam = 1.0e-7),
n0 = n0, m = m)
MCMCreg2 <- MCMCpack::MCMCregress(y~x)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{figure}[h!]
	\includegraphics[width=340pt, height=200pt]{Chapters/chapter12/figures/FigOM.png}
	\caption[List of figure caption goes here]{Posterior distribution slope parameter given omission of relevant regressor: betel and MCMCregress}\label{fig12_Omission}
\end{figure} 

\end{enumerate}