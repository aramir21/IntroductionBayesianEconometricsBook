\chapter{Univariate models}\label{chap6}

\section{Solutions of Exercises}\label{sec61}
\begin{enumerate}[leftmargin=*]

	\item Get the posterior conditional distributions of the Gaussian linear model assuming independent priors $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$.
	
\textbf{Answer}
	
The joint posterior distribution of the parameters is
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|\bf{y}, \bf{X}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, \bf{X}) \pi(\bm{\beta}) \pi(\sigma^2) \\
	&\propto (\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta} _0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&\propto\exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}} \exp\left\{-\frac{\delta_0+(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})}{2\sigma^2}\right\}\\
	&=\exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\times\underbrace{\frac{1}{(\sigma^2)^{(\frac{\alpha_n}{2}+1)}} \exp\left\{-\frac{\delta_n}{2\sigma^2}\right\}}_1.
\end{align*}

Observe that (1) is the kernel of an inverse-gamma density function. Thus, $\sigma^2|\bm{\beta},{\bf{y}},{\bf{X}} \sim IG(\alpha_n/2,\delta_n/2)$, where $\alpha_n=\alpha_0+N$ and $\delta_n=\delta_0+(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})$.

Let's see the posterior distribution of $\bm{\beta}$, 
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|{\bf{y}}, {\bf{X}}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, {\bf{X}}) \pi(\bm{\beta}) \pi(\sigma^2) \\
	&\propto (\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}({\bf{y}}-{\bf{X}}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}{\bf{B}}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&=(\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{\sigma^{-2}}{2}[{\bf{y}}^{\top}{\bf{y}}-{\bf{y}}^{\top}X\bm{\beta}-\bm{\beta}^{\top}{\bf{X}}^{\top}{\bf{y}}+\bm{\beta}^{\top}{\bf{X}}^{\top}{\bf{X}}\bm{\beta}]\right\}\\
	& \times \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}{\bf{B}}_0^{-1}\bm{\beta}-\bm{\beta}^{\top}{\bf{B}}_0^{-1}\bm{\beta}_0-\bm{\beta}_0^{\top}{\bf{B}}_0^{-1}\bm{\beta}+\bm{\beta}_0^{\top}{\bf{B}}_0^{-1}\bm{\beta}_0]\right\}\\
	&\times \frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&\propto \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}})\bm{\beta}-2\bm{\beta}^{\top}({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})]\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}}{2\sigma^2}\right\},
\end{align*}

where $\hat{\bm{\beta}}=({\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{X}}^{\top}{\bf{y}}$.

Adding and subtracting $\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n$ where
\begin{align*}
	{\bf{B}}_n & = ({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}})^{-1}\\
	\bm{\beta}_n&={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{y}}),
\end{align*}

and completing the square
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|{\bf{y}}, {\bf{X}}) & \propto \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}({\bf{B}}_0^{-1}+\sigma^{-2} {\bf{X}}^{\top}{\bf{X}})\bm{\beta}-2\bm{\beta}^{\top}{\bf{B}}_n^{-1}{\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})\right.\\
	&\left.+\bm{\beta}^{\top}_n{\bf{B}}_n^{-1}\bm{\beta}_n-\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n]\right\}\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}}{2\sigma^2}\right\}\\
	&=\exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}{\bf{B}}_n^{-1}\bm{\beta}-2\bm{\beta}^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n+\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n]\right\}\\
	& \times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}-\sigma^2\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n}{2\sigma^2}\right\}\\
	&=\underbrace{\exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_n)^{\top}{\bf{B}}_n^{-1}(\bm{\beta}-\bm{\beta}_n)\right\}}_1\\
	&\times(\sigma^2)^{-(\frac{\alpha_n}{2}+1)}\exp\left\{-\frac{\delta^{\ast}}{2\sigma^2}\right\},
\end{align*}

where $\delta^{\ast}=\delta_0+{\bf{y}}^{\top}{\bf{y}}+\sigma^2\bm{\beta}^{\top}_n{\bf{B}}_n^{-1}\bm{\beta}_n$ does not dependent on $\bm{\beta}$.

We can see that (1) is the kernel of a multivariate normal distribution with mean equal to $\bm{\beta}_n$ and covariance matrix ${\bf{B}}_n$, that is, $\bm{\beta}|\sigma^2,{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$. 

We see that the posterior distributions are from the same family as the prior distributions.

\item Show that the posterior conditional distributions of the Gaussian linear model with heteroskedasticity assuming independent priors $\pi(\bm{\beta},\sigma^2,\bm{\tau})=\pi(\bm{\beta})\times\pi(\sigma^2)\times\prod_{i=1}^N\pi(\tau_i)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$, $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$ and $\tau_i\sim G(v/2,v/2)$ are $\bm{\beta}|\sigma^2,\bm{\tau},{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$, $\sigma^2|\bm{\beta},\bm{\tau},{\bf{y}},{\bf{X}}\sim IG(\alpha_n,\delta_n)$ and $\tau_i|\bm{\beta},\sigma^2,{\bf{y}},{\bf{X}}\sim G(v_{1n},v_{2in})$, where $\bm{\tau}=[\tau_1,\dots,\tau_n]^{\top}$, ${\bf{B}}_n=({\bf{B}}_0^{-1}+\sigma^{-2}{{\bf{X}}}^{\top}\Psi{{\bf{X}}})^{-1}$, $\bm{\beta}_n={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}\Psi{\bf{y}})$, $\alpha_n=\alpha_0+N$, $\delta_n=\delta_0+({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})$, $v_{1n}=v+1$, $v_{2in}=v+\sigma^{-2}(y_i-{\bf{x}}_i^{\top}\bm{\beta})^2$, and $\Psi=\text{diagonal}\left\{\tau_i\right\}$.

\textbf{Answer}

The joint posterior distribution of the parameters is
\begin{align*}
	\pi(\bm{\beta}, \sigma^2, \bm{\tau}|\bf{y}, {\bf{X}}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, \bm{\tau}, {\bf{X}}) \pi(\bm{\beta}) \pi(\sigma^2) \prod_{i=1}^N \pi(\tau_i) \\
	&\propto \left(\prod_{i=1}^N \tau_i^{1/2}\right)(\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\prod_{i=1}^N \tau_i^{v/2-1}\exp\left\{-v\tau_i/2\right\}.
\end{align*}

Thus, the conditional posterior distribution of $\sigma^2|\bm{\beta}, \bm{\tau}, \bf{y}, {\bf{X}}$ is given by

\begin{align*}
	\pi(\sigma^2|\bm{\beta}, \bm{\tau}, \bf{y}, {\bf{X}}) & \propto (\sigma^2)^{-\frac{N+\alpha_0}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})+\delta_0\right\}.
\end{align*}

This is the kernel of an inverse-gamma distribution with shape parameter $\alpha_n$ and rate parameter $\delta_n$.

The conditional posterior distribution of $\tau_i|\bm{\beta}, \sigma^2, \bf{y}, {\bf{X}}$ is
\begin{align*}
	\pi(\tau_i|\bm{\beta}, \sigma^2,\bf{y}, {\bf{X}}) & \propto
	\tau_i^{(v+1)/2-1}\exp\left\{-\frac{\tau_i}{2}[\sigma^{-2}({\bf{y}}_i-{\bf{x}}_i^{\top}\bm{\beta})^2+v]\right\}.
\end{align*}

The conditional posterior distribution of $\bm{\beta}|\sigma^2, \bm{\tau}, \bf{y}, {\bf{X}}$ is given by

\begin{align*}
	\pi(\bm{\beta}| \sigma^2, \bm{\tau}, \bf{y}, {\bf{X}}) & \propto \exp\left\{-\frac{1}{2}[\sigma^{-2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})+(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)]\right\}.
\end{align*} 

Following same steps as in the previous exercise we get $\bm{\beta}|\sigma^2,\bm{\tau},{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$.

\item \textbf{The market value of soccer players in Europe continues}

Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the the market value of soccer players in Europe setting $v=5$ and same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case?

\textbf{Answer}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler (heteroskedastic case)}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
########################## Linear regression: Value of soccer players ##########################
Data <- read.csv("DataApplications/1ValueFootballPlayers.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
v <- 5
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
an <- a0 + N
v1n <- v + 1
# Gibbs sampling functions
PostSig2 <- function(Beta, tau){
	dn <- d0 + t(y - X%*%Beta)%*%diag(tau)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2, tau){
	Bn <- solve(B0i + sig2^(-1)*t(X)%*%diag(tau)%*%X)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*t(X)%*%diag(tau)%*%y)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostTau <- function(sig2, Beta, i){
	v2n <- v + sig2^(-1)*(y[i]-X[i,]%*%Beta)^2
	taui <- rgamma(1, v1n, v2n)
	return(taui)
}			
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler (heteroskedastic case)}
	\begin{VF}
		\begin{lstlisting}[language=R]		
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
tau <- rep(1, N)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta, tau = tau)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2, tau = tau)
	PostBetas[s,] <- Beta
	tau <- sapply(1:N, function(i){PostTau(sig2 = sig2, Beta = Beta, i)})
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))
summary(coda::mcmc(exp(PosteriorBetas[,5])-1))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean             SD       Naive SE Time-series SE 
1.246823       0.235881       0.003336       0.004010 
2. Quantiles for each variable:
2.5%   25%   50%   75% 97.5% 
0.815 1.086 1.238 1.395 1.744 					
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 
We see in this application that the value of a top soccer player in Europe increases 124\% ($\exp(0.80)-1)$) on average when he has played in the national team, the credible interval at 95\% is (81\%, 174\%). These values are not very different from the application assuming homoscedasticity in the book.


	\item \textbf{Example: Determinants of hospitalization continues}

Program a Gibbs sampling algorithm in the application of determinants of hospitalization.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
set.seed(010101)
Data <- read.csv("DataApplications/2HealthMed.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
str(Data)
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
K <- dim(X)[2] 
N <- dim(X)[1]
# Hyperparameters
b0 <- rep(0, K) # Prio mean
B0 <- diag(K) # Prior covariance
B0i <- solve(B0)
mcmc <- 1000; burnin <- 500; thin <- 2; tot <- mcmc + burnin; keep <- seq(burnin, tot, thin)
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
# Gibbs sampling functions
PostBeta <- function(Yl){
	Bn <- solve(B0i + XtX)
	bn <- Bn%*%(B0i%*%b0 + t(X)%*%Yl)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostYl <- function(Beta, i){
	Ylmean <- X[i,]%*%Beta
	if(y[i] == 1){
		Yli <- truncnorm::rtruncnorm(1, a = 0, b = Inf, mean = Ylmean, sd = 1)
	}else{
		Yli <- truncnorm::rtruncnorm(1, a = -Inf, b = 0, mean = Ylmean, sd = 1)
	}
	return(Yli)
}
PostBetas <- matrix(0, mcmc+burnin, K)
Beta <- rep(0, K)
# create progress bar in case that you want to see iterations progress
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	Yl <- sapply(1:N, function(i){PostYl(Beta = Beta, i)})
	Beta <- PostBeta(Yl = Yl)
	PostBetas[s,] <- Beta
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), "% done"))	
}
close(pb)
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent")
summary(coda::mcmc(PosteriorBetas))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

	\item \textbf{Choice of the fishing mode continues} 

Run the Algorithm A3 of the book to show the results of the Geweke \cite{Geweke1992}, Raftery \cite{Raftery1992} and Heidelberger \cite{Heidelberger1983} tests using our GUI.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
GewekeTestLocationCoef
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 
cte_1  cte_2  cte_3 NAS_1_1 NAS_1_2 NAS_1_3    AS_1   AS_2 
-1.821 -0.714  0.792  2.275 -3.944 -2.071  1.627 -2.729 

RafteryTestLocationCoef
Quantile (q) = 0.5
Accuracy (r) = +/- 0.025
Probability (s) = 0.95 
Burn-in  Total   Lower bound  Dependence
(M)      (N)     (Nmin)       factor (I)
cte_1   780      365690  1537         238.0     
cte_2   360      193950  1537         126.0     
cte_3   660      340120  1537         221.0     
NAS_1_1 120      70320   1537          45.8     
NAS_1_2 475      243960  1537         159.0     
NAS_1_3 440      248930  1537         162.0     
AS_1    3010     1438135 1537         936.0     
AS_2    550      297770  1537         194.0     

HeidelTestLocationCoef
Stationarity start     p-value 
test         iteration         
cte_1   passed       6001      6.54e-01
cte_2   failed         NA      3.72e-02
cte_3   failed         NA      4.99e-02
NAS_1_1 failed         NA      4.77e-07
NAS_1_2 failed         NA      1.82e-05
NAS_1_3 failed         NA      1.19e-04
AS_1    passed       2001      3.71e-01
AS_2    passed       8001      4.48e-01
Halfwidth Mean     Halfwidth
test                        
cte_1   passed    -0.34236 0.017025 
cte_2   <NA>            NA       NA 
cte_3   <NA>            NA       NA 
NAS_1_1 <NA>            NA       NA 
NAS_1_2 <NA>            NA       NA 
NAS_1_3 <NA>            NA       NA 
AS_1    passed    -0.00708 0.000306 
AS_2    passed     0.27982 0.009994 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

	\item \textbf{Simulation exercise of the multinomial logit model continues}

Perform inference in the simulation of the multinomial logit model using the command \textit{rmnlIndepMetrop} from the \textit{bayesm} package of \textbf{R} and using our GUI.

\textbf{Answer}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
remove(list = ls())
set.seed(12345)
# Simulation of data
N<-1000  # Sample Size
B<-c(0.5,0.8,-3)
B1<-c(-2.5,-3.5,0)
B2<-c(1,1,0)
# Alternative specific attributes of choice 1, for instance, price, quality and duration of choice 1
X1<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) 
# Alternative specific attributes of choice 2, for instance, price, quality and duration of choice 2
X2<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
# Alternative specific attributes of choice 3, for instance, price, quality and duration of choice 3
X3<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
X4<-matrix(rnorm(N,1,1),N,1)
V1<-B2[1]+X1%*%B+B1[1]*X4
V2<-B2[2]+X2%*%B+B1[2]*X4
V3<-B2[3]+X3%*%B+B1[3]*X4
suma<-exp(V1)+exp(V2)+exp(V3)
p1<-exp(V1)/suma
p2<-exp(V2)/suma
p3<-exp(V3)/suma
p<-cbind(p1,p2,p3)
y<- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE))
table(y)
L <- length(table(y))
dat <-data.frame(mode,X1[,1],X2[,1],X3[,1],X1[,2],X2[,2],X3[,2],X1[,3],X2[,3],X3[,3],X4)
colnames(dat) <- c("mode","V1.1","V1.2","V1.3","V2.1","V2.2","V2.3","V3.1","V3.2","V3.3","V4")
attach(dat)
LongData <- mlogit::mlogit.data(dat, shape = "wide", varying=2:10, choice = "mode")
Xa <- cbind(LongData$V1, LongData$V2, LongData$V3)
Xa <- cbind(X1[,1],X2[,1],X3[,1],X1[,2],X2[,2],X3[,2],X1[,3],X2[,3],X3[,3])
na <- 3
Xd <- X4
X <- bayesm::createX(p = L, na = na, nd = 1, Xa = Xa, Xd = Xd, base = L)
DataMlogit <- list(y=y, X = X, p = L)
# MCMC parameters
mcmc <- 11000+1
thin <- 5
df <- 6
mcmcpar <- list(R = mcmc, keep = 5, nu = df)
PostBeta <- bayesm::rmnlIndepMetrop(Data = DataMlogit, Mcmc = mcmcpar)
summary(PostBeta[["betadraw"]])
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

	\item \textbf{Simulation of the ordered probit model}

Simulate an ordered probit model where the first regressor distributes $N(6, 5)$ and the second distributes $G(1,1)$, the location parameter is $\bm{\beta}=\left[0.5 \ -0.25 \ 0.5\right]^{\top}$, and the cutoffs is the vector $\bm{\alpha}=\left[0 \ 1 \ 2.5\right]^{\top}$. Program from scratch a Metropolis-within-Gibbs sampling algorithm to perform inference in this simulation.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the ordered probit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101); N <- 1000
x1 <- rnorm(N, 6, 5); x2 <- rgamma(N, shape = 1, scale = 1)
X <- cbind(1, x1, x2)
beta <- c(0.5, -0.25, 0.5); cutoffs <- c(0, 1, 2.5)
e <- rnorm(N,0,1)
y_latent <- X%*%beta + e; y <- rep(0,N)
for (i in 1:N) {
	if (y_latent[i] < cutoffs[1]){
		y[i] <- 0}else{
		if (y_latent[i] >= cutoffs[1] & y_latent[i] < cutoffs[2]) {
			y[i] <- 1
		}else{
			if (y_latent[i] >= cutoffs[2] & y_latent[i] < cutoffs[3]) {
				y[i] <- 2
			}else{y[i] <- 3
			}
		}
	}
}
# Likelihood function
LogLikOP <- function(param){
	beta_g <- param[1:ncol(X)]
	delta <- param[(ncol(X)+1):(ncol(X) + dplyr::n_distinct(y) - 1)]
	Xbeta <- X%*%beta_g
	logLik <- 0
	for (i in 1:length(y)){
		if (y[i]==0){logLiki <- log(pnorm(-Xbeta[i]))
		}else if (y[i]==1){
			logLiki <- log(pnorm(exp(delta[1]) - Xbeta[i]) - pnorm(-Xbeta[i]))
		}else if (y[i]==2){
			logLiki <- log(pnorm(exp(delta[2]) + exp(delta[1]) - Xbeta[i]) - pnorm(exp(delta[1]) - Xbeta[i]))
		}else {logLiki <- log(1 - pnorm(exp(delta[2]) + exp(delta[1]) - Xbeta[i]))
		}
		logLik <- logLik + logLiki
	}
	return(-logLik)
}
# ML Estimation
param0 <- rep(0, ncol(X) + n_distinct(y)-2)
mle <- optim(param0, LogLikOP, hessian = T, method = "BFGS")
mle$par
exp(mle$par[length(beta)+1])
exp(mle$par[length(beta)+1])+exp(mle$par[length(beta)+2])
CovarML <- solve(mle$hessian)
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the ordered probit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
# M-H within Gibbs
mhop <- function(param0, G){
	betasamples <- matrix(c(0), nrow = G, ncol = ncol(X))
	betasamples[1,] <- param0[1:ncol(X)]
	tau <- matrix(c(0), nrow = G, ncol = dplyr::n_distinct(y) - 2)
	tau[1,] <- param0[(ncol(X)+1):(ncol(X) + dplyr::n_distinct(y) - 2)]
	yl <- rep(0,length(y)); ar <- rep(0,G); B1 <- solve(t(X)%*%X+solve(B0))
	pb <- winProgressBar(title = "progress bar", min = 0, max = G, width = 300)
	for(g in 2:G){
		bg <- betasamples[g-1,]; tg <- tau[g-1,]
		#Random walk M-H for delta
		delta_prime <- tg + mvtnorm::rmvnorm(1, mean = rep(0,2), sigma = VarProp)
		alpha <- min(1,(mvtnorm::dmvnorm(delta_prime, mean = d0, sigma = D0)*exp(-LogLikOP(c(bg, delta_prime)) + LogLikOP(c(bg, tg))))/mvtnorm::dmvnorm(tg, mean = d0, sigma = D0))
		if(is.nan(alpha) | is.na(alpha)) {
			alpha <- 0
		}
		#Acceptance step
		u <- runif(1, min = 0, max = 1)
		if(u<=alpha){tau[g,] <- delta_prime; ar[g] <- 1
		}else{tau[g,] <- tg
		}
		#Generation of latent variables
		for (i in 1:length(y)){
			if (y[i]==0) {
				yl[i] <- EnvStats::rnormTrunc(1, mean = X[i,]%*%bg, sd = 1, max = 0)
			}else if(y[i]==1){
				yl[i] <- EnvStats::rnormTrunc(1, mean = X[i,]%*%bg, sd = 1, min = 0, max = exp(tau[g,1]))
			}else if(y[i]==2){
				yl[i] <- EnvStats::rnormTrunc(1, mean = X[i,]%*%bg, sd = 1, min = exp(tau[g,1]), max = exp(tau[g,2])+exp(tau[g,1]))
			}else{
				yl[i] <- EnvStats::rnormTrunc(1, mean = X[i,]%*%bg, sd = 1, min = exp(tau[g,2])+exp(tau[g,1]))
			}
		}
		#Gibbs sampling for beta
		if(sum(is.nan(yl))>0 | sum(is.na(yl))>0 | sum(yl)==Inf){
			betasamples[g,] <- betasamples[g-1,]
		}else{
			b1 <- B1%*%(t(X)%*%yl + solve(B0)%*%b0)
			betasamples[g,] <- mvrnorm(1, mu = b1, Sigma = B1)
		}
		setWinProgressBar(pb, g, title=paste( round(g/G*100, 0),"% done"))
	}
	close(pb)
	return(cbind(betasamples, tau, ar))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the ordered probit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
#Hyperparameters
d0 <- rep(0,2)
D0 <- diag(2)*10000
b0 <- rep(0,ncol(X))
B0 <- diag(ncol(X))*10000
#Estimation
param0 <- rep(0, ncol(X) + dplyr::n_distinct(y)-1)
G <- 1000
tun <- 1
VarProp <- tun*solve(solve(CovarML[4:5, 4:5]) + solve(D0))
param_sample <- mhop(param0, G)
#Burn in
B <- round(0.2*G)
param_sample <- param_sample[(B+1):G,]
mcmc0 <- coda::mcmc(param_sample[, 1:(ncol(X) + dplyr::n_distinct(y) - 2)])
summary(mcmc0)
Iterations = 1:800
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 800 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean      SD  Naive SE Time-series SE
0.49120 0.08465 0.0029929       0.007140
-0.24919 0.01222 0.0004319       0.001269
0.49440 0.03942 0.0013937       0.002739
0.06716 0.06419 0.0022695       0.008479
0.41926 0.07414 0.0026212       0.009479
2. Quantiles for each variable:
2.5%      25%      50%     75%   97.5%
0.31947  0.43558  0.49710  0.5479  0.6496
-0.27180 -0.25740 -0.24948 -0.2408 -0.2238
0.42229  0.46706  0.49253  0.5189  0.5762
-0.06338  0.02328  0.06819  0.1104  0.1932
0.25857  0.37195  0.41742  0.4672  0.5558
summary(coda::mcmc(cbind(exp(param_sample[, 4]),exp(param_sample[, 4])+exp(param_sample[, 5]))))
Iterations = 1:800
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 800 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean      SD Naive SE Time-series SE
[1,] 1.072 0.06854 0.002423       0.008999
[2,] 2.597 0.13593 0.004806       0.019700
2. Quantiles for each variable:
2.5%   25%   50%   75% 97.5%
var1 0.9386 1.024 1.071 1.117 1.213
var2 2.3224 2.500 2.603 2.686 2.877
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

All posterior mean estimates are close to the population parameters, and the 95\% credible intervals encompass the population parameters. We use the definition of $\bm{\gamma}$ in the last line of the code.
	
\item \textbf{Simulation of the negative binomial model continues}

Perform inference in the simulation of the negative binomial model using the \textit{bayesm} package in \textbf{R} software.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(010101)
N <- 2000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2)
k <- dim(X)[2]
B <- rep(1, k)
alpha <- 1.2
gamma <- exp(alpha)
lambda <- exp(X%*%B)
y <- rnbinom(N, mu = lambda, size = gamma)
table(y)
# MCMC parameters
mcmc <- 10000
burnin <- 1000
thin <- 5
iter <- mcmc + burnin
keep <- seq(burnin, iter, thin)
sbeta <- 2.93/sqrt(k); salpha <- 2.93
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
alpha0 <- 0.5; delta0 <- 0.1
DataNB <- list(y = y, X = X)
mcmcNB <- list(R = mcmc, keep = thin, s_beta = sbeta, s_alpha = salpha)
PriorNB <- list(betabar = b0, A = solve(B0), a = alpha0, b = delta0)
ResultBayesm <- bayesm::rnegbinRw(Data = DataNB, Mcmc = mcmcNB, Prior = PriorNB)
summary(ResultBayesm$alphadraw)
summary(ResultBayesm$betadraw)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  

	\item \textbf{The market value of soccer players in Europe continues}

Perform the application of the value of soccer players with left censuring at one million Euros in our GUI using the Algorithm A7, and the hyperparameters of the example.

\textbf{Answer}

These are the results of running the application in our GUI. These are very similar compared with the results in the book. 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players with left censoring in our GUI}
	\begin{VF}
		\begin{lstlisting}[language=R]
Summary
Iterations = 10001:60000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 50000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
						Mean       SD  Naive SE Time-series SE
(Intercept)  1.014765 2.626395 1.175e-02      1.659e-02
Perf         0.033935 0.004529 2.025e-05      2.245e-05
Age          1.027285 0.212413 9.499e-04      1.326e-03
Age2        -0.021914 0.003989 1.784e-05      2.528e-05
NatTeam      0.847412 0.124884 5.585e-04      6.423e-04
Goals        0.010092 0.001644 7.351e-06      7.712e-06
Exp          0.175324 0.069653 3.115e-04      3.727e-04
Exp2        -0.005696 0.002950 1.319e-05      1.527e-05
sigma2       0.983337 0.096194 4.302e-04      6.724e-04
2. Quantiles for each variable:
						2.5%       25%       50%       75%      97.5%
(Intercept) -4.212563 -0.732856  1.038281  2.796603  6.051e+00
Perf         0.025121  0.030873  0.033896  0.036988  4.286e-02
Age          0.618743  0.883011  1.026760  1.168860  1.450e+00
Age2        -0.029854 -0.024580 -0.021886 -0.019203 -1.424e-02
NatTeam      0.605030  0.763154  0.846671  0.930593  1.095e+00
Goals        0.006893  0.008987  0.010097  0.011201  1.330e-02
Exp          0.038522  0.128032  0.175261  0.222110  3.118e-01
Exp2        -0.011484 -0.007671 -0.005685 -0.003706  5.502e-05
sigma2       0.813118  0.915664  0.977298  1.043661  1.189e+00
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\item \textbf{The market value of soccer players in Europe continues}

Program from scratch the Gibbs sampling algorithm in the example of the market value of soccer players at the 0.75 quantile.
       
	\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, quantile regression}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
Data <- read.csv("DataApplications/1ValueFootballPlayers.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- log(Value); X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
RegLS <- lm(y ~ X -1)
k <- dim(X)[2]; N <- dim(X)[1]
# Hyperparameters
b0 <- rep(0, k); c0 <- 1000
B0 <- c0*diag(k); B0i <- solve(B0)
# MCMC parameters
mcmc <- 5000; burnin <- 1000
tot <- mcmc + burnin; thin <- 1
# Quantile
tau <- 0.5; theta <- (1-2*tau)/(tau*(1-tau))
psi2 <- 2/(tau*(1-tau)); an2 <- 2+theta^2/psi2
# Gibbs sampler
PostBeta <- function(e){
	Bn <- solve(B0i + psi2^(-1)*t(X)%*%diag(1/e)%*%X)
	bn <- Bn%*%(B0i%*%b0 + psi2^(-1)*t(X)%*%diag(1/e)%*%(y-theta*e))
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostE <- function(Beta, i){
	dn2 <-(y[i]-X[i,]%*%Beta)^2/psi2
	ei <- GIGrvg::rgig(1, chi = dn2, psi = an2, lambda = 1/2)
	return(ei)
}
PostBetas <- matrix(0, mcmc+burnin, k)
Beta <- RegLS$coefficients
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	e <- sapply(1:N, function(i){PostE(Beta = Beta, i)})
	Beta <- PostBeta(e = e)
	PostBetas[s,] <- Beta
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
\end{lstlisting}
	\end{VF}
\end{tcolorbox}	

\item Use the \textit{bayesboot} package to perform inference in the simulation exercise of Section 7.10, and compared the results with the ones that we get using our GUI setting $S=10000$. 

	\textbf{Answer}
	
\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, quantile regression}
	\begin{VF}
		\begin{lstlisting}[language=R]
########################## Bayesian bootstrap: Simulation ##########################
rm(list = ls())
set.seed(010101)
N <- 1000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(x1, x2)
k <- dim(X)[2]
B <- rep(1, k+1)
sig2 <- 1
u <- rnorm(N, 0, sig2)
y <- cbind(1, X)%*%B + u
data <- as.data.frame(cbind(y, X))
names(data) <- c("y", "x1", "x2")
Reg <- function(d){
	Reg <- lm(y ~ x1 + x2, data = d)
	Bhat <- Reg$coef
	return(Bhat)
}
Reg(data)
S <- 10000
BB <- bayesboot::bayesboot(data = data, statistic = Reg, R = S)
plot(BB)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}	

\end{enumerate}