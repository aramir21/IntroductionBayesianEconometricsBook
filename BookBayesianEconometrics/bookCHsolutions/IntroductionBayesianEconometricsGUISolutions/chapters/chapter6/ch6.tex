\chapter{Univariate models}\label{chap6}

\section{Solutions of Exercises}\label{sec61}
\begin{enumerate}[leftmargin=*]

	\item Get the posterior conditional distributions of the Gaussian linear model assuming independent priors $\pi(\bm{\beta},\sigma^2)=\pi(\bm{\beta})\times\pi(\sigma^2)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$ and $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$.
	
\textbf{Answer}
	
The joint posterior distribution of the parameters is
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|\bf{y}, \bf{X}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, \bf{X}) \pi(\bm{\beta}) \pi(\sigma^2) \\
	&\propto (\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta} _0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&\propto\exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}} \exp\left\{-\frac{\delta_0+(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})}{2\sigma^2}\right\}\\
	&=\exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\times\underbrace{\frac{1}{(\sigma^2)^{(\frac{\alpha_n}{2}+1)}} \exp\left\{-\frac{\delta_n}{2\sigma^2}\right\}}_1.
\end{align*}

Observe that (1) is the kernel of an inverse-gamma density function. Thus, $\sigma^2|\bm{\beta},{\bf{y}},{\bf{X}} \sim IG(\alpha_n/2,\delta_n/2)$, where $\alpha_n=\alpha_0+N$ and $\delta_n=\delta_0+(\bf{y}-\bf{X}\bm{\beta})^{\top}(\bf{y}-\bf{X}\bm{\beta})$.

Let's see the posterior distribution of $\bm{\beta}$, 
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|{\bf{y}}, {\bf{X}}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, {\bf{X}}) \pi(\bm{\beta}) \pi(\sigma^2) \\
	&\propto (\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}({\bf{y}}-{\bf{X}}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}{\bf{B}}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&=(\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{\sigma^{-2}}{2}[{\bf{y}}^{\top}{\bf{y}}-{\bf{y}}^{\top}X\bm{\beta}-\bm{\beta}^{\top}{\bf{X}}^{\top}{\bf{y}}+\bm{\beta}^{\top}{\bf{X}}^{\top}{\bf{X}}\bm{\beta}]\right\}\\
	& \times \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}{\bf{B}}_0^{-1}\bm{\beta}-\bm{\beta}^{\top}{\bf{B}}_0^{-1}\bm{\beta}_0-\bm{\beta}_0^{\top}{\bf{B}}_0^{-1}\bm{\beta}+\bm{\beta}_0^{\top}{\bf{B}}_0^{-1}\bm{\beta}_0]\right\}\\
	&\times \frac{(\delta_0/2)^{(\alpha_0/2)}}{\Gamma(\alpha_0/2)}\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
	&\propto \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}})\bm{\beta}-2\bm{\beta}^{\top}({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})]\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}}{2\sigma^2}\right\},
\end{align*}

where $\hat{\bm{\beta}}=({\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{X}}^{\top}{\bf{y}}$.

Adding and subtracting $\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n$ where
\begin{align*}
	{\bf{B}}_n & = ({\bf{B}}_0^{-1}+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}})^{-1}\\
	\bm{\beta}_n&={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{y}}),
\end{align*}

and completing the square
\begin{align*}
	\pi(\bm{\beta}, \sigma^2|{\bf{y}}, {\bf{X}}) & \propto \exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}({\bf{B}}_0^{-1}+\sigma^{-2} {\bf{X}}^{\top}{\bf{X}})\bm{\beta}-2\bm{\beta}^{\top}{\bf{B}}_n^{-1}{\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}{\bf{X}}\hat{\bm{\beta}})\right.\\
	&\left.+\bm{\beta}^{\top}_n{\bf{B}}_n^{-1}\bm{\beta}_n-\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n]\right\}\times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}}{2\sigma^2}\right\}\\
	&=\exp\left\{-\frac{1}{2}[\bm{\beta}^{\top}{\bf{B}}_n^{-1}\bm{\beta}-2\bm{\beta}^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n+\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n]\right\}\\
	& \times\frac{1}{(\sigma^2)^{(\alpha_0+N)/2+1}}\exp\left\{-\frac{\delta_0+{\bf{y}}^{\top}{\bf{y}}-\sigma^2\bm{\beta}_n^{\top}{\bf{B}}_n^{-1}\bm{\beta}_n}{2\sigma^2}\right\}\\
	&=\underbrace{\exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_n)^{\top}{\bf{B}}_n^{-1}(\bm{\beta}-\bm{\beta}_n)\right\}}_1\\
	&\times(\sigma^2)^{-(\frac{\alpha_n}{2}+1)}\exp\left\{-\frac{\delta^{\ast}}{2\sigma^2}\right\},
\end{align*}

where $\delta^{\ast}=\delta_0+{\bf{y}}^{\top}{\bf{y}}+\sigma^2\bm{\beta}^{\top}_n{\bf{B}}_n^{-1}\bm{\beta}_n$ does not dependent on $\bm{\beta}$.

We can see that (1) is the kernel of a multivariate normal distribution with mean equal to $\bm{\beta}_n$ and covariance matrix ${\bf{B}}_n$, that is, $\bm{\beta}|\sigma^2,{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$. 

We see that the posterior distributions are from the same family as the prior distributions.

\item Show that the posterior conditional distributions of the Gaussian linear model with heteroskedasticity assuming independent priors $\pi(\bm{\beta},\sigma^2,\bm{\tau})=\pi(\bm{\beta})\times\pi(\sigma^2)\times\prod_{i=1}^N\pi(\tau_i)$, where $\bm{\beta} \sim N(\bm{\beta}_0, {\bf{B}}_0)$, $\sigma^2 \sim IG(\alpha_0/2, \delta_0/2)$ and $\tau_i\sim G(v/2,v/2)$ are $\bm{\beta}|\sigma^2,\bm{\tau},{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$, $\sigma^2|\bm{\beta},\bm{\tau},{\bf{y}},{\bf{X}}\sim IG(\alpha_n,\delta_n)$ and $\tau_i|\bm{\beta},\sigma^2,{\bf{y}},{\bf{X}}\sim G(v_{1n},v_{2in})$, where $\bm{\tau}=[\tau_1,\dots,\tau_n]^{\top}$, ${\bf{B}}_n=({\bf{B}}_0^{-1}+\sigma^{-2}{{\bf{X}}}^{\top}\Psi{{\bf{X}}})^{-1}$, $\bm{\beta}_n={\bf{B}}_n({\bf{B}}_0^{-1}\bm{\beta}_0+\sigma^{-2}{\bf{X}}^{\top}\Psi{\bf{y}})$, $\alpha_n=\alpha_0+N$, $\delta_n=\delta_0+({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})$, $v_{1n}=v+1$, $v_{2in}=v+\sigma^{-2}(y_i-{\bf{x}}_i^{\top}\bm{\beta})^2$, and $\Psi=\text{diagonal}\left\{\tau_i\right\}$.

\textbf{Answer}

The joint posterior distribution of the parameters is
\begin{align*}
	\pi(\bm{\beta}, \sigma^2, \bm{\tau}|\bf{y}, {\bf{X}}) & \propto p({\bf{y}}|\bm{\beta}, \sigma^2, \bm{\tau}, {\bf{X}}) \pi(\bm{\beta}) \pi(\sigma^2) \prod_{i=1}^N \pi(\tau_i) \\
	&\propto \left(\prod_{i=1}^N \tau_i^{1/2}\right)(\sigma^2)^{-\frac{N}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})\right\}\\
	&\times \exp\left\{-\frac{1}{2}(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)\right\}\\
	&\times\frac{1}{(\sigma^2)^{(\alpha_0/2+1)}}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\prod_{i=1}^N \tau_i^{v/2-1}\exp\left\{-v\tau_i/2\right\}.
\end{align*}

Thus, the conditional posterior distribution of $\sigma^2|\bm{\beta}, \bm{\tau}, \bf{y}, {\bf{X}}$ is given by

\begin{align*}
	\pi(\sigma^2|\bm{\beta}, \bm{\tau}, \bf{y}, {\bf{X}}) & \propto (\sigma^2)^{-\frac{N+\alpha_0}{2}}\exp\left\{-\frac{1}{2\sigma^2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})+\delta_0\right\}.
\end{align*}

This is the kernel of an inverse-gamma distribution with shape parameter $\alpha_n$ and rate parameter $\delta_n$.

The conditional posterior distribution of $\tau_i|\bm{\beta}, \sigma^2, \bf{y}, {\bf{X}}$ is
\begin{align*}
	\pi(\tau_i|\bm{\beta}, \sigma^2,\bf{y}, {\bf{X}}) & \propto
	\tau_i^{(v+1)/2-1}\exp\left\{-\frac{\tau_i}{2}[\sigma^{-2}({\bf{y}}_i-{\bf{x}}_i^{\top}\bm{\beta})^2+v]\right\}.
\end{align*}

The conditional posterior distribution of $\bm{\beta}|\sigma^2, \bm{\tau}, \bf{y}, {\bf{X}}$ is given by

\begin{align*}
	\pi(\bm{\beta}| \sigma^2, \bm{\tau}, \bf{y}, {\bf{X}}) & \propto \exp\left\{-\frac{1}{2}[\sigma^{-2}({\bf{y}}-{\bf{X}}\bm{\beta})^{\top}\Psi({\bf{y}}-{\bf{X}}\bm{\beta})+(\bm{\beta} - \bm{\beta}_0)^{\top}\bf{B}_0^{-1}(\bm{\beta} - \bm{\beta}_0)]\right\}.
\end{align*} 

Following same steps as in the previous exercise we get $\bm{\beta}|\sigma^2,\bm{\tau},{\bf{y}},{\bf{X}}\sim N(\bm{\beta}_n,{\bf{B}}_n)$.

\item \textbf{The market value of soccer players in Europe continues}

Use the setting of the previous exercise to perform inference using a Gibbs sampling algorithm of the the market value of soccer players in Europe setting $v=5$ and same other hyperparameters as the homoscedastic case. Is there any meaningful difference for the coefficient associated with the national team compared to the application in the homoscedastic case?

\textbf{Answer}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler (heteroskedastic case)}
	\begin{VF}
		\begin{lstlisting}[language=R]		
rm(list = ls())
set.seed(010101)
########################## Linear regression: Value of soccer players ##########################
Data <- read.csv("DataApplications/1ValueFootballPlayers.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
y <- log(Value) 
# Value: Market value in Euros (2017) of soccer players
# Regressors quantity including intercept
X <- cbind(1, Perf, Age, Age2, NatTeam, Goals, Exp, Exp2)
# Perf: Performance. Perf2: Performance squared. Age: Age; Age: Age squared. 
# NatTeam: Indicator of national team. Goals: Scored goals. Goals2: Scored goals squared
# Exp: Years of experience. Exp2: Years of experience squared. Assists: Number of assists
k <- dim(X)[2]
N <- dim(X)[1]
# Hyperparameters
d0 <- 0.001/2
a0 <- 0.001/2
b0 <- rep(0, k)
c0 <- 1000
B0 <- c0*diag(k)
B0i <- solve(B0)
v <- 5
# MCMC parameters
mcmc <- 5000
burnin <- 5000
tot <- mcmc + burnin
thin <- 1
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
an <- a0 + N
v1n <- v + 1
# Gibbs sampling functions
PostSig2 <- function(Beta, tau){
	dn <- d0 + t(y - X%*%Beta)%*%diag(tau)%*%(y - X%*%Beta)
	sig2 <- invgamma::rinvgamma(1, shape = an/2, rate = dn/2)
	return(sig2)
}
PostBeta <- function(sig2, tau){
	Bn <- solve(B0i + sig2^(-1)*t(X)%*%diag(tau)%*%X)
	bn <- Bn%*%(B0i%*%b0 + sig2^(-1)*t(X)%*%diag(tau)%*%y)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostTau <- function(sig2, Beta, i){
	v2n <- v + sig2^(-1)*(y[i]-X[i,]%*%Beta)^2
	taui <- rgamma(1, v1n, v2n)
	return(taui)
}			
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. The value of soccer players, programming our Gibbs sampler (heteroskedastic case)}
	\begin{VF}
		\begin{lstlisting}[language=R]		
PostBetas <- matrix(0, mcmc+burnin, k)
PostSigma2 <- rep(0, mcmc+burnin)
Beta <- rep(0, k)
tau <- rep(1, N)
for(s in 1:tot){
	sig2 <- PostSig2(Beta = Beta, tau = tau)
	PostSigma2[s] <- sig2
	Beta <- PostBeta(sig2 = sig2, tau = tau)
	PostBetas[s,] <- Beta
	tau <- sapply(1:N, function(i){PostTau(sig2 = sig2, Beta = Beta, i)})
}
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "Perf", "Age", "Age2", "NatTeam", "Goals", "Exp", "Exp2")
summary(coda::mcmc(PosteriorBetas))
PosteriorSigma2 <- PostSigma2[keep]
summary(coda::mcmc(PosteriorSigma2))
summary(coda::mcmc(exp(PosteriorBetas[,5])-1))
Iterations = 1:5000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 5000 
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean             SD       Naive SE Time-series SE 
1.246823       0.235881       0.003336       0.004010 
2. Quantiles for each variable:
2.5%   25%   50%   75% 97.5% 
0.815 1.086 1.238 1.395 1.744 					
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 
We see in this application that the value of a top soccer player in Europe increases 124\% ($\exp(0.80)-1)$) on average when he has played in the national team, the credible interval at 95\% is (81\%, 174\%). These values are not very different from the application assuming homoscedasticity in the book.


	\item \textbf{Example: Determinants of hospitalization continues}

Program a Gibbs sampling algorithm in the application of determinants of hospitalization.

\textbf{Answer}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
set.seed(010101)
Data <- read.csv("DataApplications/2HealthMed.csv", sep = ",", header = TRUE, fileEncoding = "latin1")
attach(Data)
str(Data)
y <- Hosp # Dependent variables
X <- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors
K <- dim(X)[2] 
N <- dim(X)[1]
# Hyperparameters
b0 <- rep(0, K) # Prio mean
B0 <- diag(K) # Prior covariance
B0i <- solve(B0)
mcmc <- 1000; burnin <- 500; thin <- 2; tot <- mcmc + burnin; keep <- seq(burnin, tot, thin)
# Posterior distributions programming the Gibbs sampling
# Auxiliary parameters
XtX <- t(X)%*%X
# Gibbs sampling functions
PostBeta <- function(Yl){
	Bn <- solve(B0i + XtX)
	bn <- Bn%*%(B0i%*%b0 + t(X)%*%Yl)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
PostYl <- function(Beta, i){
	Ylmean <- X[i,]%*%Beta
	if(y[i] == 1){
		Yli <- truncnorm::rtruncnorm(1, a = 0, b = Inf, mean = Ylmean, sd = 1)
	}else{
		Yli <- truncnorm::rtruncnorm(1, a = -Inf, b = 0, mean = Ylmean, sd = 1)
	}
	return(Yli)
}
PostBetas <- matrix(0, mcmc+burnin, K)
Beta <- rep(0, K)
# create progress bar in case that you want to see iterations progress
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	Yl <- sapply(1:N, function(i){PostYl(Beta = Beta, i)})
	Beta <- PostBeta(Yl = Yl)
	PostBetas[s,] <- Beta
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0), "% done"))	
}
close(pb)
keep <- seq((burnin+1), tot, thin)
PosteriorBetas <- PostBetas[keep,]
colnames(PosteriorBetas) <- c("Intercept", "SHI", "Female", "Age", "Age2", "Est2", "Est3", "Fair", "Good", "Excellent")
summary(coda::mcmc(PosteriorBetas))
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

	\item \textbf{Choice of the fishing mode continues} 

Run the Algorithm A3 of the book to show the results of the Geweke \cite{Geweke1992}, Raftery \cite{Raftery1992} and Heidelberger \cite{Heidelberger1983} tests using our GUI.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Determinants of hospitalization, programming our Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
GewekeTestLocationCoef
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 
cte_1  cte_2  cte_3 NAS_1_1 NAS_1_2 NAS_1_3    AS_1   AS_2 
-1.821 -0.714  0.792  2.275 -3.944 -2.071  1.627 -2.729 

RafteryTestLocationCoef
Quantile (q) = 0.5
Accuracy (r) = +/- 0.025
Probability (s) = 0.95 
Burn-in  Total   Lower bound  Dependence
(M)      (N)     (Nmin)       factor (I)
cte_1   780      365690  1537         238.0     
cte_2   360      193950  1537         126.0     
cte_3   660      340120  1537         221.0     
NAS_1_1 120      70320   1537          45.8     
NAS_1_2 475      243960  1537         159.0     
NAS_1_3 440      248930  1537         162.0     
AS_1    3010     1438135 1537         936.0     
AS_2    550      297770  1537         194.0     

HeidelTestLocationCoef
Stationarity start     p-value 
test         iteration         
cte_1   passed       6001      6.54e-01
cte_2   failed         NA      3.72e-02
cte_3   failed         NA      4.99e-02
NAS_1_1 failed         NA      4.77e-07
NAS_1_2 failed         NA      1.82e-05
NAS_1_3 failed         NA      1.19e-04
AS_1    passed       2001      3.71e-01
AS_2    passed       8001      4.48e-01
Halfwidth Mean     Halfwidth
test                        
cte_1   passed    -0.34236 0.017025 
cte_2   <NA>            NA       NA 
cte_3   <NA>            NA       NA 
NAS_1_1 <NA>            NA       NA 
NAS_1_2 <NA>            NA       NA 
NAS_1_3 <NA>            NA       NA 
AS_1    passed    -0.00708 0.000306 
AS_2    passed     0.27982 0.009994 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

	\item \textbf{Simulation exercise of the multinomial logit model continues}

Perform inference in the simulation of the multinomial logit model using the command \textit{rmnlIndepMetrop} from the \textit{bayesm} package of \textbf{R} and using our GUI.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the multinomial logit model}
	\begin{VF}
		\begin{lstlisting}[language=R]
remove(list = ls())
set.seed(12345)
# Simulation of data
N<-1000  # Sample Size
B<-c(0.5,0.8,-3)
B1<-c(-2.5,-3.5,0)
B2<-c(1,1,0)
# Alternative specific attributes of choice 1, for instance, price, quality and duration of choice 1
X1<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B)) 
# Alternative specific attributes of choice 2, for instance, price, quality and duration of choice 2
X2<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
# Alternative specific attributes of choice 3, for instance, price, quality and duration of choice 3
X3<-matrix(cbind(rnorm(N,0,1),rnorm(N,0,1),rnorm(N,0,1)),N,length(B))
X4<-matrix(rnorm(N,1,1),N,1)
V1<-B2[1]+X1%*%B+B1[1]*X4
V2<-B2[2]+X2%*%B+B1[2]*X4
V3<-B2[3]+X3%*%B+B1[3]*X4
suma<-exp(V1)+exp(V2)+exp(V3)
p1<-exp(V1)/suma
p2<-exp(V2)/suma
p3<-exp(V3)/suma
p<-cbind(p1,p2,p3)
y<- apply(p,1, function(x)sample(1:3, 1, prob = x, replace = TRUE))
table(y)
L <- length(table(y))
dat <-data.frame(mode,X1[,1],X2[,1],X3[,1],X1[,2],X2[,2],X3[,2],X1[,3],X2[,3],X3[,3],X4)
colnames(dat) <- c("mode","V1.1","V1.2","V1.3","V2.1","V2.2","V2.3","V3.1","V3.2","V3.3","V4")
attach(dat)
LongData <- mlogit::mlogit.data(dat, shape = "wide", varying=2:10, choice = "mode")
Xa <- cbind(LongData$V1, LongData$V2, LongData$V3)
Xa <- cbind(X1[,1],X2[,1],X3[,1],X1[,2],X2[,2],X3[,2],X1[,3],X2[,3],X3[,3])
na <- 3
Xd <- X4
X <- bayesm::createX(p = L, na = na, nd = 1, Xa = Xa, Xd = Xd, base = L)
DataMlogit <- list(y=y, X = X, p = L)
# MCMC parameters
mcmc <- 11000+1
thin <- 5
df <- 6
mcmcpar <- list(R = mcmc, keep = 5, nu = df)
PostBeta <- bayesm::rmnlIndepMetrop(Data = DataMlogit, Mcmc = mcmcpar)
summary(PostBeta[["betadraw"]])
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

	\item \textbf{Determinants of preventive health care visits continues}

Program from scratch a Metropolis-within-Gibbs sampling algorithm for the application of determinants of preventive health care visits continues. 

	
\item \textbf{Simulation of the negative binomial model continues}

Perform inference in the simulation of the negative binomial model using the \textit{bayesm} package in \textbf{R} software.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R. code. Simulation of the negative binomial model}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(010101)
N <- 2000 # Sample size
x1 <- runif(N); x2 <- rnorm(N)
X <- cbind(1, x1, x2)
k <- dim(X)[2]
B <- rep(1, k)
alpha <- 1.2
gamma <- exp(alpha)
lambda <- exp(X%*%B)
y <- rnbinom(N, mu = lambda, size = gamma)
table(y)
# MCMC parameters
mcmc <- 10000
burnin <- 1000
thin <- 5
iter <- mcmc + burnin
keep <- seq(burnin, iter, thin)
sbeta <- 2.93/sqrt(k); salpha <- 2.93
# Hyperparameters: Priors
B0 <- 1000*diag(k); b0 <- rep(0, k)
alpha0 <- 0.5; delta0 <- 0.1
DataNB <- list(y = y, X = X)
mcmcNB <- list(R = mcmc, keep = thin, s_beta = sbeta, s_alpha = salpha)
PriorNB <- list(betabar = b0, A = solve(B0), a = alpha0, b = delta0)
ResultBayesm <- bayesm::rnegbinRw(Data = DataNB, Mcmc = mcmcNB, Prior = PriorNB)
summary(ResultBayesm$alphadraw)
summary(ResultBayesm$betadraw)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}  
       
	
\end{enumerate}