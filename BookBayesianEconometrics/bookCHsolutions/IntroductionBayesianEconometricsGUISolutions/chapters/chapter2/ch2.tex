\chapter{Solutions of chapter 2\\
Conceptual differences: Bayesian and Frequentist approaches}\label{chap2}
\section{Solutions Exercises}\label{sec21}
\begin{enumerate}[leftmargin=*]
\item \textbf{Jeffreys-Lindley's paradox}

The \textbf{Jeffreys-Lindley's paradox} \cite{Jeffreys1961,lindley1957statistical} is an apparent disagreement between the Bayesian and Frequentist frameworks to a hypothesis testing situation.

In particular, assume that in a city 49,581 boys and 48,870 girls have been born in 20 years. Assume that the male births is distributed Binomial with probability $\theta$. We want to test the null hypothesis $H_0. \ \theta=0.5$ versus $H_1. \ \theta\neq 0.5$.

\begin{itemize}
	\item Show that the posterior model probability for the model under the null is approximately 0.95. Assume $\pi(H_0)=\pi(H_1)=0.5$, and $\pi(\theta)$ equal to $\mathcal{U}(0,1)$ under $H_1$.
	\item Show that the \textit{p}-value for this hypothesis test is equal to 0.023 using the normal approximation, $Y\sim \mathcal{N}(N\times \theta, N\times \theta \times (1-\theta))$. 
\end{itemize}

\textbf{Answer}

\begin{itemize}
	\item The marginal likelihood under the null hypothesis is $p(y|H_0)={n \choose y}\theta^y(1-\theta)^{n-y}=1.95e-04$ given $\theta=0.5$ under $H_0$, $N=49,581+48,870$ and $y=49,581$. On the other hand, the marginal likelihood under the alternative hypothesis is

\begin{align*}
	p(y|H_1)&=\int_{0}^{1}{n \choose y}\theta^y(1-\theta)^{n-y}d\theta\\
	&={n \choose y} B(y+1, n-k+1)\\
	&=\frac{\Gamma(N+1)}{\Gamma(y+1)\Gamma(n-y+1)}\frac{\Gamma(y+1)\Gamma(N-y+1)}{\Gamma(N+2)}\\
	&=\frac{N!}{(N+1)!}\\
	&=\frac{1}{N+1}\\
	&=1.016e-05. 
\end{align*}

Then, $PO_{01}=\frac{1.95e-04}{1.016e-05}=19.19$, this implies that the posterior model probability under the null hypothesis is $\pi(H_0|y)=\frac{19.19}{1+19.19}=0.95$.

\item Under the null hypothesis, 

\begin{align*}
	p & = 2\int_{49,581}^{\infty} (2\pi \sigma^2)^{-1/2}\exp\left\{-\frac{1}{2\sigma^2}(y-\mu)^2\right\}dy\\
	& = 0.0235,
\end{align*}
 
 where $\mu=N\times \theta=49,225.5$, and $\sigma{2}= N\times \theta \times (1-\theta)=24,612.75$.

\end{itemize}

Observe that the posterior model probability supports the null hypothesis, whereas the p-value implies rejection of the null hypothesis using a 5\% significance level.

Observe that actually this is not a paradox, as we are answering two different questions. The Bayes factor is comparing two models ($\theta = 0.5$ versus $\theta\sim\mathcal{U}(0,1)$), whereas the \textit{p}-value is checking the compatibility between $\theta=0.5$ and the sample information. Despite that $\theta=0.5$ is not compatible with sample information, it is better than the models assuming $\theta\sim\mathcal{U}(0,1)$ as most of these values of $\theta$ are far away from the sample mean. Thus, the model under the null is a bad description of the data, but it is better than the model under the alternative hypothesis.      

\item We want to test $H_0. \ \mu=\mu_0$ vs $H_1. \ \mu \neq \mu_0$ given $y_i\stackrel{iid}{\sim}N(\mu,\sigma^2)$.

Assume $\pi(H_0)=\pi(H_1)=0.5$, and $\pi(\mu,\sigma)\propto 1/\sigma$ under the alternative hypothesis.

Show that

$p(\mathbf{y}|\mathcal{M}_1)=\frac{\pi^{-N/2}}{2}\Gamma(N/2)2^{N/2}\left(\frac{1}{\alpha_n\hat{\sigma}^2}\right)^{N/2}\left(\frac{N}{\alpha_n\hat{\sigma}^2}\right)^{-1/2}\frac{\Gamma(1/2)\Gamma(\alpha_n/2)}{\Gamma((\alpha_N+1)/2)}$ and $p(\mathbf{y}|\mathcal{M}_0)=(2\pi)^{-N/2}\left[\frac{2}{\Gamma(N/2)}\left(\frac{N}{2}\frac{\sum_{i=1}^N(y_i-\mu_0)^2}{N}\right)^{N/2}\right]^{-1}$. Then,

\begin{align*}
	PO_{01}&=\frac{p(\mathbf{y}|\mathcal{M}_0)}{p(\mathbf{y}|\mathcal{M}_1)}\\
	& =\frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_N/2)}(\alpha_n\hat{\sigma}^2/N)^{-1/2}\left[1+\frac{(\mu_0-\bar{y})^2}{\alpha_n\hat{\sigma}^2/N}\right]^{-\left(\frac{\alpha_n+1}{2}\right)},
\end{align*}

where $\alpha_N=N-1$ and $\hat{\sigma}^2=\frac{\sum_{i=1}^N (y_i-\bar{y})^2}{N-1}$. 

Find the relationship between the posterior odds and the classical test statistic for the null hypothesis. 

\textbf{Answer}
{\footnotesize{
\begin{align*}
	p(\mathbf{y}|\mathcal{M}_1)&=\int_{-\infty}^{\infty}\int_{0}^{\infty} (2\pi)^{-N/2}\sigma^{-N}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\frac{1}{\sigma}d\sigma d\mu\\
	&=(2\pi)^{-N/2}\int_{-\infty}^{\infty}\int_{0}^{\infty} \sigma^{-(N+1)}\exp\left\{-\frac{N}{2\sigma^2}\frac{\sum_{i=1}^N (y_i-\mu)^2}{N}\right\}d\sigma d\mu\\
	&=(2\pi)^{-N/2}\frac{\Gamma(N/2)}{2}2^{N/2}\int_{-\infty}^{\infty}\left[\sum_{i=1}^N (y_i-\mu)^2\right]^{-N/2}d\mu\\
	&=(2\pi)^{-N/2}\frac{\Gamma(N/2)}{2}2^{N/2}\int_{-\infty}^{\infty}\left[\sum_{i=1}^N [(y_i-\bar{y})-(\mu-\bar{y})]^2\right]^{-N/2}d\mu\\
	&=(2\pi)^{-N/2}\frac{\Gamma(N/2)}{2}2^{N/2}\int_{-\infty}^{\infty}\left[\alpha_n\hat{\sigma}^2+N(\mu-\bar{y})^2\right]^{-N/2}d\mu\\
	&=(2\pi)^{-N/2}\frac{\Gamma(N/2)}{2}2^{N/2}\left(\frac{\alpha_n\hat{\sigma}^2}{\alpha_n\hat{\sigma}^2}\right)^{-N/2}\int_{-\infty}^{\infty}\left[\alpha_n\hat{\sigma}^2+N(\mu-\bar{y})^2\right]^{-N/2}d\mu\\
	&=(2\pi)^{-N/2}\frac{\Gamma(N/2)}{2}2^{N/2}\left(\alpha_n\hat{\sigma}^2\right)^{-N/2}\int_{-\infty}^{\infty}\left[1+\frac{N(\mu-\bar{y})^2}{\alpha_n\hat{\sigma}^2}\right]^{-N/2}d\mu\\
	&=\frac{\pi^{-N/2}}{2}\Gamma(N/2)2^{N/2}\left(\frac{1}{\alpha_n\hat{\sigma}^2}\right)^{N/2}\left(\frac{N}{\alpha_n\hat{\sigma}^2}\right)^{-1/2}\frac{\Gamma(1/2)\Gamma(\alpha_n/2)}{\Gamma((\alpha_N+1)/2)}.
\end{align*} 
}}
The third line takes into account that the integral in the second line is the kernel of an inverted-gamma distribution, and the last line takes into account that the integral in the previous line is the kernel of a student's t distribution \cite{zellner1996introduction}.


\begin{align*}
	p(\mathbf{y}|\mathcal{M}_0)&=\int_{0}^{\infty} (2\pi)^{-N/2}\sigma^{-N}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\frac{1}{\sigma}d\sigma \\
	&=(2\pi)^{-N/2}\int_{0}^{\infty} \sigma^{-(N+1)}\exp\left\{-\frac{N}{2\sigma^2}\frac{\sum_{i=1}^N (y_i-\mu)^2}{N}\right\}d\sigma \\
	&=(2\pi)^{-N/2}\left[\frac{2}{\Gamma(N/2)}\left(\frac{N}{2}\frac{\sum_{i=1}^N(y_i-\mu_0)^2}{N}\right)^{N/2}\right]^{-1}.
\end{align*} 

The third line takes into account that the integral in the second line is the kernel of an inverted-gamma distribution \cite{zellner1996introduction}.

Given these results is easy to get $PO_{01}$.

In addition, 
\begin{align*}
	PO_{01}&=\frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_N/2)}(\alpha_n\hat{\sigma}^2/N)^{-1/2}\left[1+\frac{(\mu_0-\bar{y})^2}{\alpha_n\hat{\sigma}^2/N}\right]^{-\left(\frac{\alpha_n+1}{2}\right)}\\
	&=\frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_N/2)}(\alpha_n\hat{\sigma}^2/N)^{-1/2}\left[1+\frac{1}{\alpha_n}\left(\frac{\mu_0-\bar{y}}{\hat{\sigma}/\sqrt{N}}\right)^2\right]^{-\left(\frac{\alpha_n+1}{2}\right)}\\
		&=\frac{\Gamma((\alpha_n+1)/2)}{\Gamma(1/2)\Gamma(\alpha_N/2)}(\alpha_n\hat{\sigma}^2/N)^{-1/2}\left[1+\frac{1}{\alpha_n}t^2\right]^{-\left(\frac{\alpha_n+1}{2}\right)},	
\end{align*}

where $t=\frac{\bar{y}-\mu_0}{\hat{\sigma}/\sqrt{N}}$ is the classical statistical test. Then, as $t$ increases then the $PO_{01}$ decreases, both indicating support against the null hypothesis $H_0. \ \mu=\mu_0$. However, there are other terms affecting the posterior odds, then, there is no necessary agreement between the classical test statistic and the posterior odds.  

\item Using the setting of the \textbf{Example: Math test} in subsection 2.6.1 in the book, test $H_0. \ \mu=\mu_0$ vs $H_1. \ \mu \neq \mu_0$ where $\mu_0=\left\{100, 100.5, 101, 101.5, 102 \right\}$.

\begin{itemize}
	\item What is the \textit{p}-value for these hypothesis tests?
	\item Find the posterior model probability of the null model for each $\mu_0$.
\end{itemize} 


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Example: Math test}
\begin{VF}
\begin{lstlisting}[basicstyle=\scriptsize, language=R]
N <- 50 # Sample size
y_bar <- 102 # Sample mean 
s2 <- 10 # Sample variance
alpha <- N - 1
serror <- (s2/N)^0.5 
y.H0 <- c(100, 100.5, 101, 101.5, 102)
test <- (y.H0 - y_bar)/serror
pval <- 2*pt(test, alpha)
pval
0.0000459 0.0015431 0.0299338 0.2690040 1
# p-values
PO01 <- (gamma(N/2)*((N-1)*serror^2)^(-0.5)*
(1+test^2/alpha)^(-N/2))/(gamma(1/2)*gamma((N-1)/2))
PO01/(1+PO01)
0.0001705 0.0050345 0.0725330 0.3210223 0.4702050
# Posterior model probability of the null hypothesis.

\end{lstlisting}
\end{VF}
\end{tcolorbox}

	
\end{enumerate}