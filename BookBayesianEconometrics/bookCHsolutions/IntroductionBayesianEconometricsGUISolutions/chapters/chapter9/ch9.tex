\chapter{Longitudinal/Panel data models}\label{chap9}

\section{Solutions of Exercises}\label{sec91}
\begin{enumerate}[leftmargin=*]

	\item Show that the posterior distribution of $\bm{\beta}|\sigma^2,\bm{D}$ is $N(\bm{\beta}_n,\bm{B}_n)$, where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$ and $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$.
	
	\textbf{Answer}

{\footnotesize	
	\begin{align*}
		\pi(\bm{\beta}|\sigma^2, \bm{D},\bm{y},\bm{X},\bm{W}) & \propto \exp\left\{-\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta})^{\top}\bm{V}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta})\right\}\\
		&\times \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_0)^{\top}\bm{B}_0^{-1}(\bm{\beta}-\bm{\beta}_0)\right\}\\
		& \propto \exp\left\{-\frac{1}{2}\left(-2\bm{\beta}^{\top}\left(\sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i+\bm{B}_0^{-1}\bm{\beta}_0\right)+\bm{\beta}^{\top}\left(\sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i+\bm{B}_0^{-1}\right)\bm{\beta}\right)\right\}\\
		& = \exp\left\{-\frac{1}{2}(-2\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{B}_n\left(\sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i+\bm{B}_0^{-1}\bm{\beta}_0\right)+\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{\beta})\right\}\\
		& = \exp\left\{-\frac{1}{2}(-2\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{\beta}_n+\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{\beta})\right\}. 
	\end{align*} 
}

We can complete the square in this expression by adding and subtracting $\bm{\beta}_n^{\top}\bm{B}_n^{-1}\bm{\beta}_n$. Thus,

	\begin{align*}
	\pi(\bm{\beta}|\sigma^2, \bm{D},\bm{y},\bm{X},\bm{W}) & \propto \exp\left\{-\frac{1}{2}(-2\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{\beta}_n+\bm{\beta}^{\top}\bm{B}_n^{-1}\bm{\beta}+\bm{\beta}_n^{\top}\bm{B}_n^{-1}\bm{\beta}_n-\bm{\beta}_n^{\top}\bm{B}_n^{-1}\bm{\beta}_n)\right\}\\
	&\propto \exp\left\{-\frac{1}{2}(\bm{\beta}-\bm{\beta}_n)^{\top}\bm{B}_n^{-1}(\bm{\beta}-\bm{\beta}_n)\right\}.
\end{align*} 
This is the kernel of a multivariate random variable with mean $\bm{\beta}_n$ and variance matrix $\bm{B}_n$.
	
	\item \textbf{The relation between productivity and public investment example continues}

\begin{itemize}
	\item Perform inference of this example using our GUI.
	\item Program from scratch a Gibbs sampling algorithm to perform this application. Set $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=5$ and $\bm{D}_0=\bm{I}_1$.
	\item Perform inference in this example assuming that $\mu_{it}|\tau_{it}\sim N(0, \sigma^2/\tau_{it})$ and $\tau_{it}\sim G(v/2,v/2)$ setting $v=5$. 
\end{itemize}

\textbf{Answer}
\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
DataGSP <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/8PublicCap.csv", sep = ",", header = TRUE, quote = "")
attach(DataGSP)
N <- length(unique(id))
y <- log(gsp)
NT <- length(y)
X <- cbind(1, log(pcap), log(pc), log(emp), unemp)
K1 <- dim(X)[2]
W <- matrix(rep(1, NT), NT, 1)
K2 <- dim(W)[2]
mcmc <- 10000; burnin <- 5000; thin <- 1; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- 5; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
PostBeta <- function(sig2, D){
	XVX <- matrix(0, K1, K1)
	XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Wi <- W[ids, ]
		Vi <- sig2*diag(Ti) + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi)
		Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		yi <- y[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
Postb <- function(Beta, sig2, D){
	Di <- solve(D)
	bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]
		Xi <- X[ids, ]
		yi <- y[ids]
		Wtei <- sig2^(-1)*t(Wi)%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(as.matrix(bis))
}
PostSig2 <- function(Beta, bs){
	an <- a0 + 0.5*NT
	ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]
		yi <- y[ids]
		Wi <- W[ids, ]
		ei <- yi - Xi%*%Beta - Wi*bs[i, ]
		etei <- t(ei)%*%ei
		ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
PostD <- function(bs){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]
		btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1); PostDs <- matrix(0, tot, K2*(K2+1)/2)
PostSig2s <- rep(0, tot); Postbs <- array(0, c(N, K2, tot))
RegLS <- lm(y ~ X - 1); SumLS <- summary(RegLS)
Beta <- SumLS[["coefficients"]][,1]
sig2 <- SumLS[["sigma"]]^2; D <- diag(K2)
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	bs <- Postb(Beta = Beta, sig2 = sig2, D = D)
	D <- PostD(bs = bs)
	Beta <- PostBeta(sig2 = sig2, D = D)
	# Beta <- PostBetaNew(sig2 = sig2, D = D)
	sig2 <- PostSig2(Beta = Beta, bs = bs)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	PostSig2s[s] <- sig2
	Postbs[, , s] <- bs
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]; Ds <- PostDs[keep,]
bs <- Postbs[, , keep]; sig2s <- PostSig2s[keep]
summary(coda::mcmc(Bs))
Quantiles for each variable:
				2.5%       25%       50%       75%     97.5%
var1  1.727227  1.934457  2.037411  2.138829  2.325961
var2 -0.033664  0.001015  0.018974  0.037977  0.076626
var3  0.274521  0.303619  0.318302  0.332881  0.363323
var4  0.652937  0.692479  0.712605  0.730885  0.768415
var5 -0.008724 -0.007265 -0.006548 -0.005844 -0.004508
summary(coda::mcmc(Ds))
summary(coda::mcmc(sig2s))
# Convergence diagnostics
coda::geweke.diag(Bs)
coda::raftery.diag(Bs,q=0.5,r=0.05,s = 0.95)
coda::heidel.diag(Bs)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
The likelihood function in the case with heteroskedasticity is proportional to 
	\begin{align*}
		p(\bm{\beta},\bm{b},\sigma^2,\bm{\tau}|\bm{y}, \bm{X},\bm{W}) & \propto \prod_{i=1}^N |\sigma^2 \bm{\Psi}_i|^{-1/2}\\
		&\times \exp\left\{-\frac{1}{2\sigma^2}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}\bm{\Psi}^{-1}_i(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)\right\},
	\end{align*} 
where $\bm{b}=[\bm{b}_1^{\top}, \bm{b}_2^{\top},\dots, \bm{b}_N^{\top}]^{\top}$, $\bm{\tau}=[\tau_{it}]^{\top}$ and $\bm{\Psi}_i=diag\left\{\tau_{it}^{-1}\right\}$. 

Following the same procedure as in Section 9.1 of the book, and Exercise 1 of this chapter, we have that
\begin{equation*}
	\bm{\beta}|\sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}\bm{y}_i)$ and $\bm{V}_i=\sigma^2\bm{\Psi}_i+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$.
\begin{equation*}
	\bm{b}_i|\bm{\beta},\sigma^2,\bm{\tau},\bm{D},\bm{y}, \bm{X}, \bm{W} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}))$.
\begin{equation*}
	\sigma^2| \bm{\beta}, \bm{b}, \bm{\tau}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}\bm{\Psi}_i^{-1}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.  
\begin{equation*}
	\bm{D}| \bm{b} \sim {I}{W}(d_n, \bm{D}_n),
\end{equation*}
where $d_n=d_0+N$ and $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N\bm{b}_i\bm{b}_i^{\top}$. And
\begin{equation*}
	\tau_{it}|\sigma^2, \bm{\beta}, \bm{b}, \bm{y}, \bm{X}, \bm{W} \sim {G}(v_{1n}/2, v_{2ni}/2),
\end{equation*}
where $v_{1n}=v+1$ and $v_{2ni}=v+\sigma^{-2}(y_{it}-\bm{x}_{it}^{\top}\bm{\beta}-\bm{w}_{it}^{\top}\bm{b}_i)^2$.

The following code shows to implement this for the productivity application. The results of the \textit{fixed effects} are very similar compared to the results without taken into account heteroskedasticity. 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler with heteroskedasticity}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
DataGSP <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/8PublicCap.csv", sep = ",", header = TRUE, quote = "")
attach(DataGSP)
N <- length(unique(id))
y <- log(gsp)
NT <- length(y)
X <- cbind(1, log(pcap), log(pc), log(emp), unemp)
K1 <- dim(X)[2]
W <- matrix(rep(1, NT), NT, 1)
K2 <- dim(W)[2]
mcmc <- 10000; burnin <- 5000; thin <- 1; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- 5; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001; v <- 5
# Gibbs by hand
PostBeta <- function(sig2, D, tau){
	XVX <- matrix(0, K1, K1)
	XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Wi <- W[ids, ]
		taui <- tau[ids]
		Vi <- sig2*solve(diag(1/taui)) + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi)
		Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		yi <- y[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler with heteroskedasticity}
	\begin{VF}
		\begin{lstlisting}[language=R]
Postb <- function(Beta, sig2, D, tau){
	Di <- solve(D); 	bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]; Xi <- X[ids, ]
		yi <- y[ids]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		Wtei <- sig2^(-1)*t(Wi)%*%Taui%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Taui%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostSig2 <- function(Beta, bs, tau){
	an <- a0 + 0.5*NT; ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]; yi <- y[ids]
		Wi <- W[ids, ]; taui <- tau[ids]
		Taui <- solve(diag(1/taui))
		ei <- yi - Xi%*%Beta - Wi*bs[i]
		etei <- t(ei)%*%Taui%*%ei
		ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
PostD <- function(bs){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]
		btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostTau <- function(sig2, Beta, bs){
	v1n <- v + 1
	v2n <- NULL
	for(i in 1:NT){
		Xi <- X[i, ]; yi <- y[i]
		Wi <- W[i, ]; bi <- bs[id[i],]
		v2ni <- v + sig2^(-1)*(yi - Xi%*%Beta - Wi%*%bi)^2
		v2n <- c(v2n, v2ni)
	}
	tau <- rgamma(NT, shape = rep(v1n/2, NT), rate = v2n/2)
	return(tau)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. The relationship between productivity and public investment, programming from scratch the Gibbs sampler with heteroskedasticity}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1); PostDs <- matrix(0, tot, K2*(K2+1)/2)
PostSig2s <- rep(0, tot); Postbs <- array(0, c(N, K2, tot))
PostTaus <- matrix(0, tot, NT); RegLS <- lm(y ~ X - 1)
SumLS <- summary(RegLS); Beta <- SumLS[["coefficients"]][,1]
sig2 <- SumLS[["sigma"]]^2; D <- diag(K2)
tau <- rgamma(NT, shape = v/2, rate = v/2) 
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	bs <- Postb(Beta = Beta, sig2 = sig2, D = D, tau = tau)
	D <- PostD(bs = bs)
	Beta <- PostBeta(sig2 = sig2, D = D, tau = tau)
	sig2 <- PostSig2(Beta = Beta, bs = bs, tau = tau)
	tau <- PostTau(sig2 = sig2, Beta = Beta, bs = bs)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	PostSig2s[s] <- sig2
	Postbs[, , s] <- bs
	PostTaus[s,] <- tau
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]
Ds <- PostDs[keep,]
bs <- Postbs[, , keep]
sig2s <- PostSig2s[keep]
taus <- PostTaus[keep,]
summary(coda::mcmc(Bs))
Quantiles for each variable:
				2.5%       25%       50%       75%     97.5%
var1  1.61764  1.891242  2.029985  2.165677  2.427349
var2 -0.07088 -0.016921  0.013080  0.044681  0.107360
var3  0.27259  0.312197  0.333106  0.353678  0.395123
var4  0.59507  0.664490  0.699526  0.734612  0.796977
var5 -0.01046 -0.008533 -0.007586 -0.006666 -0.004918
summary(coda::mcmc(Ds))
summary(coda::mcmc(sig2s))\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\item \textbf{Simulation exercise of the longitudinal normal model continues}

Assume that $y_{it}=\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+\beta_4 z_{i1}+b_i+w_{it1}b_{i1}+\mu_{it}$ where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $z_{i1}\sim B(0.5)$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\mu_{it}\sim N(0, 0.1^{1/2})$ $\bm{\beta}=[0.4 \ 0.6 \ -0.6 \ 0.7]^{\top}$, $i=1,2,\dots,50$, and the sample size is 2000 in an \textit{unbalanced panel structure}. In addition, we assume that $\bm{b}_i$ dependents on $\bm{z}_i=[1 \ z_{i1}]^{\top}$ such that $\bm{b}_i\sim N(\bm{Z}_i\bm{\gamma},\bm{D})$ where $\bm{Z}_i=\bm{I}_{K_2}\otimes \bm{z}_i^{\top}$, and $\bm{\gamma}=[1 \ 1 \ 1 \ 1]$. The prior for $\bm{\gamma}$ is $N(\bm{\gamma}_0,\bm{\Gamma}_0)$ where we set $\bm{\gamma}_0=\bm{0}_4$ and $\bm{\Gamma}_0=\bm{I}_4$. In addition, Set $\bm{\beta}_0=\bm{0}_4$, $\bm{B}_0=\bm{I}_4$, $\alpha_0=\delta_0=0.001$, $d_0=2$ and $\bm{D}_0=\bm{I}_2$. 
	\begin{itemize}
	\item Perform inference in this model without taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters.
	\item Perform inference in this model taking into account the dependence between $\bm{b}_i$ and $z_{i1}$, and compare the posterior estimates with the population parameters. 
\end{itemize}

\textbf{Answer}

Following the same procedure as in Section 9.1 of the book, and Exercise 1 of this chapter, we have that
\begin{equation*}
	\bm{\beta}|\sigma^2,\bm{\gamma},\bm{D},\bm{y}, \bm{X}, \bm{W}, \bm{Z} \sim {N}(\bm{\beta}_n,\bm{B}_n), 
\end{equation*}
where $\bm{B}_n = (\bm{B}_0^{-1} +\sum_{i=1}^N \bm{X}_i^{\top}\bm{V}_i^{-1}\bm{X}_i)^{-1}$, $\bm{\beta}_n= \bm{B}_n(\bm{B}_0^{-1}\bm{\beta}_0 + \sum_{i=1}^N\bm{X}_i^{\top}\bm{V}_i^{-1}(\bm{y}_i-\bm{W}_i\bm{Z}_i\bm{\gamma}))$ and $\bm{V}_i=\sigma^2\bm{I}_{T_i}+\sigma_{b}^2\bm{i}_{T_i}\bm{i}_{T_i}^{\top}$.
\begin{equation*}
	\bm{b}_i|\bm{\beta},\sigma^2,\bm{\gamma},\bm{D},\bm{y}, \bm{X}, \bm{W}, \bm{Z} \sim {N}(\bm{b}_{ni},\bm{B}_{ni}), 
\end{equation*} 
where $\bm{B}_{ni}=(\sigma^{-2}\bm{W}_i^{\top}\bm{W}_i+\bm{D}^{-1})^{-1}$ and $\bm{b}_{ni}=\bm{B}_{ni}(\sigma^{-2}\bm{W}_i^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta})+\bm{D}^{-1}\bm{Z}_i\bm{\gamma})$.
\begin{equation*}
	\sigma^2| \bm{\beta}, \bm{b}, \bm{\tau}, \bm{y}, \bm{X}, \bm{W} \sim {I}{G}(\alpha_n, \delta_n),
\end{equation*}
where $\alpha_n=\alpha_0+\frac{1}{2}\sum_{i=1}^N T_i$ and $\delta_n=\delta_0+\frac{1}{2}\sum_{i=1}^N(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)^{\top}(\bm{y}_i-\bm{X}_i\bm{\beta}-\bm{W}_i\bm{b}_i)$.  
\begin{equation*}
	\bm{D}| \bm{b}, \bm{\gamma}, \bm{Z} \sim {I}{W}(d_n, \bm{D}_n),
\end{equation*}
where $d_n=d_0+N$ and $\bm{D}_n=d_0\bm{D}_0+\sum_{i=1}^N(\bm{b}_i-\bm{Z}_i\bm{\gamma})(\bm{b}_i-\bm{Z}_i\bm{\gamma})^{\top}$. And
\begin{equation*}
	\bm{\gamma}|\bm{b}, \bm{D}, \bm{Z}\sim {N}(\bm{\gamma}_n,\bm{\Gamma}_n),
\end{equation*}
where $\bm{\Gamma}_n=(\sum_{i=1}^N \bm{Z}_i^{\top}\bm{D}^{-1}\bm{Z}_i+\bm{\Gamma}_0^{-1})^{-1}$ and $\bm{\gamma}_n=\bm{\Gamma}_n(\sum_{i=1}^N \bm{Z}_i^{\top}\bm{D}^{-1}\bm{b}_i+\bm{\Gamma}_0^{-1}\bm{\gamma}_0)$.

The following code shwos how to implement this longitudinal normal model where \textit{random effects} are correlated with regressors. We set MCMC iterations, burn-in and thinning parameters equal to 15000, 5000 and 5, respectively. In addition, $\bm{\beta}_0=\bm{0}_5$, $\bm{B}_0=\bm{I}_5$, $\alpha_0=\delta_0=0.001$, $d_0=2$, $\bm{D}_0=\bm{I}_2$, $\bm{\gamma}_0=\bm{0}_4$ and $\bm{\Gamma}_0=\bm{I}_4$.


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with fixed effects correlated to regressors}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(010101)
NT <- 2000
N <- 50
id <- c(1:N, sample(1:N, NT - N,replace=TRUE))
table(id)
x1 <- rnorm(NT); x2 <- rnorm(NT)
x3 <- rnorm(NT); z1 <- rbinom(N, 1, 0.5)
zdata <- NULL
for(i in 1:NT){
	zdatai <- z1[id[i]]
	zdata <- c(zdata, zdatai)
}
X <- cbind(x1, x2, x3, zdata)
K1 <- dim(X)[2]
w1 <- rnorm(NT) 
W <- cbind(1, w1)
K2 <- dim(W)[2]
B <- c(0.5, 0.4, 0.6, -0.6, 0.7)
D <- c(0.7, 0.6)
sig2 <- 0.1
u <- rnorm(NT, 0, sd = sig2^0.5)
Z <- cbind(1, z1)
K3 <- dim(Z)[2]
G <- rep(1, K3*K2)
b <- matrix(0, N, K2)
for(i in 1:N){
	ZGi <- t(kronecker(diag(K2), Z[i, ]))%*%G
	b[i, ] <- MASS::mvrnorm(1, ZGi, diag(D))
}
y <- NULL
for(i in 1:NT){
	yi <- X[i,]%*%B + W[i,]%*%b[id[i],] + u[i] 
	y <- c(y, yi)
}
Data <- as.data.frame(cbind(y, x1, x2, x3, zdata, w1, id))
mcmc <- 15000; burnin <- 5000; thin <- 10; tot <- mcmc + burnin
b0 <- rep(0, K1+1); B0 <- diag(K1+1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
g0 <- rep(0, K2*K3); G0 <- diag(K2*K3); G0i <- solve(G0)
Resultshreg <- MCMCpack::MCMChregress(fixed = y~x1 + x2 + x3 + zdata, random = ~w1, group="id", data = Data, burnin = burnin, mcmc = mcmc, thin = thin, 
mubeta = b0, Vbeta = B0, r = r0, R = R0, nu = a0, delta = d0)
Betas <- Resultshreg[["mcmc"]][,1:(K1+1)]
Sigma2RanEff <- Resultshreg[["mcmc"]][,c(K2*N+K1+2, 2*N+K1+1+K2^2)]
Sigma2 <- Resultshreg[["mcmc"]][,K2*N+K1+K2^2+2]
summary(Betas)
summary(Sigma2RanEff)
summary(Sigma2)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with fixed effects correlated to regressors}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBeta <- function(sig2, Gamma, D){
	XVX <- matrix(0, K1, K1)
	XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Wi <- W[ids, ]
		Vi <- sig2*diag(Ti) + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi)
		Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi
		Zi <- Z[i, ]
		yi <- y[ids]
		ZGi <- t(kronecker(diag(K2), Zi))%*%Gamma
		XVyi <- t(Xi)%*%ViInv%*%(yi - Wi%*%ZGi)
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
Postb <- function(Beta, sig2, Gamma, D){
	Di <- solve(D)
	bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]
		Xi <- X[ids, ]
		yi <- y[ids]
		Wtei <- sig2^(-1)*t(Wi)%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Wi + Di)
		Zi <- Z[i, ]
		ZGi <- t(kronecker(diag(K2), Zi))%*%Gamma
		bni <- Bni%*%(Wtei + Di%*%ZGi)
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostSig2 <- function(Beta, bs){
an <- a0 + 0.5*NT
ete <- 0
for(i in 1:N){
	ids <- which(id == i)
	Xi <- X[ids, ]
	yi <- y[ids]
	Wi <- W[ids, ]
	ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
	etei <- t(ei)%*%ei
	ete <- ete + etei
}
dn <- d0 + 0.5*ete 
sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
return(sig2)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with fixed effects correlated to regressors}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostGamma <- function(bs, D){
	Di <- solve(D)
	ZDZ <- matrix(0, K2*K3, K2*K3)
	ZDb <- matrix(0, K2*K3, 1)
	for(i in 1:N){
		Zi <- Z[i, ]
		ZZi <- t(kronecker(diag(K2), Zi))
		ZDZi <- t(ZZi)%*%Di%*%ZZi
		ZDZ <- ZDZ + ZDZi
		bi <- bs[i,]
		ZDbi <- t(ZZi)%*%Di%*%bi
		ZDb <- ZDb + ZDbi
	}
	Gn <- solve(G0i + ZDZ); gn <- Gn%*%(G0i%*%g0 + ZDb)
	Gamma <- MASS::mvrnorm(1, gn, Gn)
	return(Gamma)
}
PostD <- function(bs, Gamma){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]
		Zi <- Z[i, ]
		ZGi <- t(kronecker(diag(K2), Zi))%*%Gamma
		btbi <- (bsi-ZGi)%*%t(bsi-ZGi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostBetas <- matrix(0, tot, K1); PostGammas <- matrix(0, tot, K2*K3)
PostDs <- matrix(0, tot, K2*(K2+1)/2); PostSig2s <- rep(0, tot)
Postbs <- array(0, c(N, K2, tot)); RegLS <- lm(y ~ X - 1)
SumLS <- summary(RegLS); Beta <- SumLS[["coefficients"]][,1]
sig2 <- SumLS[["sigma"]]^2; Gamma <- rep(0, K2*K3)
D <- diag(K2); b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	bs <- Postb(Beta = Beta, sig2 = sig2, Gamma = Gamma, D = D)
	D <- PostD(bs = bs, Gamma = Gamma)
	Beta <- PostBeta(sig2 = sig2, Gamma = Gamma, D = D)
	sig2 <- PostSig2(Beta = Beta, bs = bs)
	Gamma <- PostGamma(bs = bs, D = D) 
	PostBetas[s,] <- Beta; PostDs[s,] <- matrixcalc::vech(D)
	PostSig2s[s] <- sig2; Postbs[, , s] <- bs
	PostGammas[s,] <- Gamma
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Longitudinal normal model with fixed effects correlated to regressors}
	\begin{VF}
		\begin{lstlisting}[language=R]
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]
Ds <- PostDs[keep,]
bs <- Postbs[, , keep]
sig2s <- PostSig2s[keep]
Gs <- PostGammas[keep, ]
summary(coda::mcmc(Bs))
Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
var1  0.3629  0.3812  0.3880  0.3945  0.4120
var2  0.5844  0.6032  0.6098  0.6176  0.6381
var3 -0.6114 -0.5950 -0.5880 -0.5809 -0.5605
var4 -0.4455  0.4155  0.8598  1.3047  2.0654
summary(coda::mcmc(Ds))
Quantiles for each variable:
			2.5%     25%     50%      75%  97.5%
var1  0.5858  0.7798  0.9173  1.13381 2.0153
var2 -0.3914 -0.1901 -0.1104 -0.03585 0.1250
var3  0.4285  0.5495  0.6314  0.72342 0.9662
summary(coda::mcmc(sig2s))
Quantiles for each variable:
Quantiles for each variable:
2.5%     25%     50%     75%   97.5% 
0.09609 0.11486 0.17790 0.35215 1.12537 
summary(coda::mcmc(Gs))
Quantiles for each variable:
			2.5%    25%    50%   75% 97.5%
var1  0.6552 0.8998 1.0288 1.162 1.402
var2 -0.4463 0.4056 0.7882 1.242 2.144
var3  0.8583 1.0599 1.1623 1.271 1.482
var4  0.5525 0.8430 1.0038 1.147 1.416
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
In one hand, the 95\% credible intervals of the model without taking into account the dependence between the \textit{random effects} and $z_{i1}$ overestimates the posterior mean of $\beta_4$ and the variance of $b_{i1}$. In addition, the credible intervals are very narrow. However, the variance of $\mu_i$ is well estimated. On the other hand, all the 95\% credible intervals of the model taking into account the dependence between the \textit{random effects} and $z_{i1}$ encompass the population parameters. However, there is a lot of variability in $\beta_4$ posterior estimates. The posterior mean estimate of the variance of $b_{i1}$ is close to the population value, and the location parameters associated with $z_{i1}$ in the mean of the \textit{random effects} are also well estimated by the posterior mean. However, the variance of the model is overestimated.

\item \textbf{Doctor visits in Germany continues I}

Replicate this example using our GUI, which by default does not fix the over-dispersion parameter ($\sigma^2$), and compare the results with the results of this example in Section 9.2.

\textbf{Answer}

This code replicates what we get using our GUI.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Doctor visits in Germany, results}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/9VisitDoc.csv", sep = ",", header = TRUE, quote = "")
attach(Data)
K1 <- 7; K2 <- 2; N <- 9197
b0 <- rep(0, K1); B0 <- diag(K1)
r0 <- 5; R0 <- diag(K2)
a0 <- 0.001; d0 <- 0.001
RegLogit <- glm(DocVis ~ Age + Male + Sport + LogInc + GoodHealth + BadHealth, family = binomial(link = "logit"))
SumLogit <- summary(RegLogit)
Beta0 <- SumLogit[["coefficients"]][,1]
mcmc <- 10000; burnin <- 1000; thin <- 10
# MCMChlogit
Resultshlogit <- MCMCpack::MCMChlogit(fixed = DocVis ~ Age + Male + Sport + LogInc + GoodHealth + BadHealth, random = ~Sozh, group="id", data = Data, burnin = burnin, mcmc = mcmc, thin = thin, mubeta = b0, Vbeta = B0, r = r0, R = R0, nu = a0, delta = d0, beta.start = Beta0, FixOD = 0)
Betas <- Resultshlogit[["mcmc"]][,1:K1]
Sigma2RanEff <- Resultshlogit[["mcmc"]][,c(K2*N+K1+1, 2*N+K1+K2^2)]
summary(Betas)
summary(Sigma2RanEff)
summary(Sigma2)
Quantiles for each variable:
				   2.5%        25%        50%        75%      97.5%
Intercept  -9.48e-01 -5.27e-01 -2.86e-01 -6.28e-02  2.99e-01
Age         4.88e-03  7.27e-03  8.35e-03  9.31e-03  1.09e-02
Male       -9.90e-01 -9.52e-01 -9.31e-01 -9.10e-01 -8.75e-01
Sport       1.80e-01  2.25e-01  2.55e-01  2.85e-01  3.47e-01
LogInc      1.38e-01  1.85e-01  2.15e-01  2.49e-01  3.01e-01
GoodHealth -9.56e-01 -9.18e-01 -8.97e-01 -8.75e-01 -8.36e-01
BadHealth   1.08e+00  1.14e+00  1.19e+00  1.23e+00  1.28e+00
D11		      1.43e+00  1.53e+00  1.58e+00  1.62e+00  1.70e+00
D21		     -2.67e-01  1.94e-02  1.85e-01  4.08e-01  5.81e-01
D12		     -2.67e-01  1.94e-02  1.85e-01  4.08e-01  5.81e-01
D22         6.05e-01  7.99e-01  9.75e-01  1.21e+00  1.62e+00
sigma2      6.37e-02  7.76e-02  1.01e-01  1.15e-01  1.58e-01
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

We see in this example running our GUI that we get qualitatively same results as the results fixing the over-dispersion parameter in Section 9.2 in the book. However, there small numerical differences in the \textit{fixed effects}, and the differences are greater in the posterior results of the covariance matrix of the \textit{random effects}.

\item \textbf{Simulation exercise of the longitudinal logit model}

Perform a simulation exercise to assess the performance of the hierarchical longitudinal logit model. The point of departure is to assume that $y_{it}^*=\beta_0+\beta_1x_{it1}+\beta_2x_{it2}+\beta_3x_{it3}+b_i+w_{it1}b_{i1}$ where $x_{itk}\sim N(0,1)$, $k=1,2,3$, $w_{it1}\sim N(0,1)$, $b_i\sim N(0, 0.7^{1/2})$, $b_{i1}\sim N(0, 0.6^{1/2})$, $\bm{\beta}=[0.5 \ 0.4 \ 0.6 \ -0.6]^{\top}$, $i=1,2,\dots,50$, and $y_{it}\sim B(\pi_{it}$, where $\pi_{it}=1/(1+\exp(-y_{it}^*))$. The sample size is 1000 in an \textit{unbalanced panel structure}.

\begin{itemize}
	\item Perform inference using the command $MCMChlogit$ fixing the over-dispersion parameter, and using $\bm{\beta}_0=\bm{0}_4$, $\bm{B}_0=\bm{I}_4$, $\alpha_0=\delta_0=0.001$, $d_0=2$ and $\bm{D}_0=\bm{I}_2$.
	\item Program from scratch a Metropolis-within-Gibbs algorithm to perform inference in this simulation.  
\end{itemize}

\textbf{Answer}

The following code shows the simulation setting, and the results using the \textit{MCMChlogit} command. We see that all posterior estimates encompass the population values.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal logit model using MCMChlogit}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls()); set.seed(010101)
NT <- 1000; N <- 50
id <- c(1:N, sample(1:N, NT - N,replace=TRUE))
x1 <- rnorm(NT); x2 <- rnorm(NT); x3 <- rnorm(NT) 
X <- cbind(1, x1, x2, x3); K1 <- dim(X)[2]
w1 <- rnorm(NT); W <- cbind(1, w1); K2 <- dim(W)[2]
B <- c(0.5, 0.4, 0.6, -0.6); D <- c(0.7, 0.6)
sig2 <- 0.1
b1 <- rnorm(N, 0, sd = D[1]^0.5)
b2 <- rnorm(N, 0, sd = D[2]^0.5)
b <- cbind(b1, b2)
yl <- NULL
for(i in 1:NT){
	ylmeani <- X[i,]%*%B + W[i,]%*%b[id[i],]
	yli <- rnorm(1, ylmeani, sig2^0.5)
	yl <- c(yl, yli)
}
pit <- 1/(1+exp(-yl))
y <- rbinom(NT, 1, prob = pit)
Data <- as.data.frame(cbind(y, x1, x2, x3, w1, id))
mcmc <- 15000; burnin <- 5000; thin <- 10; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
RegLogit <- glm(y ~ X - 1, family = binomial(link = "logit"))
SumLogit <- summary(RegLogit)

Beta0 <- SumLogit[["coefficients"]][,1]
sig20 <- sum(SumLogit[["deviance.resid"]]^2)/SumLogit[["df.residual"]]
Resultshlogit <- MCMCpack::MCMChlogit(fixed = y~x1 + x2 + x3, random = ~w1, group="id", data = Data, burnin = burnin, mcmc = mcmc, thin = thin, 
mubeta = b0, Vbeta = B0, r = r0, R = R0, nu = a0, delta = d0,
beta.start = Beta0, FixOD = 1, sigma2.start = sig20)
Betas <- Resultshlogit[["mcmc"]][,1:K1]
Sigma2RanEff <- Resultshlogit[["mcmc"]][,c(K2*N+K1+1, 2*N+K1+K2^2)]
Sigma2 <- Resultshlogit[["mcmc"]][,K2*N+K1+K2^2+1]
summary(Betas)
Quantiles for each variable:
							2.5%     25%     50%     75%   97.5%
beta.(Intercept)  0.4258  0.6165  0.7140  0.8155  1.0041
beta.x1           0.3294  0.4372  0.4999  0.5617  0.6684
beta.x2           0.5555  0.6763  0.7388  0.8062  0.9377
beta.x3          -0.9327 -0.7957 -0.7276 -0.6591 -0.5383
summary(Sigma2RanEff)
Quantiles for each variable:
2.5%    25%    50%    75% 97.5%
D11 0.3394 0.5366 0.6962 0.8652 1.338
D22 0.4101 0.6478 0.8091 1.0039 1.611
\end{lstlisting}
	\end{VF}
\end{tcolorbox}

The following code shows the algorithm step by step.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal logit model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
LatentMHV1 <- function(tuning, Beta, bs, sig2){
	ylhat <- rep(0, NT)
	accept <- NULL
	for(i in 1:NT){
		ids <- which(id == i)
		yi <- y[i]
		ylhatmeani <- X[i,]%*%Beta + W[i,]%*%bs[id[i],]
		ylhati <- rnorm(1, ylhatmeani, sd = sig2^0.5)
		pihati <- 1/(1+exp(-ylhati))
		ei <- rnorm(1, 0, sd = tuning)
		ylpropi <- ylhati + ei; pipropi <- 1/(1+exp(-ylpropi))
		logPosthati <- sum(dbinom(yi, 1, prob = pihati, log = TRUE) + dnorm(ylhati, ylhatmeani, sig2^0.5, log = TRUE))
		logPostpropi <- sum(dbinom(yi, 1, prob = pipropi, log = TRUE) + dnorm(ylpropi, ylhatmeani, sig2^0.5, log = TRUE))
		alphai <- min(1, exp(logPostpropi - logPosthati))
		ui <- runif(1)
		if(ui <= alphai){
			ylhati <- ylpropi; accepti <- 1
		}else{
			ylhati <- ylhati; accepti <- 0
		}
		ylhat[i] <- ylhati; accept <- c(accept, accepti)
	}
	res <- list(ylhat = ylhat, accept = mean(accept))
	return(res)
}
PostBeta <- function(D, ylhat, sig2){
	XVX <- matrix(0, K1, K1); XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i); Ti <- length(ids)
		Wi <- W[ids, ]; Vi <- diag(Ti)*sig2 + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi); Xi <- X[ids, ]
		XVXi <- t(Xi)%*%ViInv%*%Xi; XVX <- XVX + XVXi
		yi <- ylhat[ids]; XVyi <- t(Xi)%*%ViInv%*%yi
		XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX)
	bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
Postb <- function(Beta, D, ylhat, sig2){
	Di <- solve(D); bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i)
		Wi <- W[ids, ]; Xi <- X[ids, ]
		yi <- ylhat[ids]
		Wtei <- sig2^(-1)*t(Wi)%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Wi + Di)
		bni <- Bni%*%Wtei; bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal logit model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostD <- function(bs){
	rn <- r0 + N
	btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]; btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb
	Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostSig2 <- function(Beta, bs, ylhat, ss){
	an <- a0 + 0.5*NT; ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Xi <- X[ids, ]; yi <- ylhat[ids]
		Wi <- W[ids, ]
		ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
		etei <- t(ei)%*%ei; ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	if(sig2 > ss){
		sig2 <- ss
	}else{
		sig2 <- sig2
	}
	return(sig2)
}
PostBetas <- matrix(0, tot, K1)
PostDs <- matrix(0, tot, K2*(K2+1)/2)
Postbs <- array(0, c(N, K2, tot))
PostSig2s <- rep(0, tot)
Accepts <- rep(NULL, tot)
RegLogit <- glm(y ~ X - 1, family = binomial(link = "logit"))
SumLogit <- summary(RegLogit)
Beta <- SumLogit[["coefficients"]][,1]
sig2 <- sum(SumLogit[["deviance.resid"]]^2)/SumLogit[["df.residual"]]
ss0 <- sig2; D <- diag(K2)
bs1 <- rnorm(N, 0, sd = D[1]^0.5)
bs2 <- rnorm(N, 0, sd = D[2]^0.5)
bs <- cbind(bs1, bs2)
tuning <- 0.1; ropt <- 0.44
tunepariter <- seq(round(tot/10, 0), tot, round(tot/10, 0));   l <- 1
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
\end{lstlisting}
	\end{VF}
\end{tcolorbox}


\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Simulation exercise: Hierarchical longitudinal logit model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
for(s in 1:tot){
	LatY <- LatentMHV1(tuning = tuning, Beta = Beta, bs = bs, sig2 = sig2)
	ylhat <- LatY[["ylhat"]]
	bs <- Postb(Beta = Beta, D = D, ylhat = ylhat, sig2 = sig2)
	D <- PostD(bs = bs)
	Beta <- PostBeta(D = D, ylhat = ylhat, sig2 = sig2)
	sig2 <- PostSig2(Beta = Beta, bs = bs, ylhat = ylhat, ss = ss0)
	PostBetas[s,] <- Beta
	PostDs[s,] <- matrixcalc::vech(D)
	Postbs[, , s] <- bs
	PostSig2s[s] <- sig2
	AcceptRate <- LatY[["accept"]]
	Accepts[s] <- AcceptRate
	if(AcceptRate > ropt){
		tuning = tuning*(2-(1-AcceptRate)/(1-ropt))
	}else{
		tuning = tuning/(2-AcceptRate/ropt)
	}
	if(s == tunepariter[l]){
		print(AcceptRate)
		l <- l + 1
	}
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]
Ds <- PostDs[keep,]
bs <- Postbs[, , keep]
sig2s <- PostSig2s[keep]
summary(coda::mcmc(Bs))
Quantiles for each variable:
			2.5%     25%     50%     75%   97.5%
var1  0.3195  0.5266  0.6325  0.7452  0.9427
var2  0.1780  0.3563  0.4521  0.5436  0.7352
var3  0.3596  0.5197  0.6140  0.7165  0.9149
var4 -0.8994 -0.7153 -0.6125 -0.5135 -0.3240
summary(coda::mcmc(Ds))
Quantiles for each variable:
			2.5%        25%        50%       75%   97.5%
var1  0.0002003  0.0007473  0.0022547 0.0124378 0.09575
var2 -0.0534437 -0.0029207 -0.0001433 0.0009905 0.02296
var3  0.0002150  0.0009101  0.0026712 0.0119391 0.14863
summary(coda::mcmc(sig2s))
Quantiles for each variable:
2.5%    25%    50%    75%  97.5% 
0.1321 0.5174 0.8674 1.0788 1.1909 
\end{lstlisting}
	\end{VF}
\end{tcolorbox}
The 95\% credible intervals of the \textit{fixed effects} encompass all the population values. However, we do not get good posterior estimates of the covariance matrix and over-dispersion parameters.

\item \textbf{Doctor visits in Germany continues II} 

Take a sub-sample of the first 500 individuals of the datatset \textit{9VisitDoc.csv} to perform inference in the number of visits to doctors (\textit{DocNum}) with the same specification of the example of \textbf{Doctor visits in Germany} of Section 9.2.	

\textbf{Answer}

This algorithm shows how to perform inference in this application programming from scratch the algorithm.

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Doctor visits in Germany: Hierarchical longitudinal Poisson model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
rm(list = ls())
set.seed(12345)
Data <- read.csv("https://raw.githubusercontent.com/besmarter/BSTApp/refs/heads/master/DataApp/9VisitDoc.csv", sep = ",", header = TRUE, quote = "")
library(dplyr)
Data <- Data %>% 
filter(id <= 500)
attach(Data)
K1 <- 7; K2 <- 2; N <- 9197
b0 <- rep(0, K1); B0 <- diag(K1)
r0 <- 5; R0 <- diag(K2)
a0 <- 0.001; d0 <- 0.001
NT <- dim(Data)[1]
N <- length(unique(id))
X <- cbind(1, Age, Male, Sport, LogInc, GoodHealth, BadHealth)
K1 <- dim(X)[2]
W <- cbind(1, Sozh)
K2 <- dim(W)[2]
y <- DocNum
mcmc <- 15000; burnin <- 5000; thin <- 10; tot <- mcmc + burnin
b0 <- rep(0, K1); B0 <- diag(K1); B0i <- solve(B0) 
r0 <- K2; R0 <- diag(K2); a0 <- 0.001; d0 <- 0.001
LatentMHV1 <- function(tuning, Beta, bs, sig2){
	ylhat <- rep(0, NT)
	accept <- NULL
	for(i in 1:NT){
		ids <- which(id == i)
		yi <- y[i]
		ylhatmeani <- X[i,]%*%Beta + W[i,]%*%bs[id[i],]
		ylhati <- rnorm(1, ylhatmeani, sd = sig2^0.5)
		lambdahati <- exp(ylhati)
		ei <- rnorm(1, 0, sd = tuning)
		ylpropi <- ylhati + ei
		lambdapropi <- exp(ylpropi)
		logPosthati <- sum(dpois(yi, lambdahati, log = TRUE) + dnorm(ylhati, ylhatmeani, sig2^0.5, log = TRUE))
		logPostpropi <- sum(dpois(yi, lambdapropi, log = TRUE) + dnorm(ylpropi, ylhatmeani, sig2^0.5, log = TRUE))
		alphai <- min(1, exp(logPostpropi - logPosthati))
		ui <- runif(1)
		if(ui <= alphai){
			ylhati <- ylpropi; accepti <- 1
		}else{
			ylhati <- ylhati; accepti <- 0
		}
		ylhat[i] <- ylhati
		accept <- c(accept, accepti)
	}
	res <- list(ylhat = ylhat, accept = mean(accept))
	return(res)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Doctor visits in Germany: Hierarchical longitudinal Poisson model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBeta <- function(D, ylhat, sig2){
	XVX <- matrix(0, K1, K1); XVy <- matrix(0, K1, 1)
	for(i in 1:N){
		ids <- which(id == i); Ti <- length(ids)
		Wi <- matrix(W[ids, ], Ti, K2)
		Vi <- diag(Ti)*sig2 + Wi%*%D%*%t(Wi)
		ViInv <- solve(Vi); Xi <- matrix(X[ids, ], Ti, K1)
		XVXi <- t(Xi)%*%ViInv%*%Xi
		XVX <- XVX + XVXi; yi <- ylhat[ids]
		XVyi <- t(Xi)%*%ViInv%*%yi; XVy <- XVy + XVyi
	}
	Bn <- solve(B0i + XVX); bn <- Bn%*%(B0i%*%b0 + XVy)
	Beta <- MASS::mvrnorm(1, bn, Bn)
	return(Beta)
}
Postb <- function(Beta, D, ylhat, sig2){
	Di <- solve(D); bis <- matrix(0, N, K2)
	for(i in 1:N){
		ids <- which(id == i); Ti <- length(ids)
		Wi <- matrix(W[ids, ], Ti, K2)
		Xi <- matrix(X[ids, ], Ti, K1)
		yi <- ylhat[ids]
		Wtei <- sig2^(-1)*t(Wi)%*%(yi - Xi%*%Beta)
		Bni <- solve(sig2^(-1)*t(Wi)%*%Wi + Di)
		bni <- Bni%*%Wtei
		bi <- MASS::mvrnorm(1, bni, Bni)
		bis[i, ] <- bi
	}
	return(bis)
}
PostD <- function(bs){
	rn <- r0 + N; btb <- matrix(0, K2, K2)
	for(i in 1:N){
		bsi <- bs[i, ]; btbi <- bsi%*%t(bsi)
		btb <- btb + btbi
	}
	Rn <- d0*R0 + btb; Sigma <- MCMCpack::riwish(v = rn, S = Rn)
	return(Sigma)
}
PostSig2 <- function(Beta, bs, ylhat){
	an <- a0 + 0.5*NT; ete <- 0
	for(i in 1:N){
		ids <- which(id == i)
		Ti <- length(ids)
		Xi <- matrix(X[ids, ], Ti, K1); yi <- ylhat[ids]
		Wi <- matrix(W[ids, ], Ti, K2)
		ei <- yi - Xi%*%Beta - Wi%*%bs[i, ]
		etei <- t(ei)%*%ei; ete <- ete + etei
	}
	dn <- d0 + 0.5*ete 
	sig2 <- MCMCpack::rinvgamma(1, shape = an, scale = dn)
	return(sig2)
}
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

\begin{tcolorbox}[enhanced,width=4.67in,center upper,
	fontupper=\large\bfseries,drop shadow southwest,sharp corners]
	\textit{R code. Doctor visits in Germany: Hierarchical longitudinal Poisson model from scratch}
	\begin{VF}
		\begin{lstlisting}[language=R]
PostBetas <- matrix(0, tot, K1)
PostDs <- matrix(0, tot, K2*(K2+1)/2)
Postbs <- array(0, c(N, K2, tot))
PostSig2s <- rep(0, tot); ccepts <- rep(NULL, tot)
RegPois <- glm(DocNum ~ Age + Male + Sport + LogInc + GoodHealth + BadHealth, family = poisson(link = "log"))
SumPois <- summary(RegPois)
Beta <- SumPois[["coefficients"]][,1]
sig2 <- sum(SumPois[["deviance.resid"]]^2)/SumPois[["df.residual"]]
D <- diag(K2)
bs1 <- rnorm(N, 0, sd = D[1,1]^0.5)
bs2 <- rnorm(N, 0, sd = D[2,2]^0.5)
bs <- cbind(bs1, bs2); tuning <- 0.1; ropt <- 0.44
tunepariter <- seq(round(tot/10, 0), tot, round(tot/10, 0));   l <- 1
pb <- winProgressBar(title = "progress bar", min = 0, max = tot, width = 300)
for(s in 1:tot){
	LatY <- LatentMHV1(tuning = tuning, Beta = Beta, bs = bs, sig2 = sig2)
	ylhat <- LatY[["ylhat"]]
	bs <- Postb(Beta = Beta, D = D, ylhat = ylhat, sig2 = sig2)
	D <- PostD(bs = bs)
	Beta <- PostBeta(D = D, ylhat = ylhat, sig2 = sig2)
	sig2 <- PostSig2(Beta = Beta, bs = bs, ylhat = ylhat)
	PostBetas[s,] <- Beta; PostDs[s,] <- matrixcalc::vech(D)
	Postbs[, , s] <- bs; PostSig2s[s] <- sig2
	AcceptRate <- LatY[["accept"]]; Accepts[s] <- AcceptRate
	if(AcceptRate > ropt){
		tuning = tuning*(2-(1-AcceptRate)/(1-ropt))
	}else{
		tuning = tuning/(2-AcceptRate/ropt)
	}
	if(s == tunepariter[l]){
		print(AcceptRate)
		l <- l + 1
	}
	setWinProgressBar(pb, s, title=paste( round(s/tot*100, 0),"% done"))
}
close(pb)
keep <- seq((burnin+1), tot, thin)
Bs <- PostBetas[keep,]; Ds <- PostDs[keep,]
bs <- Postbs[, , keep]; sig2s <- PostSig2s[keep]
summary(coda::mcmc(Bs))
Quantiles for each variable:
2.5%       25%        50%      75%    97.5%
var1 -1.250036 -0.269211  0.1995811  0.69139  1.59234
var2 -0.001542  0.004852  0.0083935  0.01147  0.01815
var3 -0.484097 -0.331748 -0.2586891 -0.19011 -0.04787
var4 -0.023677  0.123336  0.1973927  0.26883  0.41314
var5 -0.196732 -0.071421 -0.0001036  0.06601  0.19356
var6 -0.815499 -0.657182 -0.5784964 -0.49969 -0.36882
var7  0.613990  0.799534  0.8982645  1.00161  1.18880
\end{lstlisting}
	\end{VF}
\end{tcolorbox} 

We see that males with good perception of health status visit less frequently the physicians, and women with bad perception of health status visit more frequently the physicians.

\end{enumerate}