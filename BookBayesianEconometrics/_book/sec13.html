<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Bayesian reports: Decision theory under uncertainty | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2021-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec12.html"/>
<link rel="next" href="summary-chapter-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a><ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec25A.html"><a href="sec25A.html"><i class="fa fa-check"></i><b>2.6</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.7</b> A simple working example</a></li>
<li class="chapter" data-level="2.8" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.8</b> Summary: Chapter 2</a></li>
<li class="chapter" data-level="2.9" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.9</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a><ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a><ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>4.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="4.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>4.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="4.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.5</b> Computational examples</a></li>
<li class="chapter" data-level="4.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>4.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="4.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a><ul>
<li class="chapter" data-level="6.1" data-path="normal-model.html"><a href="normal-model.html"><i class="fa fa-check"></i><b>6.1</b> Normal model</a></li>
<li class="chapter" data-level="6.2" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i><b>6.2</b> Logit model</a></li>
<li class="chapter" data-level="6.3" data-path="probit-model.html"><a href="probit-model.html"><i class="fa fa-check"></i><b>6.3</b> Probit model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec13" class="section level2">
<h2><span class="header-section-number">1.3</span> Bayesian reports: Decision theory under uncertainty</h2>
<p>The Bayesian framework allows reporting the full posterior distributions. However, some situations demand to report a specific value of the posterior distribution (point estimate), an informative interval (set), point or interval predictions and/or selecting a specific model. Decision theory offers an elegant framework to make a decision regarding what are the optimal posterior values to report <span class="citation">(Berger <a href="#ref-berger2013statistical" role="doc-biblioref">2013</a>)</span>.</p>
<p>The point of departure is a <em>loss function</em>, which is a non-negative real value function whose arguments are the unknown <em>state of nature</em> at time <span class="math inline">\(t\)</span> (<span class="math inline">\(\mathbf{\Theta}\)</span>), and a set of <em>actions</em> to be made (<span class="math inline">\(\mathcal{A}\)</span>), that is,
<span class="math display">\[\begin{equation}
L(\mathbf{\theta}, a):\mathbf{\Theta}\times \mathcal{A}\rightarrow R^+.
\end{equation}\]</span></p>
<p>This function is a mathematical expression of the loss of making mistakes. In particular, selecting action <span class="math inline">\(a\in\mathcal{A}\)</span> when <span class="math inline">\(\mathbf{\theta}\in\mathbf{\Theta}\)</span> is the true. In our case, the unknown state of nature can be population parameters, functions of them, future or unknown dgp realizations, models, etc.</p>
<p>From a Bayesian perspective, we should choose the action (<span class="math inline">\(\delta(\mathbf{y})\)</span>) that minimizes the posterior expected loss, which is the <em>posterior risk function</em> (<span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}]\)</span>),</p>
<p><span class="math display">\[\begin{equation}
  \delta(\mathbf{y})=\underset{\mathbf{\theta} \in \mathbf{\Theta}}{argmin} \  \mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}], 
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[L(\mathbf{\theta}, a)|\mathbf{y}]= \int_{\mathbf{\Theta}} L(\mathbf{\theta}, a)\pi(\mathbf{\theta}|\mathbf{y})d\mathbf{\theta}\)</span>.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>Obviously, different loss functions imply different optimal decisions. We illustrate this assuming <span class="math inline">\(\theta \in R\)</span>.</p>
<ul>
<li><span class="math inline">\(L({\theta},a)=[{\theta}-a]^2\)</span>, then</li>
</ul>
<p><span class="math display">\[\begin{equation}
  \mathbb{E}[{\theta}|\mathbf{y}] = \underset{{\theta} \in {\Theta}}{argmin} \  \int_{{\Theta}} [{\theta}-a]^2\pi({\theta}|\mathbf{y})d{\theta}.
\end{equation}\]</span></p>
<p>Using the first condition order with respect to <span class="math inline">\(a\)</span>, and interchanging differentiation with integrals, we get that the posterior mean is the Bayesian optimal action, that is, <span class="math inline">\(\delta(\mathbf{y})=\mathbb{E}[{\theta}|\mathbf{y}]\)</span>.</p>
<ul>
<li><p><span class="math inline">\(L({\theta},a)=w({\theta})[{\theta}-a]^2\)</span>, where <span class="math inline">\(w({\theta})&gt;0\)</span> is a weighting function. Then using same steps as the previous result we have that <span class="math inline">\(\delta(\mathbf{y})=\frac{\mathbb{E}[w({\theta})\times{\theta}|\mathbf{y}]}{\mathbb{E}[w({\theta})|\mathbf{y}]}\)</span>, that is, the Bayesian optimal action is a weighted average driven by <span class="math inline">\(w({\theta})\)</span>.</p></li>
<li><p><span class="math inline">\(L({\theta},a)=|{\theta}-a|\)</span>, then we have to find <span class="math inline">\(\delta(\mathbf{y})=\underset{{\theta} \in {\Theta}}{argmin} \  \int_{{\Theta}} |{\theta}-a|\pi({\theta}|\mathbf{y})d{\theta}\)</span>, this means that <span class="math inline">\(\int_{-\infty}^a \pi({\theta}|\mathbf{y})d{\theta}=1/2\)</span>, that is, <span class="math inline">\({\delta}(\mathbf{y})\)</span> is the median (exercise).</p></li>
<li><p>Given the loss function,</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
L(\theta,a)=\begin{Bmatrix} K_0(\theta-a), \theta-a\geq 0\\
K_1(a-\theta), \theta-a &lt; 0 \end{Bmatrix},
\end{equation}\]</span></p>
<p>then,</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}[L(\theta, a)|\mathbf{y}]&amp;=\int_{-\infty}^a K_1(a-\theta)\pi(\theta|\mathbf{y})d\theta + \int_a^{\infty} K_0(\theta-a)\pi(\theta|\mathbf{y})d\theta. 
\end{align}\]</span></p>
<p>Differenting w.r.t <span class="math inline">\(a\)</span>, and equaliting to zero,
<span class="math display">\[\begin{align}
  K_1\int_{-\infty}^a \pi(\theta|\mathbf{y})d\theta-K_0\int_a^{\infty} \pi(\theta|\mathbf{y})d\theta&amp;=0,
\end{align}\]</span></p>
<p>then, <span class="math inline">\(\int_{-\infty}^a \pi(\theta|\mathbf{y})d\theta=\frac{K_0}{K_0+K_1}\)</span>, that is, any <span class="math inline">\(K_0/(K_0+K_1)\)</span>-percentile of <span class="math inline">\(\pi(\theta|\mathbf{y})\)</span> is an optimal Bayesian estimate of <span class="math inline">\(\theta\)</span>.</p>
<p>We can also use decision theory under uncertatinty in hypothesis testing. In particular, testing <span class="math inline">\(H_0:\theta\in\Theta_0\)</span> versus <span class="math inline">\(H_1:\theta\in\Theta_1\)</span>, <span class="math inline">\(\Theta=\Theta_0 \cup \Theta_1\)</span> and <span class="math inline">\(\emptyset=\Theta_0 \cap \Theta_1\)</span>, there are two actions of interest, <span class="math inline">\(a_0\)</span> and <span class="math inline">\(a_1\)</span>, where <span class="math inline">\(a_j\)</span> denotes no rejecting <span class="math inline">\(H_j\)</span>, <span class="math inline">\(j=\left\{0,1\right\}\)</span>. Given the loss function,</p>
<p><span class="math display">\[\begin{equation}
L(\theta,a_j)=\begin{Bmatrix} 0, &amp; \theta\in\Theta_j\\
K_j, &amp; \theta\in\Theta_j, j\neq i \end{Bmatrix}.
\end{equation}\]</span></p>
<p>The posterior expected loss associated with <span class="math inline">\(a_j\)</span> is <span class="math inline">\(K_jP(\Theta_i|\mathbf{y})\)</span>, <span class="math inline">\(j\neq i\)</span>. Therefore, the Bayes optimal decision is the one that gives the smallest posterior expected loss, that is, the null hypothesis is rejected (<span class="math inline">\(a_1\)</span> is not rejected), when <span class="math inline">\(K_0P(\Theta_1|\mathbf{y}) &gt; K_1P(\Theta_0|\mathbf{y})\)</span>. Given our framework <span class="math inline">\((\Theta=\Theta_0 \cup \Theta_1, \emptyset=\Theta_0 \cap \Theta_1)\)</span>, then <span class="math inline">\(P(\Theta_0|\mathbf{y})=1-P(\Theta_1|\mathbf{y})\)</span>, and as a consequence, <span class="math inline">\(P(\Theta_1|\mathbf{y})&gt;\frac{K_1}{K_1+K_0}\)</span>, that is, the rejection region of the Bayesian test is <span class="math inline">\(R=\left\{\mathbf{y}:P(\Theta_1|\mathbf{y})&gt;\frac{K_1}{K_1+K_0}\right\}\)</span>.</p>
<p>Decision theory also helps to construct interval (region) estimates. Let <span class="math inline">\(\Theta_{C(\mathbf{y})}\subset \Theta\)</span> a <em>credible</em> set for <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(L(\theta,\Theta_{C(\mathbf{y})})=1-\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}\)</span>, where</p>
<p><span class="math display">\[\begin{equation}
\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}=\begin{Bmatrix}1, &amp; \theta\in \Theta_{C(\mathbf{y})}\\  
0, &amp; \theta\notin \Theta_{C(\mathbf{y})}
\end{Bmatrix}.
\end{equation}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\begin{equation}
L(\theta,\Theta_{C(\mathbf{y})})=\begin{Bmatrix}0, &amp; \theta\in \Theta_{C(\mathbf{y})}\\  
1, &amp; \theta\notin \Theta_{C(\mathbf{y})}
\end{Bmatrix}.
\end{equation}\]</span></p>
<p>Then, the risk function is <span class="math inline">\(1-P(\theta\in \Theta_{C(\mathbf{y})})\)</span>.</p>
<p>Given a measure of <em>credibility</em> (<span class="math inline">\(\alpha(\mathbf{y})\)</span>) that defines the level of trust that <span class="math inline">\(\theta\in \Theta_{C(\mathbf{y})}\)</span>. We can measure the accuracy of the report by <span class="math inline">\(L(\theta, \alpha(\mathbf{y}))=(\mathbb{I}\left\{\theta\in \Theta_{C(\mathbf{y})}\right\}-\alpha(\mathbf{y}))^2\)</span>. This loss function could be used to suggest a choice of the report <span class="math inline">\(\alpha(\mathbf{y})\)</span>. The Bayesian optimal action is <span class="math inline">\(P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})\)</span>. This can be calculated given the posterior distirbution, that is, <span class="math inline">\(P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})=\int_{\Theta_{C(\mathbf{y})}}\pi(\theta|\mathbf{y})d\theta\)</span>. This a measure of the belief that <span class="math inline">\(\theta\in \Theta_{C(\mathbf{y})}\)</span> given the prior beliefs and sample information.</p>
<p>The set <span class="math inline">\(\Theta_{C(\mathbf{y})}\in\Theta\)</span> is a <span class="math inline">\(100(1-\alpha)\%\)</span> credible set with respect to <span class="math inline">\(\pi(\theta|\mathbf{y})\)</span> if <span class="math inline">\(P(\theta\in \Theta_{C(\mathbf{y})}|\mathbf{y})=\int_{\Theta_{C(\mathbf{y})}}\pi(\theta|\mathbf{y})=1-\alpha\)</span>.</p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> <em>highest posterior density set</em> (HPD) for <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(100(1-\alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span> with the property that it has a smaller space than any other <span class="math inline">\(100(1-\alpha)\%\)</span> credible interval for <span class="math inline">\(\theta\)</span>. That is, <span class="math inline">\(C(\mathbf{y})=\left\{\theta:\pi(\theta|\mathbf{y})\geq k(\alpha)\right\}\)</span>, where <span class="math inline">\(k(\alpha)\)</span> is the largest number such that <span class="math inline">\(\int_{\theta:\pi(\theta|\mathbf{y})\geq k(\alpha)}\pi(\theta|\mathbf{y})d\theta=1-\alpha\)</span>. The HPDs can be a collection of disjoint intervals when working with multimodal posterior densities. In addition, they have the limitation of not necessary being invariant under transformations.</p>
<p>Finally, decision theory can be used to perform prediction (point, sets or probabilistic). Suppose that one has a loss <span class="math inline">\(L(Y_0,a)\)</span> involving the prediction of <span class="math inline">\(Y_0\)</span>. Then, <span class="math inline">\(L(\theta,a)=\mathbb{E}_{\theta}^{Y_0}[Y_0,a]=\int_{\mathcal{Y}_0}L(y_0,a)g(y_0|\theta)dy_0\)</span>, where <span class="math inline">\(g(y_0|\theta)\)</span> is the density function of <span class="math inline">\(Y_0\)</span>.</p>
<p>Predictive exercises can be based on predictive densities, that is, <span class="math inline">\(\pi(Y_0|\mathbf{y})\)</span>. Then, the predictive density can be used to obtain a point prediction given a loss function <span class="math inline">\(L(Y_0,y_0)\)</span>, where <span class="math inline">\(y_0\)</span> is a point prediction for <span class="math inline">\(Y_0\)</span>. We can seek <span class="math inline">\(y_0\)</span> that minimizes the mathematical expectation of the loss function.</p>
<p>Other approach is to use scoring rules to assess the quality of the predictive (probabilistic) forecasts. This is assigning a numerical score based on the predictive distribution on the event that realizes <span class="citation">(Gneiting and Raftery <a href="#ref-Gneiting2007" role="doc-biblioref">2007</a>)</span>. Then, we can use decision theory to define the most relevant scoring rule for the problem at hand, such that we assign a high ordinate to the realized value (<em>calibration</em>). In addition, it is possible to add some reward for accuracy in specific parts of the support of the density function (<em>sharpness</em>) <span class="citation">(Diks, Panchenko, and Dijk <a href="#ref-Diks2011" role="doc-biblioref">2011</a>)</span>.</p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-berger2013statistical">
<p>Berger, James O. 2013. <em>Statistical Decision Theory and Bayesian Analysis</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-Diks2011">
<p>Diks, C., V. Panchenko, and D. van Dijk. 2011. “Likelihood-Based Scoring Rules for Comparing Density Forecasts in Tails.” <em>Journal of Econometrics</em> 163: 215–30.</p>
</div>
<div id="ref-Gneiting2007">
<p>Gneiting, T., and A. Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” <em>Journal of the American Statistical Association</em> 102 (477): 359–78.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p><span class="citation">(Chernozhukov and Hong <a href="#ref-Chernozhukov2003" role="doc-biblioref">2003</a>)</span> propose Laplace type estimators (LTE) based on the <em>quasi-posterior</em>, <span class="math inline">\(p(\mathbf{\theta})=\frac{\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})}{\int_{\mathbf{\Theta}}\exp\left\{L_n(\mathbf{\theta})\right\}\pi(\mathbf{\theta})d\theta}\)</span> where <span class="math inline">\(L_n(\mathbf{\theta})\)</span> is not necessarily a log-likelihood function. The LTE minimizes the <em>quasi-posterior risk</em>.<a href="sec13.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec12.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-chapter-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
