[["unireg.html", "Chapter 6 Univariate regression", " Chapter 6 Univariate regression We will describe how to perform Bayesian inference in univariate models: normal-inverse gamma, logit, probit, multinomial probit and logit, ordered probit, negative binomial, tobit, quantile regression, and Bayesian bootstrap in linear models. We show their formal framework, some applications, and how to perform inference using our GUI as well as R. We will also have mathematical and computational exercises in our GUI and in R. "],["normal-model.html", "6.1 Normal model", " 6.1 Normal model The Gaussian linear model specifies \\({\\bf{y}}={\\bf{X}}\\beta+\\bf{\\mu}\\) such that \\(\\bf{\\mu}\\sim N(\\bf{0},\\sigma^2\\bf{I}_N)\\) is an stochastic error, \\({\\bf{X}}\\) is a \\(N \\times K\\) matrix of regressors, \\(\\beta\\) is a \\(K\\)-dimensional vector of coefficients, \\({\\bf{y}}\\) is an \\(N\\)-dimensional vector of a dependent variable, and \\(N\\) is the number of units. The conjugate independent priors for the parameters are \\(\\beta \\sim N(\\beta_0, {\\bf{B}}_0)\\) and \\(\\sigma^2 \\sim IG(\\alpha_0/2, \\delta_0/2)\\). Given the likelihood function, \\(p(\\beta, \\sigma^2|{\\bf{y}}, {\\bf{X}}) = (2\\pi\\sigma^2)^{-\\frac{N}{2}} \\exp \\left\\{-\\frac{1}{2\\sigma^2} ({\\bf{y}} - \\bf{X\\beta})^{\\top}({\\bf{y}} - \\bf{X\\beta}) \\right\\}\\), the conditional posterior distributions are \\[\\begin{align} \\beta|\\sigma^2, {\\bf{y}}, {\\bf{X}} \\sim N(\\beta_n, \\sigma^2{\\bf{B}}_n), \\end{align}\\] and \\[\\begin{align} \\sigma^2|\\beta, {\\bf{y}}, {\\bf{X}} \\sim IG(\\alpha_n/2, \\delta_n/2), \\end{align}\\] where \\({\\bf{B}}_n = ({\\bf{B}}_0^{-1} + \\sigma^{-2} {\\bf{X}}^{\\top}{\\bf{X}})^{-1}\\), \\(\\beta_n= {\\bf{B}}_n({\\bf{B}}_0^{-1}\\beta_0 + \\sigma^{-2} {\\bf{X}}^{\\top}{\\bf{y}})\\), \\(\\alpha_n = \\alpha_0 + N\\) and \\(\\delta_n = \\delta_0 + ({\\bf{y}}-{\\bf{X}}\\beta)^{\\top}({\\bf{y}}-{\\bf{X}}\\beta)\\). We can employ the Gibbs sampler in this model due to having standard conditional posterior distributions. Application: The market value of soccer players in Europe Lets analyze the determinants of the market value of soccer players. In particular, we use the dataset 1ValueFootballPlayers.csv which is in folder DataApp (see Table 13.3 for details) in our github repository (https://github.com/besmarter/BSTApp). This dataset was used by (Serna Rodríguez, Ramírez Hassan, and Coad 2019) to finding the determinants of high performance soccer players in the five most important national leagues in Europe. The specification of the model is \\[\\begin{align} \\log(\\text{Value}_i)&amp;=\\beta_1+\\beta_2\\text{Perf}_i+\\beta_3\\text{Perf}^2_i+\\beta_4\\text{Age}_i+\\beta_5\\text{Age}^2_i+\\beta_6\\text{NatTeam}_i+\\beta_7\\text{Goals}_i\\\\ &amp;+\\beta_8\\text{Goals}^2_i+\\beta_9\\text{Exp}_i+\\beta_{10}\\text{Exp}^2_i+\\beta_{11}\\text{Assists}_i, \\end{align}\\] where Value is the market value in Euros (2017), Perf is a measure of performance, Age is the players age in years, NatTem is an indicator variable that takes the value of 1 if the player has been on the national team, Goals is the number of goals scored by the player during his career, Exp is his experience in years, and Assists is the number of assist made by the player in the 20152016 season. We assume that the dependent variable distributes normal, then we use a normal-inverse gamma model using vague conjugate priors where \\({\\bf{B}}_0=1000{\\bf{I}}_{10}\\), \\(\\beta_0={\\bf{0}}_{10}\\), \\(\\alpha_0=0.001\\) and \\(\\delta_0=0.001\\). We perform a Gibbs sampler with 10,000 MCMC iterations plus a burn-in equal to 5,000, and a thinning parameter equal to 1. set.seed(010101) # Set a seed for replicability of results # Download data from github # urlfile &lt;- &#39;https://raw.githubusercontent.com/besmarter/BSTApp/master/DataApp/1ValueFootballPlayers.csv&#39; # mydata &lt;- read.csv(urlfile) mydata &lt;- read.csv(&quot;DataApplications/1ValueFootballPlayers.csv&quot;, header = T, sep = &quot;,&quot;) attach(mydata) str(mydata) ## &#39;data.frame&#39;: 335 obs. of 13 variables: ## $ Player : Factor w/ 335 levels &quot;Aaron Cresswell&quot;,..: 194 13 72 254 162 211 76 287 165 316 ... ## $ Value : int 22000000 7500000 18000000 15000000 18000000 25000000 40000000 30000000 55000000 9000000 ... ## $ ValueCens: int 22000000 7500000 18000000 15000000 18000000 25000000 40000000 30000000 55000000 9000000 ... ## $ Perf : int 7 9 33 6 34 25 31 12 35 29 ... ## $ Perf2 : int 49 81 1089 36 1156 625 961 144 1225 841 ... ## $ Age : int 24 27 23 24 30 22 31 26 25 29 ... ## $ Age2 : int 576 729 529 576 900 484 961 676 625 841 ... ## $ NatTeam : int 1 1 1 1 1 1 1 1 1 0 ... ## $ Goals : int 0 27 27 34 0 18 26 8 28 43 ... ## $ Goals2 : int 0 729 729 1156 0 324 676 64 784 1849 ... ## $ Exp : num 10.01 16.01 7.01 8.01 12.01 ... ## $ Exp2 : num 100.2 256.4 49.1 64.1 144.2 ... ## $ Assists : int 0 0 4 1 0 0 1 0 5 7 ... y &lt;- log(Value) # Dependent variable X &lt;- cbind(1,as.matrix(mydata[, 4:13])) # Regressors N &lt;- dim(X)[1] # Sample size K &lt;- dim(X)[2] # Number of regressors including a constant # Hyperparameters B0 &lt;- 1000*diag(K) # Prior covariance matrix Normal distribution B0i &lt;- solve(B0) # Prior precision matrix Normal distribution b0 &lt;- rep(0,K) # Prior mean Normal distribution a0 &lt;- 0.001 # Prior shape parameter inverse-gamma distribution d0 &lt;- 0.001 # Prior rate parameter inverse-gamma distribution # MCMC parameters it &lt;- 10000 # Iterations after burn-in burn &lt;- 5000 # Burn-in tot &lt;- burn + it # Total iterations ###### 1. Programming the Gibbs sampler ####### # Space and initial setting betaGibbs &lt;- matrix(0,tot,K) # Space for posterior beta varGibbs &lt;- matrix(1,tot,1) # Space for posterior sigma^2 sigma2_0 &lt;- 2 # Initial sigma^2 Bn &lt;- solve(B0i+sigma2_0^{-1}*t(X)%*%X) # Initial covariance matrix beta bn &lt;- Bn%*%(B0i%*%b0+sigma2_0^{-1}*t(X)%*%y) # Initial mean beta an &lt;- a0+N # Posterior alpha for(i in 1:tot){ BetaG &lt;- MASS::mvrnorm(1, mu=bn, Sigma=Bn) # Draw of posterior beta dn &lt;- d0+t(y-X%*%BetaG)%*%(y-X%*%BetaG) # Posterior delta sigma2 &lt;- pscl::rigamma(1,an/2,dn/2) # Draw of posterior variance Bn &lt;- solve(B0i+sigma2^{-1}*t(X)%*%X) # Posterior covariance beta bn &lt;- Bn%*%(B0i%*%b0+sigma2^{-1}*t(X)%*%y) # Posterior mean beta betaGibbs[i,] &lt;- BetaG varGibbs[i,] &lt;- sigma2 } # Draws after burn-in PostPar &lt;- coda::mcmc(cbind(betaGibbs[burn:it,], varGibbs[burn:it,])) # Names colnames(PostPar) &lt;- c(&quot;Cte&quot;, names(mydata)[-c(1:3)], &quot;Variance&quot;) # Summary posterior draws summary(PostPar) ## ## Iterations = 1:5001 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 5001 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Cte 3.719e+00 2.202e+00 3.114e-02 3.188e-02 ## Perf 2.798e-02 1.650e-02 2.333e-04 2.333e-04 ## Perf2 1.604e-04 4.977e-04 7.038e-06 7.206e-06 ## Age 7.744e-01 1.794e-01 2.536e-03 2.593e-03 ## Age2 -1.646e-02 3.348e-03 4.734e-05 4.845e-05 ## NatTeam 8.492e-01 1.178e-01 1.666e-03 1.704e-03 ## Goals 1.169e-02 3.503e-03 4.953e-05 4.953e-05 ## Goals2 -2.408e-05 1.874e-05 2.649e-07 2.649e-07 ## Exp 2.011e-01 6.277e-02 8.876e-04 8.876e-04 ## Exp2 -6.878e-03 2.713e-03 3.837e-05 3.837e-05 ## Assists 2.012e-02 2.425e-02 3.430e-04 3.430e-04 ## Variance 9.745e-01 7.619e-02 1.077e-03 1.112e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Cte -6.444e-01 2.243e+00 3.7431200 5.170e+00 7.978e+00 ## Perf -4.207e-03 1.661e-02 0.0281211 3.902e-02 6.023e-02 ## Perf2 -8.097e-04 -1.724e-04 0.0001627 4.935e-04 1.124e-03 ## Age 4.227e-01 6.544e-01 0.7723884 8.947e-01 1.131e+00 ## Age2 -2.297e-02 -1.871e-02 -0.0163819 -1.419e-02 -9.984e-03 ## NatTeam 6.217e-01 7.699e-01 0.8492834 9.300e-01 1.081e+00 ## Goals 4.769e-03 9.315e-03 0.0116816 1.406e-02 1.857e-02 ## Goals2 -6.188e-05 -3.643e-05 -0.0000240 -1.157e-05 1.272e-05 ## Exp 8.013e-02 1.586e-01 0.1997035 2.441e-01 3.249e-01 ## Exp2 -1.224e-02 -8.727e-03 -0.0068425 -5.056e-03 -1.714e-03 ## Assists -2.716e-02 3.785e-03 0.0202098 3.698e-02 6.760e-02 ## Variance 8.347e-01 9.219e-01 0.9701447 1.026e+00 1.132e+00 # Trace and density plots plot(PostPar) # Autocorrelation plots coda::autocorr.plot(PostPar) # Convergence diagnostics coda::geweke.diag(PostPar) ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## Cte Perf Perf2 Age Age2 NatTeam Goals Goals2 ## -0.79112 1.45439 -1.50703 0.46945 -0.24099 -0.07577 -0.68005 0.51859 ## Exp Exp2 Assists Variance ## 0.67734 -0.99537 -0.10185 0.78761 coda::raftery.diag(PostPar,q=0.5,r=0.025,s = 0.95) ## ## Quantile (q) = 0.5 ## Accuracy (r) = +/- 0.025 ## Probability (s) = 0.95 ## ## Burn-in Total Lower bound Dependence ## (M) (N) (Nmin) factor (I) ## Cte 2 1574 1537 1.020 ## Perf 2 1478 1537 0.962 ## Perf2 2 1570 1537 1.020 ## Age 1 1541 1537 1.000 ## Age2 2 1560 1537 1.010 ## NatTeam 2 1588 1537 1.030 ## Goals 2 1590 1537 1.030 ## Goals2 1 1538 1537 1.000 ## Exp 2 1555 1537 1.010 ## Exp2 1 1542 1537 1.000 ## Assists 2 1583 1537 1.030 ## Variance 2 1589 1537 1.030 coda::heidel.diag(PostPar) ## ## Stationarity start p-value ## test iteration ## Cte passed 1 0.7888 ## Perf failed NA 0.0174 ## Perf2 passed 502 0.0584 ## Age passed 1 0.6905 ## Age2 passed 1 0.6479 ## NatTeam passed 1 0.9541 ## Goals passed 1 0.3700 ## Goals2 passed 1 0.6803 ## Exp passed 1 0.2319 ## Exp2 passed 1 0.1838 ## Assists passed 1 0.2814 ## Variance passed 1 0.7429 ## ## Halfwidth Mean Halfwidth ## test ## Cte passed 3.72e+00 6.25e-02 ## Perf &lt;NA&gt; NA NA ## Perf2 passed 1.63e-04 1.49e-05 ## Age passed 7.74e-01 5.08e-03 ## Age2 passed -1.65e-02 9.50e-05 ## NatTeam passed 8.49e-01 3.34e-03 ## Goals passed 1.17e-02 9.71e-05 ## Goals2 passed -2.41e-05 5.19e-07 ## Exp passed 2.01e-01 1.74e-03 ## Exp2 passed -6.88e-03 7.52e-05 ## Assists passed 2.01e-02 6.72e-04 ## Variance passed 9.75e-01 2.18e-03 ###### 2. Using a library: MCMCpack ####### Reg &lt;- MCMCpack::MCMCregress(y~X-1, burnin = burn, mcmc = it, b0 = b0, B0 = B0i, c0 = a0, d0 = d0) summary(Reg) ## ## Iterations = 5001:15000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 3.711e+00 2.230e+00 2.230e-02 2.230e-02 ## XPerf 2.784e-02 1.643e-02 1.643e-04 1.643e-04 ## XPerf2 1.629e-04 4.938e-04 4.938e-06 4.938e-06 ## XAge 7.754e-01 1.816e-01 1.816e-03 1.816e-03 ## XAge2 -1.648e-02 3.383e-03 3.383e-05 3.383e-05 ## XNatTeam 8.500e-01 1.181e-01 1.181e-03 1.201e-03 ## XGoals 1.175e-02 3.468e-03 3.468e-05 3.468e-05 ## XGoals2 -2.447e-05 1.865e-05 1.865e-07 1.865e-07 ## XExp 2.005e-01 6.295e-02 6.295e-04 6.295e-04 ## XExp2 -6.830e-03 2.726e-03 2.726e-05 2.726e-05 ## XAssists 2.015e-02 2.468e-02 2.468e-04 2.468e-04 ## sigma2 9.739e-01 7.696e-02 7.696e-04 7.967e-04 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X -6.129e-01 2.178e+00 3.692e+00 5.207e+00 8.100e+00 ## XPerf -4.408e-03 1.660e-02 2.793e-02 3.887e-02 6.027e-02 ## XPerf2 -8.052e-04 -1.739e-04 1.604e-04 4.941e-04 1.133e-03 ## XAge 4.167e-01 6.533e-01 7.756e-01 9.001e-01 1.125e+00 ## XAge2 -2.301e-02 -1.882e-02 -1.649e-02 -1.419e-02 -9.842e-03 ## XNatTeam 6.136e-01 7.718e-01 8.502e-01 9.299e-01 1.082e+00 ## XGoals 4.959e-03 9.401e-03 1.174e-02 1.411e-02 1.859e-02 ## XGoals2 -6.116e-05 -3.724e-05 -2.418e-05 -1.179e-05 1.182e-05 ## XExp 7.644e-02 1.580e-01 1.996e-01 2.435e-01 3.234e-01 ## XExp2 -1.228e-02 -8.685e-03 -6.820e-03 -4.986e-03 -1.469e-03 ## XAssists -2.828e-02 3.625e-03 2.019e-02 3.677e-02 6.964e-02 ## sigma2 8.316e-01 9.201e-01 9.703e-01 1.023e+00 1.134e+00 When using our GUI (third approach), the first step is to type shiny::runGitHub(besmarter/BSTApp , launch.browser=T) in the R package console or any R code editor to run our GUI,1 and then select Univariate Models on the top panel. The radio button on the left hand side shows the specific models inside this generic class. Users should select Normal model (see Figure 6.1). Figure 6.1: Univariate models: Normal/normal-inverse gamma model. The right hand side panel displays a widget to upload the input dataset, which should be a csv file with headers in the first row. Users also should select the kind of separator used in the input file: comma, semicolon, or tab (use the folders DataSim and DataApp for the input file templates). Once users upload the dataset, they can see a data preview. Range sliders help to set the number of iterations of the MCMC and the amount of burn-in, and the thinning parameter can be selected as well. After this, users should specify the equation. This can be done with the formula builder, where users can select the dependent variable, and the independent variables, and then click on the Build formula tab. Users can see in the Main Equation space the formula expressed in the format used by R. See Main equation box in Figure 6.2, observe that in this case the depedent variable is \\(\\log\\text{Value}\\) rather than \\(\\text{Value}\\), then we should modify directly the Main equation box writing \\(log(Value)\\). In general, users can modify this box if necessary, for instance, including higher order or interaction terms, other transformation are also allowed. This is done directly in the Main Equation space taking into account that this extra terms should follow formula command structure (see https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/formula). Note that the class of univariate models includes the intercept by default, except ordered probit, where the specification has to do this explicitly, that is, ordered probit models do not admit an intercept for identification issues (see below). Finally, users should define the hyperparameters of the prior; for instance, in the normal-inverse gamma model, these are the mean, covariance, shape, and scale (see Figure 6.2). However, users should take into account that our GUI has \"non-informative hyperparameters by default in all our modelling frameworks, so the last part is not a requirement. Figure 6.2: Normal/normal-inverse gamma model: Formula builder and hyperparameters. After this specification process, users should click the Go! button to initiate the estimation. Our GUI displays the summary statistics and convergence diagnostics after this process is finished (see Figure 6.3). There are also widgets to download posterior chains (csv file) and graphs (pdf and eps files). Note that the order of the coefficients in the results (summary, posterior chains, and graphs) is first for the location parameters, and then for the scale parameters. Figure 6.3: Normal/normal-inverse gamma model: Results. As expected, the results using the three approach (programming, library and GUI) are very similar. These suggest that age, squared age, national team, goals, experience, and squared experience are relevant regressors. For instance, we found that the 2.5% and 97.5% percentiles of the posterior estimate associated with the variable Goals are 4.57e-03 and 1.82e-02. These values can be used to find the 95% symmetric credible interval. This means that there is a 0.95 probability that the population parameter lies in (4.57e-03, 1.82e-02), which would suggest that this variable is relevant to explain the market value of a soccer player.2 We also found that the effect of having been on the national team has a 95% credible interval equal to (0.58, 1.04) with a median equal to 0.81, that is, an increase of the market value of the player of 124.8% (\\(\\exp(0.81)-1\\)) compared with a player that has not ever been on a national team. The posterior distribution of this variable can be seen in Figure 6.4. This graph is automatically generated by the GUI, and can be downloaded in the zip file named Posterior Graphs.csv; but we should take into account that the national team is the sixth variable, remember that by default the intercept is the first variable. Figure 6.4: Posterior distribution: National team. We show all the posterior densities as well as the trace and correlation plots to visualize convergence of the posterior chains. Trace plots look stable, and autocorrelation plots decrease very quickly. In addition, convergence statistics (Gewekes (Geweke 1992), Raftery and Lewis (Raftery and Lewis 1992), and Heidelberger and Welchs tests (Heidelberger and Welch 1983)) suggest that the posterior draws come from stationary distributions (see Chapter 10 for technical details). References "],["logit-model.html", "6.2 Logit model", " 6.2 Logit model In the logit model the dependent variable is binary, \\(Y_i=\\left\\{1,0\\right\\}\\), then it follows a Bernoulli distribution, \\(Y_i\\stackrel{ind} {\\thicksim}B(\\pi_i)\\), that is, \\(p(Y_i=1)=\\pi_i\\), such that \\(\\pi_i=\\frac{\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}{1+\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}\\). The likelihood function of the logit model is \\[\\begin{align} p(\\mathbf{y}|\\beta,\\mathbf{X})&amp;=\\prod_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\\\ &amp;=\\prod_{i=1}^N\\left(\\frac{\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}{1+\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}\\right)^{y_i}\\left(\\frac{1}{1+\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}\\right)^{1-y_i}. \\end{align}\\] I specify a Normal distribution as prior, \\(\\beta\\sim N({\\bf{\\beta}}_0,{\\bf{B}}_0)\\). Then, the posterior distribution is \\[\\begin{align} \\pi(\\beta|\\mathbf{y},\\mathbf{X})&amp;\\propto\\prod_{i=1}^N\\left(\\frac{\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}{1+\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}\\right)^{y_i}\\left(\\frac{1}{1+\\exp\\left\\{{\\bf{x}}_i^{\\top}\\beta\\right\\}}\\right)^{1-y_i}\\times\\exp\\left\\{-\\frac{1}{2}(\\beta-\\beta_0)^{\\top}\\mathbf{B}_0(\\beta-\\beta_0)\\right\\}. \\end{align}\\] The logit model does not have a standard posterior distribution. Then, a random walk MetropolisHastings algorithm can be used to obtain draws from the posterior distribution. A potential proposal is a multivariate Normal centered at the current value, with covariance matrix \\(\\tau^2({\\bf{B}}_0^{-1}+\\widehat{{\\bf{\\Sigma}}}^{-1})^{-1}\\), where \\(\\tau&gt;0\\) is a tuning parameter and \\(\\widehat{\\bf{\\Sigma}}\\) is the sample covariance matrix from the maximum likelihood estimation (Martin, Quinn, and Park 2011).3 Observe that \\(\\log(p(\\mathbf{y}|\\beta,\\mathbf{X}))=\\sum_{i=1}^Ny_i{\\bf{x}}_i^{\\top}\\beta-\\log(1+\\exp({\\bf{x}}_i^{\\top}\\beta))\\). I am going to use this expression when calculating the acceptance parameter in the computational implementation of the Metropolist-Hastings algorithm. In particular, the acceptance parameter is \\(\\alpha=\\min\\left\\{1, \\exp(\\log(p(\\mathbf{y}|\\beta^{c},\\mathbf{X}))+\\log(\\pi(\\beta^c))-(\\log(p(\\mathbf{y}|\\beta^{(s-1)},\\mathbf{X}))+\\log(\\pi(\\beta^{(s-1)}))))\\right\\}\\), where \\(\\beta^c\\) and \\(\\beta^{(s-1)}\\) are draws from the proposal distribution and previous iteration of the Markov chain, respectively.4 Simulation Lets do a simulation exercise to check the performance of the algorithms. Set \\(\\beta=\\begin{bmatrix}0.5 &amp; 0.8 &amp; -1.2\\end{bmatrix}^{\\top}\\), \\(\\mathbf{x}_{ik}\\sim N(0,1)\\), \\(k=2,3\\) and \\(i=1,2,\\dots,10000\\). set.seed(010101) # Set a seed for replicability of results N &lt;- 10000 # Sample size B &lt;- c(0.5, 0.8, -1.2) # Population location parameters x2 &lt;- rnorm(N) # Regressor x3 &lt;- rnorm(N) # Regressor X &lt;- cbind(1, x2, x3) # Regressors XB &lt;- X%*%B PY &lt;- exp(XB)/(1 + exp(XB)) # Probability of Y = 1 Y &lt;- rbinom(N, 1, PY) # Draw Y&#39;s table(Y) # Frequency ## Y ## 0 1 ## 4115 5885 write.csv(cbind(Y, x2, x3), file = &quot;DataSimulations/LogitSim.csv&quot;) # Export data ###### 1. Programming the M-H sampler ####### # This function sets the M-H sampler using as default a hyperparameter mean equal to 0 # and a covariance equal to 1000 times a identity matrix, a tunning parameter equal to 1, # 1000 post burn-in iterations, and the latter is equal to 500. MHfunc &lt;- function(y, X, b0 = rep(0, dim(X)[2] + 1), B0 = 1000*diag(dim(X)[2] + 1), tau = 1, iter = 1000, burnin = 500){ Xm &lt;- cbind(1, X) # Regressors K &lt;- dim(Xm)[2] # Number of location parameters BETAS &lt;- matrix(0, iter + burnin, K) # Space for posterior chains Reg &lt;- glm(y ~ Xm - 1, family = binomial(link = &quot;logit&quot;)) # Maximum likelihood estimation BETA &lt;- Reg$coefficients # Maximum likelihood parameter estimates tot &lt;- iter + burnin # Total iterations M-H algorithm COV &lt;- vcov(Reg) # Maximum likelihood covariance matrix COVt &lt;- tau^2*solve(solve(B0) + solve(COV)) # Covariance matrix for the proposal distribution Accep &lt;- rep(0, tot) # Space for calculating the acceptance rate # create progress bar in case that you want to see iterations progress # pb &lt;- winProgressBar(title = &quot;progress bar&quot;, min = 0, # max = tot, width = 300) for(it in 1:tot){ BETAc &lt;- BETA + MASS::mvrnorm(n = 1, mu = rep(0, K), Sigma = COVt) # Candidate location parameter likecand &lt;- sum((Xm%*%BETAc) * Y - apply(Xm%*%BETAc, 1, function(x) log(1 + exp(x)))) # Log likelihood for the candidate likepast &lt;- sum((Xm%*%BETA) * Y - apply((Xm%*%BETA), 1, function(x) log(1 + exp(x)))) # Log lkelihood for the actual draw priorcand &lt;- (-1/2)*crossprod((BETAc - b0), solve(B0))%*%(BETAc - b0) # Log prior for candidate priorpast &lt;- (-1/2)*crossprod((BETA - b0), solve(B0))%*%(BETA - b0) # Log prior for actual draw alpha &lt;- min(1, exp(likecand + priorcand - likepast - priorpast)) #Probability of selecting candidate u &lt;- runif(1) # Decision rule for selecting candidate if(u &lt; alpha){ BETA &lt;- BETAc # Changing reference for candidate if selected Accep[it] &lt;- 1 # Indicator if the candidate is accepted } BETAS[it, ] &lt;- BETA # Saving draws # setWinProgressBar(pb, it, title=paste( round(it/tot*100, 0), # &quot;% done&quot;)) } # close(pb) return(list(Bs = BETAS[-c(1:burnin), ], AceptRate = mean(Accep))) } Posterior &lt;- MHfunc(y = Y, X = cbind(x2, x3), iter = 100, burnin = 5) # Runing our M-H function changing some default parameters. paste(&quot;Acceptance rate equal to&quot;, round(Posterior$AceptRate, 2), sep = &quot; &quot;) ## [1] &quot;Acceptance rate equal to 0.49&quot; PostPar &lt;- coda::mcmc(Posterior$Bs) # Names colnames(PostPar) &lt;- c(&quot;Cte&quot;, &quot;x1&quot;, &quot;x2&quot;) # Summary posterior draws summary(PostPar) ## ## Iterations = 1:100 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 100 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Cte 0.5000 0.02025 0.002025 0.005291 ## x1 0.8366 0.02057 0.002057 0.005556 ## x2 -1.1969 0.02811 0.002811 0.008643 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Cte 0.4688 0.4872 0.4989 0.5119 0.5397 ## x1 0.8092 0.8195 0.8357 0.8512 0.8809 ## x2 -1.2475 -1.2190 -1.1906 -1.1807 -1.1396 # Trace and density plots plot(PostPar) # Autocorrelation plots coda::autocorr.plot(PostPar) # Convergence diagnostics coda::geweke.diag(PostPar) ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## Cte x1 x2 ## 1.4567 -1.4614 -0.8573 coda::raftery.diag(PostPar,q=0.5,r=0.025,s = 0.95) ## ## Quantile (q) = 0.5 ## Accuracy (r) = +/- 0.025 ## Probability (s) = 0.95 ## ## You need a sample size of at least 1537 with these values of q, r and s coda::heidel.diag(PostPar) ## ## Stationarity start p-value ## test iteration ## Cte passed 1 0.0799 ## x1 failed NA 0.0350 ## x2 passed 1 0.6687 ## ## Halfwidth Mean Halfwidth ## test ## Cte passed 0.5 0.0104 ## x1 &lt;NA&gt; NA NA ## x2 passed -1.2 0.0169 ###### 2. Using a library: MCMCpack ####### RegLog &lt;- MCMCpack::MCMClogit(Y~X-1, burnin = 1000, mcmc = 10000, b0 = rep(0, 3), B0 = 1000^(-1)*diag(3), tune = 1, thin = 1) summary(RegLog) ## ## Iterations = 1001:11000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## X 0.4888 0.02499 0.0002499 0.0008527 ## Xx2 0.8325 0.02711 0.0002711 0.0009338 ## Xx3 -1.2112 0.03027 0.0003027 0.0010453 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## X 0.4395 0.4726 0.4891 0.5051 0.5385 ## Xx2 0.7787 0.8145 0.8326 0.8499 0.8877 ## Xx3 -1.2734 -1.2312 -1.2105 -1.1906 -1.1515 When using our GUI to estimate this model, we should follow these steps: Select univariate models on the top panel Select logit models using the left radio button Upload the data set (we save this in folder DataSimulations in file LogitSim.csv) Select MCMC iterations, burn-in and thinning parameters Select dependent and independent variables (see Figure 6.5) Click the Build formula button Set the hyperparameters and the tunning parameter Click the Go! button Analyze results (see Figure 6.6) Download posterior chains and diagnostic plots Figure 6.5: Univariate models: Logit/normal model We observe from our results that all 95% credible intervals embrace the population parameters. We also observe that there is a high level of autocorrelation (see autocorrelation plots) that potentially increases the dependence factor in the Raftery test (dependence factors higher than 5 are worrisome). Dependence factors are the proportional increase in the number of iterations attributable to serial dependence. Although, other diagnostics seem to be right (see Chapter 10 for details). We can potentially mitigate convergence issues running longer chains or multiple chains, using a thinning parameter greater than 1, picking a better tunning parameter or improving the mixing properties of the model using better priors or performing better math. Figure 6.6: Logit/normal model: Results. References "],["probit-model.html", "6.3 Probit model", " 6.3 Probit model The probit model also has as dependent variable a binary outcome. There is a latent (unobserved) random variable, \\(Y_i^*\\), that defines the structure of the estimation problem \\(Y_i=\\begin{Bmatrix} 1, &amp; Y_i^* \\geq 0\\\\ 0, &amp; Y_i^* &lt; 0 \\end{Bmatrix},\\) where \\(Y_i^*={\\bf{x}}_i^{\\top}\\beta+\\mu_i\\), \\(\\mu_i\\stackrel{iid} {\\sim}N(0,1)\\). Then, \\[\\begin{align} P[Y_i=1]&amp;=P[Y_i^*\\geq 0]\\\\ &amp;=P[\\mathbf{x}_i^{\\top}\\beta+\\mu_i\\geq 0]\\\\ &amp;=P[\\mu_i\\geq -\\mathbf{x}_i^{\\top}\\beta]\\\\ &amp;=1-P[\\mu_i &lt; -\\mathbf{x}_i^{\\top}\\beta]\\\\ &amp;=P[\\mu_i &lt; \\mathbf{x}_i^{\\top}\\beta], \\end{align}\\] where the last equality follows by symmetry at 0. In addition, observe that the previous calculations do not change if we multiply \\(Y_i^*\\) by a positive constant, this implies identification issues regarding scale. Intuitively, this is because we just observe 0s or 1s that are driven by an unobserved random latent variable \\(Y_i^*\\), this issue is also present in the logit model, that is why we set the variance equal to 1. (Albert and Chib 1993) implemented data augmentation (Tanner and Wong 1987) to apply a Gibbs sampling algorithm in this model. Augmenting this model with \\(Y_i^*\\), we can have the likelihood contribution from observation \\(i\\), \\(p(y_i|y_i^*)=1_{y_i=0}1_{y_i^*\\leq 0}+1_{y_i=1}1_{y_i^*&gt; 0}\\), where \\(1_A\\) is an indicator function that takes the value of 1 when condition \\(A\\) is satisfied. The posterior distribution is \\(\\pi(\\beta,{\\bf{Y^*}}|{\\bf{y}},{\\bf{X}})\\propto\\prod_{i=1}^n\\left[\\mathbf{1}_{y_i=0}1_{y_i^*&lt; 0}+1_{y_i=1}1_{y_i^*\\geq 0}\\right] \\times N_N({\\bf{Y}}^*|{\\bf{X}\\beta},{\\bf{I}}_N)\\times N_K(\\beta|\\beta_0,{\\bf{B}}_0)\\) when taking a normal distribution as prior, \\(\\beta\\sim N(\\beta_0,{\\bf{B}}_0)\\). The conditional posterior distribution of the latent variable is \\[\\begin{align} Y_i^*|\\beta,{\\bf{y}},{\\bf{X}}&amp;\\sim\\begin{Bmatrix} TN_{[0,\\infty)}({\\bf{x}}_i^{\\top}\\beta,1), &amp; y_i= 1\\\\ TN_{(-\\infty,0)}({\\bf{x}}_i^{\\top}\\beta,1), &amp; y_i= 0 \\\\ \\end{Bmatrix}, \\end{align}\\] where \\(TN_A\\) denotes a truncated normal density in the interval \\(A\\). The conditional posterior distribution of the location parameters is \\[\\begin{align} \\beta|{\\bf{Y}}^*, {\\bf{X}} &amp; \\sim N(\\beta_n,\\bf{B}_n), \\end{align}\\] where \\({\\bf{B}}_n = ({\\bf{B}}_0^{-1} + {\\bf{X}}^{\\top}{\\bf{X}})^{-1}\\), and \\(\\beta_n= {\\bf{B}}_n({\\bf{B}}_0^{-1}\\beta_0 + {\\bf{X}}^{\\top}{\\bf{Y}}^*)\\). Application: Determinants of hospitalization in Medellín We use the dataset named 2HealthMed.csv, which is in folder DataApp (see Table 13.3 for details) in our github repository (https://github.com/besmarter/BSTApp) and was used by (Ramírez Hassan, Cardona Jiménez, and Cadavid Montoya 2013). Our dependent variable is a binary indicator with a value equal to 1 if an individual was hospitalized in 2007, and 0 otherwise. The specification of the model is \\[\\begin{align} \\text{Hosp}_i&amp;=\\beta_1+\\beta_2\\text{SHI}_i+\\beta_3\\text{Female}_i+\\beta_4\\text{Age}_i+\\beta_5\\text{Age}_i^2+\\beta_6\\text{Est2}_i+\\beta_7\\text{Est3}_i\\\\ &amp;+\\beta_8\\text{Fair}_i+\\beta_9\\text{Good}_i+\\beta_{10}\\text{Excellent}_i, \\end{align}\\] where SHI is a binary variable equal to 1 if the individual is in a subsidized health care program and 0 otherwise, Female is an indicator of gender, Age in years, Est2 and Est3 are indicators of socio-economic status, the reference is Est1, which is the lowest, and self perception of health status where bad is the reference. Lets set \\(\\beta_0={\\bf{0}}_{10}\\), \\({\\bf{B}}_0={\\bf{I}}_{10}\\), iterations, burn-in and thinning parameters equal to 10000, 1000 and 1, respectively. mydata &lt;- read.csv(&quot;DataApplications/2HealthMed.csv&quot;, header = T, sep = &quot;,&quot;) attach(mydata) ## The following objects are masked from mydata (pos = 3): ## ## Age, Age2 str(mydata) ## &#39;data.frame&#39;: 12975 obs. of 22 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ MedVisPrev : int 0 0 0 0 0 0 0 0 0 0 ... ## $ MedVisPrevOr: int 1 1 1 1 1 1 1 1 1 1 ... ## $ Hosp : int 0 0 0 0 0 0 0 0 0 0 ... ## $ SHI : int 1 1 1 1 0 0 1 1 0 0 ... ## $ Female : int 0 1 1 1 0 1 0 1 0 1 ... ## $ Age : int 7 39 23 15 8 54 64 40 6 7 ... ## $ Age2 : int 49 1521 529 225 64 2916 4096 1600 36 49 ... ## $ FemaleAge : int 0 39 23 15 0 54 0 40 0 7 ... ## $ Est1 : int 1 0 0 0 0 0 0 0 0 0 ... ## $ Est2 : int 0 1 1 1 0 1 1 1 0 0 ... ## $ Est3 : int 0 0 0 0 1 0 0 0 1 1 ... ## $ Bad : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Fair : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Good : int 1 1 1 1 0 0 0 1 1 1 ... ## $ Excellent : int 0 0 0 0 1 1 0 0 0 0 ... ## $ NoEd : int 1 0 0 0 1 0 0 0 1 1 ... ## $ PriEd : int 0 0 0 0 0 1 1 1 0 0 ... ## $ HighEd : int 0 1 1 1 0 0 0 0 0 0 ... ## $ VocEd : int 0 0 0 0 0 0 0 0 0 0 ... ## $ UnivEd : int 0 0 0 0 0 0 0 0 0 0 ... ## $ PTL : num 0.43 0 0 0 0 0.06 0 0.38 0 1 ... K &lt;- 10 # Number of regressors b0 &lt;- rep(0, K) # Prio mean B0i &lt;- diag(K) # Prior precision (inverse of covariance) Prior &lt;- list(b0, B0i) # Prior list y &lt;- Hosp # Dependent variables X &lt;- cbind(1, SHI, Female, Age, Age2, Est2, Est3, Fair, Good, Excellent) # Regressors Data &lt;- list(y = y, X = X) # Data list Mcmc &lt;- list(R = 10000, nprint = 0) # MCMC parameters RegProb &lt;- bayesm::rbprobitGibbs(Data = Data, Prior = Prior, Mcmc = Mcmc) # Inference using bayesm package ## ## Starting Gibbs Sampler for Binary Probit Model ## with 12975 observations ## Table of y Values ## y ## 0 1 ## 12571 404 ## ## Prior Parms: ## betabar ## [1] 0 0 0 0 0 0 0 0 0 0 ## A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [2,] 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 ## [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 ## [9,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 ## [10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 ## ## MCMC parms: ## R= 10000 keep= 1 nprint= 0 ## PostPar &lt;- coda::mcmc(RegProb$betadraw) # Posterior draws colnames(PostPar) &lt;- c(&quot;Cte&quot;, &quot;SHI&quot;, &quot;Female&quot;, &quot;Age&quot;, &quot;Age2&quot;, &quot;Est2&quot;, &quot;Est3&quot;, &quot;Fair&quot;, &quot;Good&quot;, &quot;Excellent&quot;) # Names summary(PostPar) # Posterior summary ## ## Iterations = 1:10000 ## Thinning interval = 1 ## Number of chains = 1 ## Sample size per chain = 10000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## Cte -9.234e-01 1.389e-01 1.389e-03 3.699e-03 ## SHI -5.219e-03 5.893e-02 5.893e-04 2.212e-03 ## Female 1.278e-01 4.906e-02 4.906e-04 1.805e-03 ## Age 4.269e-05 3.634e-03 3.634e-05 1.205e-04 ## Age2 3.975e-05 4.363e-05 4.363e-07 1.323e-06 ## Est2 -8.619e-02 5.251e-02 5.251e-04 1.818e-03 ## Est3 -4.270e-02 8.089e-02 8.089e-04 2.777e-03 ## Fair -5.145e-01 1.152e-01 1.152e-03 2.123e-03 ## Good -1.225e+00 1.142e-01 1.142e-03 2.378e-03 ## Excellent -1.083e+00 1.365e-01 1.365e-03 3.626e-03 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## Cte -1.199e+00 -1.017e+00 -9.222e-01 -8.284e-01 -0.6538273 ## SHI -1.186e-01 -4.438e-02 -5.897e-03 3.293e-02 0.1144458 ## Female 3.233e-02 9.465e-02 1.284e-01 1.610e-01 0.2225963 ## Age -7.025e-03 -2.393e-03 4.003e-05 2.489e-03 0.0073117 ## Age2 -4.646e-05 1.036e-05 3.941e-05 6.987e-05 0.0001237 ## Est2 -1.895e-01 -1.219e-01 -8.577e-02 -5.082e-02 0.0168386 ## Est3 -2.011e-01 -9.709e-02 -4.210e-02 1.134e-02 0.1146267 ## Fair -7.379e-01 -5.922e-01 -5.155e-01 -4.382e-01 -0.2866942 ## Good -1.447e+00 -1.301e+00 -1.227e+00 -1.150e+00 -0.9984957 ## Excellent -1.354e+00 -1.174e+00 -1.082e+00 -9.920e-01 -0.8115961 It seems from our results that female and health status are relevant variables for hospitalization, as their 95% credible intervals do not cross 0. Women have a higher probability of being hospitalized than do men, and people with bad self perception of health condition also have a higher probability of being hospitalized. We get same results programming a Gibbs sampler algorithm (see Exercise 1) and using our GUI. We also see that there are posterior convergence issues (see Exercise 2). References "],["summary-chapter-6.html", "6.4 Summary: Chapter 6", " 6.4 Summary: Chapter 6 "],["exercises-chapter-6.html", "6.5 Exercises: Chapter 6", " 6.5 Exercises: Chapter 6 Program a Gibbs sampler for the hospitalization application. Check convergence diagnostics for the hospitalization application using our GUI. "]]
