[
["bayfre.html", "Chapter 2 Conceptual differences between the Bayesian and the Frequentist statistical approaches", " Chapter 2 Conceptual differences between the Bayesian and the Frequentist statistical approaches I give some of the main conceptual differences between the Bayesian inferential approach and the Frequentist approach. I emphasize in the Bayesian concepts as most of the readers can be familiarized with the Frequentist statistical framework. "],
["sec21.html", "2.1 The concept of probability", " 2.1 The concept of probability Let us begin with the following thought experiment: Assume that you are watching a contestant compete in the first round of the TV show “Who wants to be a millionaire?”, he is asked to answer a very simple question: What is the last name of the brothers who are credited with inventing the world’s first successful motor-operated airplane? What is the probability that the contestant answers this question correctly and, hence, survives the first round? Unless you have: watched this particular contestant participate in this show many times seen him asked this same question each time and computed the relative frequency with which he gives the correct answer, you need to answer this question as a Bayesian! Uncertainty about the event “survival” needs to be expressed as a “degree of belief” informed both by information (or “data”) on the skill of the particular participant, and how much he knows about inventors, and (possibly) prior knowledge on his performance in other game shows. Of course, your prior knowledge of the contestant may be minimal, or it may be very informed. Either way, your final answer remains a degree of belief held about an uncertain (and inherently unrepeatable) state of nature. The point of this hypothetical, light-hearted scenario is simply to highlight that a key distinction between the “Frequentist” and Bayesian approaches to inference is not the use (or nature) of prior information, but simply the manner in which probability is used. To the Bayesian, probability is the mathematical construct used to quantify uncertainty about an unknown state of nature, conditional on observed data and prior knowledge about the context in which that state of nature occurs. To the Frequentist, probability is linked intrinsically to the concept of a repeated experiment, and the relative frequency with which a particular outcome occurs, conditional on that unknown state. This distinction remains key whether the Bayesian chooses to be informative or subjective in the specification of prior information, or chooses to be noninformative or objective. Frequentists consider probability as a physical phenomenon, like mass or wavelength, whereas Bayesians stipulate that probability lives in the mind of scientists (Parmigiani and Inoue 2008). It seems that the understanding of the concept of probability for the common human being is more associated with “degrees of belief” rather than relative frequency. Peter Diggle, President of The Royal Statistical Association (2014-2016), said in an interview: “A different trend which has surged upwards in statistics during Peter’s career is the popularity of Bayesian statistics. Does Peter consider himself a Bayesian? Well, he replies, you can’t not believe in Bayes’ theorem because it’s true. But that doesn’t make you a Bayesian in the philosophical sense. When people are making personal decisions – even if they don’t formally process Bayes’ theorem in their mind – they are adapting what they think they should believe in response to new evidence as it comes in. Bayes’ theorem is just the formal mathematical machinery for doing that.” However, I should say that psychological experiments suggest that human beings suffer from anchoring, that is, a cognitive bias that causes us to rely too heavily on the previous information (prior) such that the updating process (posterior) due to new information (likelihood) is low compared to the Bayes’ rule (Kahneman 2011). References "],
["sec22.html", "2.2 Subjectivity is not the key", " 2.2 Subjectivity is not the key The concepts of subjectivity and objectivity indeed characterize both statistical paradigms in differing ways. Among Bayesians there are those who are immersed in subjective rationality ((Ramsey 1926), (Finetti 1937), (Savage 1954), (Lindley 2000)), but others who adopt objective prior distributions such as Jeffreys’, reference, empirical or robust ((Bayes 1763), (Laplace 1812), (Jeffreys 1961), (Berger 2006)) to operationalize Bayes’ rule, and thereby weight quantitative (data-based) evidence. Among Frequentists, there are choices made about significance levels which, if not explicitly subjective, are typically not grounded in any objective and documented assessment of the relative losses of Type I and Type II errors.1 In addition, both Frequentist and Bayesian statisticians make decisions about the form of the data generating process, or “model”, which - if not subject to rigorous diagnostic assessment - retains a subjective element that potentially influences the final inferential outcome. Although we all know that by definition a model is a schematic and simplified approximation to reality, “Since all models are wrong the scientist cannot obtain a correct one by excessive elaboration. On the contrary following William of Occam he should seek an economical description of natural phenomena.” (Box 1976) We also know that “All models are wrong, but some are useful” (Box 1979), that is why model diagnostics are important. This task can be performed in both approaches. Particularly, the Bayesian framework can use predictive p–values for absolute testing ((Gelman and Meng 1996), (Bayarri and Berger 2000)) or posterior odds ratios for relative statements ((Jeffreys 1935), (Kass and Raftery 1995)). This is because the marginal density, conditional on data, is interpreted as the likelihood of the prior distribution (Berger 1993). In addition, what is objectivity in a Frequentist approach? For example, why should we use a 5% or 1% significance level rather than any other value? As someone said, the apparent objectivity is really a consensus (Lindley 2000). In fact “Student” (William Gosset) saw statistical significance at any level as being “nearly valueless” in itself (Ziliak 2008). But, this is not just a situation in the Frequentist approach. The cut-offs given to “establish” scientific evidence against a null hypothesis in terms of \\(log_{10}\\) scale (Jeffreys 1961) or \\(log_{e}\\) scale (Kass and Raftery 1995) are also ad hoc. Although the true state of nature in Bayesian inference is expressed in “degrees of belief”, the distinction between the two paradigms does not reside in one being more, or less, subjective than the other. Rather, the differences are philosophical, pedagogical, and methodological. References "],
["sec23.html", "2.3 Estimation, hypothesis testing and prediction", " 2.3 Estimation, hypothesis testing and prediction All what is required to perform estimation, hypothesis testing (model selection) and prediction in the Bayesian approach is to apply the Bayes’ rule. This means coherence under a probabilistic view. But, there is no free lunch, coherence reduces flexibility. On the other hand, the frequestist approach may be not coherent from a probabilistic point of view, but it is very flexible. This approach can be seen as a tool kit that offers inferential solutions under the umbrella of understanding probability as relative frequency. For instance, a point estimator in a Frequentist approach is found such that satisfies good sampling properties such as unbiasness, efficienciency, or a large sample property such as consistency. A remarkable diference is that optimal Bayesian decisions are calculated minimizing the expected value of the loss function with respect to the posterior distribution, that is, it is conditional on observed data. On the other hand, Frequentist “optimal” actions are base on the expected values over the distribution of the estimator (a function of data) conditional on the unknown parameters, that is, it considers sampling variability. The Bayesian approach allows to obtain the posterior distribution of any unknown object such as parameters, latent variables, future or unobserved variables or models. A nice advantage is that prediction can take into account estimation error, and predictive distribution (probabilistic forecasts) can be easily recovered. Hypothesis testing (model selection) is based on inductive logic reasoning (Inverse probability); on the basis of what we see, we evaluate what hypothesis is most tenable, and is performed using posterior odds, which in turn are based on Bayes factors that evaluate evidence in favor of a null hypothesis taking explicitly the alternative (Kass and Raftery 1995), following the rules of probability (Lindley 2000) comparing how well the hypothesis predicts data (Goodman 1999), minimizing the weighted sum of type I and type II error probabilities ((DeGroot 1975), (Pericchi and Pereira 2015)), and taking the implicit balance of losses ((Jeffreys 1961), (Bernardo and Smith 1994)) into account. Posterior odds allows to use the same framework to analyze nested and non-nested models and perform model average. However, Bayes factors cannot be based on improper or vague priors (Koop 2003), the practical interplay between model selection and posterior distributions is not as easy as the Frequentist approach, and the computational burden can be more demanding due to solving potentially difficult integrals. On the other hand, the Frequentist approach establishes most of its estimators as the solution of a system of equations. Observe that optimization problems reduce to solve systems. We can potentially get the distribution of these estimators, but most of the time it is needed asymptotic arguments or resampling techniques. Hypothesis testing requires pivotal quantities2 and/or also resampling, and prediction most of the time is based on a plug-in approach, which means not taking estimation error into account. In addition, ancillary statistics can be used to build prediction intervals.3 Comparing models depends on their structure, for instance, there are different Frequentist statistical approaches to compare nested and non-nested models. A nice feature in some situations is that there is a practical interplay between hypothesis testing and confidence intervals, where you cannot reject at \\(\\alpha\\) significance level (Type I error) any null hypothesis \\(H_0. \\ \\beta_k=\\beta_k^0\\) if \\(\\beta_k^0\\) is in the \\(1-\\alpha\\) confidence interval \\(P(\\beta_k\\in[\\hat{\\beta_k}-|t_{N-K}^{\\alpha/2}|\\times\\hat \\sigma_{\\hat{\\beta_k}},\\hat{\\beta_k}+|t_{N-k}|^{\\alpha/2}\\times \\hat\\sigma_{\\hat{\\beta_k}}])=1-\\alpha\\) in a linear model, \\(\\hat{\\beta_k}\\) and \\(\\hat\\sigma_{\\hat{\\beta_k}}\\) are the least squares estimators of \\(\\beta_k\\) and its standard error, and \\(t_{N-K}^{\\alpha/2}\\) is the quantile value of the Student’s t distribution at \\(\\alpha/2\\) probability and \\(N-K\\) degrees of freedom, \\(N\\) is the sample size, and \\(K\\) the number of location parameters. A remarkable difference between the Bayesian and the Frequentist inferential frameworks is the interpretation of credible/confidence intervals. Observe that once we have estimates, such that for example the previous interval is \\([0.2, 0.4]\\) given a 95% confidence level, we cannot say that \\(P(\\beta_k\\in [0.2, 0.4])=0.95\\) in the Frequentist framework. In fact, this probability is 0 or 1 under this approach, as \\(\\beta_k\\) can be there or not, the problem is that we will never know in applied settings. This due to that \\(P(\\beta_k\\in[\\hat{\\beta_k}-|t_{N-K}^{0.025}|\\hat\\times \\sigma_{\\hat{\\beta_k}},\\hat{\\beta_k}+|t_{N-k}^{0.025}|\\times \\hat\\sigma_{\\hat{\\beta_k}}])=0.95\\) being in the sense of repeated sampling. On the other hand, once we have the posterior distribution, we can say that \\(P(\\beta_k\\in [0.2, 0.4])=0.95\\) under the Bayesian framework. Following common practice, most of researchers and practitioners do hypothesis testing based on the p-value in the Frequentist framework. But, what is a p–value? Most of the users do not know the answer due to many times statistical inference is not performed by statisticians (Berger 2006).4 A p–value is the probability of obtaining a statistical summary of the data equal to or “more extreme” than what was actually observed, assuming that the null hypothesis is true. Therefore, p–value calculations involve not just the observed data, but also more “extreme” hypothetical observations. So, “What the use of p implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.”(Jeffreys 1961) It seems that common Frequentist inferential practice intertwined two different logic reasoning arguments: the p–value (Fisher 1958) and significance level (Neyman and Pearson 1933). The former is an informal short–run criterion, whose philosophical foundation is reduction to absurdity, which measures the discrepancy between the data and the null hypothesis. So, the p–value is not a direct measure of the probability that the null hypothesis is false. The latter, whose philosophical foundations is deduction, is based on a long–run performance such that controls the overall number of incorrect inferences in the repeated sampling without care of individual cases. The p–value fallacy consists in interpreting the p–value as the strength of evidence against the null hypothesis, and using it simultaneously with the frequency of type I error under the null hypothesis (Goodman 1999). The American Statistical Association has several concerns regarding the use of the p–value as a cornerstone to perform hypothesis testing in science. This concern motivates the ASA’s statement on p–values (Wasserstein and Lazar 2016), which can be summarized in the following principles: “P–values can indicate how incompatible the data are with a specified statistical model.” “P–values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.” “Scientific conclusions and business or policy decisions should not be based only on whether a p–value passes a specific threshold.” “Proper inference requires full reporting and transparency.” “A p–value, or statistical significance, does not measure the size of an effect or the importance of a result.” “By itself, a p–value does not provide a good measure of evidence regarding a model or hypothesis.” Another difference between the Frequentists and the Bayesians is the way how scientific hypothesis are tested. The former use the p-value, whereas the latter use the Bayes factor. Observe that the p–value is associated with the probability of the data given the hypothesis, whereas the Bayes factor is associated with the probability of the hypothesis given the data. However, there is an approximate link between the \\(t\\) statistic and the Bayes factor for regression coefficients (Raftery 1995). In particular, \\(|t|&gt;(log(N)+6)^{1/2}\\), corresponds to strong evidence in favor of rejecting the not relevance of a control in a regression. Observe that in this setting the threshold of the \\(t\\) statistic, and as a consequence the significant level, depends on the sample size. Observe that this setting agrees with the idea in experimental designs of selecting the sample size such that we control Type I and Type II errors. In observational studies we cannot control the sample size, but we can select the significance level. References "],
["sec24.html", "2.4 The likelihood principle", " 2.4 The likelihood principle In making inference or decisions about the state of the nature in the Bayesian paradigm, all the relevant experimental information is given by the observed data. Then, the relevance of the likelihood principle. The Bayesian framework is conditional on data, whereas the Frequentist approach is not (Berger 1993). We follow (Berger 1993), who in turns followed (Lindley and Phillips 1976), to illustrate the likelihood principle. We are given a coin such that we are interested in the probability, \\(\\theta\\), of having it come up heads when flipped. It is desired to test \\(H_0. \\ \\theta=1/2\\) versus \\(H_1. \\ \\theta&gt;1/2\\). An experiment is conducted by flipping the coin (independently) in a series of trials, the results of which is the observation of 9 heads and 3 tails. This is not yet enough information to specify \\(f(y|\\theta)\\), since the series of trials was not explained. Two possibilities: The experiment consisted of a predetermine 12 flips, so that \\(Y=\\left[Heads\\right]\\) would be \\(\\mathcal{B}(12,\\theta)\\), then \\(l_1(\\theta)=f_1(y|\\theta)={\\binom{{n}}{{y}}}\\theta^y(1-\\theta)^{n-y}=220\\times\\theta^9(1-\\theta)^{3}.\\) The experiment consisted of flipping the coin until 3 tails were observed (\\(r=3\\)). Then, \\(Y\\), the number of failures (heads) until getting 3 tails, is \\(\\mathcal{N}\\mathcal{B}(3,1-\\theta)\\). Then, \\(l_2(\\theta)=f_2(y|\\theta)={\\binom{{y+r-1}}{{r-1}}}(1-(1-\\theta)^y(1-\\theta)^{r}=55\\times\\theta^9(1-\\theta)^{3}.\\) Using a Frequentist approach, the significance level of \\(y=9\\) using the Binomial model against \\(\\theta=1/2\\) would be: \\[\\begin{equation*} \\alpha_1=P_{1/2}(Y\\geq 9)=f_1(9|1/2)+f_1(10|1/2)+f_1(11|1/2)+f_1(12|1/2)=0.073. \\end{equation*}\\] success &lt;- 9 # Number of observed success in n trials n &lt;- 12 # Number of trials siglevel &lt;- sum(sapply(9:n, function(y) dbinom(y, n, 0.5))) paste(&quot;Significance level from binomial model is&quot;, sep = &quot; &quot;, round(siglevel, 4)) ## [1] &quot;Significance level from binomial model is 0.073&quot; For the Negative Binomial model, the significance level would be: \\[\\begin{equation*} \\alpha_2=P_{1/2}(Y\\geq 9)=f_2(9|1/2)+f_2(10|1/2)+\\ldots=0.0327. \\end{equation*}\\] success &lt;- 3 # Number of target success (tails) failures &lt;- 9 # Number of failures siglevel &lt;- 1 - pnbinom((failures - 1), success, 0.5) paste(&quot;Significance level from negative binomial model is&quot;, sep = &quot; &quot;, round(siglevel, 4)) ## [1] &quot;Significance level from negative binomial model is 0.0327&quot; We arrive to a different conclusions using a significance level equal to 5%, whereas we obtain the same outcomes using a Bayesian approach because the kernels of both distributions are the same. References "],
["sec25.html", "2.5 Logic of argumentation", " 2.5 Logic of argumentation The Frequentist approach is based on deductive logic, this means that it starts from a statement about the true state of nature (null hypothesis), and predicts what should be seen if this statement were true. On the other hand, the Bayesian approach is based on inductive logic, this means that it defines what hypothesis is more consistent with what is seen. The former inferential approach establishes that the true of the premises implies the true of the conclusion, that is why we reject or not reject hypothesis. The latter establishes that the premises supply some evidence, but not full assurance, of the true of the conclusion, that is why we get probabilistic statements. Here, there is a difference between effects of causes (forward causal inference) and causes of effects (reverse causal inference) ((Gelman and Imbens 2013), (Dawid, Musio, and Fienberg 2016)). To illustrate this point, imagine that a firm increases the price of a specific good, then economic theory would say that its demand decreases. The premise (null hypothesis) is a price increase, and the consequence is a demand reduction. Another view would be to observe a demand reduction, and try to identify which cause is more tenable. For instance, demand reduction can be caused by any positive supply shocks or any negative demand shocks. The Frequentist logic sees the first view, and the Bayesian reasoning gives the probability associated with possible causes. References "],
["sec26.html", "2.6 A simple working example", " 2.6 A simple working example We will illustrate some conceptual differences between the Bayesian and Frequentist statistical approaches performing inference given a random sample \\(\\mathbf{y}=[y_1,y_2,\\dots,y_N]\\), where \\(y_i\\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\), \\(i=1,2,\\dots,N\\). In particular, we set \\(\\pi(\\mu,\\sigma)=\\pi(\\mu)\\pi(\\sigma)\\propto \\frac{1}{\\sigma}\\). This is a standard non-informative improper prior (Jeffreys prior, see Chapter 3), that is, this prior is perfectivelly compatible with sample information. In addition, we are assuming independent priors for \\(\\mu\\) and \\(\\sigma\\). Then, \\[\\begin{align} \\pi(\\mu,\\sigma)&amp;\\propto \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2\\right\\}\\\\ &amp;= \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N ((y_i-\\bar{y}) - (\\mu-\\bar{y}))^2\\right\\}\\\\ &amp;= \\frac{1}{\\sigma}\\exp\\left\\{-\\frac{N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}\\times (\\sigma)^{-N}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\bar{y})^2\\right\\}\\\\ &amp;= \\frac{1}{\\sigma}\\exp\\left\\{-\\frac{N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}\\times (\\sigma)^{-(\\alpha_n+1)}\\exp\\left\\{-\\frac{\\alpha_n\\hat{\\sigma}^2}{2\\sigma^2}\\right\\}, \\end{align}\\] where \\(\\bar{y}=\\frac{\\sum_{i=1}^N}{N}\\), \\(\\alpha_n=N-1\\) and \\(\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^N (y_i-\\bar{y})^2}{N-1}\\). The first term in the last expression is the kernel of a normal density, \\(\\mu|\\sigma,\\mathbf{y}\\sim N(\\bar{y},\\sigma^2/N)\\). The second term is the kernel of an inverted gamma density, \\(\\sigma|\\mathbf{y}\\sim IG(\\alpha_n,\\hat{\\sigma}^2)\\). Therefore, \\(\\pi(\\mu|\\sigma,\\mathbf{y})=(2\\pi\\sigma^2/N)^{-1/2}\\exp\\left\\{\\frac{-N}{2\\sigma^2}(\\mu-\\bar{y})^2\\right\\}\\) and \\(\\pi(\\sigma|\\mathbf{y})=\\frac{2}{\\Gamma(\\alpha_n/2)}\\left(\\frac{\\alpha_n\\hat{\\sigma}^2}{2}\\right)^{\\alpha_n/2}\\frac{1}{\\sigma^{\\alpha_n+1}}\\exp\\left\\{-\\frac{\\alpha_n\\hat{\\sigma}^2}{2\\sigma^2}\\right\\}\\). Observe that \\(\\mathbb{E}[\\mu|\\sigma,\\mathbf{y}]=\\bar{y}\\), this is also the maximum likelihood (Frequentist) point estimate of \\(\\mu\\) in this setting. In addition, the Frequentist \\((1-\\alpha)\\%\\) confidence interval and the Bayesian \\((1-\\alpha)\\%\\) credible interval have exactly the same form, \\(\\bar{y}\\pm |z_{\\alpha/2}|\\frac{\\sigma}{N}\\), where \\(z_{\\alpha/2}\\) is the \\(\\alpha/2\\) percentile of a standard normal distribution. However, the interpretations are totally different. The confidence interval has a probabilistic interpretation under sampling variability of \\(\\bar{Y}\\), that is, in repeated sampling \\((1-\\alpha)\\%\\) of the intervals \\(\\bar{Y}\\pm |z_{\\alpha/2}|\\frac{\\sigma}{N}\\) would include \\(\\mu\\), but given an observed realization of \\(\\bar{Y}\\), say \\(\\bar{y}\\), the probability of \\(\\bar{y}\\pm |z_{\\alpha/2}|\\frac{\\sigma}{N}\\) including \\(\\mu\\) is 1 or 0, that is why we say a \\((1-\\alpha)\\%\\) confidence interval. On the other hand, \\(\\bar{y}\\pm |z_{\\alpha/2}|\\frac{\\sigma}{N}\\) has a simple probabilistic interpretation in the Bayesian framework, there is a \\((1-\\alpha)\\%\\) probability that \\(\\mu\\) lies in this interval. If we want to get the marginal posterior density of \\(\\mu\\), \\[\\begin{align} \\pi(\\mu|\\mathbf{y})&amp;=\\int_{0}^{\\infty} \\pi(\\mu,\\sigma|\\mathbf{y}) d\\sigma\\\\ &amp;\\propto \\int_{0}^{\\infty} \\frac{1}{\\sigma}\\times (\\sigma^2)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2\\right\\} d\\sigma\\\\ &amp;= \\int_{0}^{\\infty} \\left(\\frac{1}{\\sigma}\\right)^{N+1} \\exp\\left\\{-\\frac{N}{2\\sigma^2}\\frac{\\sum_{i=1}^N (y_i-\\mu)^2}{N}\\right\\} d\\sigma\\\\ &amp;=\\left[\\frac{2}{\\Gamma(N/2)}\\left(\\frac{N\\sum_{i=1}^N (y_i-\\mu)^2}{2N}\\right)^{N/2}\\right]^{-1}\\\\ &amp;\\propto \\left[\\sum_{i=1}^N (y_i-\\mu)^2\\right]^{-N/2}\\\\ &amp;=\\left[\\sum_{i=1}^N ((y_i-\\bar{y})-(\\mu-\\bar{y}))^2\\right]^{-N/2}\\\\ &amp;=[\\alpha_n\\hat{\\sigma}^2+N(\\mu-\\bar{y})^2]^{-N/2}\\\\ &amp;\\propto \\left[1+\\frac{1}{\\alpha_n}\\left(\\frac{\\mu-\\bar{y}}{\\hat{\\sigma}/\\sqrt{N}}\\right)^2\\right]^{-(\\alpha_n+1)/2} \\end{align}\\] The fourth line is due to having the kernel of a inverted gamma density with \\(N\\) degrees of freedom in the integral. The last expression is the kernel of a Student t density function with \\(\\alpha_n=N-1\\) degrees of freedom, expected value equal to \\(\\bar{y}\\), and variance \\(\\frac{\\hat{\\sigma}^2}{N}\\left(\\frac{\\alpha_n}{\\alpha_n-2}\\right)\\). Then, \\(\\mu|\\mathbf{y}\\sim t\\left(\\bar{y},\\frac{\\hat{\\sigma}^2}{N}\\left(\\frac{\\alpha_n}{\\alpha_n-2}\\right),\\alpha_n\\right)\\). Observe that a \\((1-\\alpha)\\%\\) confidence interval and \\((1-\\alpha)\\%\\) credible interval have exactly the same expression, \\(\\bar{y}\\pm |t_{\\alpha/2}^{\\alpha_n}|\\frac{\\hat{\\sigma}}{\\sqrt{N}}\\), where \\(t_{\\alpha/2}^{\\alpha_n}\\) is the \\(\\alpha/2\\) percentile of a Student t distribution. But again, the interpretations are totally different. The similarity between the Frequentist and Bayesian expressions are similar in this examples due to using a non-informative improper prior. "],
["sec27.html", "2.7 Summary: Chapter 2", " 2.7 Summary: Chapter 2 The differences are philosophical, including as pertains to the role of probability; pedagogical, in particular as relates to the use of inference to inform decision making; and methodological. Although at methodological level, the debate has become considerably muted, except for some aspects of inference, with the recognition that each approach has a great deal to contribute to statistical practice ((Good 1992), (Bayarri and Berger 2004), (Kass 2011)). References "],
["exercises-chapter-2.html", "2.8 Exercises: Chapter 2", " 2.8 Exercises: Chapter 2 Lindley’s paradox "]
]
