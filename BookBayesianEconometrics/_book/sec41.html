<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 Motivation of conjugate families | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 Motivation of conjugate families | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 Motivation of conjugate families | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2024-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conjfam.html"/>
<link rel="next" href="sec42.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec25A.html"><a href="sec25A.html"><i class="fa fa-check"></i><b>2.6</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.7</b> A simple working example</a></li>
<li class="chapter" data-level="2.8" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.8</b> Summary: Chapter 2</a></li>
<li class="chapter" data-level="2.9" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.9</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>4.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="4.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>4.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="4.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.5</b> Computational examples</a></li>
<li class="chapter" data-level="4.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>4.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="4.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="normal-model.html"><a href="normal-model.html"><i class="fa fa-check"></i><b>6.1</b> Normal model</a></li>
<li class="chapter" data-level="6.2" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i><b>6.2</b> Logit model</a></li>
<li class="chapter" data-level="6.3" data-path="probit-model.html"><a href="probit-model.html"><i class="fa fa-check"></i><b>6.3</b> Probit model</a></li>
<li class="chapter" data-level="6.4" data-path="summary-chapter-6.html"><a href="summary-chapter-6.html"><i class="fa fa-check"></i><b>6.4</b> Summary: Chapter 6</a></li>
<li class="chapter" data-level="6.5" data-path="exercises-chapter-6.html"><a href="exercises-chapter-6.html"><i class="fa fa-check"></i><b>6.5</b> Exercises: Chapter 6</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec41" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Motivation of conjugate families<a href="sec41.html#sec41" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Observing three fundamental pieces of Bayesian analysis: the posterior distribution (parameter inference), the marginal likelihood (hypothesis testing), and the predictive distribution (prediction), equations <a href="sec41.html#eq:411">(4.1)</a>, <a href="sec41.html#eq:412">(4.2)</a> and <a href="sec41.html#eq:413">(4.3)</a>, respectively,</p>
<p><span class="math display" id="eq:411">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp;=\frac{p(\mathbf{y}|\mathbf{\theta}) \times \pi(\mathbf{\theta})}{p(\mathbf{y})},
  \tag{4.1}
\end{align}\]</span></p>
<p><span class="math display" id="eq:412">\[\begin{equation}
p(\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{y}|\mathbf{\theta})\pi(\mathbf{\theta})d\mathbf{\theta},
\tag{4.2}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:413">\[\begin{equation}
p(\mathbf{Y}_0|\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{Y}_0|\mathbf{\theta})\pi(\mathbf{\theta}|\mathbf{y})d\mathbf{\theta},
\tag{4.3}
\end{equation}\]</span></p>
<p>we can understand that some of the initial limitations of the application of the Bayesian analysis were associated with the ausence of algorithms to draw from non-standard posterior distributions (equation <a href="sec41.html#eq:411">(4.1)</a>), and the lack of analytical solutions of the marginal likelihood (equation <a href="sec41.html#eq:412">(4.2)</a>) and the predictive distribution (equation <a href="sec41.html#eq:413">(4.3)</a>). Both issues requiring computational power.</p>
<p>Although there were algorithms to sample from non-standard posterior distributions since the second half of the last century [<span class="citation">Metropolis et al. (<a href="#ref-metropolis53">1953</a>)</span>]<span class="citation">(<a href="#ref-hastings70">Hastings 1970</a>)</span>,<span class="citation">(<a href="#ref-Geman1984">Geman and Geman 1984</a>)</span>, their particular application in the Bayesian framework emerged later <span class="citation">(<a href="#ref-Gelfand1990">A. E. Gelfand and Smith 1990</a>)</span>,<span class="citation">(<a href="#ref-tierney1994markov">Tierney 1994</a>)</span>, maybe until increasing computational power of desktop computers. However, it is also common practice nowadays to use models that have standard conditional posterior distributions to mitigate computational requirements. In addition, nice mathematical tricks plus computational algorithms <span class="citation">(<a href="#ref-gelfand1994bayesian">Alan E. Gelfand and Dey 1994</a>)</span>, <span class="citation">(<a href="#ref-chib1995marginal">Chib 1995</a>)</span>,<span class="citation">(<a href="#ref-chib2001marginal">Chib and Jeliazkov 2001</a>)</span> and approximations [<span class="citation">Tierney and Kadane (<a href="#ref-Tierney1986">1986</a>)</span>]<span class="citation">(<a href="#ref-Jordan1999">Jordan and Saul 1999</a>)</span> are used to obtain the marginal likelihood (prior predictive).</p>
<p>Despite these advances, there are two potentially conflicting desirable model specification features that we can see from equations <a href="sec41.html#eq:411">(4.1)</a>, <a href="sec41.html#eq:412">(4.2)</a> and <a href="sec41.html#eq:413">(4.3)</a>: analytical solutions and the posterior distribution in the same family as the prior distribution for a given likelihood. The latter is called <em>conjugate priors</em>, a family of priors that is closed under sampling <span class="citation">(<a href="#ref-schlaifer1961applied">Schlaifer and Raiffa 1961</a>, p.~ 43-57)</span>.</p>
<p>These features are desirable as the former implies facility to perform hypothesis testing and predictive analysis, and the latter means invariance of the prior-to-posterior updating. Both feautures imply less computational burden.</p>
<p>We can easily achieve each of these features independenly, for instance using improper priors for analytical tractability, and defining in a broad sense the family of prior distributions for prior conjugacy. However, these are in conflict.</p>
<p>Fortunately, we can achieve these two nice features if we assume that the data generating process is given by a distribution function in the <em>exponential family</em>. That is, given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span>, a probability density function <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span> belongs to the exponential family if it has the form</p>
<p><span class="math display" id="eq:414">\[\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&amp;=\prod_{i=1}^N h(y_i) C(\mathbf{\theta}) \exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(y_i)\right\}\\
  &amp;=h(\mathbf{y}) C(\mathbf{\theta})^N\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\} \tag{4.4}\\
  &amp;=h(\mathbf{y})\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})-A(\mathbf{\theta})\right\},
\end{align}\]</span></p>
<p>where <span class="math inline">\(h(\mathbf{y})=\prod_{i=1}^N h(y_i)\)</span> is a non-negative function, <span class="math inline">\(\eta(\mathbf{\theta})\)</span> is a known function of the parameters, <span class="math inline">\(A(\mathbf{\theta})=\log\left\{\int_{\mathbf{Y}}h(\mathbf{y})\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\}d\mathbf{y}\right\}=-N\log(C(\mathbf{\theta}))\)</span> is a normalization factor, and <span class="math inline">\(\mathbf{T}(\mathbf{y})=\sum_{i=1}^N\mathbf{T}(y_i)\)</span> is the vector of sufficient statistics of the distribution (by the factorization theorem).</p>
<p>If the support of <span class="math inline">\(\mathbf{y}\)</span> is independent of <span class="math inline">\(\mathbf{\theta}\)</span>, then the family is said to be <em>regular</em>, otherwise it is <em>irregular</em>. In addition, if we set <span class="math inline">\(\eta=\eta(\mathbf{\theta})\)</span>, then the exponential family is said to be in the <em>canonical form</em></p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&amp;=h(\mathbf{y})D(\mathbf{\eta})^N\exp\left\{\eta^{\top}\mathbf{T}(\mathbf{y})\right\}\\
  &amp;=h(\mathbf{y})\exp\left\{\eta^{\top}\mathbf{T}(\mathbf{y})-B(\mathbf{\eta})\right\}.
\end{align}\]</span></p>
<p>A nice feature of this representation is that <span class="math inline">\(\mathbb{E}[\mathbf{T}(\mathbf{y})|\mathbf{\eta}]=\nabla B(\mathbf{\eta})\)</span> and <span class="math inline">\(Var[\mathbf{T}(\mathbf{y})|\mathbf{\eta}]=\nabla^2 B(\mathbf{\eta})\)</span>.</p>
<p><strong>Examples of exponential family distributions</strong></p>
<ol style="list-style-type: decimal">
<li>Discrete distributions</li>
</ol>
<ul>
<li>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Poisson distribution</strong> let’s show that <span class="math inline">\(p(\mathbf{y}|\lambda)\)</span> is in the exponential family.</li>
</ul>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\lambda)&amp;=\prod_{i=1}^N \frac{\lambda^{y_i} \exp(-\lambda)}{y_i!}\\
  &amp;=\frac{\lambda^{\sum_{i=1}^N y_i}\exp(-N\lambda)}{\prod_{i=1}^N y_i!}\\
  &amp;=\frac{\exp(-N\lambda)\exp(\sum_{i=1}^Ny_i\log(\lambda))}{\prod_{i=1}^N y_i!},
\end{align}\]</span></p>
<p>then <span class="math inline">\(h(\mathbf{y})=\left[\prod_{i=1}^N y_i!\right]^{-1}\)</span>, <span class="math inline">\(\eta(\lambda)=\log(\lambda)\)</span>, <span class="math inline">\(T(\mathbf{y})=\sum_{i=1}^N y_i\)</span> (sufficient statistic) and <span class="math inline">\(C(\lambda)=\exp(-\lambda)\)</span>.</p>
<p>If we set <span class="math inline">\(\eta=\log(\lambda)\)</span>, then</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\eta)&amp;=\frac{\exp(\sum_{i=1}^Ny_i\eta-N\exp(\eta))}{\prod_{i=1}^N y_i!},
\end{align}\]</span></p>
<p>such that <span class="math inline">\(B(\eta)=N\exp(\eta)\)</span>, then <span class="math inline">\(\nabla(B(\eta))=N\exp(\eta)=N\lambda=\mathbb{E}\left[\sum_{i=1}^N y_i\biggr\rvert\lambda\right]\)</span>, that is, <span class="math inline">\(\mathbb{E}\left[\frac{\sum_{i=1}^N y_i}{N}\biggr\rvert\lambda\right]=\mathbb{E}[\bar{y}|\lambda]=\lambda\)</span>, and <span class="math inline">\(\nabla^2(B(\eta))=N\exp(\eta)=N\lambda=Var\left[\sum_{i=1}^N y_i\biggr\rvert\lambda\right]=N^2 \times Var\left[\bar{y}\rvert\lambda\right]\)</span>, then <span class="math inline">\(Var\left[\bar{y}\rvert\lambda\right]=\frac{\lambda}{N}\)</span>.</p>
<ul>
<li>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Bernoulli distribution</strong> let’s show that <span class="math inline">\(p(\mathbf{y}|\theta)\)</span> is in the exponential family.</li>
</ul>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\theta)&amp;=\prod_{i=1}^N \theta^{y_i}(1-\theta)^{1-y_i}\\
  &amp;=\theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}\\
  &amp;=(1-\theta)^N\exp\left\{\sum_{i=1}^N y_i\log\left(\frac{\theta}{1-\theta}\right)\right\},
\end{align}\]</span></p>
<p>then <span class="math inline">\(h(\mathbf{y})=\mathbb{I}[y_i\in\left\{0,1\right\}]\)</span>, <span class="math inline">\(\eta(\theta)=\log\left(\frac{\theta}{1-\theta}\right)\)</span>, <span class="math inline">\(T(\mathbf{y})=\sum_{i=1}^N y_i\)</span> and <span class="math inline">\(C(\theta)=1-\theta\)</span>.</p>
<p>Write this distribution in the canonical form, and find the mean and variance of the sufficient statistic (exercise 1).</p>
<ul>
<li>Given a random sample <span class="math inline">\(\mathbf{y}=[\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_N]\)</span> from a <strong>m-dimensional multinomial distribution</strong>, where <span class="math inline">\(\mathbf{y}_i=\left[y_{i1},\dots,y_{im}\right]\)</span>, <span class="math inline">\(\sum_{l=1}^m y_{il}=n\)</span>, <span class="math inline">\(n\)</span> independent trials each of which leads to a success for exactly one of <span class="math inline">\(m\)</span> categories with probabilities <span class="math inline">\(\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_m]\)</span>, <span class="math inline">\(\sum_{l=1}^m \theta_l=1\)</span>. Let’s show that <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span> is in the exponential family.</li>
</ul>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\mathbf{\theta})&amp;=\prod_{i=1}^N \frac{n!}{\prod_{l=1}^m y_{il}!} \prod_{l=1}^m\theta_l^{y_il}\\
  &amp;=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\exp\left\{\sum_{i=1}^N\sum_{l=1}^m y_{il}\log(\theta_l)\right\}\\
  &amp;=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\exp\left\{\left(N\times n-\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\right)\log(\theta_m)+\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\log(\theta_l)\right\}\\
  &amp;=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\theta_m^{N\times n}\exp\left\{\sum_{i=1}^N\sum_{l=1}^{m-1}y_{il}\log(\theta_l/\theta_m)\right\},
\end{align}\]</span></p>
<p>then <span class="math inline">\(h(\mathbf{y})=\frac{(n!)^N}{\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\)</span>, <span class="math inline">\(\eta(\mathbf{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right)\dots \log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]\)</span>, <span class="math inline">\(T(\mathbf{y})=\left[\sum_{i=1}^N y_{i1}\dots \sum_{i=1}^N y_{im-1}\right]\)</span> and <span class="math inline">\(C(\mathbf{\theta})=\theta_m^n\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Continuos distributions</li>
</ol>
<ul>
<li>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>normal distribution</strong> let’s show that <span class="math inline">\(p(\mathbf{y}|\mu,\sigma^2)\)</span> is in the exponential family.</li>
</ul>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\mu,\sigma^2)&amp;=\prod_{i=1}^N \frac{1}{2\pi\sigma^2}\exp\left\{-\frac{1}{2\sigma^2}\left(y_i-\mu\right)^2\right\}\\
  &amp;= (2\pi)^{-N/2}(\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N\left(y_i-\mu\right)^2\right\}\\
  &amp;= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^Ny_i^2+\frac{\mu}{\sigma^2}\sum_{i=1}^N y_i-N\frac{\mu^2}{2\sigma^2}-\frac{N}{2}\log(\sigma^2)\right\}

\end{align}\]</span></p>
<p>then <span class="math inline">\(h(\mathbf{y})=(2\pi)^{-N/2}\)</span>, <span class="math inline">\(\eta(\mu,\sigma^2)=\left[\frac{\mu}{\sigma^2} \ \frac{-1}{\sigma^2}\right]\)</span>, <span class="math inline">\(T(\mathbf{y})=\left[\sum_{i=1}^N y_i \ \sum_{i=1}^N y_i^2\right]\)</span> and <span class="math inline">\(C(\mu,\sigma^2)=\exp\left\{-\frac{\mu^2}{2\sigma^2}-\frac{\log(\sigma^2)}{2}\right\}\)</span>.</p>
<p>Observe that</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}|\mu,\sigma^2)&amp;= (2\pi)^{-N/2}\exp\left\{\eta_1\sum_{i=1}^N y_i+\eta_2\sum_{i=1}^Ny_i^2-\frac{N}{2}\log(-2\eta_2)+\frac{N}{4}\frac{\eta_1^2}{\eta_2}\right\},
\end{align}\]</span></p>
<p>where <span class="math inline">\(B(\mathbf{\eta})=\frac{N}{2}\log(-2\eta_2)-\frac{N}{4}\frac{\eta_1^2}{\eta_2}\)</span>. Then,</p>
<p><span class="math display">\[\begin{align}
  \nabla B(\mathbf{\eta}) &amp; = \begin{bmatrix}
    -\frac{N}{2}\frac{\eta_1}{\eta_2}\\
    -\frac{N}{2}\frac{1}{\eta_2}+\frac{N}{4}\frac{\eta_1^2}{\eta_2^2}
  \end{bmatrix}
   =
  \begin{bmatrix}
    N\times\mu\\
    N\times(\mu^2+\sigma^2)
  \end{bmatrix}  = \begin{bmatrix}
    \mathbb{E}\left[\sum_{i=1}^N y_i\bigr\rvert \mu,\sigma^2\right]\\
    \mathbb{E}\left[\sum_{i=1}^N y_i^2\bigr\rvert \mu,\sigma^2\right]
  \end{bmatrix}.
\end{align}\]</span></p>
<!-- * Given $\mathbf{y}\sim N_N(\mathbf{\mu},\mathbf{\Sigma})$, that is, a **multivariate normal distribution** show that $p(\mathbf{y}|\mathbf{\mu},\mathbf{\Sigma})$ is in the exponential family. -->
<!-- \begin{align} -->
<!--   p(\mathbf{y}|\mathbf{\mu},\mathbf{\Sigma})&= (2\pi)^{-N/2}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}\left(\mathbf{y}-\mathbf{\mu}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{y}-\mathbf{\mu}\right)\right\}\\ -->
<!--   &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{y}-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}\\ -->
<!--   &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(tr\left\{\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{y}\right\}-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}\\ -->
<!--   &= (2\pi)^{-N/2}\exp\left\{-\frac{1}{2}\left(vec\left(\mathbf{y}\mathbf{y}^{\top}\right)^{\top}vec\left(\mathbf{\Sigma}^{-1}\right)-2\mathbf{y}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}, -->
<!-- \end{align} -->
<!-- where $tr$ and $vec$ are the trace and vectorization operators, respectively. -->
<!-- Then $h(\mathbf{y})=(2\pi)^{-N/2}$, $\eta(\mathbf{\mu},\mathbf{\Sigma})=\left[\mathbf{\Sigma}^{-1}\mathbf{\mu} \ \ vec\left(\mathbf{\Sigma}^{-1}\right)\right]$, $T(\mathbf{y})=\left[\mathbf{y} \ \ -\frac{1}{2}vec(\mathbf{y}\mathbf{y}^{\top})\right]$ and $C(\mathbf{\mu},\mathbf{\Sigma})=\exp\left\{-\frac{1}{2N}\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}+\log(|\Sigma|)\right)\right\}$. -->
<ul>
<li>Given <span class="math inline">\(\mathbf{Y}=[\mathbf{y}_1 \ \mathbf{y}_2 \ \dots \ \mathbf{y}_p]\)</span> a <span class="math inline">\(N\times p\)</span> matrix such that <span class="math inline">\(\mathbf{y}_i\sim N_p(\mathbf{\mu},\mathbf{\Sigma})\)</span>, <span class="math inline">\(i=1,2,\dots,N\)</span>, that is, each <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\mathbf{Y}\)</span> follows a <strong>multivariate normal distribution</strong>. Then, assuming independence between rows, let’s show that <span class="math inline">\(p(\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_N|\mathbf{\mu},\mathbf{\Sigma})\)</span> is in the exponential family.</li>
</ul>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_N|\mathbf{\mu},\mathbf{\Sigma})&amp;=\prod_{i=1}^N (2\pi)^{-p/2}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}\left(\mathbf{y}_i-\mathbf{\mu}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{y}_i-\mathbf{\mu}\right)\right\}\\
  &amp;= (2\pi)^{-pN/2}|\Sigma|^{-N/2}\exp\left\{-\frac{1}{2}tr\left[\sum_{i=1}^N\left(\mathbf{y}_i-\mathbf{\mu}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{y}_i-\mathbf{\mu}\right)\right]\right\}\\
  &amp;= (2\pi)^{-p N/2}|\Sigma|^{-N/2}\exp\left\{-\frac{1}{2}tr\left[\left(\mathbf{S}+N\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)^{\top}\right)\mathbf{\Sigma}^{-1}\right]\right\}\\
  &amp;= (2\pi)^{-p N/2}\exp\left\{-\frac{1}{2}\left[\left(vec\left(\mathbf{S}\right)^{\top}+N vec\left(\hat{\mathbf{\mu}}\hat{\mathbf{\mu}}^{\top}\right)^{\top}\right)vec \left(\mathbf{\Sigma}^{-1}\right)\right.\right.\\
  &amp;\left.\left.-2N\hat{\mathbf{\mu}}^{\top}vec\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)+N tr\left(\mathbf{\mu}\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)+N\log (|\mathbf{\Sigma}|)\right]\right\}\\

\end{align}\]</span></p>
<p>where the second line uses the trace operator (<span class="math inline">\(tr\)</span>), and its invariability under cyclic permutation is used in the third line. In addition, we add and subtract <span class="math inline">\(\hat{\mathbf{\mu}}=\frac{1}{N}\sum_{i=1}^N\mathbf{y}_i\)</span> in each parenthesis such that we get <span class="math inline">\(\mathbf{S}=\sum_{i=1}^N\left(\mathbf{y}_i-\hat{\mathbf{\mu}}\right)\left(\mathbf{y}_i-\hat{\mathbf{\mu}}\right)^{\top}\)</span>. We get the fourth line after using some properties of the trace operator to introduce the vectorization operator (<span class="math inline">\(vec\)</span>), and collecting terms.</p>
<p>Then <span class="math inline">\(h(\mathbf{y})=(2\pi)^{-pN/2}\)</span>, <span class="math inline">\(\eta(\mathbf{\mu},\mathbf{\Sigma})^{\top}=\left[\left(vec\left(\mathbf{\Sigma}^{-1}\right)\right)^{\top} \ \ \left(vec\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)\right)^{\top}\right]\)</span>, <span class="math inline">\(T(\mathbf{y})=\left[-\frac{1}{2}\left(vec\left(\mathbf{S}\right)^{\top}+N vec\left(\hat{\mathbf{\mu}}\hat{\mathbf{\mu}}^{\top}\right)^{\top}\right) \ \ -N\hat{\mathbf{\mu}}^{\top}\right]^{\top}\)</span> and <span class="math inline">\(C(\mathbf{\mu},\mathbf{\Sigma})=\exp\left\{-\frac{1}{2}\left(tr\left(\mathbf{\mu}\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)+\log(|\Sigma|)\right)\right\}\)</span>.</p>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chib1995marginal" class="csl-entry">
Chib, Siddhartha. 1995. <span>“Marginal Likelihood from the Gibbs Output.”</span> <em>Journal of the American Statistical Association</em> 90 (432): 1313–21.
</div>
<div id="ref-chib2001marginal" class="csl-entry">
Chib, Siddhartha, and Ivan Jeliazkov. 2001. <span>“Marginal Likelihood from the Metropolis–Hastings Output.”</span> <em>Journal of the American Statistical Association</em> 96 (453): 270–81.
</div>
<div id="ref-Gelfand1990" class="csl-entry">
Gelfand, A. E., and A. F. M. Smith. 1990. <span>“Sampling-Based Approaches to Calculating Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 85: 398–409.
</div>
<div id="ref-gelfand1994bayesian" class="csl-entry">
Gelfand, Alan E, and Dipak K Dey. 1994. <span>“Bayesian Model Choice: Asymptotics and Exact Calculations.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 56 (3): 501–14.
</div>
<div id="ref-Geman1984" class="csl-entry">
Geman, S, and D. Geman. 1984. <span>“Stochastic Relaxation, <span>G</span>ibbs Distributions and the <span>B</span>ayesian Restoration of Images.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 6: 721–41.
</div>
<div id="ref-hastings70" class="csl-entry">
Hastings, W. 1970. <span>“Monte <span>C</span>arlo Sampling Methods Using <span>M</span>arkov Chains and Their Application.”</span> <em>Biometrika</em> 57: 97–109.
</div>
<div id="ref-Jordan1999" class="csl-entry">
Jordan, Ghahramani, M. I., and L. Saul. 1999. <span>“Introduction to Variational Methods for Graphical Models.”</span> <em>Machine Learning</em> 37: 183–233.
</div>
<div id="ref-metropolis53" class="csl-entry">
Metropolis, N., A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. 1953. <span>“Equations of State Calculations by Fast Computing Machines.”</span> <em>J. Chem. Phys</em> 21: 1087–92.
</div>
<div id="ref-schlaifer1961applied" class="csl-entry">
Schlaifer, Robert, and Howard Raiffa. 1961. <em>Applied Statistical Decision Theory</em>.
</div>
<div id="ref-tierney1994markov" class="csl-entry">
Tierney, Luke. 1994. <span>“Markov Chains for Exploring Posterior Distributions.”</span> <em>The Annals of Statistics</em>, 1701–28.
</div>
<div id="ref-Tierney1986" class="csl-entry">
Tierney, Luke, and Joseph B Kadane. 1986. <span>“Accurate Approximations for Posterior Moments and Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 81 (393): 82–86.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conjfam.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec42.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
