<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2024-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec41.html"/>
<link rel="next" href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec25A.html"><a href="sec25A.html"><i class="fa fa-check"></i><b>2.6</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.7</b> A simple working example</a></li>
<li class="chapter" data-level="2.8" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.8</b> Summary: Chapter 2</a></li>
<li class="chapter" data-level="2.9" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.9</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a>
<ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>4.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="4.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>4.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="4.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.5</b> Computational examples</a></li>
<li class="chapter" data-level="4.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>4.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="4.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="normal-model.html"><a href="normal-model.html"><i class="fa fa-check"></i><b>6.1</b> Normal model</a></li>
<li class="chapter" data-level="6.2" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i><b>6.2</b> Logit model</a></li>
<li class="chapter" data-level="6.3" data-path="probit-model.html"><a href="probit-model.html"><i class="fa fa-check"></i><b>6.3</b> Probit model</a></li>
<li class="chapter" data-level="6.4" data-path="summary-chapter-6.html"><a href="summary-chapter-6.html"><i class="fa fa-check"></i><b>6.4</b> Summary: Chapter 6</a></li>
<li class="chapter" data-level="6.5" data-path="exercises-chapter-6.html"><a href="exercises-chapter-6.html"><i class="fa fa-check"></i><b>6.5</b> Exercises: Chapter 6</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec42" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Conjugate prior to exponential family<a href="sec42.html#sec42" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Theorem 4.2.1</strong></p>
<p>The prior distribution <span class="math inline">\(\pi(\mathbf{\theta})\propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}\)</span> is conjugate to the exponential family (equation <a href="sec41.html#eq:414">(4.4)</a>).</p>
<p><strong>Proof</strong></p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp; \propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\} \times h(\mathbf{y}) C(\mathbf{\theta})^N\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\}\\
  &amp; \propto C(\mathbf{\theta})^{N+b_0} \exp\left\{\eta(\mathbf{\theta})^{\top}(\mathbf{T}(\mathbf{y})+\mathbf{a}_0\right\}.
\end{align}\]</span></p>
<p>Observe that the posterior is in the exponential family, <span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y})\propto C(\mathbf{\theta})^{\beta_n} \exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{\alpha}_n\right\}\)</span>, <span class="math inline">\(\beta_n=N+b_0\)</span> and <span class="math inline">\(\mathbf{\alpha}_n=\mathbf{T}(\mathbf{y})+\mathbf{a}_0\)</span>.</p>
<p><em>Remarks</em></p>
<p>We see comparing the prior and the likelihood that <span class="math inline">\(b_0\)</span> plays the role of a hypothetical sample size, and <span class="math inline">\(\mathbf{a}_0\)</span> plays the role of hypothetical sufficient statistics. This view helps the elicitation process.</p>
<p>In addition, we stablished the result in the <em>standard form</em> of the exponential family. We can also stablish this result in the <em>canonical form</em> of the exponential family. Observe that given <span class="math inline">\(\mathbf{\eta}=\mathbf{\eta}(\mathbf{\theta})\)</span> another way to get a prior for <span class="math inline">\(\mathbf{\eta}\)</span> is to use the change of variables theorem given a bijective function.</p>
<p>In the setting where there is a prior regular conjugate prior <span class="citation">(<a href="#ref-diaconis1979conjugate">Diaconis, Ylvisaker, et al. 1979</a>)</span> show that we obtain a posterior expectation of the sufficient statistics that is a weighted average between the prior expectation and the likelihood estimate.</p>
<p><strong>Examples: Theorem 4.2.1</strong></p>
<ol style="list-style-type: decimal">
<li>Likelihood functions from discrete distributions</li>
</ol>
<ul>
<li><strong>The Poisson-gamma model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Poisson distribution</strong> then a conjugate prior density for <span class="math inline">\(\lambda\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda)&amp;\propto \left(\exp(-\lambda)\right)^{b_0} \exp\left\{a_0\log(\lambda)\right\}\\
&amp; = \exp(-\lambda b_0) \lambda^{a_0}\\
&amp; = \exp(-\lambda \beta_0) \lambda^{\alpha_0-1}.
\end{align}\]</span></p>
<p>This is the kernel of a gamma density in the <em>rate parametrization</em>, <span class="math inline">\(G(\alpha_0,\beta_0)\)</span>, <span class="math inline">\(\alpha_0=a_0+1\)</span> and <span class="math inline">\(\beta_0=b_0\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Then, a prior conjugate distribution for the Poisson likelihood is a gamma distribution.</p>
<p>Taking into account that <span class="math inline">\(\sum_{i=1}^N y_i\)</span> is a sufficient statistic for the Poisson distribution, then we can think about <span class="math inline">\(a_0\)</span> as the number of occurrences in <span class="math inline">\(b_0\)</span> experiments.
Observe that</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda|\mathbf{y})&amp;\propto \exp(-\lambda \beta_0) \lambda^{\alpha_0-1} \times \exp(-N\lambda)\lambda^{\sum_{i=1}^Ny_i}\\
  &amp;= \exp(-\lambda(N+\beta_0)) \lambda^{\sum_{i=1}^Ny_i+\alpha_0-1}.
\end{align}\]</span></p>
<p>As expected, this is the kernel of a gamma distribution, which means <span class="math inline">\(\lambda|\mathbf{y}\sim G(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\sum_{i=1}^Ny_i+\alpha_0\)</span> and <span class="math inline">\(\beta_n=N+\beta_0\)</span>.</p>
<p>Observe that <span class="math inline">\(\alpha_0/\beta_0\)</span> is the prior mean, and <span class="math inline">\(\alpha_0/\beta_0^2\)</span> is the prior variance. Then, <span class="math inline">\(\alpha_0\rightarrow 0\)</span> and <span class="math inline">\(\beta_0\rightarrow 0\)</span> imply a non-informative prior such that the posterior mean converges to the maximum likelihood estimator <span class="math inline">\(\bar{y}=\frac{\sum_{i=1}^N y_i}{N}\)</span>,</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}\left[\lambda|\mathbf{y}\right]&amp;=\frac{\alpha_n}{\beta_n}\\
  &amp;=\frac{\sum_{i=1}^Ny_i+\alpha_0}{N+\beta_0}\\
  &amp;=\frac{N\bar{y}}{N+\beta_0}+\frac{\alpha_0}{N+\beta_0}.
\end{align}\]</span></p>
<p>The posterior mean is a weighted average between sample and prior information. This is a general result from regular conjugate priors <span class="citation">(<a href="#ref-diaconis1979conjugate">Diaconis, Ylvisaker, et al. 1979</a>)</span>. Observe that <span class="math inline">\(\mathbb{E}\left[\lambda|\mathbf{y}\right]=\bar{y}, \lim N\rightarrow\infty\)</span>.</p>
<p>In addition, <span class="math inline">\(\alpha_0\rightarrow 0\)</span> and <span class="math inline">\(\beta_0\rightarrow 0\)</span> corresponds to <span class="math inline">\(\pi(\lambda)\propto \frac{1}{\lambda}\)</span>, which is an improper prior. Improper priors have bad consequences on Bayes factors (hypothesis testing). In this setting, we can get analytical solutions for the marginal likelihood and the predictive distribution (see the health insurance example and exercise 3 in Chapter <a href="basics.html#basics">1</a>).</p>
<ul>
<li><strong>The Bernoulli-beta model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Bernoulli distribution</strong> then a conjugate prior density for <span class="math inline">\(\lambda\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\theta)&amp;\propto (1-\theta)^{b_0} \exp\left\{a_0\log\left(\frac{\theta}{1-\theta}\right)\right\}\\
&amp; = (1-\theta)^{b_0-a_0}\theta^{a_0}\\
&amp; = \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}.
\end{align}\]</span></p>
<p>This is the kernel of a beta density, <span class="math inline">\(B(\alpha_0,\beta_0)\)</span>, <span class="math inline">\(\alpha_0=a_0+1\)</span> and <span class="math inline">\(\beta_0=b_0-a_0+1\)</span>. A prior conjugate distribution for the Bernoulli likelihood is a beta distribution. Given that <span class="math inline">\(b_0\)</span> is the hypothetical sample size, and <span class="math inline">\(a_0\)</span> is the hypothetical sufficient statistic, which is the number of successes, then <span class="math inline">\(b_0-a_0\)</span> is the number of failures. This implies that <span class="math inline">\(\alpha_0\)</span> is the number of prior successes plus one, and <span class="math inline">\(\beta_0\)</span> is the number of prior failures plus one. Given that the mode of a beta distribuited random variable is <span class="math inline">\(\frac{\alpha_0-1}{\alpha_0+\beta_0-2}=\frac{a_0}{b_0}\)</span>, then we have the a priori probability of success. Setting <span class="math inline">\(\alpha_0=1\)</span> and <span class="math inline">\(\beta_0=1\)</span>, which implies a 0-1 uniform distribution, corresponds to a setting with 0 successes (and 0 failures) in 0 experiments.</p>
<p>Observe that</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda|\mathbf{y})&amp;\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^Ny_i}\\
  &amp;= \theta^{\alpha_0+\sum_{i=1}^N y_i-1}(1-\theta)^{\beta_0+N-\sum_{i=1}^Ny_i-1}.
\end{align}\]</span></p>
<p>The posterior distribution is beta, <span class="math inline">\(\theta|\mathbf{y}\sim B(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\alpha_0+\sum_{i=1}^N y_i\)</span> and <span class="math inline">\(\beta_n=\beta_0+N-\sum_{i=1}^Ny_i\)</span>, where the posterior mean <span class="math inline">\(\mathbf{E}[\theta|\mathbf{y}]=\frac{\alpha_n}{\alpha_n+\beta_n}=\frac{\alpha_0+N\bar{y}}{\alpha_0+\beta_0+N}=\frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+N}\frac{\alpha_0}{\alpha_0+\beta_0}+\frac{N}{\alpha_0+\beta_0+N}\bar{y}\)</span>. The posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>El marginal likelihood in this setting is</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})=&amp;\int_{0}^1 \frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0,\beta_0)}\times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}d\theta\\
  =&amp; \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)},
\end{align}\]</span></p>
<p>where <span class="math inline">\(B(\cdot ,\cdot)\)</span> is the beta function.</p>
<p>In addition, the predictive density is</p>
<p><span class="math display">\[\begin{align}
  p(Y_0|\mathbf{y})&amp;=\int_0^1 \theta^{y_0}(1-\theta)^{1-y_0}\times \frac{\theta^{\alpha_n-1}(1-\theta)^{\beta_n-1}}{B(\alpha_n,\beta_n)}d\theta\\
  &amp;=\frac{B(\alpha_n+y_0,\beta_n+1-y_0)}{B(\alpha_n,\beta_n)}\\
  &amp;=\frac{\Gamma(\alpha_n+\beta_n)\Gamma(\alpha_n+y_0)\Gamma(\beta_n+y_0)}{\Gamma(\alpha_n+\beta_n+1\Gamma(\alpha_n)\Gamma(\beta_n)}\\
  &amp;=\begin{Bmatrix}
  \frac{\alpha_n}{\alpha_n+\beta_n}, &amp; y_0=1\\
  \frac{\beta_n}{\alpha_n+\beta_n}, &amp; y_0=0\\
  \end{Bmatrix}.
\end{align}\]</span></p>
<p>This is a Bernoulli distribution with probability of success equal to <span class="math inline">\(\frac{\alpha_n}{\alpha_n+\beta_n}\)</span>.</p>
<ul>
<li><strong>The multinomial-Dirichlet model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>multinomial distribution</strong> then a conjugate prior density for <span class="math inline">\(\mathbf{\theta}=\left[\theta_1,\theta_2,\dots,\theta_m\right]\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta})&amp;\propto \theta_m^{b_0} \exp\left\{\mathbf{\eta}(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}\\
&amp; = \prod_{l=1}^{m-1}\theta_l^{a_{0l}}\theta_m^{b_0-\sum_{l=1}^{m-1}a_{0l}}\\
&amp; = \prod_{l=1}^{m}\theta_l^{\alpha_{0l}-1},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\eta}(\mathbf{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right),\dots,\log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]\)</span>, <span class="math inline">\(\mathbf{a}_0=\left[a_{01},\dots,a_{am-1}\right]^{\top}\)</span>, <span class="math inline">\(\mathbf{\alpha}_0=\left[\alpha_{01},\alpha_{02},\dots,\alpha_{0m}\right]\)</span>, <span class="math inline">\(\alpha_{0l}=a_{0l}+1\)</span>, <span class="math inline">\(l=1,2,\dots,m-1\)</span> and <span class="math inline">\(\alpha_{0m}=b_0-\sum_{l=1}^{m-1} a_{0l}+1\)</span>.</p>
<p>This is the kernel of a Dirichlet distribution, that is, the prior distribution is <span class="math inline">\(D(\mathbf{\alpha}_0)\)</span>.</p>
<p>Observe that <span class="math inline">\(a_{0l}\)</span> is the number of hypothetical number of times outcome <span class="math inline">\(l\)</span> is observed over the hypothetical <span class="math inline">\(b_0\)</span> trials. Setting <span class="math inline">\(\alpha_{0l}=1\)</span>, that is a uniform distribution over the open standard simplex, implicitly we set <span class="math inline">\(a_{0l}=0\)</span>, which means that there are 0 occurrences of category <span class="math inline">\(l\)</span> in <span class="math inline">\(b_0=0\)</span> experiments.</p>
<p>The posterior distribution of the multinomial-Dirichlet model is given by</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp;\propto \prod_{l=1}^m \theta_l^{\alpha_{0l}-1}\times\prod_{l=1}^m \theta_l^{\sum_{i=1}^{N} y_{il}}\\
  &amp;=\prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^{N} y_{il}-1}
\end{align}\]</span></p>
<p>This is the kernel of a Dirichlet distribution <span class="math inline">\(D(\mathbf{\alpha}_n)\)</span>, <span class="math inline">\(\mathbf{\alpha}_n=\left[\alpha_{n1},\alpha_{n2},\dots,\alpha_{nm}\right]\)</span>, <span class="math inline">\(\alpha_{nl}=\alpha_{0l}+\sum_{i=1}^{N}y_{il}\)</span>, <span class="math inline">\(l=1,2,\dots,m\)</span>. Observe that</p>
<p><span class="math display">\[\begin{align}

\mathbb{E}[\theta_{j}|\mathbf{y}]&amp;=\frac{\alpha_{nj}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\\
&amp;=\frac{\sum_{l=1}^m \alpha_{0l}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\alpha_{0j}}{\sum_{l=1}^m \alpha_{0l}}+\frac{\sum_{l=1}^m\sum_{i=1}^N y_{il}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\sum_{i=1}^N y_{ij}}{\sum_{l=1}^m\sum_{i=1}^N y_{il}}.
\end{align}\]</span></p>
<p>We have again that the posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>The marginal likelihood is</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})&amp;=\int_{\mathbf{\Theta}}\frac{\prod_{l=1}^m \theta_l^{\alpha_{0l}-1}}{B(\mathbf{\alpha}_0)}\times \prod_{i=1}^N\frac{n!}{\prod_{l=1}^m y_{il}}\prod_{l=1}^m \theta_{l}^{y_{il}}d\mathbf{\theta}\\
  &amp;=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\int_{\mathbf{\Theta}} \prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^N y_{il}-1} d\mathbf{\theta}\\
  &amp;=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}B(\mathbf{\alpha}_n)\\
  &amp;=\frac{N\times n! \Gamma\left(\sum_{l=1}^n \alpha_{0l}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{0l}+N\times n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}\right)}{\Gamma\left(\alpha_{0l}\right)\prod_{i=1}^N y_{il}!},
\end{align}\]</span></p>
<p>where <span class="math inline">\(B(\mathbf{\alpha})=\frac{\prod_{l=1}^m\Gamma(\alpha_l)}{\Gamma\left(\sum_{l=1}^m \alpha_l\right)}\)</span>.</p>
<p>Following similar steps we get the predictive density</p>
<p><span class="math display">\[\begin{align}
  p(Y_0|\mathbf{y})&amp;=\frac{ n! \Gamma\left(\sum_{l=1}^n \alpha_{nl}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{nl}+ n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}+y_{0l}\right)}{\Gamma\left(\alpha_{nl}\right) y_{0l}!}.
\end{align}\]</span></p>
<p>This is a Dirichlet-multinomial distribution with parameters <span class="math inline">\(\mathbf{\alpha}_n\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Likelihood functions from continuos distributions</li>
</ol>
<ul>
<li><strong>The normal-normal/inverse-gamma model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a normal distribution, then the conjugate prior density has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\mu,\sigma^2)&amp;\propto \exp\left\{b_0\left(-\frac{\mu^2}{2\sigma^2}-\frac{\log \sigma^2}{2}\right)\right\}\exp\left\{a_{01}\frac{\mu}{\sigma^2}-a_{02}\frac{1}{\sigma^2}\right\}\\
  &amp;=\exp\left\{b_0\left(-\frac{\mu^2}{2\sigma^2}-\frac{\log \sigma^2}{2}\right)\right\}\exp\left\{a_{01}\frac{\mu}{\sigma^2}-a_{02}\frac{1}{\sigma^2}\right\}\exp\left\{-\frac{a_{01}^2}{2\sigma^2b_0}\right\}\exp\left\{\frac{a_{01}^2}{2\sigma^2b_0}\right\}\\
  &amp;=\exp\left\{-\frac{b_0}{2\sigma^2}\left(\mu-\frac{a_{01}}{b_0}\right)^2\right\}\left(\frac{1}{\sigma^2}\right)^{\frac{b_0+1-1}{2}}\exp\left\{\frac{1}{\sigma^2}\frac{-2b_0a_{02}+a_{01}^2}{2b_0}\right\}\\
  &amp;=\underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{1}{2}}\exp\left\{-\frac{b_0}{2\sigma^2}\left(\mu-\frac{a_{01}}{b_0}\right)^2\right\}}_{1}\underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{b_0-1}{2}}\exp\left\{-\frac{1}{\sigma^2}\frac{2b_0a_{02}-a_{01}^2}{2b_0}\right\}}_{2}.
\end{align}\]</span></p>
<p>The first part is the kernel of a normal density with mean <span class="math inline">\(\mu_0=a_{01}/\beta_0\)</span> and variance <span class="math inline">\(\sigma^2/\beta_0\)</span>, <span class="math inline">\(\beta_0=b_0\)</span> that is, <span class="math inline">\(\mu|\sigma^2\sim N(\mu_0,\sigma^2/\beta_0)\)</span>. The second part is the kernel of an inverse gamma density with shape parameter <span class="math inline">\(\alpha_0/2=\frac{\beta_0-3}{2}\)</span>, and scale parameter <span class="math inline">\(\delta_0/2=\frac{2\beta_0a_{02}-a_{01}^2}{2\beta_0}\)</span>, <span class="math inline">\(\sigma^2\sim IG(\alpha_0/2,\delta_0/2)\)</span>. Observe that <span class="math inline">\(b_0=\beta_0\)</span> is the hypothetical sample size, and <span class="math inline">\(a_{01}\)</span> is the hypothetical sum of prior observations, then, it makes sense that <span class="math inline">\(a_{01}/\beta_0\)</span> and <span class="math inline">\(\sigma^2/\beta_0\)</span> are the prior mean and variance, respectively.</p>
<p>Therefore, the posterior distribution is also a normal-inverse gamma distribution,</p>
<p><span class="math display">\[\begin{align}
  \pi(\mu,\sigma^2|\mathbf{y})&amp;\propto \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{\beta_0}{2\sigma^2}(\mu-\mu_0)^2\right\}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}\\
  &amp;\times(\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N (y_i-\mu)^2\right\}\\
  &amp; = \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}\left(\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N (y_i-\bar{y})^2+N(\mu-\bar{y})^2+\delta_0\right)\right\}\\
  &amp; \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1} + \frac{(\beta_0\mu_0+N\bar{y})^2}{\beta_0+N} - \frac{(\beta_0\mu_0+N\bar{y})^2}{\beta_0+N}\\
  &amp; = \underbrace{\left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}\left((\beta_0+N)\left(\mu-\left(\frac{\beta_0\mu_0+N\bar{y}}{\beta_0+N}\right)\right)^2\right)\right\}}_{1}\\
  &amp; \times \underbrace{\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}\left(\sum_{i=1}^N (y_i-\bar{y})^2+\delta_0+\frac{\beta_0N}{\beta_0+N}(\bar{y}-\mu_0)^2\right)\right\}}_{2}.
\end{align}\]</span></p>
<p>The first term is the kernel of a normal density, <span class="math inline">\(\mu|\sigma^2,\mathbf{y}\sim N \left(\mu_n, \sigma_n^2\right)\)</span>, where <span class="math inline">\(\mu_n=\frac{\beta_0\mu_0+N\bar{y}}{\beta_0+N}\)</span> and <span class="math inline">\(\sigma_n^2=\frac{\sigma^2}{\beta_n}\)</span>, <span class="math inline">\(\beta_n=\beta_0+N\)</span>. The second term is the kernel of an inverse gamma density, <span class="math inline">\(\sigma^2|\mathbf{y}\sim IG(\alpha_n/2,\delta_n/2)\)</span> where <span class="math inline">\(\alpha_n=\alpha_0+N\)</span> and <span class="math inline">\(\delta_n=\sum_{i=1}^N (y_i-\bar{y})^2+\delta_0+\frac{\beta_0N}{\beta_0+N}(\bar{y}-\mu_0)^2\)</span>. Observe that the posterior mean is a weighted average between prior and sample information. The weights depends on the sample sizes (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(N\)</span>).</p>
<p>The marginal posterior for <span class="math inline">\(\sigma^2\)</span> is inverse gamma with shape and scale parameters <span class="math inline">\(\alpha_n/2\)</span> and <span class="math inline">\(\delta_n/2\)</span>, respectively. The marginal posterior of <span class="math inline">\(\mu\)</span> is</p>
<p><span class="math display">\[\begin{align}
  \pi(\mu|\mathbf{y})&amp;\propto \int_{0}^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+1}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\beta_n(\mu-\mu_n)^2+\delta_n)\right\}\right\}d\sigma^2\\
  &amp;=\frac{\Gamma\left(\frac{\alpha_n+1}{2}\right)}{\left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{\frac{\alpha_n+1}{2}}}\\
  &amp;\propto \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}\left(\frac{\delta_n}{\delta_n}\right)^{-\frac{\alpha_n+1}{2}}\\
  &amp;\propto \left[\frac{\alpha_n\beta_n(\mu-\mu_n)^2}{\alpha_n\delta_n}+1\right]^{-\frac{\alpha_n+1}{2}},
\end{align}\]</span></p>
<p>where the second line due to having the kernel of an inverse gamma density with parameters <span class="math inline">\((\alpha_n+1)/2\)</span> and <span class="math inline">\(-\frac{1}{2\sigma^2}(\beta_n(\mu-\mu_n)^2+\delta_n)\)</span>.</p>
<p>This is the kernel of a Student’s t distribution, <span class="math inline">\(\mu|\mathbf{y}\sim t(\mu_n,\delta_n/\beta_n\alpha_n,\alpha_n)\)</span>, where <span class="math inline">\(\mathbb{E}[\mu|\mathbf{y}]=\mu_n\)</span> and <span class="math inline">\(Var[\mu|\mathbf{y}]=\frac{\alpha_n}{\alpha_n-2}\left(\frac{\delta_n}{\beta_n\alpha_n}\right)=\frac{\delta_n}{(\alpha_n-2)\beta_n}\)</span>, <span class="math inline">\(\alpha_n&gt;2\)</span>. Observe that the marginal posterior distribution for <span class="math inline">\(\mu\)</span> has heavier tails than the conditional posterior distribution due to incorporating uncertainty regarding <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The marginal likelihood is</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})&amp;=\int_{-\infty}^{\infty}\int_{0}^{\infty}\left\{ (2\pi\sigma^2/\beta_0)^{-1/2}\exp\left\{-\frac{1}{2\sigma^2/\beta_0}(\mu-\mu_0)^2\right\}\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\right.\\
  &amp;\times\left.\exp\left\{-\frac{\delta_0}{2\sigma^2}\right\}(2\pi\sigma^2)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^N(y_i-\mu)^2\right\}\right\}d\sigma^2d\mu\\
  &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\int_{-\infty}^{\infty}\int_{0}^{\infty}\left\{\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N+1}{2}+1}\right.\\
  &amp;\times\left.\exp\left\{-\frac{1}{2\sigma^2}(\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N (y_i-\mu)^2+\delta_0)\right\}\right\}d\sigma^2d\mu\\
  &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{N+1+\alpha_0}{2}\right)\\
  &amp;\times \int_{-\infty}^{\infty} \left[\frac{\beta_0(\mu-\mu_0)^2+\sum_{i=1}^N(y_i-\mu)^2+\delta_0}{2}\right]^{-\frac{\alpha_0+N+1}{2}}d\mu\\
  &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{N+1+\alpha_0}{2}\right)\\
  &amp;\times \int_{-\infty}^{\infty} \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n/2}{\delta_n/2}\right)^{-\frac{\alpha_n+1}{2}}\\
  &amp;=\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}(2\pi)^{-\left(\frac{N+1}{2}\right)}\beta_0^{1/2}\Gamma\left(\frac{\alpha_n+1}{2}\right)\left(\frac{\delta_n}{2}\right)^{-\frac{\alpha_n+1}{2}}\frac{\left(\frac{\delta_n\pi}{\beta_n}\right)^{1/2}\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_n+1}{2}\right)}\\
  &amp;=\frac{\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_0}{2}\right)}\frac{(\delta_0/2)^{\alpha_0/2}}{(\delta_n/2)^{\alpha_n/2}}\left(\frac{\beta_0}{\beta_n}\right)^{1/2}(\pi)^{-N/2},
\end{align}\]</span></p>
<p>where we take into account that <span class="math inline">\(\int_{-\infty}^{\infty} \left[\frac{\beta_n(\mu-\mu_n)^2+\delta_n}{2}\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n/2}{\delta_n/2}\right)^{-\frac{\alpha_n+1}{2}}=\int_{-\infty}^{\infty} \left[\frac{\beta_n\alpha_n(\mu-\mu_n)^2}{\delta_n\alpha_n}+1\right]^{-\frac{\alpha_n+1}{2}}d\mu\left(\frac{\delta_n}{2}\right)^{-\frac{\alpha_n+1}{2}}\)</span>. The term in the integral is the kernel of a Student’s t density, this means that the integral is equal to <span class="math inline">\(\frac{\left(\frac{\delta_n\pi}{\beta_n}\right)^{1/2}\Gamma\left(\frac{\alpha_n}{2}\right)}{\Gamma\left(\frac{\alpha_n+1}{2}\right)}\)</span>.</p>
<p>The predictive density is</p>
<p><span class="math display">\[\begin{align}
  \pi(Y_0|\mathbf{y})&amp;\propto\int_{-\infty}^{\infty}\int_0^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{1}{2\sigma^2}(y_0-\mu)^2\right\}\left(\frac{1}{\sigma^2}\right)^{1/2}\exp\left\{-\frac{\beta_n}{2\sigma^2}(\mu-\mu_n)^2\right\}\right.\\
  &amp;\times \left.\left(\frac{1}{\sigma^2}\right)^{\alpha_n/2+1}\exp\left\{-\frac{\delta_n}{2\sigma^2}\right\}\right\}d\sigma^2d\mu\\
  &amp;=\int_{-\infty}^{\infty}\int_0^{\infty}\left\{ \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+2}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}((y_0-\mu)^2+\beta_n(\mu-\mu_n)^2+\delta_n)\right\}\right\}d\sigma^2d\mu\\
  &amp;\propto\int_{-\infty}^{\infty}\left[\beta_n(\mu-\mu_n)^2+(y_0-\mu)^2+\delta_n\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\\
  &amp;=\int_{-\infty}^{\infty}\left[(\beta_n+1)\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2+\frac{\beta_n(y_0-\mu_n)^2}{\beta_n+1}+\delta_n\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\\
  &amp;=\int_{-\infty}^{\infty}\left[1+\frac{(\beta_n+1)^2\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2}{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}\right]^{-\left(\frac{\alpha_n}{2}+1\right)}d\mu\\
  &amp;\times\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{\beta_n+1}\right)^{-\left(\frac{\alpha_n}{2}+1\right)}\\
  &amp;\propto\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{(\beta_n+1)^2(\alpha_n+1)}\right)^{\frac{1}{2}}\left(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{\beta_n+1}\right)^{-\left(\frac{\alpha_n}{2}+1\right)}\\
  &amp;\propto (\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n)^{\left(\frac{\alpha_n+1}{2}\right)}\\
  &amp;\propto\left[1+\frac{\beta_n\alpha_n}{(\beta_n+1)\delta_n\alpha_n}(y_0-\mu_n)^2\right]^{-\left(\frac{\alpha_n+1}{2}\right)},
\end{align}\]</span></p>
<p>where we have that <span class="math inline">\(\left[1+\frac{(\beta_n+1)^2\left(\mu-\left(\frac{\beta_n\mu_n+y_0}{\beta_n+1}\right)\right)^2}{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}\right]^{-\left(\frac{\alpha_n}{2}+1\right)}\)</span> is the kernel of a Student’s t density with degrees of freedom <span class="math inline">\(\alpha_n+1\)</span> and scale <span class="math inline">\(\frac{\beta_n(y_0-\mu_n)^2+(\beta_n+1)\delta_n}{(\beta_n+1)^2(\alpha_n+1)}\)</span>.</p>
<p>The last expression is the kernel of a Student’s t density, that is, <span class="math inline">\(Y_0|\mathbf{y}\sim t\left(\mu_n,\frac{(\beta_n+1)\delta_n}{\beta_n\alpha_n},\alpha_n\right)\)</span>.</p>
<ul>
<li><strong>The multivariate normal-normal/inverse-Wishart model</strong></li>
</ul>
<p>We show in the subsection <a href="sec41.html#sec41">4.1</a> that the multivariate normal distribution is in the exponential family where <span class="math inline">\(h(\mathbf{y})=(2\pi)^{-pN/2}\)</span>, <span class="math inline">\(\eta(\mathbf{\mu},\mathbf{\Sigma})^{\top}=\left[\left(vec\left(\mathbf{\Sigma}^{-1}\right)\right)^{\top} \ \ \left(vec\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)\right)^{\top}\right]\)</span>, <span class="math inline">\(T(\mathbf{y})=\left[-\frac{1}{2}\left(vec\left(\mathbf{S}\right)^{\top}+N vec\left(\hat{\mathbf{\mu}}\hat{\mathbf{\mu}}^{\top}\right)^{\top}\right) \ \ -N\hat{\mathbf{\mu}}^{\top}\right]^{\top}\)</span> and <span class="math inline">\(C(\mathbf{\mu},\mathbf{\Sigma})=\exp\left\{-\frac{1}{2}\left(tr\left(\mathbf{\mu}\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)+\log(|\Sigma|)\right)\right\}\)</span>.</p>
<p>Then, its conjugate prior distribution should have the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\mu},\mathbf{\Sigma})&amp;\propto \exp\left\{-\frac{b_0}{2}\left(tr\left(\mathbf{\mu}\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)+\log(|\Sigma|)\right)\right\}\\
  &amp;\times \exp\left\{\mathbf{a}_{01}^{\top} vec\left(\mathbf{\Sigma}^{-1}\right)+\mathbf{a}_{02}^{\top}vec\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\right)\right\}\\
  &amp;=|\Sigma|^{-b_0/2}\exp\left\{-\frac{b_0}{2}\left(tr\left(\mathbf{\mu}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}\right)\right)+tr\left(\mathbf{a}_{02}^{\top}\mathbf{\Sigma}^{-1}\mathbf{\mu}\right)\right\}\\
  &amp;\times \exp\left\{\mathbf{a}_{01}^{\top} vec\left(\mathbf{\Sigma}^{-1}\right)+\frac{\mathbf{a}_{02}^{\top}\mathbf{\Sigma}^{-1}\mathbf{a}_{02}}{2b_0}-\frac{\mathbf{a}_{02}^{\top}\mathbf{\Sigma}^{-1}\mathbf{a}_{02}}{2b_0}\right\}\\
  &amp;=|\Sigma|^{-b_0/2}\exp\left\{-\frac{b_0}{2}\left(\mathbf{\mu}-\frac{\mathbf{a}_{02}}{b_0}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{\mu}-\frac{\mathbf{a}_{02}}{b_0}\right)\right\}\\
  &amp;\times \exp\left\{-\frac{1}{2}tr\left(\left(\mathbf{A}_{01}-\frac{\mathbf{a}_{02}\mathbf{a}_{02}^{\top}}{b_0}\right)\mathbf{\Sigma}^{-1}\right)\right\}\\
  &amp;=\underbrace{|\Sigma|^{-1/2}\exp\left\{-\frac{b_0}{2}\left(\mathbf{\mu}-\frac{\mathbf{a}_{02}}{b_0}\right)^{\top}\mathbf{\Sigma}^{-1}\left(\mathbf{\mu}-\frac{\mathbf{a}_{02}}{b_0}\right)\right\}}_1\\
  &amp;\times \underbrace{|\Sigma|^{-(\alpha_0+p+1)/2}\exp\left\{-\frac{1}{2}tr\left(\left(\mathbf{A}_{01}-\frac{\mathbf{a}_{02}\mathbf{a}_{02}^{\top}}{b_0}\right)\mathbf{\Sigma}^{-1}\right)\right\}}_2,
\end{align}\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> is the hypothetical sample size, and <span class="math inline">\(\mathbf{a}_{01}\)</span> and <span class="math inline">\(\mathbf{a}_{02}\)</span> are <span class="math inline">\(p^2\)</span> and <span class="math inline">\(p\)</span> dimensional vectors of prior sufficient statistics, and <span class="math inline">\(\mathbf{a}_{01}=-\frac{1}{2}vec(\mathbf{A}_{01})\)</span> such that <span class="math inline">\(\mathbf{A}_{01}\)</span> is a <span class="math inline">\(p\times p\)</span> positive semi-definite matrix. Setting <span class="math inline">\(b_0=1+\alpha_0+p+1\)</span> we have that the first part in the last expression is the kernel of a multivariate normal density with mean <span class="math inline">\(\mathbf{\mu}_0=\mathbf{a}_{02}/b_0\)</span> and covariance <span class="math inline">\(\frac{\mathbf{\Sigma}}{b_0}\)</span>, that is, <span class="math inline">\(\mathbf{\mu}|\mathbf{\Sigma}\sim N_p\left(\mathbf{\mu}_0,\frac{\mathbf{\Sigma}}{\beta_0}\right)\)</span>, <span class="math inline">\(b_0=\beta_0\)</span>. It makes sense these hyperparameters because <span class="math inline">\(\mathbf{a}_{02}\)</span> is the hypothetical sum of prior observations and <span class="math inline">\(b_0\)</span> is the hypothetical prior sample size. On the other hand, the second expression in the last line is the kernel of a Inverse-Wishart distribution with scale matrix <span class="math inline">\(\mathbf{\Psi}_0=\left(\mathbf{A}_{01}-\frac{\mathbf{a}_{02}\mathbf{a}_{02}^{\top}}{b_0}\right)\)</span> and degrees of freedom <span class="math inline">\(\alpha_0\)</span>, that is, <span class="math inline">\(\mathbf{\Sigma}\sim IW_p(\mathbf{\Psi}_0,\alpha_0)\)</span>. Observe that <span class="math inline">\(\mathbf{\Psi}_0\)</span> has the same structure as the first part of the sufficient statistics in <span class="math inline">\(T(\mathbf{y})\)</span>, just that it should be understood as coming from prior hypothetical observations.</p>
<p>Therefore, the prior distribution in this setting is <strong>normal/inverse-Wishart</strong>, and given conjugacy, the posterior distribution is in the same family.</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\mu},\mathbf{\Sigma}|\mathbf{Y})&amp;\propto
  (2\pi)^{-p N/2}|\Sigma|^{-N/2}\exp\left\{-\frac{1}{2}tr\left[\left(\mathbf{S}+N\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)^{\top}\right)\mathbf{\Sigma}^{-1}\right]\right\}\\
  &amp;\times |\mathbf{\Sigma}|^{-1/2}\exp\left\{-\frac{\beta_0}{2}tr\left[(\mathbf{\mu}-\mathbf{\mu}_0)(\mathbf{\mu}-\mathbf{\mu}_0)^{\top}\mathbf{\Sigma}^{-1}\right]\right\}|\mathbf{\Sigma}|^{-(\alpha_0+p+1)/2}\exp\left\{-\frac{1}{2}tr(\mathbf{\Psi}_0\mathbf{\Sigma}^{-1})\right\}.
\end{align}\]</span></p>
<p>Taking into account that
<span class="math display">\[\begin{align}
N\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)\left(\mathbf{\mu}-\hat{\mathbf{\mu}}\right)^{\top}+\beta_0\left(\mathbf{\mu}-\mathbf{\mu}_0\right)\left(\mathbf{\mu}-\mathbf{\mu}_0\right)^{\top}&amp;=(N+\beta_0)\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}\\
&amp;+\frac{N\beta_0}{N+\beta_0}\left(\hat{\mathbf{\mu}}-\mathbf{\mu}_0\right)\left(\hat{\mathbf{\mu}}-\mathbf{\mu}_0\right)^{\top},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\mu}_n=\frac{N}{N+\beta_0}\hat{\mathbf{\mu}}+\frac{\beta_0}{N+\beta_0}\mathbf{\mu}_0\)</span> is the posterior mean. We have</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\mu},\mathbf{\Sigma}|\mathbf{Y})&amp;\propto |\Sigma|^{-1/2}\exp\left\{-\frac{N+\beta_0}{2}tr\left[\left(\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}\right)\mathbf{\Sigma}^{-1}\right]\right\}\\
  &amp;\times |\mathbf{\Sigma}|^{-(N+\alpha_0+p+1)/2}\exp\left\{-\frac{1}{2}tr\left[\left(\mathbf{\Psi}_0+\mathbf{S}+\frac{N\beta_0}{N+\beta_0}(\hat{\mathbf{\mu}}-\mathbf{\mu}_0)(\hat{\mathbf{\mu}}-\mathbf{\mu}_0)^{\top}\right)\mathbf{\Sigma}^{-1}\right]\right\}.
\end{align}\]</span></p>
<p>Then, <span class="math inline">\(\mathbf{\mu}|\mathbf{\Sigma},\mathbf{Y}\sim N_p\left(\mathbf{\mu}_n,\frac{1}{\beta_n}\mathbf{\Sigma}\right)\)</span>, and <span class="math inline">\(\mathbf{\Sigma}|\mathbf{Y}\sim W\left(\alpha_n,\mathbf{\Psi}_n\right)\)</span> where <span class="math inline">\(\beta_n=N+\beta_0\)</span>, <span class="math inline">\(\alpha_n=N+\alpha_0\)</span> and <span class="math inline">\(\mathbf{\Psi}_n=\mathbf{\Psi}_0+\mathbf{S}+\frac{N\beta_0}{N+\beta_0}(\hat{\mathbf{\mu}}-\mathbf{\mu}_0)(\hat{\mathbf{\mu}}-\mathbf{\mu}_0)^{\top}\)</span>.</p>
<p>The marginal posterior of <span class="math inline">\(\mathbf{\mu}\)</span> is given by <span class="math inline">\(\int_{\mathcal{S}} \pi(\mathbf{\mu},\mathbf{\Sigma})d\mathbf{\Sigma}\)</span> where <span class="math inline">\(\mathcal{S}\)</span> is the space of positive semi-definite matrices. Then,</p>
<p><span class="math display">\[\begin{align}
\pi(\mathbf{\mu}|\mathbf{Y})&amp;\propto\int_{\mathcal{S}}\left\{|\mathbf{\Sigma}|^{-(\alpha_n+p+2)/2} \exp\left\{-\frac{1}{2}tr\left[\left(\beta_n\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}+\mathbf{\Psi}_n\right)\mathbf{\Sigma}^{-1}\right]\right\} \right\}d\mathbf{\Sigma}\\
&amp;\propto \big\lvert\left(\beta_n\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}+\mathbf{\Psi}_n\right)\big\lvert^{-(\alpha_n+1)/2}\\
&amp;=\left[\big\lvert\mathbf{\Psi}_n\big\lvert\times \big\lvert1+\beta_n\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}\mathbf{\Psi}_n^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\big\lvert\right]^{-(\alpha_n+1)/2}\\
&amp;\propto \left(1+\frac{1}{\alpha_n+1-p}\left(\mathbf{\mu}-\mathbf{\mu}_n\right)^{\top}\left(\frac{\mathbf{\Psi}_n}{(\alpha_n+1-p)\beta_n}\right)^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_n\right)\right)^{-(\alpha_n+1-p+p)/2},
\end{align}\]</span></p>
<p>where the second line uses properties of the inverse Wishart distribution, and the third line uses a particular case of the Sylvester’s determinant theorem.</p>
<p>We observe that the last line is the kernel of a Multivariate Student’s t distribution, that is, <span class="math inline">\(\mathbf{\mu}|\mathbf{Y}\sim t_p(v_n,\mathbf{\mu}_n,\mathbf{\Sigma}_n)\)</span> where <span class="math inline">\(v_n=\alpha_n+1-p\)</span> and <span class="math inline">\(\mathbf{\Sigma}_n=\frac{\mathbf{\Psi}_n}{(\alpha_n+1-p)\beta_n}\)</span>.</p>
<p>The marginal likelihood is given by</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{Y})=\frac{\Gamma_p\left(\frac{\alpha_n}{2}\right)}{\Gamma_p\left(\frac{\alpha_0}{2}\right)}\frac{|\mathbf{\Psi}_0|^{\alpha_0/2}}{|\mathbf{\Psi}_n|^{\alpha_n/2}}\left(\frac{\beta_0}{\beta_n}\right)^{p/2}(2\pi)^{-Np/2},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\Gamma_p\)</span> is the multivariate gamma function (see Exercise 4).</p>
<p>The posterior predictive distribution is <span class="math inline">\(\mathbf{Y}_0|\mathbf{Y}\sim t_p(v_n,\mathbf{\mu}_n,(\beta_n+1)\mathbf{\Sigma}_n)\)</span> (see Exercise 5).</p>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-diaconis1979conjugate" class="csl-entry">
Diaconis, Persi, Donald Ylvisaker, et al. 1979. <span>“Conjugate Priors for Exponential Families.”</span> <em>The Annals of Statistics</em> 7 (2): 269–81.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Another parametrization of the gamma density is the <em>scale parametrization</em> where <span class="math inline">\(\kappa_0=1/\beta_0\)</span>. See the health insurance example in Chapter <a href="basics.html#basics">1</a>.<a href="sec42.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec41.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
