<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Conjugate prior to exponential family | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2021-03-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec41.html"/>
<link rel="next" href="computational-examples.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a><ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.6</b> A simple working example</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html#sec26"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.8</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a><ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a><ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.3</b> Computational examples</a></li>
<li class="chapter" data-level="4.4" data-path="summary.html"><a href="summary.html#summary"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.5</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec42" class="section level2">
<h2><span class="header-section-number">4.2</span> Conjugate prior to exponential family</h2>
<p><strong>Theorem 4.2.1</strong></p>
<p>The prior distribution <span class="math inline">\(\pi(\mathbf{\theta})\propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}\)</span> is conjugate to the exponential family (equation <a href="sec41.html#eq:414">(4.4)</a>).</p>
<p><strong>Proof</strong></p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp; \propto C(\mathbf{\theta})^{b_0}\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{a}_0\right\} \times h(\mathbf{y}) C(\mathbf{\theta})^N\exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{T}(\mathbf{y})\right\}\\
  &amp; \propto C(\mathbf{\theta})^{N+b_0} \exp\left\{\eta(\mathbf{\theta})^{\top}(\mathbf{T}(\mathbf{y})+\mathbf{a}_0\right\}. 
\end{align}\]</span></p>
<p>Observe that the posterior is in the exponential family, <span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y})\propto C(\mathbf{\theta})^{\beta_n} \exp\left\{\eta(\mathbf{\theta})^{\top}\mathbf{\alpha}_n\right\}\)</span>, <span class="math inline">\(\beta_n=N+b_0\)</span> and <span class="math inline">\(\mathbf{\alpha}_n=\mathbf{T}(\mathbf{y})+\mathbf{a}_0\)</span>.</p>
<p><em>Remarks</em></p>
<p>We see comparing the prior and the likelihood that <span class="math inline">\(b_0\)</span> plays the role of a hypothetical sample size, and <span class="math inline">\(\mathbf{a}_0\)</span> plays the role of hypothetical sufficient statistics. This view helps the elicitation process.</p>
<p>In addition, we stablished the result in the <em>standard form</em> of the exponential family. We can also stablish this result in the <em>canonical form</em> of the exponential family. Observe that given <span class="math inline">\(\mathbf{\eta}=\mathbf{\eta}(\mathbf{\theta})\)</span> another way to get a prior for <span class="math inline">\(\mathbf{\eta}\)</span> is to use the change of variables theorem given a bijective function.</p>
<p>In the setting where there is a prior regular conjugate prior <span class="citation">(Diaconis, Ylvisaker, and others <a href="#ref-diaconis1979conjugate" role="doc-biblioref">1979</a>)</span> show that we obtain a posterior expectation of the sufficient statistics that is a weighted average between the prior expectation and the likelihood estimate.</p>
<p><strong>Examples: Theorem 4.2.1</strong></p>
<ol style="list-style-type: decimal">
<li>Likelihood functions from discrete distributions</li>
</ol>
<ul>
<li><strong>The Poisson-gamma model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Poisson distribution</strong> then a conjugate prior density for <span class="math inline">\(\lambda\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda)&amp;\propto \left(\exp(-\lambda)\right)^{b_0} \exp\left\{a_0\log(\lambda)\right\}\\
 &amp; = \exp(-\lambda b_0) \lambda^{a_0}\\
 &amp; = \exp(-\lambda \beta_0) \lambda^{\alpha_0-1}.
\end{align}\]</span></p>
<p>This is the kernel of a gamma density in the <em>rate parametrization</em>, <span class="math inline">\(G(\alpha_0,\beta_0)\)</span>, <span class="math inline">\(\alpha_0=a_0+1\)</span> and <span class="math inline">\(\beta_0=b_0\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Then, a prior conjugate distribution for the Poisson likelihood is a gamma distribution.</p>
<p>Taking into account that <span class="math inline">\(\sum_{i=1}^N y_i\)</span> is a sufficient statistic for the Poisson distribution, then we can think about <span class="math inline">\(a_0\)</span> as the number of occurrences in <span class="math inline">\(b_0\)</span> experiments.
Observe that</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda|\mathbf{y})&amp;\propto \exp(-\lambda \beta_0) \lambda^{\alpha_0-1} \times \exp(-N\lambda)\lambda^{\sum_{i=1}^Ny_i}\\
  &amp;= \exp(-\lambda(N+\beta_0)) \lambda^{\sum_{i=1}^Ny_i+\alpha_0-1}. 
\end{align}\]</span></p>
<p>As expected, this is the kernel of a gamma distribution, which means <span class="math inline">\(\lambda|\mathbf{y}\sim G(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\sum_{i=1}^Ny_i+\alpha_0\)</span> and <span class="math inline">\(\beta_n=N+\beta_0\)</span>.</p>
<p>Observe that <span class="math inline">\(\alpha_0/\beta_0\)</span> is the prior mean, and <span class="math inline">\(\alpha_0/\beta_0^2\)</span> is the prior variance. Then, <span class="math inline">\(\alpha_0\rightarrow 0\)</span> and <span class="math inline">\(\beta_0\rightarrow 0\)</span> imply a non-informative prior such that the posterior mean converges to the maximum likelihood estimator <span class="math inline">\(\bar{y}=\frac{\sum_{i=1}^N y_i}{N}\)</span>,</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}\left[\lambda|\mathbf{y}\right]&amp;=\frac{\alpha_n}{\beta_n}\\
  &amp;=\frac{\sum_{i=1}^Ny_i+\alpha_0}{N+\beta_0}\\
  &amp;=\frac{N\bar{y}}{N+\beta_0}+\frac{\alpha_0}{N+\beta_0}
\end{align}\]</span></p>
<p>The posterior mean is a weighted average between sample and prior information. This is a general result from regular conjugate priors <span class="citation">(Diaconis, Ylvisaker, and others <a href="#ref-diaconis1979conjugate" role="doc-biblioref">1979</a>)</span>. Observe that <span class="math inline">\(\mathbb{E}\left[\lambda|\mathbf{y}\right]=\bar{y}, \lim N\rightarrow\infty\)</span>.</p>
<p>In addition, <span class="math inline">\(\alpha_0\rightarrow 0\)</span> and <span class="math inline">\(\beta_0\rightarrow 0\)</span> corresponds to <span class="math inline">\(\pi(\lambda)\propto \frac{1}{\lambda}\)</span>, which is an improper prior. Improper priors have bad consequences on Bayes factors (hypothesis testing). In this setting, we can get analytical solutions for the marginal likelihood and the predictive distribution (see the health insurance example and exercise 3 in Chapter <a href="basics.html#basics">1</a>).</p>
<ul>
<li><strong>The Bernoulli-beta model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>Bernoulli distribution</strong> then a conjugate prior density for <span class="math inline">\(\lambda\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\theta)&amp;\propto (1-\theta)^{b_0} \exp\left\{a_0\log\left(\frac{\theta}{1-\theta}\right)\right\}\\
 &amp; = (1-\theta)^{b_0-a_0}\theta^{a_0}\\
 &amp; = \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}.
\end{align}\]</span></p>
<p>This is the kernel of a beta density, <span class="math inline">\(B(\alpha_0,\beta_0)\)</span>, <span class="math inline">\(\alpha_0=a_0+1\)</span> and <span class="math inline">\(\beta_0=b_0-a_0+1\)</span>. A prior conjugate distribution for the Bernoulli likelihood is a beta distribution. Given that <span class="math inline">\(b_0\)</span> is the hypothetical sample size, and <span class="math inline">\(a_0\)</span> is the hypothetical sufficient statistic, which is the number of successes, then <span class="math inline">\(b_0-a_0\)</span> is the number of failures. This implies that <span class="math inline">\(\alpha_0\)</span> is the number of prior successes plus one, and <span class="math inline">\(\beta_0\)</span> is the number of prior failures plus one. Given that the mode of a beta distribuited random variable is <span class="math inline">\(\frac{\alpha_0-1}{\alpha_0+\beta_0-2}=\frac{a_0}{b_0}\)</span>, then we have the a priori probability of success. Setting <span class="math inline">\(\alpha_0=1\)</span> and <span class="math inline">\(\beta_0=1\)</span>, which implies a 0-1 uniform distribution, corresponds to a setting with 0 successes (and 0 failures) in 0 experiments.</p>
<p>Observe that</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda|\mathbf{y})&amp;\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^Ny_i}\\
  &amp;= \theta^{\alpha_0+\sum_{i=1}^N y_i-1}(1-\theta)^{\beta_0+N-\sum_{i=1}^Ny_i-1}. 
\end{align}\]</span></p>
<p>The posterior distribution is beta, <span class="math inline">\(\theta|\mathbf{y}\sim B(\alpha_n,\beta_n)\)</span>, <span class="math inline">\(\alpha_n=\alpha_0+\sum_{i=1}^N y_i\)</span> and <span class="math inline">\(\beta_n=\beta_0+N-\sum_{i=1}^Ny_i\)</span>, where the posterior mean <span class="math inline">\(\mathbf{E}[\theta|\mathbf{y}]=\frac{\alpha_n}{\alpha_n+\beta_n}=\frac{\alpha_0+N\bar{y}}{\alpha_0+\beta_0+N}=\frac{\alpha_0+\beta_0}{\alpha_0+\beta_0+N}\frac{\alpha_0}{\alpha_0+\beta_0}+\frac{N}{\alpha_0+\beta_0+N}\bar{y}\)</span>. The posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>El marginal likelihood in this setting is</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})=&amp;\int_{0}^1 \frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0,\beta_0)}\times \theta^{\sum_{i=1}^N y_i}(1-\theta)^{N-\sum_{i=1}^N y_i}d\theta\\
  =&amp; \frac{B(\alpha_n,\beta_n)}{B(\alpha_0,\beta_0)},
\end{align}\]</span></p>
<p>where <span class="math inline">\(B(\cdot ,\cdot)\)</span> is the beta function.</p>
<p>In addition, the predictive density is</p>
<p><span class="math display">\[\begin{align}
  p(Y_0|\mathbf{y})&amp;=\int_0^1 \theta^{y_0}(1-\theta)^{1-y_0}\times \frac{\theta^{\alpha_n-1}(1-\theta)^{\beta_n-1}}{B(\alpha_n,\beta_n)}d\theta\\
  &amp;=\frac{B(\alpha_n+y_0,\beta_n+1-y_0)}{B(\alpha_n,\beta_n)}\\
  &amp;=\frac{\Gamma(\alpha_n+\beta_n)\Gamma(\alpha_n+y_0)\Gamma(\beta_n+y_0)}{\Gamma(\alpha_n+\beta_n+1\Gamma(\alpha_n)\Gamma(\beta_n)}\\
  &amp;=\begin{Bmatrix}
  \frac{\alpha_n}{\alpha_n+\beta_n}, &amp; y_0=1\\
  \frac{\beta_n}{\alpha_n+\beta_n}, &amp; y_0=0\\
  \end{Bmatrix}.
\end{align}\]</span></p>
<p>This is a Bernoulli distribution with probability of success equal to <span class="math inline">\(\frac{\alpha_n}{\alpha_n+\beta_n}\)</span>.</p>
<ul>
<li><strong>The multinomial-Dirichlet model</strong></li>
</ul>
<p>Given a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a <strong>multinomial distribution</strong> then a conjugate prior density for <span class="math inline">\(\mathbf{\theta}=\left[\theta_1,\theta_2,\dots,\theta_m\right]\)</span> has the form</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta})&amp;\propto \theta_m^{b_0} \exp\left\{\mathbf{\eta}(\mathbf{\theta})^{\top}\mathbf{a}_0\right\}\\
 &amp; = \prod_{l=1}^{m-1}\theta_l^{a_{0l}}\theta_m^{b_0-\sum_{l=1}^{m-1}a_{0l}}\\
 &amp; = \prod_{l=1}^{m}\theta_l^{\alpha_{0l}-1},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\eta}(\mathbf{\theta})=\left[\log\left(\frac{\theta_1}{\theta_m}\right),\dots,\log\left(\frac{\theta_{m-1}}{\theta_m}\right)\right]\)</span>, <span class="math inline">\(\mathbf{a}_0=\left[a_{01},\dots,a_{am-1}\right]^{\top}\)</span>, <span class="math inline">\(\mathbf{\alpha}_0=\left[\alpha_{01},\alpha_{02},\dots,\alpha_{0m}\right]\)</span>, <span class="math inline">\(\alpha_{0l}=a_{0l}+1\)</span>, <span class="math inline">\(l=1,2,\dots,m-1\)</span> and <span class="math inline">\(\alpha_{0m}=b_0-\sum_{l=1}^{m-1} a_{0l}+1\)</span>.</p>
<p>This is the kernel of a Dirichlet distribution, that is, the prior distribution is <span class="math inline">\(D(\mathbf{\alpha}_0)\)</span>.</p>
<p>Observe that <span class="math inline">\(a_{0l}\)</span> is the number of hypothetical number of times outcome <span class="math inline">\(l\)</span> is observed over the hypothetical <span class="math inline">\(b_0\)</span> trials. Setting <span class="math inline">\(\alpha_{0l}=1\)</span>, that is a uniform distribution over the open standard simplex, implicitly we set <span class="math inline">\(a_{0l}=0\)</span>, which means that there are 0 occurrences of category <span class="math inline">\(l\)</span> in <span class="math inline">\(b_0=0\)</span> experiments.</p>
<p>The posterior distribution of the multinomial-Dirichlet model is given by</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp;\propto \prod_{l=1}^m \theta_l^{\alpha_{0l}-1}\times\prod_{l=1}^m \theta_l^{\sum_{i=1}^{N} y_{il}}\\
  &amp;=\prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^{N} y_{il}-1}
\end{align}\]</span></p>
<p>This is the kernel of a Dirichlet distribution <span class="math inline">\(D(\mathbf{\alpha}_n)\)</span>, <span class="math inline">\(\mathbf{\alpha}_n=\left[\alpha_{n1},\alpha_{n2},\dots,\alpha_{nm}\right]\)</span>, <span class="math inline">\(\alpha_{nl}=\alpha_{0l}+\sum_{i=1}^{N}y_{il}\)</span>, <span class="math inline">\(l=1,2,\dots,m\)</span>. Observe that</p>
<p><span class="math display">\[\begin{align}

\mathbb{E}[\theta_{j}|\mathbf{y}]&amp;=\frac{\alpha_{nj}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\\
&amp;=\frac{\sum_{l=1}^m \alpha_{0l}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\alpha_{0j}}{\sum_{l=1}^m \alpha_{0l}}+\frac{\sum_{l=1}^m\sum_{i=1}^N y_{il}}{\sum_{l=1}^m \left[\alpha_{0l}+\sum_{i=1}^N y_{il}\right]}\frac{\sum_{i=1}^N y_{ij}}{\sum_{l=1}^m\sum_{i=1}^N y_{il}}.
\end{align}\]</span></p>
<p>We have again that the posterior mean is a weighted average between the prior mean and the maximum likelihood estimate.</p>
<p>The marginal likelihood is</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})&amp;=\int_{\mathbf{\Theta}}\frac{\prod_{l=1}^m \theta_l^{\alpha_{0l}-1}}{B(\mathbf{\alpha}_0)}\times \prod_{i=1}^N\frac{n!}{\prod_{l=1}^m y_{il}}\prod_{l=1}^m \theta_{l}^{y_{il}}d\mathbf{\theta}\\
  &amp;=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}\int_{\mathbf{\Theta}} \prod_{l=1}^m \theta_l^{\alpha_{0l}+\sum_{i=1}^N y_{il}-1} d\mathbf{\theta}\\
  &amp;=\frac{N\times n!}{B(\mathbf{\alpha}_0)\prod_{i=1}^N\prod_{l=1}^m y_{il}!}B(\mathbf{\alpha}_n)\\
  &amp;=\frac{N\times n! \Gamma\left(\sum_{l=1}^n \alpha_{0l}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{0l}+N\times n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}\right)}{\Gamma\left(\alpha_{0l}\right)\prod_{i=1}^N y_{il}!},
\end{align}\]</span></p>
<p>where <span class="math inline">\(B(\mathbf{\alpha})=\frac{\prod_{l=1}^m\Gamma(\alpha_l)}{\Gamma\left(\sum_{l=1}^m \alpha_l\right)}\)</span>.</p>
<p>Following similar steps we get the predictive density</p>
<p><span class="math display">\[\begin{align}
  p(Y_0|\mathbf{y})&amp;=\frac{ n! \Gamma\left(\sum_{l=1}^n \alpha_{nl}\right)}{\Gamma\left(\sum_{l=1}^n \alpha_{nl}+ n\right)}\prod_{l=1}^m \frac{\Gamma\left( \alpha_{nl}+y_{0l}\right)}{\Gamma\left(\alpha_{nl}\right) y_{0l}!}.
\end{align}\]</span></p>
<p>This is a Dirichlet-multinomial distribution with parameters <span class="math inline">\(\mathbf{\alpha}_n\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Likelihood functions from continuos distributions</li>
</ol>
<ul>
<li><strong>The normal-normal/gamma model</strong></li>
</ul>
<p>What can the prior distribution be of a random sample <span class="math inline">\(\mathbf{y}=[y_1,y_2,\dots,y_N]^{\top}\)</span> from a normal distribution?</p>
<p>Find the posterior distribution of the normal-normal/gamma model.</p>
<p>What is the posterior distribution of the multivariate normal likelihood with a normal-Wishart prior?</p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-diaconis1979conjugate">
<p>Diaconis, Persi, Donald Ylvisaker, and others. 1979. “Conjugate Priors for Exponential Families.” <em>The Annals of Statistics</em> 7 (2): 269–81.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Another parametrization of the gamma density is the <em>scale parametrization</em> where <span class="math inline">\(\kappa_0=1/\beta_0\)</span>. See the health insurance example in Chapter <a href="basics.html#basics">1</a>.<a href="sec42.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec41.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computational-examples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
