<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 Bayesian framework: A brief summary of theory | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2021-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec11.html"/>
<link rel="next" href="sec13.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a><ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec25A.html"><a href="sec25A.html"><i class="fa fa-check"></i><b>2.6</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.7</b> A simple working example</a></li>
<li class="chapter" data-level="2.8" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.8</b> Summary: Chapter 2</a></li>
<li class="chapter" data-level="2.9" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.9</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a><ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a><ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>4.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="4.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>4.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="4.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.5</b> Computational examples</a></li>
<li class="chapter" data-level="4.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>4.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="4.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a><ul>
<li class="chapter" data-level="6.1" data-path="normal-model.html"><a href="normal-model.html"><i class="fa fa-check"></i><b>6.1</b> Normal model</a></li>
<li class="chapter" data-level="6.2" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i><b>6.2</b> Logit model</a></li>
<li class="chapter" data-level="6.3" data-path="probit-model.html"><a href="probit-model.html"><i class="fa fa-check"></i><b>6.3</b> Probit model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec12" class="section level2">
<h2><span class="header-section-number">1.2</span> Bayesian framework: A brief summary of theory</h2>
<p>For two random objects <span class="math inline">\(\mathbf{\theta}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, the Bayes’ rule may be analogously used,<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p><span class="math display" id="eq:121">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp;=\frac{p(\mathbf{y}|\mathbf{\theta}) \times \pi(\mathbf{\theta})}{p(\mathbf{y})},
  \tag{1.3}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y})\)</span> is the posterior density function, <span class="math inline">\(\pi(\mathbf{\theta})\)</span> is the prior density, <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span> is the likelihood (statistical model), and</p>
<p><span class="math display" id="eq:121a">\[\begin{equation}
p(\mathbf{y})=\int_{\mathbf{\Theta}}p(\mathbf{y}|\mathbf{\theta})\pi(\mathbf{\theta})d\mathbf{\theta}=\mathbb{E}\left[p(\mathbf{y}|\mathbf{\theta})\right]
\tag{1.4}
\end{equation}\]</span></p>
<p>is the marginal likelihood or prior predictive.</p>
<p><em>Remarks</em></p>
<p>Observe that <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span> is not a density in <span class="math inline">\(\mathbf{\theta}\)</span>. In addition, <span class="math inline">\(\pi(\mathbf{\theta})\)</span> does not have to integrate to 1, that is, <span class="math inline">\(\pi(\mathbf{\theta})\)</span> can be an improper density function, <span class="math inline">\(\int_{\mathbf{\Theta}}\pi(\mathbf{\theta})d\mathbf{\theta}=\infty\)</span>. However, <span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y})\)</span> is a proper density function, that is, <span class="math inline">\(\int_{\mathbf{\Theta}}\pi(\mathbf{\theta}|\mathbf{y})=1\)</span>. For instance, set <span class="math inline">\(\pi(\mathbf{\theta})=c\)</span>, where <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(\int_{\mathbf{\Theta}}cd\mathbf{\theta}=\infty\)</span>. However, <span class="math inline">\(\int_{\mathbf{\Theta}}\pi(\mathbf{\theta}|\mathbf{y})d\mathbf{\theta}=\int_{\mathbf{\Theta}}\frac{p(\mathbf{y}|\mathbf{\theta})\times c}{\int_{\mathbf{\Theta}} p(\mathbf{y}|\mathbf{\theta})\times c d\mathbf{\theta}}d\mathbf{\theta}=1\)</span> where <span class="math inline">\(c\)</span> cancels out.</p>
<p><span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y})\)</span> is a sample updated “probabilistic belief” version of <span class="math inline">\(\pi(\mathbf{\theta})\)</span>, where <span class="math inline">\(\pi(\mathbf{\theta})\)</span> is a prior probabilistic belief which can be constructed from previous empirical work, theory foundations, expert knowledge and/or mathematical convenience. This prior usually depends on parameters, which are named <em>hyperparameters</em>. In addition, the Bayesian approach implies using a probabilistic model about <span class="math inline">\(\mathbf{y}\)</span> given <span class="math inline">\(\mathbf{\theta}\)</span>, that is, <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span>, where its integral over <span class="math inline">\(\mathbf{\Theta}\)</span>, <span class="math inline">\(p(\mathbf{y})\)</span> is named <em>the model evidence</em> due to being a measure of model fit to the data.</p>
<p>Observe that the Bayesian inferential approach is conditional, that is, what can we learn about an unknown object <span class="math inline">\(\mathbf{\theta}\)</span> given that we already observed <span class="math inline">\(\mathbf{y}\)</span>? The answer is also conditional on the probabilistic model, that is <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta})\)</span>. So, what if we want to compare different models, let’s say <span class="math inline">\(\mathcal{M}_m\)</span>, <span class="math inline">\(m=\left\{1,2,\dots,M\right\}\)</span>. Then, we should make explicit this in the Bayes’ rule formulation,</p>
<p><span class="math display" id="eq:122">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y},\mathcal{M}_m)&amp;=\frac{p(\mathbf{y}|\mathbf{\theta},\mathcal{M}_m) \times \pi(\mathbf{\theta}|\mathcal{M}_m)}{p(\mathbf{y}|\mathcal{M}_m)}.
  \tag{1.5}
\end{align}\]</span></p>
<p>The posterior model probability is</p>
<p><span class="math display" id="eq:123">\[\begin{align}
  \pi(\mathcal{M}_m|\mathbf{y})&amp;=\frac{p(\mathbf{y}|\mathcal{M}_m) \times \pi(\mathcal{M}_m)}{p(\mathbf{y})}, 
  \tag{1.6}
\end{align}\]</span></p>
<p>where <span class="math inline">\(p(\mathbf{y}|\mathcal{M}_m)=\int_{\mathbf{\Theta}}p(\mathbf{y}|\mathbf{\theta},\mathcal{M}_m) \times \pi(\mathbf{\theta}|\mathcal{M}_m)d\mathbf{\theta}\)</span> due to equation <a href="sec12.html#eq:122">(1.5)</a>, and <span class="math inline">\(\pi(\mathcal{M}_m)\)</span> is the prior model probability.</p>
<p>Calculating <span class="math inline">\(p(\mathbf{y})\)</span> in equations <a href="sec12.html#eq:121">(1.3)</a> and <a href="sec12.html#eq:123">(1.6)</a> is very demanding most of the realistic cases. Fortunately, it is not required when performing inference about <span class="math inline">\(\mathbf{\theta}\)</span> as this is integrated out from it. Then, all what you need to know about the shape of <span class="math inline">\(\mathbf{\theta}\)</span> is in <span class="math inline">\(p(\mathbf{y}|\mathbf{\theta},\mathcal{M}_m) \times \pi(\mathbf{\theta}|\mathcal{M}_m)\)</span> or without explicitly conditioning on <span class="math inline">\(\mathcal{M}_m\)</span>,<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p><span class="math display" id="eq:124">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp; \propto p(\mathbf{y}|\mathbf{\theta}) \times \pi(\mathbf{\theta}).
  \tag{1.7}
\end{align}\]</span></p>
<p>Equation <a href="sec12.html#eq:124">(1.7)</a> is a very good shortcut to perform Bayesian inference about <span class="math inline">\(\mathbf{\theta}\)</span>.</p>
<p>We also can avoid calculating <span class="math inline">\(p(\mathbf{y})\)</span> when performing model selection (hypothesis testing) using posterior odds ratio, that is, comparing models <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span>,</p>
<p><span class="math display" id="eq:125">\[\begin{align}
  PO_{12}&amp;=\frac{\pi(\mathcal{M}_1|\mathbf{y})}{\pi(\mathcal{M}_2|\mathbf{y})}\\
  &amp;=\frac{\pi(\mathbf{y}|\mathcal{M}_1)}{\pi(\mathbf{y}|\mathcal{M}_2)}\times\frac{\pi(\mathcal{M}_1)}{\pi(\mathcal{M}_2)},
  \tag{1.8}
\end{align}\]</span></p>
<p>where the first term in equation <a href="sec12.html#eq:125">(1.8)</a> is named the Bayes Factor, and the second term is the prior odds. Observe that the Bayes Factor is a ratio of ordinates for <span class="math inline">\(\mathbf{y}\)</span> under different models. Then, the Bayes Factor is a measure of relative sample evidence in favor of model 1 compared to model 2.</p>
<p>However, we still need to calculate <span class="math inline">\(p(\mathbf{y}|\mathcal{M}_m)=\int_{\mathbf{\Theta}}p(\mathbf{y}|\mathbf{\theta},\mathcal{M}_m)\pi(\mathbf{\theta}|\mathcal{M}_m)d\mathbf{\theta}=\mathbb{E}\left[p(\mathbf{y}|\mathbf{\theta},\mathcal{M}_m)\right]\)</span>. For this integral to be meaningful, the prior must be proper. Using improper prior has unintended consequences when comparing models.</p>
<p>A nice feature of this approach is that if we have an exhaustive set of compiting models such that <span class="math inline">\(\sum_{m=1}^M \pi(\mathcal{M}_m|\mathbf{y})=1\)</span>, then we can recover <span class="math inline">\(\pi(\mathcal{M}_m|\mathbf{y})\)</span> without calculating <span class="math inline">\(p(\mathbf{y})\)</span>. In particular, given two models <span class="math inline">\(\mathcal{M}_1\)</span> and <span class="math inline">\(\mathcal{M}_2\)</span> such that <span class="math inline">\(\pi(\mathcal{M}_1|\mathbf{y})+\pi(\mathcal{M}_2|\mathbf{y})=1\)</span>. Then, <span class="math inline">\(\pi(\mathcal{M}_1|\mathbf{y})=\frac{PO_{12}}{1+PO_{12}}\)</span> and <span class="math inline">\(\pi(\mathcal{M}_2|\mathbf{y})=1-\pi(\mathcal{M}_1|\mathbf{y})\)</span>. In general, <span class="math inline">\(\pi(\mathcal{M}_m|\mathbf{y})=\frac{\pi(\mathbf{y}|\mathcal{M}_m)\times \pi(\mathcal{M}_m)}{\sum_{l=1}^M \pi(\mathbf{y}|\mathcal{M}_l)\times \pi(\mathcal{M}_l)}\)</span>.</p>
<p>Table <a href="sec12.html#tab:guide">1.1</a> shows guidelines for the interpretation of <span class="math inline">\(2\log(PO_{12})\)</span> <span class="citation">(Kass and Raftery <a href="#ref-Kass1995" role="doc-biblioref">1995</a>)</span>. This is done to replicate the structure of the likelihood ratio test statistic. However, posterior odds do not require nested models as the likelihood ratio test does.</p>
<table>
<caption><span id="tab:guide">Table 1.1: </span>Kass and Raftery guidelines</caption>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(2\log(PO_{12})\)</span></th>
<th align="left"><span class="math inline">\(PO_{12}\)</span></th>
<th align="left">Evidence against <span class="math inline">\(\mathcal{M}_{2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0 to 2</td>
<td align="left">1 to 3</td>
<td align="left">Not worth more than a bare mention</td>
</tr>
<tr class="even">
<td align="left">2 to 6</td>
<td align="left">3 to 20</td>
<td align="left">Positive</td>
</tr>
<tr class="odd">
<td align="left">6 to 10</td>
<td align="left">20 to 150</td>
<td align="left">Strong</td>
</tr>
<tr class="even">
<td align="left">&gt; 10</td>
<td align="left">&gt; 150</td>
<td align="left">Very strong</td>
</tr>
</tbody>
</table>
<p>The Bayesian approach is also suitable to get probabilistic predictions, that is, we can obtain a posterior predictive density</p>
<p><span class="math display" id="eq:126">\[\begin{align}
  \pi(\mathbf{Y}_0|\mathbf{y},\mathcal{M}_m) &amp; =\int_{\mathbf{\Theta}}\pi(\mathbf{Y}_0,\mathbf{\theta}|\mathbf{y},\mathcal{M}_m)d\mathbf{\theta}\\
  &amp;=\int_{\mathbf{\Theta}}\pi(\mathbf{Y}_0|\mathbf{\theta},\mathbf{y},\mathcal{M}_m)\pi(\mathbf{\theta}|\mathbf{y},\mathcal{M}_m)d\mathbf{\theta}.
  \tag{1.9}
\end{align}\]</span></p>
<p>Observe that equation <a href="sec12.html#eq:126">(1.9)</a> is again an expectation <span class="math inline">\(\mathbb{E}[\pi(\mathbf{Y}_0|\mathbf{\theta},\mathbf{y},\mathcal{M}_m)]\)</span>, this time using the posterior distribution. Therefore, the Bayesian approach takes estimation error into account when performing prediction.</p>
<p>As we have shown many times, expectation (integration) is a common feature in Bayesian inference. That is why the remarkable relevance of computation based on Monte Carlo integration in the Bayesian framework.</p>
<p>If we want to consider model uncertainty in prediction or any unknown probabilistic object, we can follow same arguments. In the prediction case,</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{Y}_0|\mathbf{y})&amp;=\sum_{m=1}^M \pi(\mathcal{M}_m|\mathbf{y})\pi(\mathbf{Y}_0|\mathbf{y},\mathcal{M}_m),
\end{align}\]</span></p>
<p>and parameters case,</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y})&amp;=\sum_{m=1}^M \pi(\mathcal{M}_m|\mathbf{y})\pi(\mathbf{\theta}|\mathbf{y},\mathcal{M}_m),
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display" id="eq:127">\[\begin{align}
\mathbb{E}(\mathbf{\theta}|\mathbf{y})=\sum_{m=1}^{M}\hat{\mathbf{\theta}}_m \pi(\mathcal{M}_m|\mathbf{y}),
\tag{1.10}
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:128">\[\begin{align}
Var(\mathbf{\theta}|\mathbf{y})= \sum_{m=1}^{M}\pi(\mathcal{M}_m|\mathbf{y}) \widehat{Var} (\mathbf{\theta}|\mathbf{y},\mathcal{M}_m)+\sum_{m=1}^{M} \pi(M_m|\mathbf{y}) (\hat{\mathbf{\theta}}_m-\mathbb{E}[\mathbf{\theta}|\mathbf{y})]^2,
\tag{1.11}
\end{align}\]</span></p>
<p><span class="math inline">\(\hat{\mathbf{\theta}}_m\)</span> and <span class="math inline">\(\widehat{Var}(\mathbf{\theta}|\mathbf{y},\mathcal{M}_m)\)</span> are the posterior mean and variance under model <span class="math inline">\(m\)</span>, respectively.</p>
<p>Observe how the variance in equation <a href="sec12.html#eq:128">(1.11)</a> encloses extra variability due to potential differences between mean posterior estimates associated with each model, and the posterior mean involving model uncertainty in equation <a href="sec12.html#eq:127">(1.10)</a>. The previous equations illustrates Bayesian model average (BMA).</p>
<p>A nice advantage of the Bayesian approach, which is very useful in <em>state space</em> models (See Chapter <a href="time.html#time">8</a>), is the way that the posterior distribution updates with new sample information. Given <span class="math inline">\(\mathbf{y}=\mathbf{y}_{1:t+1}\)</span> a sequence of observations, then</p>
<p><span class="math display">\[\begin{align}
  \pi(\mathbf{\theta}|\mathbf{y}_{1:t+1})&amp;\propto p(\mathbf{y}_{1:t+1}|\mathbf{\theta})\times \pi(\mathbf{\theta})\\
  &amp;= p(y_{t+1}|\mathbf{y}_{1:t},\mathbf{\theta})\times p(\mathbf{y}_{1:t}|\mathbf{\theta})\times \pi(\mathbf{\theta})\\
  &amp;\propto p(y_{t+1}|\mathbf{y}_{1:t},\mathbf{\theta})\times \pi(\mathbf{\theta}|\mathbf{y}_{1:t}). 
\end{align}\]</span></p>
<p>We observe that the new prior is just the posterior distribution using the previous observation. This is particular useful under the assumption of <em>conditional independence</em>, that is, <span class="math inline">\(y_{t+1}\perp\mathbf{y}_{1:t}|\mathbf{\theta}\)</span>,<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> then <span class="math inline">\(p(y_{t+1}|\mathbf{y}_{1:t},\mathbf{\theta})=p(y_{t+1}|\mathbf{\theta})\)</span> such that the posterior can be recovered recursively <span class="citation">(Petris, Petrone, and Campagnoli <a href="#ref-petris2009dynamic" role="doc-biblioref">2009</a>)</span>. This facilities online updating due to all information up to <span class="math inline">\(t\)</span> being in <span class="math inline">\(\mathbf{\theta}\)</span>. Then, <span class="math inline">\(\pi(\mathbf{\theta}|\mathbf{y}_{1:t+1})\propto p(y_{t+1}|\mathbf{\theta})\times \pi(\mathbf{\theta}|\mathbf{y}_{1:t})\propto\prod_{h=1}^{t+1} p(y_h|\mathbf{\theta})\times \pi(\mathbf{\theta})\)</span>. This recursive expression can be calculated faster at some specific point in time <span class="math inline">\(t\)</span> compared to a batch mode algoritm, which requires procesing simultaneously all information up to <span class="math inline">\(t\)</span>.</p>
<p>It also important to wonder about the sampling properties of Bayesian <em>estimators</em>. This topic has attracted attention of econometricians (and statisticians) long time ago. For instance, asymptotic posterior concentration at the population parameter vector is discussed by <span class="citation">(Bickel and Yahav <a href="#ref-bickel1969some" role="doc-biblioref">1969</a>)</span>. Convergence of posterior distributions is stated by the Bernstein-von Mises theorem <span class="citation">(Lehmann and Casella <a href="#ref-Lehmann2003" role="doc-biblioref">2003</a>)</span>, which creates a link between credible intervals (sets) and confidence intervals (sets). In particular, it shows that Bayesian credible intervals with <span class="math inline">\(\alpha\)</span> level convergences asymptotically to confidence intervals at <span class="math inline">\(\alpha\)</span> level. This suggests that Bayesian inference is asymptotically correct from a sampling perspective.</p>
<p>A heuristic approach to show this in the simpliest case where we assume random sampling and <span class="math inline">\(\theta\in R\)</span> is the following: <span class="math inline">\(p(\mathbf{y}|\theta)=\prod_{i=1}^N p(y_i|\theta)\)</span> such that the log likelihood is <span class="math inline">\(\mathcal{l}(\mathbf{y}|\theta)\equiv\log p(\mathbf{y}|\theta)=\sum_{i=1}^N \log p(y_i|\theta)=N\times \bar{\mathcal{l}}(\mathbf{y}|\theta)\)</span> where <span class="math inline">\(\bar{\mathcal{l}}\equiv\frac{1}{N}\sum_{i=1}^N \log p(y_i|\theta)\)</span> is the mean likelihood. Then, the posterior distribution is proportional to</p>
<p><span class="math display">\[\begin{align}
  \pi(\theta|\mathbf{y})&amp;\propto p(\mathbf{y}|\theta) \times \pi(\theta)\\
  &amp;=\exp\left\{N\times \bar{\mathcal{l}}(\mathbf{y}|\theta)\right\} \times \pi(\theta).
\end{align}\]</span></p>
<p>Observe that as the sample size gets large, that is, <span class="math inline">\(N\rightarrow \infty\)</span>, the exponential term should dominate the prior distribution as long as this does not depend on <span class="math inline">\(N\)</span> such that the likelihood determines the posterior distribution.</p>
<p>Maximum likelihood theory shows that <span class="math inline">\(\lim_{N\to\infty} \bar{\mathcal{l}}(\mathbf{y}|\theta)\rightarrow \bar{\mathcal{l}}(\mathbf{y}|\theta_0)\)</span> where <span class="math inline">\(\theta_0\)</span> is the population parameter of the data generating process. In addition, doing a second order Taylor expansion of the log likelihood at the Maximum likelihood estimator,</p>
<p><span class="math display">\[\begin{align}
\mathcal{l}(\mathbf{y}|\theta)&amp;\approx \mathcal{l}(\mathbf{y}|\hat{\theta})+\left.\frac{1}{2}\frac{d\mathcal{l}(\mathbf{y}|{\theta})}{d\theta}\right\vert_{\hat{\theta}}(\hat{\theta}-\theta_0)+\left.\frac{d^2\mathcal{l}(\mathbf{y}|{\theta})}{d\theta^2}\right\vert_{\hat{\theta}}(\hat{\theta}-\theta_0)^2\\
&amp;= \mathcal{l}(\mathbf{y}|\hat{\theta})+\frac{1}{2}\left.\sum_{i=1}^N\frac{d^2\mathcal{l}(y_i|{\theta})}{d\theta^2}\right\vert_{\hat{\theta}}(\hat{\theta}-\theta_0)^2\\
&amp;= \mathcal{l}(\mathbf{y}|\hat{\theta})-\frac{1}{2}\left.N\left[-\bar{\mathcal{l}}&#39;&#39;\right\vert_{\hat{\theta}}\right](\hat{\theta}-\theta_0)^2\\
&amp;= \mathcal{l}(\mathbf{y}|\hat{\theta})-\frac{N}{2\sigma^2}(\hat{\theta}-\theta_0)^2\\ 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\left.\frac{d\mathcal{l}(\mathbf{y}|\theta)}{d\theta}\right\vert_{\hat{\theta}}=0\)</span>, <span class="math inline">\(\bar{\mathcal{l}}&#39;&#39;\equiv\frac{1}{N}\left.\sum_{i=1}^N\frac{d^2\mathcal{l}(y_i|{\theta})}{d\theta^2}\right\vert_{\hat{\theta}}\)</span> and <span class="math inline">\(\sigma^2=\left[\left.-\bar{\mathcal{l}}&#39;&#39;\right\vert_{\hat{\theta}}\right]^{-1}\)</span>. Then,</p>
<p><span class="math display">\[\begin{align}
  \pi(\theta|\mathbf{y})&amp;\propto \exp\left\{{\mathcal{l}}(\mathbf{y}|\theta)\right\} \times \pi(\theta)\\
  &amp;\approx \exp\left\{\mathcal{l}(\mathbf{y}|\hat{\theta})-\frac{N}{2\sigma^2}(\hat{\theta}-\theta_0)^2\right\} \times \pi(\theta)\\
  &amp;\propto \exp\left\{-\frac{N}{2\sigma^2}(\hat{\theta}-\theta_0)^2\right\} \times \pi(\theta)\\ 
\end{align}\]</span></p>
<p>Observe that we have that the posterior density is proportional to the kernel of a normal density with mean <span class="math inline">\(\hat{\theta}\)</span> and variance <span class="math inline">\(\sigma^2/N\)</span> as long as <span class="math inline">\(\pi(\hat{\theta})\neq 0\)</span>. This kernel dominates as the sample size gets large due to N in the exponential term. Observe that the prior should not exclude values of <span class="math inline">\(\theta\)</span> that are logically possible, such as <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><strong>Example: Health insurance</strong></p>
<p>Suppose that you are analyzing to buy a health insurance next year. To make a better decision you want to know <strong>what is the probability that you visit your Doctor at least once next year?</strong> To answer this question you have records of the number of times that you have visited your Doctor the last 5 years, <span class="math inline">\(\mathbf{y}=[0, 3, 2, 1, 0]\)</span>. How to proceed?</p>
<p>Assuming that this is a random sample<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> from a data generating process (statistical model) that is Poisson, that is, <span class="math inline">\(Y_i\sim P(\lambda)\)</span>, and your probabilistic prior beliefs about <span class="math inline">\(\lambda\)</span> are well described by a Gamma distribution with shape and scale parameters <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\lambda\sim G(\alpha_0, \beta_0)\)</span>, then, you are interested in calculating the probability <span class="math inline">\(P(y_0&gt;0|\mathbf{y})\)</span>. You need to calculate the posterior predictive density <span class="math inline">\(\pi(y_0|\mathbf{y})\)</span> to answer this question in a Bayesian way.</p>
<p>In this example, <span class="math inline">\(p(\mathbf{y}|\lambda)\)</span> is Poisson, and <span class="math inline">\(\pi(\lambda)\)</span> is Gamma. Then, using <a href="sec12.html#eq:126">(1.9)</a></p>
<p><span class="math display">\[\begin{align}
  \pi(Y_0|\mathbf{y})=&amp;\int_{0}^{\infty}\frac{\lambda^{y_0}\exp\left\{-\lambda\right\}}{y_0!}\times \pi(\lambda|\mathbf{y})d\lambda,\\
\end{align}\]</span></p>
<p>where the posterior distribution is</p>
<p><span class="math display">\[\begin{align}
  \pi(\lambda|\mathbf{y})&amp;\propto\frac{1}{\Gamma(\alpha_0)\beta_0^{\alpha_0}}\lambda^{\alpha_0-1}\exp\left\{-\lambda/\beta_0\right\}\prod_{i=1}^N \frac{\lambda^{y_i}\exp\left\{-\lambda\right\}}{y_i!}\\
  &amp;\propto \lambda^{\sum_{i=1}^N y_i + \alpha_0 - 1}\exp\left\{-\lambda\left(\frac{\beta_0 N+1}{\beta_0}\right)\right\},
  
\end{align}\]</span></p>
<p>by equation <a href="sec12.html#eq:121">(1.3)</a>. <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function.</p>
<p>Observe that the last expression is the kernel of a Gamma distribution with parameters <span class="math inline">\(\alpha_n=\sum_{i=1}^N y_i + \alpha_0\)</span> and <span class="math inline">\(\beta_n=\frac{\beta_0}{\beta_0 N + 1}\)</span>. Given that <span class="math inline">\(\int_0^{\infty}\pi(\lambda|\mathbf{y})d\lambda=1\)</span>, then the constant of proportionality in the last expression is <span class="math inline">\(\Gamma(\alpha_n)\beta_n^{\alpha_n}\)</span>. The posterior density function <span class="math inline">\(\pi(\lambda|\mathbf{y})\)</span> is <span class="math inline">\(G(\alpha_n,\beta_n)\)</span>.</p>
<p>Observe that</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}[\lambda|\mathbf{y}]&amp;=\alpha_n\beta_n\\
  &amp;=\left(\sum_{i=1}^N y_i + \alpha_0\right)\left(\frac{\beta_0}{\beta_0 N + 1}\right)\\
  &amp;=\bar{y}\left(\frac{N\beta_0}{N\beta_0+1}\right)+\alpha_0\beta_0\left(\frac{1}{N\beta_0+1}\right)\\
  &amp;=w\bar{y}+(1-w)\mathbb{E}[\lambda],
\end{align}\]</span></p>
<p>where <span class="math inline">\(\bar{y}\)</span> is the sample mean, which is the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(w=\left(\frac{N\beta_0}{N\beta_0+1}\right)\)</span> and <span class="math inline">\(\mathbb{E}[\lambda]=\alpha_0\beta_0\)</span> is the prior mean. The posterior mean is a weighted average of the maximum likelihood estimator (sample information) and the prior mean. Observe that <span class="math inline">\(\lim_{N\rightarrow\infty}w= 1\)</span>, that is, the sample information asymptotically dominates.</p>
<p>The predictive distribution is</p>
<p><span class="math display">\[\begin{align}
  \pi(Y_0|\mathbf{y})=&amp;\int_{0}^{\infty}\frac{\lambda^{y_0}\exp\left\{-\lambda\right\}}{y_0!}\times \frac{1}{\Gamma(\alpha_n)\beta_n^{\alpha_n}}\lambda^{\alpha_n-1}\exp\left\{-\lambda/\beta_n\right\} d\lambda\\
  =&amp;\frac{1}{y_0!\Gamma(\alpha_n)\beta_n^{\alpha_n}}\int_{0}^{\infty}\lambda^{y_0+\alpha_n-1}\exp\left\{-\lambda\left(\frac{1+\beta_n}{\beta_n}\right)\right\}d\lambda\\
  =&amp;\frac{\Gamma(y_0+\alpha_n)\left(\frac{\beta_n}{\beta_n+1}\right)^{y_0+\alpha_n}}{y_0!\Gamma(\alpha_n)\beta_n^{\alpha_n}}\\
  =&amp;{y_0+\alpha_n-1 \choose y_0}\left(\frac{\beta_n}{\beta_n+1}\right)^{y_0}\left(\frac{1}{\beta_n+1}\right)^{\alpha_n}.
\end{align}\]</span></p>
<p>The third equality follows from the kernel of a Gamma density, and the fourth from <span class="math inline">\({y_0+\alpha_n-1 \choose y_0}=\frac{(y_0+\alpha_n-1)(y_0+\alpha_n-2)\dots\alpha_n}{y_0!}=\frac{\Gamma(y_0+\alpha_n)}{\Gamma(\alpha_n)y_0!}\)</span> using a property of the Gamma function.</p>
<p>Observe that this is a Negative Binomial density, that is <span class="math inline">\(Y_0|\mathbf{y}\sim NB(\alpha_n,p_n)\)</span> where <span class="math inline">\(p_n=\frac{\beta_n}{\beta_n+1}\)</span>.</p>
<p>A key question is how to fix the <em>hyperparameters</em>. In this exercise we use two approaches for exposition purposes. We set <span class="math inline">\(\alpha_0=0.001\)</span> and <span class="math inline">\(\beta_0=1/0.001\)</span> which imply vague prior information about <span class="math inline">\(\lambda\)</span> due to having a large degree of variability compared to the mean information.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> In particular, <span class="math inline">\(\mathbb{E}[\lambda]=1\)</span> and <span class="math inline">\(\mathbb{V}ar[\lambda]=1000\)</span>.</p>
<p>In this setting, <span class="math inline">\(P(Y_0&gt;0|\mathbf{y})=1-P(Y_0=0|\mathbf{y})\approx 0.67\)</span>. That is, the probability of visiting the Doctor at least once next year is approximately 0.67.</p>
<p>Another approach is using <em>Empirical Bayes</em>, where we set the hyperparameters maximizing the logarithm of the marginal likelihood, that is, <span class="math inline">\([\hat{\alpha}_0,\hat{\beta}_0]=\underset{\alpha_0,\beta_0}{\mathrm{argmax}} \ \ln p(\mathbf{y})\)</span> where</p>
<p><span class="math display">\[\begin{align}
  p(\mathbf{y})&amp;=\int_0^{\infty}\left\{\frac{1}{\Gamma(\alpha_0)\beta_0^{\alpha_0}}\lambda^{\alpha_0-1}\exp\left\{-\lambda/\beta_0\right\} \prod_{i=1}^N\frac{\lambda^{y_i}\exp\left\{-\lambda\right\}}{ y_i!}\right\}d\lambda\\
  &amp;=\frac{\int_0^{\infty}\lambda^{\sum_{i=1}^N y_i+\alpha_0-1}\exp\left\{-\lambda \left(\frac{\beta_0 N +1}{\beta_0}\right) \right\}d\lambda}{\Gamma(\alpha_0)\beta_0^{\alpha_0}\prod_{i=1}^N y_i!}\\
  &amp;=\frac{\Gamma(\sum_{i=1}^N y_i+\alpha_0)\left(\frac{\beta_0}{N\beta_0+1}\right)^{\sum_{i=1}^N y_i}\left(\frac{1}{N\beta_0+1}\right)^{\alpha_0}}{\Gamma(\alpha_0)\prod_{i=1}^N y_i}
\end{align}\]</span></p>
<p>Using the empirical Bayes approach, we get <span class="math inline">\(\hat{\alpha}_0=51.8\)</span> and <span class="math inline">\(\hat{\beta}_0=0.023\)</span>, then <span class="math inline">\(P(Y_0&gt;0|\mathbf{y})=1-P(Y_0=0|\mathbf{y})\approx 0.70\)</span>.</p>
<p>Observe that we can calculate the posterior odds comparing the model using an Empirical Bayes prior (model 1) versus the vague prior (model 2). We assume that <span class="math inline">\(\pi(\mathcal{M}_1)=\pi(\mathcal{M}_2)=0.5\)</span>, then</p>
<p><span class="math display">\[\begin{align}
  PO_{12}&amp;=\frac{\pi(\mathbf{y}|\text{Empirical Bayes})}{\pi(\mathbf{y}|\text{Vague prior})}\\
        &amp;=\frac{\frac{\Gamma(\sum_{i=1}^N y_i+51.807)\left(\frac{0.023}{N\times 0.023+1}\right)^{\sum_{i=1}^N y_i}\left(\frac{1}{N\times 0.023+1}\right)^{51.807}}{\Gamma(51.807)}}{\frac{\Gamma(\sum_{i=1}^N y_i+0.001)\left(\frac{1/0.001}{N/0.001+1}\right)^{\sum_{i=1}^N y_i}\left(\frac{1}{N/0.001+1}\right)^{0.001}}{\Gamma(0.001)}}\\
        &amp;\approx 919.
\end{align}\]</span></p>
<p>Then, <span class="math inline">\(2\times \log(PO_{12})=13.64\)</span>, there is very strong evidence against the vague prior model (see Table <a href="sec12.html#tab:guide">1.1</a>). In particular, <span class="math inline">\(\pi(\text{Empirical Bayes}|\mathbf{y})=\frac{919}{1+919}=0.999\)</span> and <span class="math inline">\(\pi(\text{Vague prior}|\mathbf{y})=1-0.999=0.001\)</span>. These probabilities can be used to perform Bayesian model average (BMA). In particular,</p>
<p><span class="math display">\[\begin{align}
  \mathbb{E}(\lambda|\mathbf{y})&amp;=1.2\times 0.999+1.2\times 0.001=1.2\\
  Var(\lambda|\mathbf{y})&amp;=0.025\times 0.999+0.24\times 0.001\\
  &amp; + (1.2-1.2)^2\times 0.999 + (1.2-1.2)^2\times 0.001= 0.025.
\end{align}\]</span></p>
<p>The BMA predictive distribution is a mix of negative binomial distributions, that is, <span class="math inline">\(y_0|\mathbf{y}\sim 0.999\times NB(57.8, 0.02)+0.001\times NB(6.001, 0.17)\)</span>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="sec12.html#cb9-1"></a><span class="kw">set.seed</span>(<span class="dv">010101</span>)</span>
<span id="cb9-2"><a href="sec12.html#cb9-2"></a>y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># Data</span></span>
<span id="cb9-3"><a href="sec12.html#cb9-3"></a>N &lt;-<span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb9-4"><a href="sec12.html#cb9-4"></a></span>
<span id="cb9-5"><a href="sec12.html#cb9-5"></a><span class="kw">paste</span>(<span class="st">&quot;The sample mean is&quot;</span>, <span class="kw">mean</span>(y), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The sample mean is 1.2&quot;</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="sec12.html#cb11-1"></a><span class="kw">paste</span>(<span class="st">&quot;The sample variance is&quot;</span>, <span class="kw">var</span>(y), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The sample variance is 1.7&quot;</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="sec12.html#cb13-1"></a>ProbBo &lt;-<span class="st"> </span><span class="cf">function</span>(y, a0, b0){</span>
<span id="cb13-2"><a href="sec12.html#cb13-2"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb13-3"><a href="sec12.html#cb13-3"></a>  an &lt;-<span class="st"> </span>a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y) <span class="co"># Posterior shape parameter</span></span>
<span id="cb13-4"><a href="sec12.html#cb13-4"></a>  bn &lt;-<span class="st"> </span>b0 <span class="op">/</span><span class="st"> </span>((b0 <span class="op">*</span><span class="st"> </span>N) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Posterior scale parameter</span></span>
<span id="cb13-5"><a href="sec12.html#cb13-5"></a>  p &lt;-<span class="st"> </span>bn <span class="op">/</span><span class="st"> </span>(bn <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Probability negative binomial density</span></span>
<span id="cb13-6"><a href="sec12.html#cb13-6"></a>  Pr &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnbinom</span>(<span class="dv">0</span>, <span class="dt">size =</span> an, <span class="dt">prob =</span> (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)) <span class="co"># Probability of visiting the Doctor at least once next year</span></span>
<span id="cb13-7"><a href="sec12.html#cb13-7"></a>  <span class="co"># Observe that in R there is a slightly different parametrization.</span></span>
<span id="cb13-8"><a href="sec12.html#cb13-8"></a>  <span class="kw">return</span>(Pr)</span>
<span id="cb13-9"><a href="sec12.html#cb13-9"></a>} </span>
<span id="cb13-10"><a href="sec12.html#cb13-10"></a></span>
<span id="cb13-11"><a href="sec12.html#cb13-11"></a><span class="co"># Using a vague prior:</span></span>
<span id="cb13-12"><a href="sec12.html#cb13-12"></a></span>
<span id="cb13-13"><a href="sec12.html#cb13-13"></a>a0 &lt;-<span class="st"> </span><span class="fl">0.001</span> <span class="co"># Prior shape parameter</span></span>
<span id="cb13-14"><a href="sec12.html#cb13-14"></a>b0 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="fl">0.001</span> <span class="co"># Prior scale parameter</span></span>
<span id="cb13-15"><a href="sec12.html#cb13-15"></a>PriMeanV &lt;-<span class="st"> </span>a0 <span class="op">*</span><span class="st"> </span>b0 <span class="co"># Prior mean</span></span>
<span id="cb13-16"><a href="sec12.html#cb13-16"></a>PriVarV &lt;-<span class="st"> </span>a0 <span class="op">*</span><span class="st"> </span>b0<span class="op">^</span><span class="dv">2</span> <span class="co"># Prior variance</span></span>
<span id="cb13-17"><a href="sec12.html#cb13-17"></a><span class="kw">paste</span>(<span class="st">&quot;Prior mean and prior variance using vague information are&quot;</span>, PriMeanV, <span class="st">&quot;and&quot;</span>, PriVarV, <span class="st">&quot;respectively&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Prior mean and prior variance using vague information are 1 and 1000 respectively&quot;</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="sec12.html#cb15-1"></a>Pp &lt;-<span class="st"> </span><span class="kw">ProbBo</span>(y, <span class="dt">a0 =</span> <span class="fl">0.001</span>, <span class="dt">b0 =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="fl">0.001</span>) <span class="co"># This setting is defining vague prior information.</span></span>
<span id="cb15-2"><a href="sec12.html#cb15-2"></a><span class="kw">paste</span>(<span class="st">&quot;The probability of visiting the Doctor at least once next year using a vague prior is&quot;</span>, Pp, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The probability of visiting the Doctor at least once next year using a vague prior is 0.665096103908558&quot;</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="sec12.html#cb17-1"></a><span class="co"># Using Emprirical Bayes</span></span>
<span id="cb17-2"><a href="sec12.html#cb17-2"></a>LogMgLik &lt;-<span class="st"> </span><span class="cf">function</span>(theta, y){</span>
<span id="cb17-3"><a href="sec12.html#cb17-3"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(y) <span class="co">#sample size</span></span>
<span id="cb17-4"><a href="sec12.html#cb17-4"></a>  a0 &lt;-<span class="st"> </span>theta[<span class="dv">1</span>] <span class="co"># prior shape hyperparameter</span></span>
<span id="cb17-5"><a href="sec12.html#cb17-5"></a>  b0 &lt;-<span class="st"> </span>theta[<span class="dv">2</span>] <span class="co"># prior scale hyperparameter</span></span>
<span id="cb17-6"><a href="sec12.html#cb17-6"></a>  an &lt;-<span class="st"> </span><span class="kw">sum</span>(y) <span class="op">+</span><span class="st"> </span>a0 <span class="co"># posterior shape parameter</span></span>
<span id="cb17-7"><a href="sec12.html#cb17-7"></a>  <span class="cf">if</span>(a0 <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>b0 <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>){ <span class="co">#Avoiding negative values</span></span>
<span id="cb17-8"><a href="sec12.html#cb17-8"></a>    lnp &lt;-<span class="st"> </span><span class="op">-</span><span class="ot">Inf</span></span>
<span id="cb17-9"><a href="sec12.html#cb17-9"></a>  }<span class="cf">else</span>{lnp &lt;-<span class="st"> </span><span class="kw">lgamma</span>(an) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)<span class="op">*</span><span class="kw">log</span>(b0<span class="op">/</span>(N<span class="op">*</span>b0<span class="op">+</span><span class="dv">1</span>)) <span class="op">-</span><span class="st"> </span>a0<span class="op">*</span><span class="kw">log</span>(N<span class="op">*</span>b0<span class="op">+</span><span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="kw">lgamma</span>(a0)} <span class="co"># log marginal likelihood</span></span>
<span id="cb17-10"><a href="sec12.html#cb17-10"></a>  <span class="kw">return</span>(<span class="op">-</span>lnp)</span>
<span id="cb17-11"><a href="sec12.html#cb17-11"></a>}</span>
<span id="cb17-12"><a href="sec12.html#cb17-12"></a></span>
<span id="cb17-13"><a href="sec12.html#cb17-13"></a>theta0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.01</span>, <span class="dv">1</span><span class="op">/</span><span class="fl">0.1</span>) <span class="co"># Initial values</span></span>
<span id="cb17-14"><a href="sec12.html#cb17-14"></a>control &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">1000</span>) <span class="co"># Number of iterations in optimization</span></span>
<span id="cb17-15"><a href="sec12.html#cb17-15"></a>EmpBay &lt;-<span class="st"> </span><span class="kw">optim</span>(theta0, LogMgLik, <span class="dt">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="dt">control =</span> control, <span class="dt">hessian =</span> <span class="ot">TRUE</span>, <span class="dt">y =</span> y) <span class="co"># Optimization</span></span>
<span id="cb17-16"><a href="sec12.html#cb17-16"></a>EmpBay<span class="op">$</span>convergence <span class="co"># Checking convergence</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="sec12.html#cb19-1"></a>EmpBay<span class="op">$</span>value <span class="co"># Maximum</span></span></code></pre></div>
<pre><code>## [1] 4.961032</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="sec12.html#cb21-1"></a>a0EB &lt;-<span class="st"> </span>EmpBay<span class="op">$</span>par[<span class="dv">1</span>] <span class="co"># Prior shape using empirical Bayes</span></span>
<span id="cb21-2"><a href="sec12.html#cb21-2"></a>b0EB &lt;-<span class="st"> </span>EmpBay<span class="op">$</span>par[<span class="dv">2</span>] <span class="co"># Prior scale using empirical Bayes</span></span>
<span id="cb21-3"><a href="sec12.html#cb21-3"></a><span class="kw">paste</span>(<span class="st">&quot;The prior shape and scale parameters are&quot;</span>, a0EB, <span class="st">&quot;and&quot;</span>, b0EB, <span class="st">&quot;respectively&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The prior shape and scale parameters are 51.8069610269937 and 0.0231834128698478 respectively&quot;</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="sec12.html#cb23-1"></a>PriMeanEB &lt;-<span class="st"> </span>a0EB <span class="op">*</span><span class="st"> </span>b0EB <span class="co"># Prior mean</span></span>
<span id="cb23-2"><a href="sec12.html#cb23-2"></a>PriVarEB &lt;-<span class="st"> </span>a0EB <span class="op">*</span><span class="st"> </span>b0EB<span class="op">^</span><span class="dv">2</span> <span class="co"># Prior variance</span></span>
<span id="cb23-3"><a href="sec12.html#cb23-3"></a><span class="kw">paste</span>(<span class="st">&quot;Prior mean and variance using empirical Bayes are&quot;</span>, PriMeanEB, <span class="st">&quot;and&quot;</span>, PriVarEB, <span class="st">&quot;respectively&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Prior mean and variance using empirical Bayes are 1.20106216702091 and 0.0278447201003999 respectively&quot;</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="sec12.html#cb25-1"></a>PpEB &lt;-<span class="st"> </span><span class="kw">ProbBo</span>(y, <span class="dt">a0 =</span> a0EB, <span class="dt">b0 =</span> b0EB) <span class="co"># This setting is using emprical Bayes.</span></span>
<span id="cb25-2"><a href="sec12.html#cb25-2"></a><span class="kw">paste</span>(<span class="st">&quot;The probability of visiting the Doctor at least once next year using empirical Bayes is&quot;</span>, PpEB, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The probability of visiting the Doctor at least once next year using empirical Bayes is 0.695366831279072&quot;</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="sec12.html#cb27-1"></a><span class="co"># Density figures</span></span>
<span id="cb27-2"><a href="sec12.html#cb27-2"></a>lambda &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="fl">0.01</span>) <span class="co"># Values of lambda</span></span>
<span id="cb27-3"><a href="sec12.html#cb27-3"></a>VaguePrior &lt;-<span class="st"> </span><span class="kw">dgamma</span>(lambda, <span class="dt">shape =</span> a0, <span class="dt">scale =</span> b0)</span>
<span id="cb27-4"><a href="sec12.html#cb27-4"></a>EBPrior &lt;-<span class="st"> </span><span class="kw">dgamma</span>(lambda, <span class="dt">shape =</span> a0EB, <span class="dt">scale =</span> b0EB)</span>
<span id="cb27-5"><a href="sec12.html#cb27-5"></a>PosteriorV &lt;-<span class="st"> </span><span class="kw">dgamma</span>(lambda, <span class="dt">shape =</span> a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y), <span class="dt">scale =</span> b0 <span class="op">/</span><span class="st"> </span>((b0 <span class="op">*</span><span class="st"> </span>N) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) </span>
<span id="cb27-6"><a href="sec12.html#cb27-6"></a>PosteriorEB &lt;-<span class="st"> </span><span class="kw">dgamma</span>(lambda, <span class="dt">shape =</span> a0EB <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y), <span class="dt">scale =</span> b0EB <span class="op">/</span><span class="st"> </span>((b0EB <span class="op">*</span><span class="st"> </span>N) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb27-7"><a href="sec12.html#cb27-7"></a></span>
<span id="cb27-8"><a href="sec12.html#cb27-8"></a><span class="co"># Likelihood function</span></span>
<span id="cb27-9"><a href="sec12.html#cb27-9"></a>Likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta, y){</span>
<span id="cb27-10"><a href="sec12.html#cb27-10"></a>  LogL &lt;-<span class="st"> </span><span class="kw">dpois</span>(y, theta, <span class="dt">log =</span> <span class="ot">TRUE</span>)</span>
<span id="cb27-11"><a href="sec12.html#cb27-11"></a>  Lik &lt;-<span class="st"> </span><span class="kw">prod</span>(<span class="kw">exp</span>(LogL))</span>
<span id="cb27-12"><a href="sec12.html#cb27-12"></a>  <span class="kw">return</span>(Lik)</span>
<span id="cb27-13"><a href="sec12.html#cb27-13"></a>}</span>
<span id="cb27-14"><a href="sec12.html#cb27-14"></a>Liks &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambda, <span class="cf">function</span>(par) {<span class="kw">Likelihood</span>(par, <span class="dt">y =</span> y)})</span>
<span id="cb27-15"><a href="sec12.html#cb27-15"></a>Sc &lt;-<span class="st"> </span><span class="kw">max</span>(PosteriorEB)<span class="op">/</span><span class="kw">max</span>(Liks) <span class="co">#Scale for displaying in figure</span></span>
<span id="cb27-16"><a href="sec12.html#cb27-16"></a>LiksScale &lt;-<span class="st"> </span>Liks <span class="op">*</span><span class="st"> </span>Sc</span>
<span id="cb27-17"><a href="sec12.html#cb27-17"></a></span>
<span id="cb27-18"><a href="sec12.html#cb27-18"></a>data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(lambda, VaguePrior, EBPrior, PosteriorV, PosteriorEB, LiksScale)) <span class="co">#Data frame</span></span>
<span id="cb27-19"><a href="sec12.html#cb27-19"></a></span>
<span id="cb27-20"><a href="sec12.html#cb27-20"></a><span class="kw">require</span>(ggplot2) <span class="co"># Cool figures</span></span></code></pre></div>
<pre><code>## Loading required package: ggplot2</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="sec12.html#cb29-1"></a><span class="kw">require</span>(latex2exp) <span class="co"># LaTeX equations in figures</span></span></code></pre></div>
<pre><code>## Loading required package: latex2exp</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="sec12.html#cb31-1"></a><span class="kw">require</span>(ggpubr) <span class="co"># Multiple figures in one page</span></span></code></pre></div>
<pre><code>## Loading required package: ggpubr</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="sec12.html#cb33-1"></a>fig1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> data, <span class="kw">aes</span>(lambda, VaguePrior)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb33-2"><a href="sec12.html#cb33-2"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st">  </span></span>
<span id="cb33-3"><a href="sec12.html#cb33-3"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb33-4"><a href="sec12.html#cb33-4"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Prior: Vague Gamma&quot;</span>) </span>
<span id="cb33-5"><a href="sec12.html#cb33-5"></a></span>
<span id="cb33-6"><a href="sec12.html#cb33-6"></a>fig2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> data, <span class="kw">aes</span>(lambda, EBPrior)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb33-7"><a href="sec12.html#cb33-7"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st">  </span></span>
<span id="cb33-8"><a href="sec12.html#cb33-8"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb33-9"><a href="sec12.html#cb33-9"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Prior: Empirical Bayes Gamma&quot;</span>)</span>
<span id="cb33-10"><a href="sec12.html#cb33-10"></a></span>
<span id="cb33-11"><a href="sec12.html#cb33-11"></a>fig3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> data, <span class="kw">aes</span>(lambda, PosteriorV)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb33-12"><a href="sec12.html#cb33-12"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st">  </span></span>
<span id="cb33-13"><a href="sec12.html#cb33-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb33-14"><a href="sec12.html#cb33-14"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Posterior: Vague Gamma&quot;</span>)</span>
<span id="cb33-15"><a href="sec12.html#cb33-15"></a></span>
<span id="cb33-16"><a href="sec12.html#cb33-16"></a>fig4 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> data, <span class="kw">aes</span>(lambda, PosteriorEB)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb33-17"><a href="sec12.html#cb33-17"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st">  </span></span>
<span id="cb33-18"><a href="sec12.html#cb33-18"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb33-19"><a href="sec12.html#cb33-19"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Posterior: Empirical Bayes Gamma&quot;</span>)</span>
<span id="cb33-20"><a href="sec12.html#cb33-20"></a></span>
<span id="cb33-21"><a href="sec12.html#cb33-21"></a>FIG &lt;-<span class="st"> </span><span class="kw">ggarrange</span>(fig1, fig2, fig3, fig4,</span>
<span id="cb33-22"><a href="sec12.html#cb33-22"></a>                 <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb33-23"><a href="sec12.html#cb33-23"></a></span>
<span id="cb33-24"><a href="sec12.html#cb33-24"></a><span class="kw">annotate_figure</span>(FIG,</span>
<span id="cb33-25"><a href="sec12.html#cb33-25"></a>                <span class="dt">top =</span> <span class="kw">text_grob</span>(<span class="st">&quot;Vague versus Empirical Bayes: Poisson-Gamma model&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>, <span class="dt">size =</span> <span class="dv">14</span>))</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-1.svg" width="672" /></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="sec12.html#cb34-1"></a>dataNew &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(lambda, <span class="dv">3</span>), <span class="kw">c</span>(EBPrior, PosteriorEB, LiksScale),</span>
<span id="cb34-2"><a href="sec12.html#cb34-2"></a>                            <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each =</span> <span class="dv">1000</span>))) <span class="co">#Data frame</span></span>
<span id="cb34-3"><a href="sec12.html#cb34-3"></a></span>
<span id="cb34-4"><a href="sec12.html#cb34-4"></a><span class="kw">colnames</span>(dataNew) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Lambda&quot;</span>, <span class="st">&quot;Density&quot;</span>, <span class="st">&quot;Factor&quot;</span>)</span>
<span id="cb34-5"><a href="sec12.html#cb34-5"></a>dataNew<span class="op">$</span>Factor &lt;-<span class="st"> </span><span class="kw">factor</span>(dataNew<span class="op">$</span>Factor, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;3&quot;</span>, <span class="st">&quot;2&quot;</span>), </span>
<span id="cb34-6"><a href="sec12.html#cb34-6"></a>                         <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;Prior&quot;</span>, <span class="st">&quot;Likelihood&quot;</span>, <span class="st">&quot;Posterior&quot;</span>))</span>
<span id="cb34-7"><a href="sec12.html#cb34-7"></a></span>
<span id="cb34-8"><a href="sec12.html#cb34-8"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> dataNew, <span class="kw">aes_string</span>(<span class="dt">x =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>, <span class="dt">group =</span> <span class="st">&quot;Factor&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb34-9"><a href="sec12.html#cb34-9"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Factor)) <span class="op">+</span></span>
<span id="cb34-10"><a href="sec12.html#cb34-10"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb34-11"><a href="sec12.html#cb34-11"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Prior, likelihood and posterior: Empirical Bayes Poisson-Gamma model&quot;</span>) <span class="op">+</span></span>
<span id="cb34-12"><a href="sec12.html#cb34-12"></a><span class="st">  </span><span class="kw">guides</span>(<span class="dt">color=</span><span class="kw">guide_legend</span>(<span class="dt">title=</span><span class="st">&quot;Information&quot;</span>)) <span class="op">+</span></span>
<span id="cb34-13"><a href="sec12.html#cb34-13"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;yellow&quot;</span>, <span class="st">&quot;blue&quot;</span>))</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-2.svg" width="672" /></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="sec12.html#cb35-1"></a><span class="co"># Predictive distributions</span></span>
<span id="cb35-2"><a href="sec12.html#cb35-2"></a>PredDen &lt;-<span class="st"> </span><span class="cf">function</span>(y, y0, a0, b0){</span>
<span id="cb35-3"><a href="sec12.html#cb35-3"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb35-4"><a href="sec12.html#cb35-4"></a>  an &lt;-<span class="st"> </span>a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y) <span class="co"># Posterior shape parameter</span></span>
<span id="cb35-5"><a href="sec12.html#cb35-5"></a>  bn &lt;-<span class="st"> </span>b0 <span class="op">/</span><span class="st"> </span>((b0 <span class="op">*</span><span class="st"> </span>N) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Posterior scale parameter</span></span>
<span id="cb35-6"><a href="sec12.html#cb35-6"></a>  p &lt;-<span class="st"> </span>bn <span class="op">/</span><span class="st"> </span>(bn <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Probability negative binomial density</span></span>
<span id="cb35-7"><a href="sec12.html#cb35-7"></a>  Pr &lt;-<span class="st"> </span><span class="kw">dnbinom</span>(y0, <span class="dt">size =</span> an, <span class="dt">prob =</span> (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)) <span class="co"># Predictive density</span></span>
<span id="cb35-8"><a href="sec12.html#cb35-8"></a>  <span class="co"># Observe that in R there is a slightly different parametrization.</span></span>
<span id="cb35-9"><a href="sec12.html#cb35-9"></a>  <span class="kw">return</span>(Pr)</span>
<span id="cb35-10"><a href="sec12.html#cb35-10"></a>}</span>
<span id="cb35-11"><a href="sec12.html#cb35-11"></a>y0 &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb35-12"><a href="sec12.html#cb35-12"></a>PredVague &lt;-<span class="st"> </span><span class="kw">PredDen</span>(<span class="dt">y =</span> y, <span class="dt">y0 =</span> y0, <span class="dt">a0 =</span> a0, <span class="dt">b0 =</span> b0)</span>
<span id="cb35-13"><a href="sec12.html#cb35-13"></a>PredEB &lt;-<span class="st"> </span><span class="kw">PredDen</span>(<span class="dt">y =</span> y, <span class="dt">y0 =</span> y0, <span class="dt">a0 =</span> a0EB, <span class="dt">b0 =</span> b0EB)</span>
<span id="cb35-14"><a href="sec12.html#cb35-14"></a>dataPred &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(y0, PredVague, PredEB))</span>
<span id="cb35-15"><a href="sec12.html#cb35-15"></a><span class="kw">colnames</span>(dataPred) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y0&quot;</span>, <span class="st">&quot;PredictiveVague&quot;</span>, <span class="st">&quot;PredictiveEB&quot;</span>)</span>
<span id="cb35-16"><a href="sec12.html#cb35-16"></a></span>
<span id="cb35-17"><a href="sec12.html#cb35-17"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> dataPred) <span class="op">+</span><span class="st"> </span></span>
<span id="cb35-18"><a href="sec12.html#cb35-18"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(y0, PredictiveVague, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)) <span class="op">+</span><span class="st">  </span></span>
<span id="cb35-19"><a href="sec12.html#cb35-19"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$y_0$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb35-20"><a href="sec12.html#cb35-20"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Predictive density: Vague and Empirical Bayes priors&quot;</span>) <span class="op">+</span></span>
<span id="cb35-21"><a href="sec12.html#cb35-21"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(y0, PredictiveEB, <span class="dt">color =</span> <span class="st">&quot;yellow&quot;</span>)) <span class="op">+</span></span>
<span id="cb35-22"><a href="sec12.html#cb35-22"></a><span class="st">  </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">title=</span><span class="st">&quot;Prior&quot;</span>)) <span class="op">+</span></span>
<span id="cb35-23"><a href="sec12.html#cb35-23"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Vague&quot;</span>, <span class="st">&quot;Empirical Bayes&quot;</span>), <span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;yellow&quot;</span>)) <span class="op">+</span></span>
<span id="cb35-24"><a href="sec12.html#cb35-24"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dt">by=</span><span class="dv">1</span>))</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-3.svg" width="672" /></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="sec12.html#cb36-1"></a><span class="co"># Posterior odds: Vague vs Empirical Bayes</span></span>
<span id="cb36-2"><a href="sec12.html#cb36-2"></a></span>
<span id="cb36-3"><a href="sec12.html#cb36-3"></a>PO12 &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="kw">LogMgLik</span>(<span class="kw">c</span>(a0EB, b0EB), <span class="dt">y =</span> y))<span class="op">/</span><span class="kw">exp</span>(<span class="op">-</span><span class="kw">LogMgLik</span>(<span class="kw">c</span>(a0, b0), <span class="dt">y =</span> y))</span>
<span id="cb36-4"><a href="sec12.html#cb36-4"></a><span class="kw">paste</span>(<span class="st">&quot;The posterior odds: Empirical Bayes vs Vague prior prior is&quot;</span>, PO12, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The posterior odds: Empirical Bayes vs Vague prior prior is 919.006897935592&quot;</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="sec12.html#cb38-1"></a>PostProMEM &lt;-<span class="st"> </span>PO12<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>PO12) <span class="co"># Posterior model probability Empirical Bayes</span></span>
<span id="cb38-2"><a href="sec12.html#cb38-2"></a>PostProbMV &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>PostProMEM <span class="co"># Posterior model probability vague prior</span></span>
<span id="cb38-3"><a href="sec12.html#cb38-3"></a></span>
<span id="cb38-4"><a href="sec12.html#cb38-4"></a><span class="kw">paste</span>(<span class="st">&quot;These are the posterior model probabilities&quot;</span>, PostProMEM, PostProbMV, <span class="st">&quot;for the Empirical Bayes and vague priors, respectively&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;These are the posterior model probabilities 0.998913051627935 0.00108694837206535 for the Empirical Bayes and vague priors, respectively&quot;</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="sec12.html#cb40-1"></a><span class="co"># Bayesian model average (BMA)</span></span>
<span id="cb40-2"><a href="sec12.html#cb40-2"></a>PostMeanEB &lt;-<span class="st"> </span>(a0EB <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)) <span class="op">*</span><span class="st"> </span>(b0EB <span class="op">/</span><span class="st"> </span>(b0EB <span class="op">*</span><span class="st"> </span>N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) <span class="co"># Posterior mean Empirical Bayes </span></span>
<span id="cb40-3"><a href="sec12.html#cb40-3"></a>PostMeanV &lt;-<span class="st"> </span>(a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)) <span class="op">*</span><span class="st"> </span>(b0 <span class="op">/</span><span class="st"> </span>(b0 <span class="op">*</span><span class="st"> </span>N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) <span class="co"># Posterior mean vague priors</span></span>
<span id="cb40-4"><a href="sec12.html#cb40-4"></a>BMAmean &lt;-<span class="st"> </span>PostProMEM <span class="op">*</span><span class="st"> </span>PostMeanEB <span class="op">+</span><span class="st"> </span>PostProbMV <span class="op">*</span><span class="st"> </span>PostMeanV  <span class="co"># BMA posterior mean</span></span>
<span id="cb40-5"><a href="sec12.html#cb40-5"></a></span>
<span id="cb40-6"><a href="sec12.html#cb40-6"></a>PostVarEB &lt;-<span class="st"> </span>(a0EB <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)) <span class="op">*</span><span class="st"> </span>(b0EB <span class="op">/</span><span class="st"> </span>(b0EB <span class="op">*</span><span class="st"> </span>N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))<span class="op">^</span><span class="dv">2</span> <span class="co"># Posterior variance Empirical Bayes</span></span>
<span id="cb40-7"><a href="sec12.html#cb40-7"></a>PostVarV &lt;-<span class="st"> </span>(a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y)) <span class="op">*</span><span class="st"> </span>(b0 <span class="op">/</span><span class="st"> </span>(b0 <span class="op">*</span><span class="st"> </span>N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))<span class="op">^</span><span class="dv">2</span> <span class="co"># Posterior variance vague prior </span></span>
<span id="cb40-8"><a href="sec12.html#cb40-8"></a></span>
<span id="cb40-9"><a href="sec12.html#cb40-9"></a>BMAVar &lt;-<span class="st"> </span>PostProMEM <span class="op">*</span><span class="st"> </span>PostVarEB <span class="op">+</span><span class="st"> </span>PostProbMV <span class="op">*</span><span class="st"> </span>PostVarV <span class="op">+</span><span class="st"> </span>PostProMEM <span class="op">*</span><span class="st"> </span>(PostMeanEB <span class="op">-</span><span class="st"> </span>BMAmean)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>PostProbMV <span class="op">*</span><span class="st"> </span>(PostMeanV <span class="op">-</span><span class="st"> </span>BMAmean)<span class="op">^</span><span class="dv">2</span><span class="co"># BMA posterior variance   </span></span>
<span id="cb40-10"><a href="sec12.html#cb40-10"></a></span>
<span id="cb40-11"><a href="sec12.html#cb40-11"></a><span class="kw">paste</span>(<span class="st">&quot;The BMA posterior mean and variance are&quot;</span>, BMAmean, <span class="st">&quot;and&quot;</span>, BMAVar, <span class="st">&quot;respectively&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The BMA posterior mean and variance are 1.20095075523623 and 0.0251837165972447 respectively&quot;</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="sec12.html#cb42-1"></a><span class="co"># BMA: Predictive</span></span>
<span id="cb42-2"><a href="sec12.html#cb42-2"></a>BMAPred &lt;-<span class="st"> </span>PostProMEM <span class="op">*</span><span class="st"> </span>PredEB <span class="op">+</span><span class="st"> </span>PostProbMV <span class="op">*</span><span class="st"> </span>PredVague    </span>
<span id="cb42-3"><a href="sec12.html#cb42-3"></a>dataPredBMA &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(y0, BMAPred))</span>
<span id="cb42-4"><a href="sec12.html#cb42-4"></a><span class="kw">colnames</span>(dataPredBMA) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y0&quot;</span>, <span class="st">&quot;PredictiveBMA&quot;</span>)</span>
<span id="cb42-5"><a href="sec12.html#cb42-5"></a></span>
<span id="cb42-6"><a href="sec12.html#cb42-6"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> dataPredBMA) <span class="op">+</span><span class="st"> </span></span>
<span id="cb42-7"><a href="sec12.html#cb42-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(y0, PredictiveBMA, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)) <span class="op">+</span><span class="st">  </span></span>
<span id="cb42-8"><a href="sec12.html#cb42-8"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$y_0$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb42-9"><a href="sec12.html#cb42-9"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Predictive density: BMA&quot;</span>) </span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-4.svg" width="672" /></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="sec12.html#cb43-1"></a><span class="co"># Bayesian updating</span></span>
<span id="cb43-2"><a href="sec12.html#cb43-2"></a>BayUp &lt;-<span class="st"> </span><span class="cf">function</span>(y, lambda, a0, b0){</span>
<span id="cb43-3"><a href="sec12.html#cb43-3"></a>  N &lt;-<span class="st"> </span><span class="kw">length</span>(y)</span>
<span id="cb43-4"><a href="sec12.html#cb43-4"></a>  an &lt;-<span class="st"> </span>a0 <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(y) <span class="co"># Posterior shape parameter</span></span>
<span id="cb43-5"><a href="sec12.html#cb43-5"></a>  bn &lt;-<span class="st"> </span>b0 <span class="op">/</span><span class="st"> </span>((b0 <span class="op">*</span><span class="st"> </span>N) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Posterior scale parameter</span></span>
<span id="cb43-6"><a href="sec12.html#cb43-6"></a>  p &lt;-<span class="st"> </span><span class="kw">dgamma</span>(lambda, <span class="dt">shape =</span> an, <span class="dt">scale =</span> bn) <span class="co"># Posterior density</span></span>
<span id="cb43-7"><a href="sec12.html#cb43-7"></a>  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">Post =</span> p, <span class="dt">a0New =</span> an, <span class="dt">b0New =</span> bn))</span>
<span id="cb43-8"><a href="sec12.html#cb43-8"></a>}</span>
<span id="cb43-9"><a href="sec12.html#cb43-9"></a></span>
<span id="cb43-10"><a href="sec12.html#cb43-10"></a>PostUp &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb43-11"><a href="sec12.html#cb43-11"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){</span>
<span id="cb43-12"><a href="sec12.html#cb43-12"></a>  <span class="cf">if</span>(i <span class="op">==</span><span class="st"> </span><span class="dv">1</span>){</span>
<span id="cb43-13"><a href="sec12.html#cb43-13"></a>    PostUpi &lt;-<span class="st"> </span><span class="kw">BayUp</span>(y[i], lambda, <span class="dt">a0 =</span> <span class="fl">0.001</span>, <span class="dt">b0 =</span> <span class="dv">1</span><span class="op">/</span><span class="fl">0.001</span>)}</span>
<span id="cb43-14"><a href="sec12.html#cb43-14"></a>  <span class="cf">else</span>{</span>
<span id="cb43-15"><a href="sec12.html#cb43-15"></a>    PostUpi &lt;-<span class="st"> </span><span class="kw">BayUp</span>(y[i], lambda, <span class="dt">a0 =</span> PostUpi<span class="op">$</span>a0New, <span class="dt">b0 =</span> PostUpi<span class="op">$</span>b0New)</span>
<span id="cb43-16"><a href="sec12.html#cb43-16"></a>  }</span>
<span id="cb43-17"><a href="sec12.html#cb43-17"></a>  PostUp &lt;-<span class="st"> </span><span class="kw">cbind</span>(PostUp, PostUpi<span class="op">$</span>Post)</span>
<span id="cb43-18"><a href="sec12.html#cb43-18"></a>}</span>
<span id="cb43-19"><a href="sec12.html#cb43-19"></a></span>
<span id="cb43-20"><a href="sec12.html#cb43-20"></a>DataUp &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(lambda, <span class="dv">5</span>), <span class="kw">c</span>(PostUp),</span>
<span id="cb43-21"><a href="sec12.html#cb43-21"></a>                            <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">each =</span> <span class="dv">1000</span>))) <span class="co">#Data frame</span></span>
<span id="cb43-22"><a href="sec12.html#cb43-22"></a></span>
<span id="cb43-23"><a href="sec12.html#cb43-23"></a><span class="kw">colnames</span>(DataUp) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Lambda&quot;</span>, <span class="st">&quot;Density&quot;</span>, <span class="st">&quot;Factor&quot;</span>)</span>
<span id="cb43-24"><a href="sec12.html#cb43-24"></a></span>
<span id="cb43-25"><a href="sec12.html#cb43-25"></a>DataUp<span class="op">$</span>Factor &lt;-<span class="st"> </span><span class="kw">factor</span>(DataUp<span class="op">$</span>Factor, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>, <span class="st">&quot;4&quot;</span>, <span class="st">&quot;5&quot;</span>), </span>
<span id="cb43-26"><a href="sec12.html#cb43-26"></a>                         <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;Iter 1&quot;</span>, <span class="st">&quot;Iter 2&quot;</span>, <span class="st">&quot;Iter 3&quot;</span>, <span class="st">&quot;Iter 4&quot;</span>, <span class="st">&quot;Iter 5&quot;</span>))</span>
<span id="cb43-27"><a href="sec12.html#cb43-27"></a></span>
<span id="cb43-28"><a href="sec12.html#cb43-28"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> DataUp, <span class="kw">aes_string</span>(<span class="dt">x =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>, <span class="dt">group =</span> <span class="st">&quot;Factor&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb43-29"><a href="sec12.html#cb43-29"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Factor)) <span class="op">+</span></span>
<span id="cb43-30"><a href="sec12.html#cb43-30"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">TeX</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">lambda$&quot;</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>) <span class="op">+</span></span>
<span id="cb43-31"><a href="sec12.html#cb43-31"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Bayesian updating: Poisson-Gamma model with vague prior&quot;</span>) <span class="op">+</span></span>
<span id="cb43-32"><a href="sec12.html#cb43-32"></a><span class="st">  </span><span class="kw">guides</span>(<span class="dt">color=</span><span class="kw">guide_legend</span>(<span class="dt">title=</span><span class="st">&quot;Update&quot;</span>)) <span class="op">+</span></span>
<span id="cb43-33"><a href="sec12.html#cb43-33"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;purple&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;yellow&quot;</span>, <span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="BayesianEconometrics_files/figure-html/unnamed-chunk-4-5.svg" width="672" /></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="sec12.html#cb44-1"></a>S &lt;-<span class="st"> </span><span class="dv">100000</span> <span class="co"># Posterior draws</span></span>
<span id="cb44-2"><a href="sec12.html#cb44-2"></a>PostMeanLambdaUps &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>N, <span class="cf">function</span>(i) {<span class="kw">mean</span>(<span class="kw">sample</span>(lambda, S, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> PostUp[ , i]))}) <span class="co">#Posterior mean update i</span></span>
<span id="cb44-3"><a href="sec12.html#cb44-3"></a><span class="kw">paste</span>(<span class="st">&quot;Posterior means using all information and sequential updating are:&quot;</span>, <span class="kw">round</span>(PostMeanV, <span class="dv">2</span>), <span class="st">&quot;and&quot;</span>, <span class="kw">round</span>(PostMeanLambdaUps[<span class="dv">5</span>], <span class="dv">2</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) </span></code></pre></div>
<pre><code>## [1] &quot;Posterior means using all information and sequential updating are: 1.2 and 1.2&quot;</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="sec12.html#cb46-1"></a>PostVarLambdaUps &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>N, <span class="cf">function</span>(i) {<span class="kw">var</span>(<span class="kw">sample</span>(lambda, S, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> PostUp[ , i]))}) <span class="co">#Posterior variance update i</span></span>
<span id="cb46-2"><a href="sec12.html#cb46-2"></a><span class="kw">paste</span>(<span class="st">&quot;Posterior variances using all information and sequential updating are:&quot;</span>, <span class="kw">round</span>(PostVarV, <span class="dv">2</span>), <span class="st">&quot;and&quot;</span>, <span class="kw">round</span>(PostVarLambdaUps[<span class="dv">5</span>], <span class="dv">2</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Posterior variances using all information and sequential updating are: 0.24 and 0.24&quot;</code></pre>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bickel1969some">
<p>Bickel, Peter J, and Joseph A Yahav. 1969. “Some Contributions to the Asymptotic Theory of Bayes Solutions.” <em>Zeitschrift Für Wahrscheinlichkeitstheorie Und Verwandte Gebiete</em> 11 (4): 257–76.</p>
</div>
<div id="ref-Kass1995">
<p>Kass, Robert E., and Adrian E. Raftery. 1995. “Bayes Factorss.” <em>Journal of American Statistical Association</em> 90 (430): 773–95.</p>
</div>
<div id="ref-Lehmann2003">
<p>Lehmann, E. L., and George Casella. 2003. <em>Theory of Point Estimation</em>. Second Edition. Springer.</p>
</div>
<div id="ref-petris2009dynamic">
<p>Petris, Giovanni, Sonia Petrone, and Patrizia Campagnoli. 2009. “Dynamic Linear Models.” In <em>Dynamic Linear Models with R</em>, 31–84. Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>From a Bayesian perspective <span class="math inline">\(\mathbf{\theta}\)</span> is fixed, but unknown. Then, it is treated as a random object.<a href="sec12.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><span class="math inline">\(\propto\)</span> is the proportional symbol.<a href="sec12.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p><span class="math inline">\(\perp\)</span> is the independence symbol.<a href="sec12.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Independent and identically distribuited draws.<a href="sec12.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>We should be aware that there may be technical problems using this king of hyperparameters in this setting <span class="citation">(Gelman and others <a href="#ref-gelman2006prior" role="doc-biblioref">2006</a>)</span>.<a href="sec12.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec11.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec13.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/01-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
