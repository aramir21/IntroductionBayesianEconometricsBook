<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Econometrics</title>
  <meta name="description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  <meta name="github-repo" content="https://github.com/aramir21/IntroductionBayesianEconometricsBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Linear regression: The conjugate normal-normal/inverse gamma model | Introduction to Bayesian Econometrics" />
  
  <meta name="twitter:description" content="The subject of this textbook is Bayesian regression analysis, and its main aim is to provide introductory level theory foundation, and facilitate applicability of Bayesian inference." />
  

<meta name="author" content="Andrés Ramírez-Hassan" />


<meta name="date" content="2021-05-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec42.html"/>
<link rel="next" href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Bayesian Econometrics: A GUIded tour</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="to-instructors-and-students.html"><a href="to-instructors-and-students.html"><i class="fa fa-check"></i>To instructors and students</a></li>
<li class="chapter" data-level="" data-path="a-brief-presentation-of-r-software.html"><a href="a-brief-presentation-of-r-software.html"><i class="fa fa-check"></i>A brief presentation of R software</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basic formal concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="sec11.html"><a href="sec11.html"><i class="fa fa-check"></i><b>1.1</b> The Bayes’ rule</a></li>
<li class="chapter" data-level="1.2" data-path="sec12.html"><a href="sec12.html"><i class="fa fa-check"></i><b>1.2</b> Bayesian framework: A brief summary of theory</a></li>
<li class="chapter" data-level="1.3" data-path="sec13.html"><a href="sec13.html"><i class="fa fa-check"></i><b>1.3</b> Bayesian reports: Decision theory under uncertainty</a></li>
<li class="chapter" data-level="1.4" data-path="summary-chapter-1.html"><a href="summary-chapter-1.html"><i class="fa fa-check"></i><b>1.4</b> Summary: Chapter 1</a></li>
<li class="chapter" data-level="1.5" data-path="exercises-chapter-1.html"><a href="exercises-chapter-1.html"><i class="fa fa-check"></i><b>1.5</b> Exercises: Chapter 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayfre.html"><a href="bayfre.html"><i class="fa fa-check"></i><b>2</b> Conceptual differences between the Bayesian and the Frequentist statistical approaches</a><ul>
<li class="chapter" data-level="2.1" data-path="sec21.html"><a href="sec21.html"><i class="fa fa-check"></i><b>2.1</b> The concept of probability</a></li>
<li class="chapter" data-level="2.2" data-path="sec22.html"><a href="sec22.html"><i class="fa fa-check"></i><b>2.2</b> Subjectivity is not the key</a></li>
<li class="chapter" data-level="2.3" data-path="sec23.html"><a href="sec23.html"><i class="fa fa-check"></i><b>2.3</b> Estimation, hypothesis testing and prediction</a></li>
<li class="chapter" data-level="2.4" data-path="sec24.html"><a href="sec24.html"><i class="fa fa-check"></i><b>2.4</b> The likelihood principle</a></li>
<li class="chapter" data-level="2.5" data-path="sec25.html"><a href="sec25.html"><i class="fa fa-check"></i><b>2.5</b> Logic of argumentation</a></li>
<li class="chapter" data-level="2.6" data-path="sec25A.html"><a href="sec25A.html"><i class="fa fa-check"></i><b>2.6</b> Why is not the Bayesian approach that popular?</a></li>
<li class="chapter" data-level="2.7" data-path="sec26.html"><a href="sec26.html"><i class="fa fa-check"></i><b>2.7</b> A simple working example</a></li>
<li class="chapter" data-level="2.8" data-path="sec27.html"><a href="sec27.html"><i class="fa fa-check"></i><b>2.8</b> Summary: Chapter 2</a></li>
<li class="chapter" data-level="2.9" data-path="exercises-chapter-2.html"><a href="exercises-chapter-2.html"><i class="fa fa-check"></i><b>2.9</b> Exercises: Chapter 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="objsub.html"><a href="objsub.html"><i class="fa fa-check"></i><b>3</b> Objective and subjective Bayesian approaches</a><ul>
<li class="chapter" data-level="3.1" data-path="sec31.html"><a href="sec31.html"><i class="fa fa-check"></i><b>3.1</b> Objective Bayesian priors</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec31.html"><a href="sec31.html#empirical-bayes"><i class="fa fa-check"></i><b>3.1.1</b> Empirical Bayes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec32.html"><a href="sec32.html"><i class="fa fa-check"></i><b>3.2</b> Subjective Bayesian priors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sec32.html"><a href="sec32.html#human-heuristics"><i class="fa fa-check"></i><b>3.2.1</b> Human heuristics</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec32.html"><a href="sec32.html#elicitation"><i class="fa fa-check"></i><b>3.2.2</b> Elicitation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conjfam.html"><a href="conjfam.html"><i class="fa fa-check"></i><b>4</b> Basic statistical models: Conjugate families</a><ul>
<li class="chapter" data-level="4.1" data-path="sec41.html"><a href="sec41.html"><i class="fa fa-check"></i><b>4.1</b> Motivation of conjugate families</a></li>
<li class="chapter" data-level="4.2" data-path="sec42.html"><a href="sec42.html"><i class="fa fa-check"></i><b>4.2</b> Conjugate prior to exponential family</a></li>
<li class="chapter" data-level="4.3" data-path="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html"><i class="fa fa-check"></i><b>4.3</b> Linear regression: The conjugate normal-normal/inverse gamma model</a></li>
<li class="chapter" data-level="4.4" data-path="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html"><i class="fa fa-check"></i><b>4.4</b> Multivariate linear regression: The conjugate normal-normal/inverse Wishart model</a></li>
<li class="chapter" data-level="4.5" data-path="computational-examples.html"><a href="computational-examples.html"><i class="fa fa-check"></i><b>4.5</b> Computational examples</a></li>
<li class="chapter" data-level="4.6" data-path="summary-chapter-4.html"><a href="summary-chapter-4.html"><i class="fa fa-check"></i><b>4.6</b> Summary: Chapter 4</a></li>
<li class="chapter" data-level="4.7" data-path="exercises-chapter-4.html"><a href="exercises-chapter-4.html"><i class="fa fa-check"></i><b>4.7</b> Exercises: Chapter 4</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sim.html"><a href="sim.html"><i class="fa fa-check"></i><b>5</b> Simulation methods</a></li>
<li class="chapter" data-level="6" data-path="unireg.html"><a href="unireg.html"><i class="fa fa-check"></i><b>6</b> Univariate regression</a><ul>
<li class="chapter" data-level="6.1" data-path="normal-model.html"><a href="normal-model.html"><i class="fa fa-check"></i><b>6.1</b> Normal model</a></li>
<li class="chapter" data-level="6.2" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i><b>6.2</b> Logit model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multi.html"><a href="multi.html"><i class="fa fa-check"></i><b>7</b> Multivariate regression</a></li>
<li class="chapter" data-level="8" data-path="time.html"><a href="time.html"><i class="fa fa-check"></i><b>8</b> Time series</a></li>
<li class="chapter" data-level="9" data-path="longi.html"><a href="longi.html"><i class="fa fa-check"></i><b>9</b> Longitudinal regression</a></li>
<li class="chapter" data-level="10" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>10</b> Convergence diagnostics</a></li>
<li class="chapter" data-level="11" data-path="bma.html"><a href="bma.html"><i class="fa fa-check"></i><b>11</b> Bayesian model averaging in variable selection</a></li>
<li class="chapter" data-level="12" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>12</b> Nonparametric regression</a></li>
<li class="chapter" data-level="13" data-path="recent.html"><a href="recent.html"><i class="fa fa-check"></i><b>13</b> Recent developments</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Bayesian Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-the-conjugate-normal-normalinverse-gamma-model" class="section level2">
<h2><span class="header-section-number">4.3</span> Linear regression: The conjugate normal-normal/inverse gamma model</h2>
<p>In this setting we analyze the conjugate normal-normal/inverse gamma model which is the workhorse in econometrics. In this model, the dependent variable <span class="math inline">\(y_i\)</span> is related to a set of regressors <span class="math inline">\({\mathbf{x}}_i=(x_{i1},x_{i2},\ldots,x_{iK})^{\top}\)</span> in a linear way, that is, <span class="math inline">\(y_i=\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_Kx_{iK}+\mu_i={\bf{x}}_i^{\top}\beta+\mu_i\)</span> where <span class="math inline">\(\mathbf{\beta}=(\beta_1,\beta_2,\ldots,\beta_K)^{\top}\)</span> and <span class="math inline">\(\mu_i\stackrel{iid} {\thicksim}N(0,\sigma^2)\)</span> is an stochastic error that is independent of the regressors, <span class="math inline">\({\bf{x}}_i\perp\mu_i\)</span>.</p>
<p>Defining <span class="math inline">\(\mathbf{y}=\begin{bmatrix} y_1\\ y_2\\ \vdots \\ y_N \end{bmatrix}\)</span>, <span class="math inline">\(\mathbf{X}=\begin{bmatrix} x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1K}\\ x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2K}\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\ x_{N1} &amp; x_{N2} &amp; \ldots &amp; x_{NK}\\ \end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{\mu}=\begin{bmatrix} \mu_1\\ \mu_2\\ \vdots \\ \mu_N \end{bmatrix}\)</span>, we can write the model in matrix form: <span class="math inline">\({\bf{y}}={\bf{X}}\beta+\mu\)</span>, where <span class="math inline">\(\mu\sim N(\bf{0},\sigma^2{\bf{I}})\)</span> which implies that <span class="math inline">\({\bf{y}}\sim N({\bf{X}}\beta,\sigma^2\bf{I})\)</span>. Then, the likelihood function is</p>
<p><span class="math display">\[\begin{align}
    p({\bf{y}}|\beta, \sigma^2, {{\bf{X}}}) &amp; = (2\pi\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{y}} - {\bf{X}}\beta)^{\top}({\bf{y}} - {\bf{X}}\beta) \right\}  \\
    &amp; \propto (\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{y}} - {\bf{X}}\beta)^{\top}({\bf{y}} - {\bf{X}}\beta) \right\}.
\end{align}\]</span></p>
<p>The conjugate priors for the parameters are
<span class="math display">\[\begin{align}
    \beta|\sigma^2 &amp; \sim N(\beta_0, \sigma^2 {\bf{B}}_0),\\
    \sigma^2 &amp; \sim IG(\alpha_0/2, \delta_0/2).
\end{align}\]</span></p>
<p>Then, the posterior distribution is</p>
<p><span class="math display">\[\begin{align}
  \pi(\beta,\sigma^2|\mathbf{y},\mathbf{X})&amp;\propto (\sigma^2)^{-\frac{N}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{y}} - {\bf{X}}\beta)^{\top}({\bf{y}} - {\bf{X}}\beta) \right\} \\
    &amp; \times (\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\beta - \beta_0)^{\top}{\bf{B}}_0^{-1}(\beta - \beta_0)\right\} \\
    &amp; \times \frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp \left\{-\frac{\delta_0}{2\sigma^2} \right\} \\
    &amp; \propto (\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} [\beta^{\top}({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})\beta - 2\beta^{\top}({\bf{B}}_0^{-1}\beta_0 + {\bf{X}}^{\top}{\bf{X}}\hat{\beta})] \right\} \\
    &amp; \times \left(\frac{1}{\sigma^2}\right)^{(\alpha_0+N)/2+1}\exp \left\{-\frac{\delta_0+ {\bf{y}}^{\top}{\bf{y}} + \beta_0^{\top}{\bf{B}}_0^{-1}\beta_0}{2\sigma^2} \right\},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}=({\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{X}}^{\top}{\bf{y}}\)</span> is the maximum likelihood estimator.</p>
<p>Adding and subtracting <span class="math inline">\(\beta_n^{\top}{{\bf{B}}}_n^{-1} \beta_n\)</span> to complete the square, where <span class="math inline">\({{\bf{B}}}_n = ({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}\)</span> and <span class="math inline">\(\beta_n = {{\bf{B}}}_n({\bf{B}}_0^{-1}\beta_0 + {\bf{X}}^{\top}{\bf{X}}\hat{\beta})\)</span>,</p>
<p><span class="math display">\[\begin{align}
  \pi(\beta,\sigma^2|\mathbf{y},\mathbf{X})&amp;\propto \underbrace{(\sigma^2)^{-\frac{K}{2}} \exp \left\{-\frac{1}{2\sigma^2} (\beta-\beta_n)^{\top}{\bf{B}}^{-1}_n(\beta-\beta_n) \right\}}_1 \\
    &amp; \times \underbrace{(\sigma^2)^{-\left(\frac{\alpha_n}{2}+1 \right)} \exp \left\{-\frac{\delta_n}{2\sigma^2} \right\}}_2.
\end{align}\]</span></p>
<p>The first expression is the kernel of a normal density function, <span class="math inline">\(\beta|\sigma^2, {\bf{y}}, {\bf{X}} \sim N(\beta_n, \sigma^2{\bf{B}}_n)\)</span>. The second expression is the kernel of a inverse gamma density, <span class="math inline">\(\sigma^2| {\bf{y}}, {\bf{X}}\sim IG(\alpha_n/2, \delta_n/2)\)</span>, where <span class="math inline">\(\alpha_n = \alpha_0 + N\)</span> and <span class="math inline">\(\delta_n = \delta_0 + {\bf{y}}^{\top}{\bf{y}} + \beta_0^{\top}{\bf{B}}_0^{-1}\beta_0 - \beta_n^{\top}{\bf{B}}_n^{-1}\beta_n\)</span>.</p>
<p>Taking into account that
<span class="math display">\[\begin{align}\beta_n &amp; = ({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}({\bf{B}}_0^{-1}\beta_0 + {\bf{X}}^{\top}{\bf{X}}\hat{\beta})\\
&amp; = ({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{B}}_0^{-1}\beta_0 + ({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1} {\bf{X}}^{\top}{\bf{X}}\hat{\beta}, 
\end{align}\]</span></p>
<p>where <span class="math inline">\(({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{B}}_0^{-1}=\bf{I}_K-({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{X}}^{\top}{\bf{X}}\)</span> <span class="citation">(Smith <a href="#ref-Smith1973" role="doc-biblioref">1973</a>)</span>. Setting <span class="math inline">\({\bf{W}}=({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}{\bf{X}}^{\top}{\bf{X}}\)</span> we have <span class="math inline">\(\beta_n=(\bf{I}_K-{\bf{W}})\beta_0+{\bf{W}}\hat{\beta}\)</span>, that is, the posterior mean of <span class="math inline">\(\beta\)</span> is a weighted average between the sample and prior information, where the weights depend on the precision of each piece of information. Observe that when the prior covariance matrix is highly vague (non–informative), such that <span class="math inline">\({\bf{B}}_0^{-1}\rightarrow \bf{0}_K\)</span>, we obtain <span class="math inline">\({\bf{W}} \rightarrow I_K\)</span>, such that <span class="math inline">\(\beta_n \rightarrow \hat{\beta}\)</span>, that is, the posterior mean location parameter converges to the maximum likelihood estimator.</p>
<p>In addition, we know that the posterior conditional covariance matrix of the location parameters <span class="math inline">\(\sigma^2({\bf{B}}_0^{-1} + {\bf{X}}^{\top}{\bf{X}})^{-1}=\sigma^2({\bf{X}}^{\top}{\bf{X}})^{-1}-\sigma^2\left(({\bf{X}}^{\top}{\bf{X}})^{-1}({\bf{B}}_0 + ({\bf{X}}^{\top}{\bf{X}})^{-1})^{-1}({\bf{X}}^{\top}{\bf{X}})^{-1}\right)\)</span> is positive semi-definite.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> Given that <span class="math inline">\(\sigma^2({\bf{X}}^{\top}{\bf{X}})^{-1}\)</span> is the covariance matrix of the maximum likelihood estimator, we observe that prior information reduces estimation uncertainty.</p>
<p>Now, we calculate the posterior marginal distribution of <span class="math inline">\(\beta\)</span>,</p>
<p><span class="math display">\[\begin{align}
    \pi(\beta|{\bf{y}},{\bf{X}}) &amp; = \int_0^{\infty} \pi(\beta, \sigma^2|{\bf{y}},{\bf{X}}) d\sigma^2 \\
    &amp; = \int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+K}{2} + 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2,
\end{align}\]</span>
where <span class="math inline">\(s = \delta_n + (\beta - \beta_n)^{\top}{{\bf{B}}}_n^{-1}(\beta - \beta_n)\)</span>. Then we can write
<span class="math display">\[\begin{align}
    \pi(\beta|{\bf{y}},{\bf{X}}) &amp; = \int_0^{\infty} \left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_n+K}{2} + 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2 \\
    &amp; = \frac{\Gamma((\alpha_n+K)/2)}{(s/2)^{(\alpha_n+K)/2}} \int_0^{\infty} \frac{(s/2)^{(\alpha_n+K)/2}}{\Gamma((\alpha_n+K)/2)} (\sigma^2)^{-(\alpha_n+K)/2 - 1} \exp \left\{-\frac{s}{2\sigma^2}\right\} d\sigma^2.
\end{align}\]</span></p>
<p>The right term is the integral of the probability density function of an inverse gamma distribution with parameters <span class="math inline">\(\nu = (\alpha_n+K)/2\)</span> and <span class="math inline">\(\tau = s/2\)</span>. Since we are integrating over the whole support of <span class="math inline">\(\sigma^2\)</span>, the integral is equal to 1, and therefore
<span class="math display">\[\begin{align*}
    \pi(\beta|{\bf{y}},{\bf{X}}) &amp; = \frac{\Gamma((\alpha_n+K)/2)}{(s/2)^{(\alpha_n+K)/2}} \\
    &amp; \propto s^{-(\alpha_n+K)/2} \\
    &amp; = [\delta_n + (\beta - \beta_n)^{\top}{{\bf{B}}}_n^{-1}(\beta - \beta_n)]^{-(\alpha_n+K)/2} \\
    &amp; = \left[1 + \frac{(\beta - \beta_n)^{\top}\left(\frac{\delta_n}{\alpha_n}{{\bf{B}}}_n\right)^{-1}(\beta - \beta_n)}{\alpha_n}\right]^{-(\alpha_n+K)/2}(\delta_n)^{-(\alpha_N+K)/2} \\
    &amp; \propto \left[1 + \frac{(\beta - \beta_n)^{\top}{\bf{H}}_n^{-1}(\beta - \beta_n)}{\alpha_n}\right]^{-(\alpha_n+K)/2},
\end{align*}\]</span>
where <span class="math inline">\({\bf{H}}_n = \frac{\delta_n}{\alpha_n}{\bf{B}}_n\)</span>. This last expression is a multivariate Student’s <span class="math inline">\(t\)</span> distribution for <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\beta|{\bf{y}},{\bf{X}} \sim t_K(\alpha_n, \beta_n, {\bf{H}}_n)\)</span>.</p>
<p>Observe that as we have incorporated the uncertainty of the variance, the posterior for <span class="math inline">\(\beta\)</span> changes from a normal to a Students’ t distribution, which has heavier tails.</p>
<p>The marginal likelihood of this model is</p>
<p><span class="math display">\[\begin{align}
  p({\bf{y}})=\int_0^{\infty}\int_{R^K}\pi (\beta | \sigma^2,{\bf{B}}_0,\beta_0 )\pi(\sigma^2| \alpha_0/2, \delta_0/2)p({\bf{y}}|\beta, \sigma^2, {\bf{X}})d\sigma^2 d\beta.
\end{align}\]</span></p>
<p>Taking into account that <span class="math inline">\(({\bf{y}}-{\bf{X}}\beta)^{\top}({\bf{y}}-{\bf{X}}\beta)+(\beta-\beta_0)^{\top}{\bf{B}}_0^{-1}(\beta-\beta_0)=(\beta-\beta_n)^{\top}{\bf{B}}_n^{-1}(\beta-\beta_n)+m\)</span>, where <span class="math inline">\(m={\bf{y}}^{\top}{\bf{y}}+\beta_0^{\top}{\bf{B}}_0^{-1}\beta_0-\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n\)</span>, we have that</p>
<p><span class="math display">\[\begin{align}
  p({\bf{y}})&amp;=\int_0^{\infty}\int_{R^K}\pi (\beta | \sigma^2)\pi(\sigma^2)p({\bf{y}}|\beta, \sigma^2, {\bf{X}})d\sigma^2 d\beta\\
  &amp;=\int_0^{\infty}\pi(\sigma^2) \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{1}{(2\pi\sigma^2)^{K/2}|{\bf{B}}_0|^{1/2}}\\
  &amp;\times\int_{R^K}\exp\left\{-\frac{1}{2\sigma^2}(\beta-\beta_n)^{\top}{\bf{B}}_n^{-1}(\beta-\beta_n)\right\}d\sigma^2 d\beta\\
  &amp;=\int_0^{\infty}\pi(\sigma^2) \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{|{\bf{B}}_n|^{1/2}}{|{\bf{B}}_0|^{1/2}}d\sigma^2\\
  &amp;=\int_{0}^{\infty} \frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_0/2+1}\exp\left\{\left(-\frac{\delta_0}{2\sigma^2}\right)\right\} \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{-\frac{1}{2\sigma^2}m \right\}   \frac{|{\bf{B}}_n|^{1/2}}{|{\bf{B}}_0|^{1/2}} d\sigma^2\\
  &amp;= \frac{1}{(2\pi)^{N/2}}\frac{(\delta_0/2)^{\alpha_0/2}}{\Gamma(\alpha_0/2)}\frac{|{\bf{B}}_n|^{1/2}}{|{\bf{B}}_0|^{1/2}}\int_{0}^{\infty}\left(\frac{1}{\sigma^2}\right)^{\frac{\alpha_0+N}{2}+1}\exp\left\{\left(-\frac{\delta_0+m}{2\sigma^2}\right)\right\}d\sigma^2\\
  &amp;= \frac{1}{\pi^{N/2}}\frac{\delta_0^{\alpha_0/2}}{\delta_n^{\alpha_n/2}}\frac{|{\bf{B}}_n|^{1/2}}{|{\bf{B}}_0|^{1/2}}\frac{\Gamma(\alpha_n/2)}{\Gamma(\alpha_0/2)}.
\end{align}\]</span></p>
<p>The posterior predictive is equal to</p>
<p><span class="math display">\[\begin{align}
\pi({\bf{Y}}_0|{\bf{y}})&amp;=\int_{0}^{\infty}\int_{R^K}p({\bf{Y}}_0|\beta,\sigma^2,{\bf{y}})\pi(\beta|\sigma^2,{\bf{y}})\pi(\sigma^2|{\bf{y}})d\beta d\sigma^2\\
&amp;=\int_{0}^{\infty}\int_{R^K}p({\bf{Y}}_0|\beta,\sigma^2)\pi(\beta|\sigma^2,{\bf{y}})\pi(\sigma^2|{\bf{y}})d\beta d\sigma^2,
\end{align}\]</span></p>
<p>where we take into account independence between <span class="math inline">\({\bf{Y}}_0\)</span> and <span class="math inline">\({\bf{Y}}\)</span>. Given <span class="math inline">\({\bf{X}}_0\)</span>, which is the <span class="math inline">\(N_0\times K\)</span> matrix of regressors associated with <span class="math inline">\({\bf{Y}}_0\)</span>, Then,</p>
<p><span class="math display">\[\begin{align}
\pi({\bf{Y}}_0|{\bf{y}})&amp;=\int_{0}^{\infty}\int_{R^K}\left\{ (2\pi\sigma^2)^{-\frac{N_0}{2}} \exp \left\{-\frac{1}{2\sigma^2} ({\bf{Y}}_0 - {\bf{X}}_0\beta)^{\top}({\bf{Y}}_0 - {\bf{X}}_0\beta)^{\top} \right\}\right. \\
    &amp; \times (2\pi\sigma^2)^{-\frac{K}{2}} |{\bf{B}}_n|^{-1/2} \exp \left\{-\frac{1}{2\sigma^2} (\beta - \beta_n)^{\top}{\bf{B}}_n^{-1}(\beta - \beta_n)\right\} \\
    &amp; \left. \times \frac{(\delta_n/2)^{\alpha_n/2}}{\Gamma(\alpha_n/2)}\left(\frac{1}{\sigma^2}\right)^{\alpha_n/2+1}\exp \left\{-\frac{\delta_n}{2\sigma^2} \right\}\right\}d\beta d\sigma^2. \\
\end{align}\]</span></p>
<p>Setting <span class="math inline">\({\bf{M}}=({\bf{X}}_0^{\top}{\bf{X}}_0+{\bf{B}}_n^{-1})\)</span> and <span class="math inline">\(\beta_*={\bf{M}}^{-1}({\bf{B}}_n^{-1}\beta_n+{\bf{X}}_0^{\top}{\bf{Y}}_0)\)</span>, we have</p>
<p><span class="math display">\[\begin{align}
({\bf{Y}}_0 - {\bf{X}}_0\beta)^{\top}({\bf{Y}}_0 - {\bf{X}}_0\beta)^{\top}+(\beta - \beta_n)^{\top}{\bf{B}}_n^{-1}(\beta - \beta_n)&amp;=(\beta - \beta_*)^{\top}{\bf{M}}(\beta - \beta_*)\\
&amp;+\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-\beta_*^{\top}{\bf{M}}\beta_*,
\end{align}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\begin{align}
\pi({\bf{Y}}_0|{\bf{y}})&amp;\propto\int_{0}^{\infty}\left\{\left(\frac{1}{\sigma^2}\right)^{-\frac{K+N_0+\alpha_n}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-\beta_*^{\top}{\bf{M}}\beta_*+\delta_n)\right\}\right.\\
&amp;\times\left.\int_{R^K}\exp\left\{-\frac{1}{2\sigma^2}(\beta - \beta_*)^{\top}{\bf{M}}(\beta - \beta_*)\right\}d\beta\right\} d\sigma^2,\\

\end{align}\]</span>
where the term in the second integral is the kernel of a multivariate normal density with mean <span class="math inline">\(\beta_*\)</span> and covariance matrix <span class="math inline">\(\sigma^2{\bf{M}}^{-1}\)</span>. Then,</p>
<p><span class="math display">\[\begin{align}
\pi({\bf{Y}}_0|{\bf{y}})&amp;\propto\int_{0}^{\infty}\left(\frac{1}{\sigma^2}\right)^{\frac{N_0+\alpha_n}{2}+1}\exp\left\{-\frac{1}{2\sigma^2}(\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-\beta_*^{\top}{\bf{M}}\beta_*+\delta_n)\right\}d\sigma^2,\\
\end{align}\]</span></p>
<p>which is the kernel of an inverse gamma density. Thus,</p>
<p><span class="math display">\[\begin{align}
  \pi({\bf{Y}}_0|{\bf{y}})&amp;\propto \left[\frac{\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-\beta_*^{\top}{\bf{M}}\beta_*+\delta_n}{2}\right]^{-\frac{\alpha_n+N_0}{2}}.
\end{align}\]</span></p>
<p>Setting <span class="math inline">\({\bf{C}}^{-1}={\bf{I}}_{N_0}+{\bf{X}}_0{\bf{B}}_n{\bf{X}}_0^{\top}\)</span> such that <span class="math inline">\({\bf{C}}={\bf{I}}_{N_0}-{\bf{X}}_0({\bf{B}}_n^{-1}+{\bf{X}}_0^{\top}{\bf{X}}_0)^{-1}{\bf{X}}_0^{\top}={\bf{I}}_{N_0}-{\bf{X}}_0{\bf{M}}^{-1}{\bf{X}}_0^{\top}\)</span>,<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> and <span class="math inline">\({\bf{\beta}}_{**}={\bf{C}}^{-1}{\bf{X}}_0{\bf{M}}^{-1}{\bf{B}}_n^{-1}\beta_n\)</span>, then</p>
<p><span class="math display">\[\begin{align}
\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-\beta_*^{\top}{\bf{M}}\beta_*&amp;=
\beta_n^{\top}{\bf{B}}_n^{-1}\beta_n+{\bf{Y}}_0^{\top}{\bf{Y}}_0-(\beta_n^{\top}{\bf{B}}_n^{-1}+{\bf{Y}}_0^{\top}{\bf{X}}_0){\bf{M}}^{-1}({\bf{B}}_n^{-1}\beta_n+{\bf{X}}_0^{\top}{\bf{Y}}_0)\\
&amp;=\beta_n^{\top}({\bf{B}}_n^{-1}-{\bf{B}}_n^{-1}{\bf{M}}^{-1}{\bf{B}}_n^{-1})\beta_n+{\bf{Y}}_0^{\top}{\bf{C}}{\bf{Y}}_0\\
&amp;-2{\bf{Y}}_0^{\top}{\bf{C}}{\bf{C}}^{-1}{\bf{X}}_0{\bf{M}}^{-1}{\bf{B}}_n^{-1}\beta_n+{\bf{\beta}}_{**}^{\top}{\bf{C}}{\bf{\beta}}_{**}-{\bf{\beta}}_{**}^{\top}{\bf{C}}{\bf{\beta}}_{**}\\
&amp;=\beta_n^{\top}({\bf{B}}_n^{-1}-{\bf{B}}_n^{-1}{\bf{M}}^{-1}{\bf{B}}_n^{-1})\beta_n+({\bf{Y}}_0-{\bf{\beta}}_{**})^{\top}{\bf{C}}({\bf{Y}}_0-{\bf{\beta}}_{**})\\
&amp;-{\bf{\beta}}_{**}^{\top}{\bf{C}}{\bf{\beta}}_{**},
\end{align}\]</span></p>
<p>where <span class="math inline">\(\beta_n^{\top}({\bf{B}}_n^{-1}-{\bf{B}}_n^{-1}{\bf{M}}^{-1}{\bf{B}}_n^{-1})\beta_n={\bf{\beta}}_{**}^{\top}{\bf{C}}{\bf{\beta}}_{**}\)</span> and <span class="math inline">\(\beta_{**}={\mathbf{X}}_0\beta_n\)</span> (see Exercise 6).</p>
<p>Then,</p>
<p><span class="math display">\[\begin{align}
  \pi({\bf{Y}}_0|{\bf{y}})&amp;\propto\left[\frac{({\bf{Y}}_0-{\mathbf{X}}_0\beta_n)^{\top}{\bf{C}}({\bf{Y}}_0-{\mathbf{X}}_0\beta_n)+\delta_n}{2}\right]^{-\frac{\alpha_n+N_0}{2}}\\
  &amp;\propto\left[\frac{({\bf{Y}}_0-{\mathbf{X}}_0\beta_n)^{\top}\left(\frac{{\bf{C}}\alpha_n}{\delta_n}\right)({\bf{Y}}_0-{\mathbf{X}}_0\beta_n)}{\alpha_n}+1\right]^{-\frac{\alpha_n+N_0}{2}}.
  
\end{align}\]</span></p>
<p>Then, the posterior predictive is a multivariate Student’s t, <span class="math inline">\({\bf{Y}}_0|{\bf{y}}\sim t\left({\bf{X}}_0\beta_n,\frac{\delta_n({\bf{I}}_{N_0}+{\bf{X}}_0{\bf{B}}_n{\bf{X}}_0^{\top})}{\alpha_n},\alpha_n\right)\)</span>.</p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Smith1973">
<p>Smith, A. F. M. 1973. “A General Bayesian Linear Model.” <em>Journal of the Royal Statistical Society. Series B (Methodological).</em> 35 (1): 67–75.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p>A particular case of the Woodbury matrix identity<a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>Using this result <span class="math inline">\(({\bf{A}}+{\bf{B}}{\bf{D}}{\bf{C}})^{-1}={\bf{A}}^{-1}-{\bf{A}}^{-1}{\bf{B}}({\bf{D}}^{-1}+{\bf{C}}{\bf{A}}^{-1}{\bf{B}})^{-1}{\bf{C}}{\bf{A}}^{-1}\)</span><a href="linear-regression-the-conjugate-normal-normalinverse-gamma-model.html#fnref18" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec42.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-linear-regression-the-conjugate-normal-normalinverse-wishart-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/aramir21/IntroductionBayesianEconometricsBook/04-Conjugate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
